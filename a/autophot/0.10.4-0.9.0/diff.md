# Comparing `tmp/autophot-0.10.4-py2.py3-none-any.whl.zip` & `tmp/autophot-0.9.0-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,89 +1,85 @@
-Zip file size: 170579 bytes, number of entries: 87
--rw-rw-r--  2.0 unx     3186 b- defN 23-Jun-27 15:03 autophot/AP_config.py
--rw-rw-r--  2.0 unx     5720 b- defN 23-Jul-28 03:07 autophot/__init__.py
+Zip file size: 154642 bytes, number of entries: 83
+-rw-rw-r--  2.0 unx     3171 b- defN 23-Jun-13 21:12 autophot/AP_config.py
+-rw-rw-r--  2.0 unx     5719 b- defN 23-Jun-13 21:20 autophot/__init__.py
 -rw-rw-r--  2.0 unx      281 b- defN 23-Jun-13 21:12 autophot/__main__.py
--rw-rw-r--  2.0 unx     1210 b- defN 23-Jul-28 03:07 autophot/fit/__init__.py
--rw-rw-r--  2.0 unx     6406 b- defN 23-Jul-02 15:14 autophot/fit/base.py
+-rw-rw-r--  2.0 unx     1092 b- defN 23-Jun-13 21:12 autophot/fit/__init__.py
+-rw-rw-r--  2.0 unx     6406 b- defN 23-Jun-13 21:12 autophot/fit/base.py
 -rw-rw-r--  2.0 unx       30 b- defN 23-Jun-13 21:12 autophot/fit/gp.py
 -rw-rw-r--  2.0 unx     6961 b- defN 23-Jun-13 21:12 autophot/fit/gradient.py
 -rw-rw-r--  2.0 unx     6939 b- defN 23-Jun-13 21:12 autophot/fit/hmc.py
--rw-rw-r--  2.0 unx    13710 b- defN 23-Jul-28 03:07 autophot/fit/iterative.py
--rw-rw-r--  2.0 unx    18661 b- defN 23-Jul-28 03:07 autophot/fit/lm.py
+-rw-rw-r--  2.0 unx    13401 b- defN 23-Jun-13 21:12 autophot/fit/iterative.py
+-rw-rw-r--  2.0 unx    31419 b- defN 23-Jun-13 21:12 autophot/fit/lm.py
 -rw-rw-r--  2.0 unx     4381 b- defN 23-Jun-13 21:12 autophot/fit/mhmcmc.py
--rw-rw-r--  2.0 unx     7150 b- defN 23-Jul-28 03:07 autophot/fit/nuts.py
--rw-rw-r--  2.0 unx    28502 b- defN 23-Jul-28 03:07 autophot/fit/oldlm.py
+-rw-rw-r--  2.0 unx     7150 b- defN 23-Jun-13 21:12 autophot/fit/nuts.py
 -rw-rw-r--  2.0 unx     1150 b- defN 23-Jun-13 21:12 autophot/image/__init__.py
--rw-rw-r--  2.0 unx    18011 b- defN 23-Jun-27 15:03 autophot/image/image_header.py
--rw-rw-r--  2.0 unx    20710 b- defN 23-Jun-27 15:03 autophot/image/image_object.py
--rw-rw-r--  2.0 unx     5514 b- defN 23-Jun-27 15:03 autophot/image/jacobian_image.py
--rw-rw-r--  2.0 unx     6411 b- defN 23-Jun-19 02:11 autophot/image/model_image.py
--rw-rw-r--  2.0 unx     6629 b- defN 23-Jun-27 15:03 autophot/image/psf_image.py
--rw-rw-r--  2.0 unx    14446 b- defN 23-Jun-27 15:03 autophot/image/target_image.py
--rw-rw-r--  2.0 unx    24450 b- defN 23-Jun-27 15:03 autophot/image/window_object.py
--rw-rw-r--  2.0 unx      745 b- defN 23-Jul-28 03:07 autophot/models/__init__.py
--rw-rw-r--  2.0 unx     8292 b- defN 23-Jul-28 03:07 autophot/models/_model_methods.py
--rw-rw-r--  2.0 unx    22590 b- defN 23-Jul-28 03:07 autophot/models/_shared_methods.py
--rw-rw-r--  2.0 unx    13125 b- defN 23-Jul-28 03:07 autophot/models/core_model.py
--rw-rw-r--  2.0 unx     7177 b- defN 23-Jul-01 15:17 autophot/models/edgeon_model.py
--rw-rw-r--  2.0 unx    14412 b- defN 23-Jun-19 02:11 autophot/models/exponential_model.py
--rw-rw-r--  2.0 unx     2047 b- defN 23-Jun-19 02:11 autophot/models/flatsky_model.py
+-rw-rw-r--  2.0 unx    16790 b- defN 23-Jun-13 21:12 autophot/image/image_header.py
+-rw-rw-r--  2.0 unx    20545 b- defN 23-Jun-13 21:12 autophot/image/image_object.py
+-rw-rw-r--  2.0 unx     5269 b- defN 23-Jun-13 21:12 autophot/image/jacobian_image.py
+-rw-rw-r--  2.0 unx     6379 b- defN 23-Jun-13 21:12 autophot/image/model_image.py
+-rw-rw-r--  2.0 unx     6791 b- defN 23-Jun-13 21:12 autophot/image/psf_image.py
+-rw-rw-r--  2.0 unx    14763 b- defN 23-Jun-13 21:12 autophot/image/target_image.py
+-rw-rw-r--  2.0 unx    23813 b- defN 23-Jun-13 21:12 autophot/image/window_object.py
+-rw-rw-r--  2.0 unx      654 b- defN 23-Jun-13 21:12 autophot/models/__init__.py
+-rw-rw-r--  2.0 unx     1742 b- defN 23-Jun-13 21:12 autophot/models/_model_methods.py
+-rw-rw-r--  2.0 unx    19240 b- defN 23-Jun-13 21:12 autophot/models/_shared_methods.py
+-rw-rw-r--  2.0 unx    12502 b- defN 23-Jun-13 21:12 autophot/models/core_model.py
+-rw-rw-r--  2.0 unx     6850 b- defN 23-Jun-13 21:12 autophot/models/edgeon_model.py
+-rw-rw-r--  2.0 unx    14419 b- defN 23-Jun-13 21:12 autophot/models/exponential_model.py
+-rw-rw-r--  2.0 unx     2108 b- defN 23-Jun-13 21:12 autophot/models/flatsky_model.py
 -rw-rw-r--  2.0 unx    10650 b- defN 23-Jun-13 21:12 autophot/models/foureirellipse_model.py
--rw-rw-r--  2.0 unx     5249 b- defN 23-Jun-27 15:03 autophot/models/galaxy_model_object.py
--rw-rw-r--  2.0 unx    13247 b- defN 23-Jun-19 02:11 autophot/models/gaussian_model.py
--rw-rw-r--  2.0 unx    10888 b- defN 23-Jun-23 00:07 autophot/models/group_model_object.py
--rw-rw-r--  2.0 unx    25903 b- defN 23-Jul-28 03:07 autophot/models/model_object.py
--rw-rw-r--  2.0 unx     5755 b- defN 23-Jul-28 03:07 autophot/models/moffat_model.py
--rw-rw-r--  2.0 unx    18952 b- defN 23-Jun-19 02:11 autophot/models/nuker_model.py
--rw-rw-r--  2.0 unx    16039 b- defN 23-Jul-28 03:07 autophot/models/parameter_group.py
--rw-rw-r--  2.0 unx    18866 b- defN 23-Jul-28 03:07 autophot/models/parameter_object.py
--rw-rw-r--  2.0 unx     2570 b- defN 23-Jun-27 15:03 autophot/models/planesky_model.py
--rw-rw-r--  2.0 unx     3889 b- defN 23-Jun-19 02:11 autophot/models/psf_model.py
--rw-rw-r--  2.0 unx     4878 b- defN 23-Jun-19 02:11 autophot/models/ray_model.py
--rw-rw-r--  2.0 unx     3178 b- defN 23-Jul-28 03:07 autophot/models/relspline_model.py
--rw-rw-r--  2.0 unx    18049 b- defN 23-Jul-28 03:07 autophot/models/sersic_model.py
+-rw-rw-r--  2.0 unx     5306 b- defN 23-Jun-13 21:12 autophot/models/galaxy_model_object.py
+-rw-rw-r--  2.0 unx    13246 b- defN 23-Jun-13 21:12 autophot/models/gaussian_model.py
+-rw-rw-r--  2.0 unx    11139 b- defN 23-Jun-13 21:12 autophot/models/group_model_object.py
+-rw-rw-r--  2.0 unx    23870 b- defN 23-Jun-13 21:12 autophot/models/model_object.py
+-rw-rw-r--  2.0 unx     3904 b- defN 23-Jun-13 21:12 autophot/models/moffat_model.py
+-rw-rw-r--  2.0 unx    18951 b- defN 23-Jun-13 21:12 autophot/models/nuker_model.py
+-rw-rw-r--  2.0 unx    12454 b- defN 23-Jun-13 21:12 autophot/models/parameter_group.py
+-rw-rw-r--  2.0 unx    18097 b- defN 23-Jun-13 21:12 autophot/models/parameter_object.py
+-rw-rw-r--  2.0 unx     2450 b- defN 23-Jun-13 21:12 autophot/models/planesky_model.py
+-rw-rw-r--  2.0 unx     3719 b- defN 23-Jun-13 21:12 autophot/models/psf_model.py
+-rw-rw-r--  2.0 unx     4998 b- defN 23-Jun-13 21:12 autophot/models/ray_model.py
+-rw-rw-r--  2.0 unx    16161 b- defN 23-Jun-13 21:12 autophot/models/sersic_model.py
 -rw-rw-r--  2.0 unx      875 b- defN 23-Jun-13 21:12 autophot/models/sky_model_object.py
--rw-rw-r--  2.0 unx    10912 b- defN 23-Jun-19 02:11 autophot/models/spline_model.py
--rw-rw-r--  2.0 unx      838 b- defN 23-Jun-27 15:03 autophot/models/star_model_object.py
--rw-rw-r--  2.0 unx     3178 b- defN 23-Jun-19 02:11 autophot/models/superellipse_model.py
--rw-rw-r--  2.0 unx     4783 b- defN 23-Jun-19 02:11 autophot/models/warp_model.py
--rw-rw-r--  2.0 unx     3861 b- defN 23-Jun-19 02:11 autophot/models/wedge_model.py
--rw-rw-r--  2.0 unx     4102 b- defN 23-Jun-19 02:11 autophot/models/zernike_model.py
+-rw-rw-r--  2.0 unx    10695 b- defN 23-Jun-13 21:12 autophot/models/spline_model.py
+-rw-rw-r--  2.0 unx     1226 b- defN 23-Jun-13 21:12 autophot/models/star_model_object.py
+-rw-rw-r--  2.0 unx     3259 b- defN 23-Jun-13 21:12 autophot/models/superellipse_model.py
+-rw-rw-r--  2.0 unx     4741 b- defN 23-Jun-13 21:12 autophot/models/warp_model.py
+-rw-rw-r--  2.0 unx     3981 b- defN 23-Jun-13 21:12 autophot/models/wedge_model.py
 -rw-rw-r--  2.0 unx       57 b- defN 23-Jun-13 21:12 autophot/parse_config/__init__.py
--rw-rw-r--  2.0 unx     4143 b- defN 23-Jul-28 03:07 autophot/parse_config/basic_config.py
+-rw-rw-r--  2.0 unx     4147 b- defN 23-Jun-13 21:12 autophot/parse_config/basic_config.py
 -rw-rw-r--  2.0 unx     4590 b- defN 23-Jun-13 21:12 autophot/parse_config/galfit_config.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 21:12 autophot/parse_config/shared_methods.py
--rw-rw-r--  2.0 unx       93 b- defN 23-Jul-28 03:07 autophot/plots/__init__.py
--rw-rw-r--  2.0 unx     3174 b- defN 23-Jul-28 03:07 autophot/plots/diagnostic.py
--rw-rw-r--  2.0 unx    15086 b- defN 23-Jun-27 15:03 autophot/plots/image.py
--rw-rw-r--  2.0 unx     7558 b- defN 23-Jun-19 02:11 autophot/plots/profile.py
+-rw-rw-r--  2.0 unx       67 b- defN 23-Jun-13 21:12 autophot/plots/__init__.py
+-rw-rw-r--  2.0 unx     8074 b- defN 23-Jun-13 21:12 autophot/plots/image.py
+-rw-rw-r--  2.0 unx     7556 b- defN 23-Jun-13 21:12 autophot/plots/profile.py
 -rw-rw-r--  2.0 unx     3048 b- defN 23-Jun-13 21:12 autophot/plots/shared_elements.py
--rw-rw-r--  2.0 unx    20928 b- defN 23-Jun-27 15:03 autophot/plots/visuals.py
+-rw-rw-r--  2.0 unx    21022 b- defN 23-Jun-13 21:12 autophot/plots/visuals.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 21:12 autophot/utils/__init__.py
 -rw-rw-r--  2.0 unx      800 b- defN 23-Jun-13 21:12 autophot/utils/angle_operations.py
 -rw-rw-r--  2.0 unx      976 b- defN 23-Jun-13 21:12 autophot/utils/decorators.py
--rw-rw-r--  2.0 unx    11781 b- defN 23-Jul-28 03:07 autophot/utils/interpolate.py
--rw-rw-r--  2.0 unx     8668 b- defN 23-Jul-28 03:07 autophot/utils/operations.py
--rw-rw-r--  2.0 unx      964 b- defN 23-Jun-15 21:05 autophot/utils/optimization.py
--rw-rw-r--  2.0 unx     5748 b- defN 23-Jun-19 02:11 autophot/utils/parametric_profiles.py
+-rw-rw-r--  2.0 unx     9815 b- defN 23-Jun-13 21:12 autophot/utils/interpolate.py
+-rw-rw-r--  2.0 unx     6728 b- defN 23-Jun-13 21:12 autophot/utils/operations.py
+-rw-rw-r--  2.0 unx      963 b- defN 23-Jun-13 21:12 autophot/utils/optimization.py
+-rw-rw-r--  2.0 unx     5990 b- defN 23-Jun-13 21:12 autophot/utils/parametric_profiles.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 21:12 autophot/utils/conversions/__init__.py
 -rw-rw-r--  2.0 unx     1840 b- defN 23-Jun-13 21:12 autophot/utils/conversions/coordinates.py
 -rw-rw-r--  2.0 unx     1095 b- defN 23-Jun-13 21:12 autophot/utils/conversions/dict_to_hdf5.py
--rw-rw-r--  2.0 unx     2609 b- defN 23-Jul-28 03:07 autophot/utils/conversions/functions.py
+-rw-rw-r--  2.0 unx     1829 b- defN 23-Jun-13 21:12 autophot/utils/conversions/functions.py
 -rw-rw-r--  2.0 unx     3260 b- defN 23-Jun-13 21:12 autophot/utils/conversions/optimization.py
 -rw-rw-r--  2.0 unx     2539 b- defN 23-Jun-13 21:12 autophot/utils/conversions/units.py
 -rw-rw-r--  2.0 unx      281 b- defN 23-Jun-13 21:12 autophot/utils/initialize/__init__.py
 -rw-rw-r--  2.0 unx     3103 b- defN 23-Jun-13 21:12 autophot/utils/initialize/center.py
 -rw-rw-r--  2.0 unx     4542 b- defN 23-Jun-13 21:12 autophot/utils/initialize/construct_psf.py
--rw-rw-r--  2.0 unx     3972 b- defN 23-Jun-27 15:03 autophot/utils/initialize/initialize.py
--rw-rw-r--  2.0 unx     7036 b- defN 23-Jun-27 00:09 autophot/utils/initialize/segmentation_map.py
+-rw-rw-r--  2.0 unx     3921 b- defN 23-Jun-13 21:12 autophot/utils/initialize/initialize.py
+-rw-rw-r--  2.0 unx     7036 b- defN 23-Jun-13 21:12 autophot/utils/initialize/segmentation_map.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 21:12 autophot/utils/isophote/__init__.py
 -rw-rw-r--  2.0 unx     1085 b- defN 23-Jun-13 21:12 autophot/utils/isophote/ellipse.py
 -rw-rw-r--  2.0 unx     8531 b- defN 23-Jun-13 21:12 autophot/utils/isophote/extract.py
 -rw-rw-r--  2.0 unx     7012 b- defN 23-Jun-13 21:12 autophot/utils/isophote/integrate.py
--rw-rw-r--  2.0 unx    35149 b- defN 23-Jul-28 03:08 autophot-0.10.4.dist-info/LICENSE
--rw-rw-r--  2.0 unx     3557 b- defN 23-Jul-28 03:08 autophot-0.10.4.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 23-Jul-28 03:08 autophot-0.10.4.dist-info/WHEEL
--rw-rw-r--  2.0 unx       56 b- defN 23-Jul-28 03:08 autophot-0.10.4.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        9 b- defN 23-Jul-28 03:08 autophot-0.10.4.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     7621 b- defN 23-Jul-28 03:08 autophot-0.10.4.dist-info/RECORD
-87 files, 636529 bytes uncompressed, 158495 bytes compressed:  75.1%
+-rw-rw-r--  2.0 unx    35149 b- defN 23-Jun-13 21:20 autophot-0.9.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     3572 b- defN 23-Jun-13 21:20 autophot-0.9.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 23-Jun-13 21:20 autophot-0.9.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       56 b- defN 23-Jun-13 21:20 autophot-0.9.0.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        9 b- defN 23-Jun-13 21:20 autophot-0.9.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     7270 b- defN 23-Jun-13 21:20 autophot-0.9.0.dist-info/RECORD
+83 files, 575760 bytes uncompressed, 143104 bytes compressed:  75.1%
```

## zipnote {}

```diff
@@ -30,17 +30,14 @@
 
 Filename: autophot/fit/mhmcmc.py
 Comment: 
 
 Filename: autophot/fit/nuts.py
 Comment: 
 
-Filename: autophot/fit/oldlm.py
-Comment: 
-
 Filename: autophot/image/__init__.py
 Comment: 
 
 Filename: autophot/image/image_header.py
 Comment: 
 
 Filename: autophot/image/image_object.py
@@ -114,17 +111,14 @@
 
 Filename: autophot/models/psf_model.py
 Comment: 
 
 Filename: autophot/models/ray_model.py
 Comment: 
 
-Filename: autophot/models/relspline_model.py
-Comment: 
-
 Filename: autophot/models/sersic_model.py
 Comment: 
 
 Filename: autophot/models/sky_model_object.py
 Comment: 
 
 Filename: autophot/models/spline_model.py
@@ -138,17 +132,14 @@
 
 Filename: autophot/models/warp_model.py
 Comment: 
 
 Filename: autophot/models/wedge_model.py
 Comment: 
 
-Filename: autophot/models/zernike_model.py
-Comment: 
-
 Filename: autophot/parse_config/__init__.py
 Comment: 
 
 Filename: autophot/parse_config/basic_config.py
 Comment: 
 
 Filename: autophot/parse_config/galfit_config.py
@@ -156,17 +147,14 @@
 
 Filename: autophot/parse_config/shared_methods.py
 Comment: 
 
 Filename: autophot/plots/__init__.py
 Comment: 
 
-Filename: autophot/plots/diagnostic.py
-Comment: 
-
 Filename: autophot/plots/image.py
 Comment: 
 
 Filename: autophot/plots/profile.py
 Comment: 
 
 Filename: autophot/plots/shared_elements.py
@@ -237,26 +225,26 @@
 
 Filename: autophot/utils/isophote/extract.py
 Comment: 
 
 Filename: autophot/utils/isophote/integrate.py
 Comment: 
 
-Filename: autophot-0.10.4.dist-info/LICENSE
+Filename: autophot-0.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: autophot-0.10.4.dist-info/METADATA
+Filename: autophot-0.9.0.dist-info/METADATA
 Comment: 
 
-Filename: autophot-0.10.4.dist-info/WHEEL
+Filename: autophot-0.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: autophot-0.10.4.dist-info/entry_points.txt
+Filename: autophot-0.9.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: autophot-0.10.4.dist-info/top_level.txt
+Filename: autophot-0.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: autophot-0.10.4.dist-info/RECORD
+Filename: autophot-0.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## autophot/AP_config.py

```diff
@@ -2,15 +2,14 @@
 import logging
 import torch
 
 __all__ = ["ap_dtype", "ap_device", "ap_logger", "set_logging_output"]
 
 ap_dtype = torch.float64
 ap_device = "cuda:0" if torch.cuda.is_available() else "cpu"
-ap_verbose = 0
 
 logging.basicConfig(
     filename="AutoPhot.log",
     level=logging.INFO,
     format="%(asctime)s:%(levelname)s: %(message)s",
 )
 ap_logger = logging.getLogger()
```

## autophot/__init__.py

```diff
@@ -1,15 +1,15 @@
 import sys
 import argparse
 import requests
 from .parse_config import galfit_config, basic_config
 from . import models, image, plots, utils, fit, AP_config
 
 # meta data
-__version__ = "0.10.4"
+__version__ = "0.9.0"
 __author__ = "Connor Stone"
 __email__ = "connorstone628@gmail.com"
 
 
 def run_from_terminal() -> None:
     """
     Execute AutoPhot from the command line with various options.
```

## autophot/fit/__init__.py

```diff
@@ -1,18 +1,14 @@
 from .base import *
 from .lm import *
-from .oldlm import *
 from .gradient import *
 from .iterative import *
-try:
-    from .hmc import *
-    from .nuts import *
-except AssertionError as e:
-    print("Could not load HMC or NUTS due to:", str(e))
+from .hmc import *
 from .mhmcmc import *
+from .nuts import *
 
 """
 base: This module defines the base class BaseOptimizer, 
       which is used as the parent class for all optimization algorithms in AutoPhot. 
       This module contains helper functions used across multiple optimization algorithms, 
       such as computing gradients and making copies of models.
```

## autophot/fit/iterative.py

```diff
@@ -50,18 +50,14 @@
         method_kwargs: Dict[str, Any] = {},
         **kwargs: Dict[str, Any],
     ) -> None:
         super().__init__(model, initial_state, max_iter=max_iter, **kwargs)
 
         self.method = method
         self.method_kwargs = method_kwargs
-        if "relative_tolerance" not in method_kwargs and isinstance(method, LM):
-            # Lower tolerance since it's not worth fine tuning a model when it's neighbors will be shifting soon anyway
-            self.method_kwargs["relative_tolerance"] = 1e-3
-            self.method_kwargs["max_iter"] = 15
         #          # pixels      # parameters
         self.ndf = self.model.target[self.model.window].flatten("data").size(0) - len(
             self.current_state
         )
         if self.model.target.has_mask:
             # subtract masked pixels from degrees of freedom
             self.ndf -= torch.sum(
```

## autophot/fit/lm.py

```diff
@@ -1,417 +1,812 @@
 # Levenberg-Marquardt algorithm
 import os
 from time import time
 from typing import List, Callable, Optional, Union, Sequence, Any
-from functools import partial
 
 import torch
 from torch.autograd.functional import jacobian
 import numpy as np
 import matplotlib.pyplot as plt
 
 from .base import BaseOptimizer
 from .. import AP_config
 
-__all__ = ("LM",)
+__all__ = ["LM", "LM_Constraint"]
+
+
+@torch.no_grad()
+@torch.jit.script
+def Broyden_step(J, h, Yp, Yph):
+    delta = torch.matmul(J, h)
+    # avoid constructing a second giant jacobian matrix, instead go one row at a time
+    for j in range(J.shape[1]):
+        J[:, j] += (Yph - Yp - delta) * h[j] / torch.linalg.norm(h)
+    return J
 
-class OptimizeStopFail(Exception):
-    pass
 
 class LM(BaseOptimizer):
-    """The LM class is an implementation of the Levenberg-Marquardt
-    optimization algorithm. This method is used to solve non-linear
-    least squares problems and is known for its robustness and
-    efficiency.
-
-    The Levenberg-Marquardt (LM) algorithm is an iterative method used
-    for solving non-linear least squares problems. It can be seen as a
-    combination of the Gauss-Newton method and the gradient descent
-    method: it works like the Gauss-Newton method when the parameters
-    are approximately correct, and like the gradient descent method
-    when the parameters are far from their optimal values.
-
-    The cost function that the LM algorithm tries to minimize is of
-    the form:
-    
-    .. math::
-        f(\\boldsymbol{\\beta}) = \\frac{1}{2}\\sum_{i=1}^{N} r_i(\\boldsymbol{\\beta})^2
-
-    where :math:`\\boldsymbol{\\beta}` is the vector of parameters,
-    :math:`r_i` are the residuals, and :math:`N` is the number of
-    observations.
-
-    The LM algorithm iteratively performs the following update to the parameters:
-
-    .. math::
-        \\boldsymbol{\\beta}_{n+1} = \\boldsymbol{\\beta}_{n} - (J^T J + \\lambda diag(J^T J))^{-1} J^T \\boldsymbol{r}
-
-    where:
-        - :math:`J` is the Jacobian matrix whose elements are :math:`J_{ij} = \\frac{\\partial r_i}{\\partial \\beta_j}`,
-        - :math:`\\boldsymbol{r}` is the vector of residuals :math:`r_i(\\boldsymbol{\\beta})`,
-        - :math:`\\lambda` is a damping factor which is adjusted at each iteration. 
-
-    When :math:`\\lambda = 0` this can be seen as the Gauss-Newton
-    method. In the limit that :math:`\\lambda` is large, the
-    :math:`J^T J` matrix (an approximation of the Hessian) becomes
-    subdominant and the update essentially points along :math:`J^T
-    \\boldsymbol{r}` which is the gradient. In this scenario the
-    gradient descent direction is also modified by the :math:`\\lambda
-    diag(J^T J)` scaling which in some sense makes each gradient
-    unitless and further improves the step. Note as well that as
-    :math:`\\lambda` gets larger the step taken will be smaller, which
-    helps to ensure convergence when the initial guess of the
-    parameters are far from the optimal solution.
-
-    Note that the residuals :math:`r_i` are typically also scaled by
-    the variance of the pixels, but this does not change the equations
-    above. For a detailed explanation of the LM method see the article
-    by Henri Gavin on which much of the AutoPhot LM implementation is
-    based::
-    
-        @article{Gavin2019,
-            title={The Levenberg-Marquardt algorithm for nonlinear least squares curve-fitting problems},
-            author={Gavin, Henri P},
-            journal={Department of Civil and Environmental Engineering, Duke University},
-            volume={19},
-            year={2019}
-        }
-
-    as well as the paper on LM geodesic acceleration by Mark
-    Transtrum::
-    
-        @article{Tanstrum2012,
-           author = {{Transtrum}, Mark K. and {Sethna}, James P.},
-            title = "{Improvements to the Levenberg-Marquardt algorithm for nonlinear least-squares minimization}",
-             year = 2012,
-              doi = {10.48550/arXiv.1201.5885},
-           adsurl = {https://ui.adsabs.harvard.edu/abs/2012arXiv1201.5885T},
-        }
-
-    The damping factor :math:`\\lambda` is adjusted at each iteration:
-    it is effectively increased when we are far from the solution, and
-    decreased when we are close to it. In practice, the algorithm
-    attempts to pick the smallest :math:`\\lambda` that is can while
-    making sure that the :math:`\\chi^2` decreases at each step.
-
-    The main advantage of the LM algorithm is its adaptability. When
-    the current estimate is far from the optimum, the algorithm
-    behaves like a gradient descent to ensure global
-    convergence. However, when it is close to the optimum, it behaves
-    like Gauss-Newton, which provides fast local convergence.
-
-    In practice, the algorithm is typically implemented with various
-    enhancements to improve its performance. For example, the Jacobian
-    may be approximated with finite differences, geodesic acceleration
-    can be used to speed up convergence, and more sophisticated
-    strategies can be used to adjust the damping factor :math:`\\lambda`.
-
-    The exact performance of the LM algorithm will depend on the
-    nature of the problem, including the complexity of the function
-    f(x), the quality of the initial estimate x0, and the distribution
-    of the data.
-
-    The LM class implemented for AutoPhot takes a model, initial
-    state, and various other optional parameters as inputs and seeks
-    to find the parameters that minimize the cost function.
+    """based heavily on:
+    @article{gavin2019levenberg,
+        title={The Levenberg-Marquardt algorithm for nonlinear least squares curve-fitting problems},
+        author={Gavin, Henri P},
+        journal={Department of Civil and Environmental Engineering, Duke University},
+        volume={19},
+        year={2019}
+    }
+
+    The Levenberg-Marquardt algorithm bridges the gap between a
+    gradient descent optimizer and a Newton's Method optimizer. The
+    Hessian for the Newton's Method update is too complex to evaluate
+    with automatic differentiation (memory scales roughly as
+    parameters^2 * pixels^2) and so an approximation is made using the
+    Jacobian of the image pixels wrt to the parameters of the
+    model. Automatic differentiation provides an exact Jacobian as
+    opposed to a finite differences approximation.
+
+    Once a Hessian H and gradient G have been determined, the update
+    step is defined as h which is the solution to the linear equation:
+
+    (H + L*I)h = G
+
+    where L is the Levenberg-Marquardt damping parameter and I is the
+    identity matrix. For small L this is just the Newton's method, for
+    large L this is just a small gradient descent step (approximately
+    h = grad/L). The method implimented is modified from Gavin 2019.
 
     Args:
-      model: The model to be optimized.
-      initial_state (Sequence): Initial values for the parameters to be optimized.
-      max_iter (int): Maximum number of iterations for the algorithm.
-      relative_tolerance (float): Tolerance level for relative change in cost function value to trigger termination of the algorithm.
-      fit_parameters_identity: Used to select a subset of parameters .
-      verbose: Controls the verbosity of the output during optimization. A higher value results in more detailed output. If not provided, defaults to 0 (no output).
-      max_step_iter (optional): The maximum number of steps while searching for chi^2 improvement on a single Jacobian evaluation. Default is 10.
-      curvature_limit (optional): Controls how cautious the optimizer is for changing curvature. It should be a number greater than 0, where smaller is more cautious. Default is 0.75.
-      Lup and Ldn (optional): These are the adjustment step sizes for the damping parameter. Default is 5 and 3 respectively.
-      L0 (optional): This is the starting damping parameter. For easy problems with good initialization, this can be set lower. Default is 1.
-      acceleration (optional): Controls the use of geodesic acceleration, which can be helpful in some scenarios. Set 1 for full acceleration, 0 for no acceleration. Default is 0.
-        
+        model (AutoPhot_Model): object with which to perform optimization
+        initial_state (Optional[Sequence]): an initial state for optimization
+        epsilon4 (Optional[float]): approximation accuracy requirement, for any rho < epsilon4 the step will be rejected. Default 0.1
+        epsilon5 (Optional[float]): numerical stability factor, added to the diagonal of the Hessian. Default 1e-8
+        constraints (Optional[Union[LM_Constraint,tuple[LM_Constraint]]]): Constraint objects which control the fitting process.
+        L0 (Optional[float]): initial value for L factor in (H +L*I)h = G. Default 1.
+        Lup (Optional[float]): amount to increase L when rejecting an update step. Default 11.
+        Ldn (Optional[float]): amount to decrease L when accetping an update step. Default 9.
+
     """
 
     def __init__(
-            self,
-            model,
-            initial_state: Sequence = None,
-            max_iter: int = 100,
-            relative_tolerance: float = 1e-5,
-            fit_parameters_identity=None,
-            **kwargs,
+        self,
+        model: "AutoPhot_Model",
+        initial_state: Sequence = None,
+        max_iter: int = 100,
+        fit_parameters_identity: Optional[tuple] = None,
+        **kwargs,
     ):
-
         super().__init__(
             model,
             initial_state,
             max_iter=max_iter,
             fit_parameters_identity=fit_parameters_identity,
-            relative_tolerance = relative_tolerance,
             **kwargs,
         )
-        # The forward model which computes the output image given input parameters
-        self.forward = partial(model, as_representation = True, parameters_identity = fit_parameters_identity)
-        # Compute the jacobian in representation units (defined for -inf, inf)
-        self.jacobian = partial(model.jacobian, as_representation = True, parameters_identity = fit_parameters_identity)
-        # Compute the jacobian in natural units
-        self.jacobian_natural = partial(model.jacobian, as_representation = False, parameters_identity = fit_parameters_identity)
-        # Function to transform between true parameter values and representation values
-        self.transform = partial(model.parameters.transform, to_representation = False, parameters_identity = fit_parameters_identity)
-        # Maximum number of iterations of the algorithm
-        self.max_iter = max_iter
-        # Maximum number of steps while searching for chi^2 improvement on a single jacobian evaluation
-        self.max_step_iter = kwargs.get("max_step_iter", 10)
-        # sets how cautious the optimizer is for changing curvature, should be number greater than 0, where smaller is more cautious
-        self.curvature_limit = kwargs.get("curvature_limit", 0.75) 
-        # These are the adjustment step sized for the damping parameter
-        self._Lup = kwargs.get("Lup", 5.)
-        self._Ldn = kwargs.get("Ldn", 3.)
-        # This is the starting damping parameter, for easy problems with good initialization, this can be set lower
-        self.L = kwargs.get("L0", 1.)
-        # Geodesic acceleration is helpful in some scenarios. By default it is turned off. Set 1 for full acceleration, 0 for no acceleration.
-        self.acceleration = kwargs.get("acceleration", 0.)
+
+        # Set optimizer parameters
+        self.epsilon4 = kwargs.get("epsilon4", 0.1)
+        self.epsilon5 = kwargs.get("epsilon5", 1e-8)
+        self.Lup = kwargs.get("Lup", 11.0)
+        self.Ldn = kwargs.get("Ldn", 9.0)
+        self.L = kwargs.get("L0", 1.0)
+        self.use_broyden = kwargs.get("use_broyden", False)
+
         # Initialize optimizer atributes
         self.Y = self.model.target[self.fit_window].flatten("data")
-        
-        # Degrees of freedom
+        #        1 / sigma^2
+        self.W = (
+            1.0 / self.model.target[self.fit_window].flatten("variance")
+            if model.target.has_variance
+            else 1.0
+        )
+        #          # pixels      # parameters
         self.ndf = len(self.Y) - len(self.current_state)
+        self.J = None
+        self.full_jac = False
+        self.current_Y = None
+        self.prev_Y = [None, None]
+        if self.model.target.has_mask:
+            self.mask = self.model.target[self.fit_window].flatten("mask")
+            # subtract masked pixels from degrees of freedom
+            self.ndf -= torch.sum(self.mask)
+        self.L_history = []
+        self.decision_history = []
+        self.rho_history = []
+        self._count_grad_step = 0
+        self._count_converged = 0
+        self.ndf = kwargs.get("ndf", self.ndf)
+        self._covariance_matrix = None
 
-        # 1 / (2 * sigma^2)
-        if model.target.has_variance:
-            self.W = 1. / self.model.target[self.fit_window].flatten("variance")
+        # update attributes with constraints
+        self.constraints = kwargs.get("constraints", None)
+        if self.constraints is not None and isinstance(self.constraints, LM_Constraint):
+            self.constraints = (self.constraints,)
+
+        if self.constraints is not None:
+            for con in self.constraints:
+                self.Y = torch.cat((self.Y, con.reference_value))
+                self.W = torch.cat((self.W, 1 / con.weight))
+                self.ndf -= con.reduce_ndf
+                if self.model.target.has_mask:
+                    self.mask = torch.cat(
+                        (
+                            self.mask,
+                            torch.zeros_like(con.reference_value, dtype=torch.bool),
+                        )
+                    )
+
+    def L_up(self, Lup=None):
+        if Lup is None:
+            Lup = self.Lup
+        self.L = min(1e9, self.L * Lup)
+
+    def L_dn(self, Ldn=None):
+        if Ldn is None:
+            Ldn = self.Ldn
+        self.L = max(1e-9, self.L / Ldn)
+
+    @torch.no_grad()
+    def grad_step(self) -> None:
+        L = 0.1
+        self.iteration += 1
+        self._count_grad_step += 1
+        if self.verbose > 1:
+            AP_config.ap_logger.info(
+                f"taking grad step. Loss to beat: {np.nanmin(self.loss_history[:-1])}"
+            )
+        for count in range(20):
+            self.update_Yp(self.grad * L)
+            loss = self.update_chi2()
+
+            if not torch.isfinite(loss):
+                L /= 10
+                continue
+            if self.verbose > 1:
+                AP_config.ap_logger.info(f"grad step loss: {loss.item()}, L: {L}")
+            if np.nanmin(self.loss_history[:-1]) > loss.item():
+                self.loss_history.append(loss.detach().cpu().item())
+                self.L = 1.0
+                self.L_history.append(self.L)
+                self.current_state += self.grad * L
+                self.lambda_history.append(
+                    np.copy(self.current_state.detach().cpu().numpy())
+                )
+                self.decision_history.append("accept grad")
+                if self.verbose > 0:
+                    AP_config.ap_logger.info("accept grad")
+                self.rho_history.append(1.0)
+                self.prev_Y[0] = self.prev_Y[1]
+                self.prev_Y[1] = torch.clone(self.current_Y)
+                break
+            elif (
+                np.abs(np.nanmin(self.loss_history[:-1]) - loss.item())
+                < (self.relative_tolerance * 1e-3)
+                and L < 1e-5
+            ):
+                self.loss_history.append(loss.detach().cpu().item())
+                self.L = 1.0
+                self.L_history.append(self.L)
+                self.current_state += self.grad * L
+                self.lambda_history.append(
+                    np.copy(self.current_state.detach().cpu().numpy())
+                )
+                self.decision_history.append("accept bad grad")
+                if self.verbose > 0:
+                    AP_config.ap_logger.info("accept bad grad")
+                self.rho_history.append(1.0)
+                self.prev_Y[0] = self.prev_Y[1]
+                self.prev_Y[1] = torch.clone(self.current_Y)
+                break
+            else:
+                L /= 10
+                continue
         else:
-            self.W = torch.ones_like(self.Y)
+            raise RuntimeError(
+                "Unable to take gradient step! LM has found itself in a very bad place of parameter space, try adjusting initial parameters"
+            )
 
-        # mask
-        if model.target.has_mask:
-            mask = self.model.target[self.fit_window].flatten("mask")
-            self.mask = torch.logical_not(mask)
-            self.ndf -= torch.sum(mask).item()
+    def step(self, current_state=None) -> None:
+        """
+        Levenberg-Marquardt update step
+        """
+        if current_state is not None:
+            self.current_state = current_state
+
+        if self.iteration > 0:
+            if self.verbose > 0:
+                AP_config.ap_logger.info("---------iter---------")
         else:
-            self.mask = None
+            if self.verbose > 0:
+                AP_config.ap_logger.info("---------init---------")
 
-        # variable to store covariance matrix if it is ever computed
-        self._covariance_matrix = None
+        h = self.update_h()
+        if self.verbose > 1:
+            AP_config.ap_logger.info(f"h: {h.detach().cpu().numpy()}")
 
-    def Lup(self):
-        self.L = min(1e9, self.L * self._Lup)
-    def Ldn(self):
-        self.L = max(1e-9, self.L / self._Ldn)
-        
-    @torch.no_grad()
-    def step(self, chi2):
+        self.update_Yp(h)
+        loss = self.update_chi2()
+        if self.verbose > 0:
+            AP_config.ap_logger.info(f"LM loss: {loss.item()}")
 
-        Y0 = self.forward(parameters = self.current_state).flatten("data")
-        J = self.jacobian(parameters = self.current_state).flatten("data")
-        r = -self.W * (self.Y - Y0)
-        self.hess = J.T @ (self.W.view(len(self.W), -1) * J)
-        self.grad = J.T @ (self.W * (self.Y - Y0))
-
-        init_chi2 = chi2
-        nostep = True
-        best = (torch.zeros_like(self.current_state), init_chi2, self.L)
-        scarry_best = (None, init_chi2, self.L)
-        direction = "none"
-        iteration = 0
-        d = 0.1
-        for iteration in range(self.max_step_iter):
-            # In a scenario where LM is having a hard time proposing a good step, but the damping is really low, just jump up to normal damping levels
-            if iteration > self.max_step_iter/2 and self.L < 1e-3:
-                self.L = 1.
-
-            # compute LM update step
-            h = self._h(self.L, self.grad, self.hess)
-
-            # Compute goedesic acceleration
-            Y1 = self.forward(parameters = self.current_state + d*h).flatten("data")
-            rh = -self.W * (self.Y - Y1)
-            rpp = (2 / d) * ((rh - r) / d - self.W*(J @ h))
-            if self.L > 1e-4:
-                a = -self._h(self.L, J.T @ rpp, self.hess) / 2
-            else:
-                a = torch.zeros_like(h)
+        if self.iteration == 0:
+            self.prev_Y[1] = self.current_Y
+        self.loss_history.append(loss.detach().cpu().item())
+        self.L_history.append(self.L)
+        self.lambda_history.append(
+            np.copy((self.current_state + h).detach().cpu().numpy())
+        )
 
-            # Evaluate new step
-            ha = h + a*self.acceleration
-            Y1 = self.forward(parameters = self.current_state + ha).flatten("data")
+        if self.iteration > 0 and not torch.isfinite(loss):
+            if self.verbose > 0:
+                AP_config.ap_logger.warning("nan loss")
+            self.decision_history.append("nan")
+            self.rho_history.append(None)
+            self._count_reject += 1
+            self.iteration += 1
+            self.L_up()
+            return
+        elif self.iteration > 0:
+            rho = self.rho(np.nanmin(self.loss_history[:-1]), loss, h)
+            if self.verbose > 1:
+                AP_config.ap_logger.debug(
+                    f"LM loss: {loss.item()}, best loss: {np.nanmin(self.loss_history[:-1])}, loss diff: {np.nanmin(self.loss_history[:-1]) - loss.item()}, L: {self.L}"
+                )
+            self.rho_history.append(rho)
+            if self.verbose > 1:
+                AP_config.ap_logger.debug(f"rho: {rho.item()}")
 
-            # Compute and report chi^2
-            chi2 = self._chi2(Y1.detach()).item()
+            if rho > self.epsilon4:
+                if self.verbose > 0:
+                    AP_config.ap_logger.info("accept")
+                self.decision_history.append("accept")
+                self.prev_Y[0] = self.prev_Y[1]
+                self.prev_Y[1] = torch.clone(self.current_Y)
+                self.current_state += h
+                self.L_dn()
+                self._count_reject = 0
+                if (
+                    0
+                    < (self.ndf * (np.nanmin(self.loss_history[:-1]) - loss) / loss)
+                    < self.relative_tolerance
+                ):
+                    self._count_finish += 1
+                else:
+                    self._count_finish = 0
+            # elif self._count_reject == 4:
+            #     if self.verbose > 0:
+            #         AP_config.ap_logger.info("reject, resetting jacobian")
+            #     self.decision_history.append("reject")
+            #     self.L = min(1e-2, self.L / self.Lup**4)
+            #     self._count_reject += 1
+            else:
+                if self.verbose > 0:
+                    AP_config.ap_logger.info("reject")
+                self.decision_history.append("reject")
+                self.L_up()
+                self._count_reject += 1
+                return
+        else:
+            self.decision_history.append("init")
+            self.rho_history.append(None)
+
+        if (
+            (not self.use_broyden)
+            or self.J is None
+            or self.iteration < 2
+            or "reset" in self.decision_history[-2:]
+            or rho < self.epsilon4
+            or self._count_reject > 0
+            or self.iteration >= (2 * len(self.current_state))
+            or self.decision_history[-1] == "nan"
+        ):
             if self.verbose > 1:
-                AP_config.ap_logger.info(f"sub step L: {self.L}, Chi^2: {chi2}")
+                AP_config.ap_logger.debug("full jac")
+            self.update_J_AD()
+        else:
+            if self.verbose > 1:
+                AP_config.ap_logger.debug("Broyden jac")
+            self.update_J_Broyden(h, self.prev_Y[0], self.current_Y)
+
+        self.update_hess()
+        self.update_grad(self.prev_Y[1])
+        self.iteration += 1
+
+    def fit(self):
+        self.iteration = 0
+        self._count_reject = 0
+        self._count_finish = 0
+        self.grad_only = False
+
+        start_fit = time()
+        try:
+            while True:
+                if self.verbose > 0:
+                    AP_config.ap_logger.info(f"L: {self.L}")
 
-            # Skip if chi^2 is nan
-            if not np.isfinite(chi2):
-                if self.verbose > 1:
-                    AP_config.ap_logger.info("Skip due to non-finite values")
-                self.Lup()
-                if direction == "better":
+                # take LM step
+                self.step()
+
+                # Save the state of the model
+                if (
+                    self.save_steps is not None
+                    and self.decision_history[-1] == "accept"
+                ):
+                    self.model.save(
+                        os.path.join(
+                            self.save_steps,
+                            f"{self.model.name}_Iteration_{self.iteration:03d}.yaml",
+                        )
+                    )
+
+                lam, L, loss = self.progress_history()
+
+                # Check for convergence
+                if (
+                    self.decision_history.count("accept") > 2
+                    and self.decision_history[-1] == "accept"
+                    and L[-1] < 0.1
+                    and ((loss[-2] - loss[-1]) / loss[-1])
+                    < (self.relative_tolerance / 10)
+                ):
+                    self._count_grad_step = 0
+                    self._count_converged += 1
+                elif self._count_grad_step >= 5:
+                    self.message = (
+                        self.message
+                        + "success by immobility, unable to find improvement either converged or bad area of parameter space."
+                    )
                     break
-                direction = "worse"
-                continue
-            
-            # Keep track of chi^2 improvement even if it fails curvature test
-            if chi2 <= scarry_best[1]:
-                scarry_best = (ha, chi2, self.L)
-                nostep = False
-
-            # Check for high curvature, in which case linear approximation is not valid. avoid this step
-            if torch.linalg.norm(a) / torch.linalg.norm(h) > self.curvature_limit:
-                if self.verbose > 1:
-                    AP_config.ap_logger.info("Skip due to large curvature")
-                self.Lup()
-                if direction == "better":
+                elif self.iteration >= self.max_iter:
+                    self.message = (
+                        self.message + f"fail max iterations reached: {self.iteration}"
+                    )
                     break
-                direction = "worse"
-                continue
-
-            # Check for Chi^2 improvement
-            if chi2 <= best[1]:
-                if self.verbose > 1:
-                    AP_config.ap_logger.info("new best chi^2")
-                best = (ha, chi2, self.L)
-                nostep = False
-                self.Ldn()
-                if self.L <= 1e-8 or direction == "worse":
+                elif not torch.all(torch.isfinite(self.current_state)):
+                    self.message = self.message + "fail non-finite step taken"
                     break
-                direction = "better"
-            elif chi2 > best[1] and direction in ["none", "worse"]:
-                if self.verbose > 1:
-                    AP_config.ap_logger.info("chi^2 is worse")
-                self.Lup()
-                if self.L == 1e9:
+                elif (
+                    self.L >= (1e9 - 1)
+                    and self._count_reject >= 12
+                    and not self.take_low_rho_step()
+                ):
+                    if not self.full_jac:
+                        self.update_J_AD()
+                        self.update_hess()
+                        self.update_grad(self.prev_Y[1])
+                    try:
+                        self.grad_step()
+                    except RuntimeError:
+                        self.message = (
+                            self.message
+                            + "fail by immobility, unable to find improvement or even small bad step"
+                        )
+                        break
+                if self._count_converged >= 2:
+                    self.message = self.message + "success"
                     break
-                direction = "worse"
-            else:
-                break
-
-            # If a step substantially improves the chi^2, stop searching for better step, simply exit the loop and accept the good step
-            if (best[1] - init_chi2) / init_chi2 < -0.1:
-                if self.verbose > 1:
-                    AP_config.ap_logger.info("Large step taken, ending search for good step")
-                break
+                lam, L, loss = self.accept_history()
+                if len(loss) >= 10:
+                    loss10 = np.array(loss[-10:])
+                    if np.all(
+                        np.abs((loss10[1:] - loss10[:-1]) / loss10[:-1])
+                        < self.relative_tolerance
+                    ):
+                        self.message = self.message + "success"
+                        break
+        except KeyboardInterrupt:
+            self.message = self.message + "fail interrupted"
+
+        if self.message.startswith("fail") and self._count_finish > 0:
+            self.message = (
+                self.message
+                + ". possibly converged to numerical precision and could not make a better step."
+            )
+        self.model.parameters.set_values(
+            self.res(),
+            as_representation=True,
+            parameters_identity=self.fit_parameters_identity,
+        )
+        if self.verbose > 1:
+            AP_config.ap_logger.info(
+                f"LM Fitting complete in {time() - start_fit} sec with message: {self.message}"
+            )
 
-        if nostep:
-            if scarry_best[0] is not None:
-                if self.verbose > 1:
-                    AP_config.ap_logger.warn("no low curvature step found, taking high curvature step")
-                return scarry_best
-            raise OptimizeStopFail("Could not find step to improve chi^2")
+        return self
 
-        return best
+    def update_uncertainty(self):
+        # set the uncertainty for each parameter
+        cov = self.covariance_matrix
+        if torch.all(torch.isfinite(cov)):
+            try:
+                self.model.parameters.set_uncertainty(
+                    torch.sqrt(torch.abs(torch.diag(cov))),
+                    as_representation=False,
+                    parameters_identity=self.fit_parameters_identity,
+                )
+            except RuntimeError as e:
+                AP_config.ap_logger.warning(f"Unable to update uncertainty due to: {e}")
 
-    @staticmethod
     @torch.no_grad()
-    def _h(L, grad, hess):
+    def undo_step(self) -> None:
+        AP_config.ap_logger.info("undoing step, trying to recover")
+        assert (
+            self.decision_history.count("accept") >= 2
+        ), "cannot undo with not enough accepted steps, retry with new parameters"
+        assert len(self.decision_history) == len(self.lambda_history)
+        assert len(self.decision_history) == len(self.L_history)
+        found_accept = False
+        for i in reversed(range(len(self.decision_history))):
+            if not found_accept and self.decision_history[i] == "accept":
+                found_accept = True
+                continue
+            if self.decision_history[i] != "accept":
+                continue
+            self.current_state = torch.tensor(
+                self.lambda_history[i],
+                dtype=AP_config.ap_dtype,
+                device=AP_config.ap_device,
+            )
+            self.L = self.L_history[i] * self.Lup
+
+    def take_low_rho_step(self) -> bool:
+        for i in reversed(range(len(self.decision_history))):
+            if "accept" in self.decision_history[i]:
+                return False
+            if self.rho_history[i] is not None and self.rho_history[i] > 0:
+                if self.verbose > 0:
+                    AP_config.ap_logger.info(
+                        f"taking a low rho step for some progress: {self.rho_history[i]}"
+                    )
+                self.current_state = torch.tensor(
+                    self.lambda_history[i],
+                    dtype=AP_config.ap_dtype,
+                    device=AP_config.ap_device,
+                )
+                self.L = self.L_history[i]
 
-        I = torch.eye(len(grad), dtype=grad.dtype, device=grad.device)
+                self.loss_history.append(self.loss_history[i])
+                self.L_history.append(self.L)
+                self.lambda_history.append(
+                    np.copy((self.current_state).detach().cpu().numpy())
+                )
+                self.decision_history.append("low rho accept")
+                self.rho_history.append(self.rho_history[i])
+
+                with torch.no_grad():
+                    self.update_Yp(torch.zeros_like(self.current_state))
+                    self.prev_Y[0] = self.prev_Y[1]
+                    self.prev_Y[1] = self.current_Y
+                self.update_J_AD()
+                self.update_hess()
+                self.update_grad(self.prev_Y[1])
+                self.iteration += 1
+                self.count_reject = 0
+                return True
+
+    @torch.no_grad()
+    def update_h(self) -> torch.Tensor:
+        """Solves the LM update linear equation (H + L*I)h = G to determine
+        the proposal for how to adjust the parameters to decrease the
+        chi2.
+
+        """
+        h = torch.zeros_like(self.current_state)
+        if self.iteration == 0:
+            return h
 
         h = torch.linalg.solve(
-            (hess + 1e-2 * L**2 * I) * (1 + L**2 * I) ** 2 / (1 + L**2),
-            grad,
+            (
+                self.hess
+                + 1e-3
+                * self.L
+                * torch.eye(
+                    len(self.grad), dtype=AP_config.ap_dtype, device=AP_config.ap_device
+                )
+            )
+            * (
+                1
+                + self.L
+                * torch.eye(
+                    len(self.grad), dtype=AP_config.ap_dtype, device=AP_config.ap_device
+                )
+            )
+            ** 2
+            / (1 + self.L),
+            self.grad,
         )
-        
         return h
 
     @torch.no_grad()
-    def _chi2(self, Ypred):
-        if self.mask is None:
-            return torch.sum(self.W * (self.Y - Ypred)**2) / self.ndf
-        else:
-            return torch.sum((self.W * (self.Y - Ypred)**2)[self.mask]) / self.ndf
-            
+    def update_Yp(self, h):
+        """
+        Updates the current model values for each pixel
+        """
+        # Sample model at proposed state
+        self.current_Y = self.model(
+            parameters=self.current_state + h,
+            as_representation=True,
+            parameters_identity=self.fit_parameters_identity,
+            window=self.fit_window,
+        ).flatten("data")
 
-    @torch.no_grad()
-    def update_hess_grad(self, natural = False):
+        # Add constraint evaluations
+        if self.constraints is not None:
+            for con in self.constraints:
+                self.current_Y = torch.cat((self.current_Y, con(self.model)))
 
-        if natural:
-            J = self.jacobian_natural(parameters = self.transform(self.current_state)).flatten("data")
-        else:
-            J = self.jacobian(parameters = self.current_state).flatten("data")
-        Ypred = self.forward(parameters = self.current_state).flatten("data")
-        self.hess = torch.matmul(J.T, (self.W.view(len(self.W), -1) * J))
-        self.grad = torch.matmul(J.T, self.W * (self.Y - Ypred))
-        
     @torch.no_grad()
-    def fit(self):
+    def update_chi2(self):
+        """
+        Updates the chi squared / ndf value
+        """
+        # Apply mask if needed
+        if self.model.target.has_mask:
+            loss = (
+                torch.sum(
+                    ((self.Y - self.current_Y) ** 2 * self.W)[
+                        torch.logical_not(self.mask)
+                    ]
+                )
+                / self.ndf
+            )
+        else:
+            loss = torch.sum((self.Y - self.current_Y) ** 2 * self.W) / self.ndf
 
-        self._covariance_matrix = None
-        self.loss_history = [self._chi2(self.forward(parameters = self.current_state).flatten("data")).item()]
-        self.L_history = [self.L]
-        self.lambda_history = [self.current_state.detach().clone().cpu().numpy()]
-        
-        for iteration in range(self.max_iter):
-            if self.verbose > 0:
-                AP_config.ap_logger.info(f"Chi^2: {self.loss_history[-1]}, L: {self.L}")
-            try:
-                res = self.step(chi2 = self.loss_history[-1])
-            except OptimizeStopFail:
-                if self.verbose > 0:
-                    AP_config.ap_logger.warning("Could not find step to improve Chi^2, stopping")
-                self.message = self.message + "fail. Could not find step to improve Chi^2"
-                break
+        return loss
 
-            self.L = res[2]
-            self.current_state = (self.current_state + res[0]).detach()
-            
-            self.L_history.append(self.L)
-            self.loss_history.append(res[1])
-            self.lambda_history.append(self.current_state.detach().clone().cpu().numpy())
-            
-            self.Ldn()
-            
-            if len(self.loss_history) >= 3:
-                if (self.loss_history[-3] - self.loss_history[-1]) / self.loss_history[-1] < self.relative_tolerance and self.L < 0.1:
-                    self.message = self.message + "success"
-                    break
-            if len(self.loss_history) > 10:
-                if (self.loss_history[-10] - self.loss_history[-1]) / self.loss_history[-1] < self.relative_tolerance:
-                    self.message = self.message + "success by immobility. Convergence not guaranteed"
-                    break
-                
-        else:
-            self.message = self.message + "fail. Maximum iterations"
-                
-        if self.verbose > 0:
-            AP_config.ap_logger.info(f"Final Chi^2: {self.loss_history[-1]}, L: {self.L_history[-1]}. Converged: {self.message}")
-        self.model.parameters.set_values(
-            self.res(),
+    def update_J_AD(self) -> None:
+        """
+        Update the jacobian using automatic differentiation, produces an accurate jacobian at the current state.
+        """
+        # Free up memory
+        del self.J
+        if "cpu" not in AP_config.ap_device:
+            torch.cuda.empty_cache()
+
+        # Compute jacobian on image
+        self.J = self.model.jacobian(
+            torch.clone(self.current_state).detach(),
             as_representation=True,
             parameters_identity=self.fit_parameters_identity,
-        )
+            window=self.fit_window,
+        ).flatten("data")
 
-        return self
+        # compute the constraint jacobian if needed
+        if self.constraints is not None:
+            for con in self.constraints:
+                self.J = torch.cat((self.J, con.jacobian(self.model)))
+
+        # Apply mask if needed
+        if self.model.target.has_mask:
+            self.J[self.mask] = 0.0
+
+        # Note that the most recent jacobian was a full autograd jacobian
+        self.full_jac = True
+
+    def update_J_natural(self) -> None:
+        """
+        Update the jacobian using automatic differentiation, produces an accurate jacobian at the current state. Use this method to get the jacobian in the parameter space instead of representation space.
+        """
+        # Free up memory
+        del self.J
+        if "cpu" not in AP_config.ap_device:
+            torch.cuda.empty_cache()
+
+        # Compute jacobian on image
+        self.J = self.model.jacobian(
+            torch.clone(
+                self.model.parameters.transform(
+                    self.current_state,
+                    to_representation=False,
+                    parameters_identity=self.fit_parameters_identity,
+                )
+            ).detach(),
+            as_representation=False,
+            parameters_identity=self.fit_parameters_identity,
+            window=self.fit_window,
+        ).flatten("data")
+
+        # compute the constraint jacobian if needed
+        if self.constraints is not None:
+            for con in self.constraints:
+                self.J = torch.cat((self.J, con.jacobian(self.model)))
+
+        # Apply mask if needed
+        if self.model.target.has_mask:
+            self.J[self.mask] = 0.0
+
+        # Note that the most recent jacobian was a full autograd jacobian
+        self.full_jac = False
+
+    @torch.no_grad()
+    def update_J_Broyden(self, h, Yp, Yph) -> None:
+        """
+        Use the Broyden update to approximate the new Jacobian tensor at the current state. Less accurate, but far faster.
+        """
+
+        # Update the Jacobian
+        self.J = Broyden_step(self.J, h, Yp, Yph)
+
+        # Apply mask if needed
+        if self.model.target.has_mask:
+            self.J[self.mask] = 0.0
+
+        # compute the constraint jacobian if needed
+        if self.constraints is not None:
+            for con in self.constraints:
+                self.J = torch.cat((self.J, con.jacobian(self.model)))
+
+        # Note that the most recent jacobian update was with Broyden step
+        self.full_jac = False
+
+    @torch.no_grad()
+    def update_hess(self) -> None:
+        """
+        Update the Hessian using the jacobian most recently computed on the image.
+        """
+
+        if isinstance(self.W, float):
+            self.hess = torch.matmul(self.J.T, self.J)
+        else:
+            self.hess = torch.matmul(self.J.T, self.W.view(len(self.W), -1) * self.J)
+        self.hess += self.epsilon5 * torch.eye(
+            len(self.current_state),
+            dtype=AP_config.ap_dtype,
+            device=AP_config.ap_device,
+        )
 
     @property
     @torch.no_grad()
     def covariance_matrix(self) -> torch.Tensor:
         if self._covariance_matrix is not None:
             return self._covariance_matrix
-        self.update_hess_grad(natural = True)
+        self.update_J_natural()
+        self.update_hess()
         try:
             self._covariance_matrix = torch.linalg.inv(self.hess)
         except:
             AP_config.ap_logger.warning(
                 "WARNING: Hessian is singular, likely at least one model is non-physical. Will massage Hessian to continue but results should be inspected."
             )
             self.hess += torch.eye(
                 len(self.grad), dtype=AP_config.ap_dtype, device=AP_config.ap_device
             ) * (torch.diag(self.hess) == 0)
             self._covariance_matrix = torch.linalg.inv(self.hess)
         return self._covariance_matrix
 
     @torch.no_grad()
-    def update_uncertainty(self):
-        # set the uncertainty for each parameter
-        cov = self.covariance_matrix
-        if torch.all(torch.isfinite(cov)):
-            try:
-                self.model.parameters.set_uncertainty(
-                    torch.sqrt(
-                        torch.abs(torch.diag(cov))
-                    ),
-                    as_representation=False,
-                    parameters_identity=self.fit_parameters_identity,
+    def update_grad(self, Yph) -> None:
+        """
+        Update the gradient using the model evaluation on all pixels
+        """
+        self.grad = torch.matmul(self.J.T, self.W * (self.Y - Yph))
+
+    @torch.no_grad()
+    def rho(self, Xp, Xph, h) -> torch.Tensor:
+        return (
+            self.ndf
+            * (Xp - Xph)
+            / abs(
+                torch.dot(
+                    h,
+                    self.L * (torch.abs(torch.diag(self.hess) - self.epsilon5) * h)
+                    + self.grad,
                 )
-            except RuntimeError as e:
-                AP_config.ap_logger.warning(f"Unable to update uncertainty due to: {e}")
-        else:
-            AP_config.ap_logger.warning(f"Unable to update uncertainty due to non finite covariance matrix")
+            )
+        )
+
+    def accept_history(self) -> (List[np.ndarray], List[np.ndarray], List[float]):
+        lambdas = []
+        Ls = []
+        losses = []
+
+        for l in range(len(self.decision_history)):
+            if "accept" in self.decision_history[l] and np.isfinite(
+                self.loss_history[l]
+            ):
+                lambdas.append(self.lambda_history[l])
+                Ls.append(self.L_history[l])
+                losses.append(self.loss_history[l])
+        return lambdas, Ls, losses
+
+    def progress_history(self) -> (List[np.ndarray], List[np.ndarray], List[float]):
+        lambdas = []
+        Ls = []
+        losses = []
+
+        for l in range(len(self.decision_history)):
+            if self.decision_history[l] == "accept":
+                lambdas.append(self.lambda_history[l])
+                Ls.append(self.L_history[l])
+                losses.append(self.loss_history[l])
+        return lambdas, Ls, losses
+
+
+class LM_Constraint:
+    """Add an arbitrary constraint to the LM optimization algorithm.
+
+    Expresses a constraint between parameters in the LM optimization
+    routine. Constraints may be used to bias parameters to have
+    certain behaviour, for example you may require the radius of one
+    model to be larger than that of another, or may require two models
+    to have the same position on the sky. The constraints defined in
+    this object are fuzzy constraints and so can be broken to some
+    degree, the amount of constraint breaking is determined my how
+    informative the data is and how strong the constraint weight is
+    set. To create a constraint, first construct a function which
+    takes as argument a 1D tensor of the model parameters and gives as
+    output a real number (or 1D tensor of real numbers) which is zero
+    when the constraint is satisfied and non-zero increasing based on
+    how much the constraint is violated. For example:
+
+    def example_constraint(P):
+        return (P[1] - P[0]) * (P[1] > P[0]).int()
+
+    which enforces that parameter 1 is less than parameter 0. Note
+    that we do not use any control flow "if" statements and instead
+    incorporate the condition through multiplication, this is
+    important as it allows pytorch to compute derivatives through the
+    expression and performs far faster on GPU since no communication
+    is needed back and forth to handle the if-statement. Keep this in
+    mind while constructing your constraint function. Also, make sure
+    that any math operations are performed by pytorch so it can
+    construct a computational graph. Bayond the requirement that the
+    constraint be differentiable, there is no limitation on what
+    constraints can be built with this system.
+
+    Args:
+      constraint_func (Callable[torch.Tensor, torch.Tensor]): python function which takes in a 1D tensor of parameters and generates real values in a tensor.
+      constraint_args (Optional[tuple]): An optional tuple of arguments for the constraint function that will be unpacked when calling the function.
+      weight (torch.Tensor): The weight of this constraint in the range (0,inf). Smaller values mean a stronger constraint, larger values mean a weaker constraint. Default 1.
+      representation_parameters (bool): if the constraint_func expects the parameters in the form of their representation or their standard value. Default False
+      out_len (int): the length of the output tensor by constraint_func. Default 1
+      reference_value (torch.Tensor): The value at which the constraint is satisfied. Default 0.
+      reduce_ndf (float): Amount by which to reduce the degrees of freedom. Default 0.
+
+    """
+
+    def __init__(
+        self,
+        constraint_func: Callable[[torch.Tensor, Any], torch.Tensor],
+        constraint_args: tuple = (),
+        representation_parameters: bool = False,
+        out_len: int = 1,
+        reduce_ndf: float = 0.0,
+        weight: Optional[torch.Tensor] = None,
+        reference_value: Optional[torch.Tensor] = None,
+        **kwargs,
+    ):
+        self.constraint_func = constraint_func
+        self.constraint_args = constraint_args
+        self.representation_parameters = representation_parameters
+        self.out_len = out_len
+        self.reduce_ndf = reduce_ndf
+        self.reference_value = torch.as_tensor(
+            reference_value if reference_value is not None else torch.zeros(out_len),
+            dtype=AP_config.ap_dtype,
+            device=AP_config.ap_device,
+        )
+        self.weight = torch.as_tensor(
+            weight if weight is not None else torch.ones(out_len),
+            dtype=AP_config.ap_dtype,
+            device=AP_config.ap_device,
+        )
+
+    def jacobian(self, model: "AutoPhot_Model"):
+        jac = jacobian(
+            lambda P: self.constraint_func(P, *self.constraint_args),
+            model.parameters.get_vector(
+                as_representation=self.representation_parameters
+            ),
+            strategy="forward-mode",
+            vectorize=True,
+            create_graph=False,
+        )
+
+        return jac.reshape(-1, np.sum(model.parameters.vector_len()))
+
+    def __call__(self, model: "AutoPhot_Model"):
+        return self.constraint_func(
+            model.parameters.get_vector(
+                as_representation=self.representation_parameters
+            ),
+            *self.constraint_args,
+        )
```

## autophot/fit/nuts.py

```diff
@@ -95,15 +95,15 @@
         initial_state: Optional[Sequence] = None,
         max_iter: int = 1000,
         **kwargs
     ):
         super().__init__(model, initial_state, max_iter=max_iter, **kwargs)
 
         self.inv_mass = kwargs.get("inv_mass", None)
-        self.epsilon = kwargs.get("epsilon", 1e-4)
+        self.epsilon = kwargs.get("epsilon", 1e-3)
         self.progress_bar = kwargs.get("progress_bar", True)
         self.prior = kwargs.get("prior", None)
         self.warmup = kwargs.get("warmup", 100)
         self.nuts_kwargs = kwargs.get("nuts_kwargs", {})
         self.mcmc_kwargs = kwargs.get("mcmc_kwargs", {})
 
     def fit(
```

## autophot/image/image_header.py

```diff
@@ -9,28 +9,28 @@
 from .window_object import Window, Window_List
 from .. import AP_config
 
 __all__ = ["Image_Header"]
 
 
 class Image_Header(object):
-    north = np.pi / 2.0
-
+    north = np.pi/2.
+    
     def __init__(
         self,
         data_shape: Optional[torch.Tensor] = None,
         pixelscale: Optional[Union[float, torch.Tensor]] = None,
-        wcs: Optional["astropy.wcs.wcs.WCS"] = None,
+        wcs: Optional['astropy.wcs.wcs.WCS'] = None,
         window: Optional[Window] = None,
         filename: Optional[str] = None,
         zeropoint: Optional[Union[float, torch.Tensor]] = None,
         note: Optional[str] = None,
         origin: Optional[Sequence] = None,
         center: Optional[Sequence] = None,
-        identity: str = None,
+        _identity: str = None,
         **kwargs: Any,
     ) -> None:
         """Initialize an instance of the APImage class.
 
         Parameters:
         -----------
         pixelscale : float or None, optional
@@ -49,26 +49,24 @@
             The center of the image in the coordinate system, as a 1D array of length 2. Default is None.
 
         Returns:
         --------
         None
         """
         # Record identity
-        if identity is None:
+        if _identity is None:
             self.identity = str(id(self))
         else:
-            self.identity = identity
+            self.identity = _identity
 
         if filename is not None:
             self.load(filename)
             return
 
-        self.data_shape = torch.as_tensor(
-            data_shape, dtype=torch.int32, device=AP_config.ap_device
-        )
+        self.data_shape = torch.as_tensor(data_shape, dtype=torch.int32, device=AP_config.ap_device)
         # set Zeropoint
         if zeropoint is None:
             self.zeropoint = None
         else:
             self.zeropoint = torch.as_tensor(
                 zeropoint, dtype=AP_config.ap_dtype, device=AP_config.ap_device
             )
@@ -76,37 +74,33 @@
         # set a note for the image
         self.note = note
 
         if wcs is not None and pixelscale is None:
             self.pixelscale = wcs.pixel_scale_matrix
         else:
             self.pixelscale = pixelscale
-
+            
         # Set Window
         if window is None:
             # If window is not provided, create one based on pixelscale and data shape
             assert (
                 self.pixelscale is not None
             ), "pixelscale cannot be None if window is not provided"
-
-            end = self.pixel_to_world_delta(
-                torch.flip(
+            
+            end = (self.pixel_to_world_delta(torch.flip(
                     torch.tensor(
                         data_shape, dtype=AP_config.ap_dtype, device=AP_config.ap_device
                     ),
                     (0,),
                 )
-            )
+            ))
             shape = torch.linalg.solve(self.pixelscale / self.pixel_length, end)
             if wcs is not None:
-                wcs_origin = wcs.pixel_to_world(-0.5, -0.5)
                 origin = torch.as_tensor(
-                    [wcs_origin.ra.arcsec, wcs_origin.dec.arcsec],
-                    dtype=AP_config.ap_dtype,
-                    device=AP_config.ap_device,
+                    wcs.pixel_to_world(-0.5, -0.5), dtype=AP_config.ap_dtype, device=AP_config.ap_device
                 )
             elif origin is None and center is None:
                 origin = torch.zeros(
                     2, dtype=AP_config.ap_dtype, device=AP_config.ap_device
                 )
             elif center is None:
                 origin = torch.as_tensor(
@@ -115,103 +109,85 @@
             else:
                 origin = (
                     torch.as_tensor(
                         center, dtype=AP_config.ap_dtype, device=AP_config.ap_device
                     )
                     - end / 2
                 )
-
-            self.window = Window(origin=origin, shape=shape, projection=self.pixelscale)
+            
+            self.window = Window(origin=origin, shape=shape, projection = self.pixelscale)
         else:
             # When The Window object is provided
             self.window = window
             if self.pixelscale is None:
                 pixelscale = self.window.shape[0] / data_shape[1]
-                self.pixelscale = torch.tensor(
-                    [[pixelscale, 0.0], [0.0, pixelscale]],
-                    dtype=AP_config.ap_dtype,
-                    device=AP_config.ap_device,
-                )
+                self.pixelscale = torch.tensor([[pixelscale, 0.],[0., pixelscale]], dtype=AP_config.ap_dtype, device=AP_config.ap_device)
 
     @property
     def pixelscale(self):
         return self._pixelscale
 
     @pixelscale.setter
     def pixelscale(self, pix):
         if pix is None:
             self._pixelscale = None
             return
-
-        self._pixelscale = (
-            torch.as_tensor(pix, dtype=AP_config.ap_dtype, device=AP_config.ap_device)
-            .clone()
-            .detach()
-        )
+        
+        self._pixelscale = torch.as_tensor(pix, dtype=AP_config.ap_dtype, device=AP_config.ap_device).clone().detach()
         if self._pixelscale.numel() == 1:
-            self._pixelscale = torch.tensor(
-                [[self._pixelscale.item(), 0.0], [0.0, self._pixelscale.item()]],
-                dtype=AP_config.ap_dtype,
-                device=AP_config.ap_device,
-            )
+            self._pixelscale = torch.tensor([[self._pixelscale.item(), 0.], [0., self._pixelscale.item()]], dtype=AP_config.ap_dtype, device=AP_config.ap_device)
         self._pixel_area = torch.linalg.det(self.pixelscale).abs()
         self._pixel_length = self._pixel_area.sqrt()
         self._pixel_origin = None
-        self._pixelscale_inv = torch.linalg.inv(self.pixelscale)
-
+        
     @property
     def pixel_area(self):
         return self._pixel_area
-
+    
     @property
     def pixel_length(self):
         return self._pixel_length
 
     @property
     def pixel_origin(self):
         if self._pixel_origin is None:
-            self._pixel_origin = self.origin + self.pixel_to_world_delta(
-                0.5 * torch.ones_like(self.origin)
-            )
+            self._pixel_origin = self.origin + self.pixel_to_world_delta(0.5*torch.ones_like(self.origin))
         return self._pixel_origin
 
-    def pixel_to_world(self, pixel_coordinate, internal_transpose=False):
+    def pixel_to_world(self, pixel_coordinate, internal_transpose = False):
         """Take in a coordinate on the regular cartesian pixel grid, where
         0,0 is the center of the first pixel. This coordinate is
         transformed into the world coordiante system based on the
         pixel scale and origin position for this image. In the world
         coordinate system the origin is placed with respect to the
         bottom corner of the 0,0 pixel.
 
         """
         if internal_transpose:
-            return (self.pixelscale @ pixel_coordinate).T + self.pixel_origin
-        return (self.pixelscale @ pixel_coordinate) + self.pixel_origin
+            return (self.pixelscale @ (pixel_coordinate)).T + self.pixel_origin
+        return (self.pixelscale @ (pixel_coordinate)) + self.pixel_origin
 
-    def world_to_pixel(self, world_coordinate, unsqueeze_origin=False):
-        if unsqueeze_origin:
-            O = self.pixel_origin.unsqueeze(-1)
-        else:
-            O = self.pixel_origin
-        return self._pixelscale_inv @ (world_coordinate - O)
+    def world_to_pixel(self, world_coordinate):
+        return torch.linalg.solve(self.pixelscale, world_coordinate - self.pixel_origin)
 
     def pixel_to_world_delta(self, pixel_delta):
         """Take in a coordinate on the regular cartesian pixel grid, where
         0,0 is the center of the first pixel. This coordinate is
         transformed into the world coordiante system based on the
         pixel scale and origin position for this image. In the world
         coordinate system the origin is placed with respect to the
         bottom corner of the 0,0 pixel.
 
         """
         return self.pixelscale @ pixel_delta
 
     def world_to_pixel_delta(self, world_delta):
-        return self._pixelscale_inv @ world_delta
-
+        return torch.linalg.solve(self.pixelscale, world_delta)
+    
+    
     @property
     def origin(self) -> torch.Tensor:
         """
         Returns the origin (bottom-left corner) of the image window.
 
         Returns:
             torch.Tensor: A 1D tensor of shape (2,) containing the (x, y) coordinates of the origin.
@@ -246,172 +222,128 @@
 
         """
         self.window.shift_origin(shift)
         self._pixel_origin = None
 
     def pixel_shift_origin(self, shift):
         self.shift_origin(self.pixel_to_world_delta(shift))
-
+        
     def copy(self, **kwargs):
         """Produce a copy of this image with all of the same properties. This
         can be used when one wishes to make temporary modifications to
         an image and then will want the original again.
 
         """
         return self.__class__(
             data_shape=self.data_shape,
             pixelscale=self.pixelscale,
             zeropoint=self.zeropoint,
             note=self.note,
             window=self.window.copy(),
-            identity=self.identity,
+            _identity=self.identity,
             **kwargs,
         )
 
     def get_window(self, window, **kwargs):
         """Get a sub-region of the image as defined by a window on the sky."""
         indices = window.get_indices(self)
         return self.__class__(
-            data_shape=(
-                indices[0].stop - indices[0].start,
-                indices[1].stop - indices[1].start,
-            ),
+            data_shape=(indices[0].stop - indices[0].start, indices[1].stop - indices[1].start),
             pixelscale=self.pixelscale,
             zeropoint=self.zeropoint,
             note=self.note,
             window=self.window & window,
-            identity=self.identity,
+            _identity=self.identity,
             **kwargs,
         )
 
     def to(self, dtype=None, device=None):
         if dtype is None:
             dtype = AP_config.ap_dtype
         if device is None:
             device = AP_config.ap_device
         self.window.to(dtype=dtype, device=device)
         self.pixelscale.to(dtype=dtype, device=device)
         if self.zeropoint is not None:
             self.zeropoint.to(dtype=dtype, device=device)
         return self
 
-    def crop(self, pixels):  # fixme data_shape
+    def crop(self, pixels):# fixme data_shape
         if len(pixels) == 1:  # same crop in all dimension
             self.window -= self.pixel_to_world_delta(
                 torch.as_tensor(
-                    [pixels[0], pixels[0]],
-                    dtype=AP_config.ap_dtype,
-                    device=AP_config.ap_device,
+                    [pixels[0], pixels[0]], dtype=AP_config.ap_dtype, device=AP_config.ap_device
                 )
             )
         elif len(pixels) == 2:  # different crop in each dimension
             self.window -= self.pixel_to_world_delta(
                 torch.as_tensor(
                     pixels, dtype=AP_config.ap_dtype, device=AP_config.ap_device
                 )
             )
         elif len(pixels) == 4:  # different crop on all sides
             pixels = torch.as_tensor(
                 pixels, dtype=AP_config.ap_dtype, device=AP_config.ap_device
             )
             low = self.pixel_to_world_delta(pixels[:2])
             high = self.pixel_to_world_delta(pixels[2:])
-            self.window -= torch.cat((low, high))
+            self.window -= torch.cat((low,high))
         else:
             raise ValueError(f"Unrecognized pixel crop format: {pixels}")
         self._pixel_origin = None
         return self
-
+    
     @torch.no_grad()
     def get_coordinate_meshgrid(self):
         n_pixels = self.data_shape
-        xsteps = torch.arange(
-            n_pixels[1], dtype=AP_config.ap_dtype, device=AP_config.ap_device
-        )
-        ysteps = torch.arange(
-            n_pixels[0], dtype=AP_config.ap_dtype, device=AP_config.ap_device
-        )
+        xsteps = torch.arange(n_pixels[1], dtype=AP_config.ap_dtype, device=AP_config.ap_device)
+        ysteps = torch.arange(n_pixels[0], dtype=AP_config.ap_dtype, device=AP_config.ap_device)
         meshx, meshy = torch.meshgrid(
-            xsteps,
-            ysteps,
+            xsteps, ysteps,
             indexing="xy",
         )
-        Coords = self.pixel_to_world(
-            torch.stack((meshx, meshy)).view(2, -1), internal_transpose=True
-        ).T
+        Coords = self.pixel_to_world(torch.stack((meshx, meshy)).view(2,-1), internal_transpose = True).T
         return Coords.reshape((2, *meshx.shape))
 
     @torch.no_grad()
     def get_coordinate_corner_meshgrid(self):
         n_pixels = self.data_shape
-        xsteps = (
-            torch.arange(
-                n_pixels[1] + 1, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-            )
-            - 0.5
-        )
-        ysteps = (
-            torch.arange(
-                n_pixels[0] + 1, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-            )
-            - 0.5
-        )
+        xsteps = torch.arange(n_pixels[1]+1, dtype=AP_config.ap_dtype, device=AP_config.ap_device) - 0.5
+        ysteps = torch.arange(n_pixels[0]+1, dtype=AP_config.ap_dtype, device=AP_config.ap_device) - 0.5
         meshx, meshy = torch.meshgrid(
-            xsteps,
-            ysteps,
+            xsteps, ysteps,
             indexing="xy",
         )
-        Coords = self.pixel_to_world(
-            torch.stack((meshx, meshy)).view(2, -1), internal_transpose=True
-        ).T
+        Coords = self.pixel_to_world(torch.stack((meshx, meshy)).view(2,-1), internal_transpose = True).T
         return Coords.reshape((2, *meshx.shape))
 
     @torch.no_grad()
     def get_coordinate_simps_meshgrid(self):
         n_pixels = self.data_shape
-        xsteps = (
-            0.5
-            * torch.arange(
-                2 * (n_pixels[1]) + 1,
-                dtype=AP_config.ap_dtype,
-                device=AP_config.ap_device,
-            )
-            - 0.5
-        )
-        ysteps = (
-            0.5
-            * torch.arange(
-                2 * (n_pixels[0]) + 1,
-                dtype=AP_config.ap_dtype,
-                device=AP_config.ap_device,
-            )
-            - 0.5
-        )
+        xsteps = 0.5*torch.arange(2*(n_pixels[1])+1, dtype=AP_config.ap_dtype, device=AP_config.ap_device) - 0.5
+        ysteps = 0.5*torch.arange(2*(n_pixels[0])+1, dtype=AP_config.ap_dtype, device=AP_config.ap_device) - 0.5
         meshx, meshy = torch.meshgrid(
-            xsteps,
-            ysteps,
+            xsteps, ysteps,
             indexing="xy",
         )
-        Coords = self.pixel_to_world(
-            torch.stack((meshx, meshy)).view(2, -1), internal_transpose=True
-        ).T
+        Coords = self.pixel_to_world(torch.stack((meshx, meshy)).view(2,-1), internal_transpose = True).T
         return Coords.reshape((2, *meshx.shape))
 
     def super_resolve(self, scale: int, **kwargs):
         assert isinstance(scale, int) or scale.dtype is torch.int32
         if scale == 1:
             return self
 
         return self.__class__(
             data_shape=self.data_shape,
             pixelscale=self.pixelscale / scale,
             zeropoint=self.zeropoint,
             note=self.note,
             window=self.window.copy(),
-            identity=self.identity,
+            _identity=self.identity,
             **kwargs,
         )
 
     def reduce(self, scale: int, **kwargs):
         """This operation will downsample an image by the factor given. If
         scale = 2 then 2x2 blocks of pixels will be summed together to
         form individual larger pixels. A new image object will be
@@ -430,15 +362,15 @@
 
         return self.__class__(
             data_shape=self.data_shape,
             pixelscale=self.pixelscale * scale,
             zeropoint=self.zeropoint,
             note=self.note,
             window=self.window.copy(),
-            identity=self.identity,
+            _identity=self.identity,
             **kwargs,
         )
 
     def expand(self, padding: Tuple[float]) -> None:
         """
         Args:
           padding tuple[float]: length 4 tuple with amounts to pad each dimension in physical units
@@ -477,25 +409,17 @@
             hdul.writeto(filename, overwrite=overwrite)
         return hdul
 
     def load(self, filename):
         hdul = fits.open(filename)
         for hdu in hdul:
             if "IMAGE" in hdu.header and hdu.header["IMAGE"] == "PRIMARY":
-                self.pixelscale = torch.tensor(
-                    eval(hdu.header.get("PXLSCALE")),
-                    dtype=AP_config.ap_dtype,
-                    device=AP_config.ap_device,
-                )
-                self.zeropoint = torch.tensor(
-                    eval(hdu.header.get("ZEROPNT")),
-                    dtype=AP_config.ap_dtype,
-                    device=AP_config.ap_device,
-                )
+                self.pixelscale = torch.tensor(eval(hdu.header.get("PXLSCALE")), dtype=AP_config.ap_dtype, device=AP_config.ap_device)
+                self.zeropoint = torch.tensor(eval(hdu.header.get("ZEROPNT")), dtype=AP_config.ap_dtype, device=AP_config.ap_device)
                 self.note = hdu.header.get("NOTE")
-                self.window = Window(state=eval(hdu.header.get("WINDOW")))
+                self.window = Window(state = eval(hdu.header.get("WINDOW")))
                 break
         return hdul
 
     def __str__(self):
         state = self.get_state()
         return "\n".join(f"{key}: {state[key]}" for key in state)
```

## autophot/image/image_object.py

```diff
@@ -30,23 +30,23 @@
         origin: The origin of the image in the coordinate system.
     """
 
     def __init__(
         self,
         data: Optional[Union[torch.Tensor]] = None,
         header: Optional[Image_Header] = None,
-        wcs: Optional["astropy.wcs.wcs.WCS"] = None,
+        wcs: Optional['astropy.wcs.wcs.WCS'] = None,
         pixelscale: Optional[Union[float, torch.Tensor]] = None,
         window: Optional[Window] = None,
         filename: Optional[str] = None,
         zeropoint: Optional[Union[float, torch.Tensor]] = None,
         note: Optional[str] = None,
         origin: Optional[Sequence] = None,
         center: Optional[Sequence] = None,
-        identity: str = None,
+        _identity: str = None,
         **kwargs: Any,
     ) -> None:
         """Initialize an instance of the APImage class.
 
         Parameters:
         -----------
         data : numpy.ndarray or None, optional
@@ -81,54 +81,51 @@
                 wcs=wcs,
                 window=window,
                 filename=filename,
                 zeropoint=zeropoint,
                 note=note,
                 origin=origin,
                 center=center,
-                identity=identity,
+                _identity=_identity,
                 **kwargs,
             )
         else:
             self.header = header
 
         if filename is not None:
             self.load(filename)
             return
 
         # set the data
         self.data = data
-        self.to()
 
     @property
     def north(self):
         return self.header.north
 
     @property
     def pixel_area(self):
         return self.header.pixel_area
 
     @property
     def pixel_length(self):
         return self.header.pixel_length
-
-    def pixel_to_world(self, pixel_coordinate, internal_transpose=False):
-        return self.header.pixel_to_world(
-            pixel_coordinate, internal_transpose=internal_transpose
-        )
-
-    def world_to_pixel(self, world_coordinate, unsqueeze_origin=False):
-        return self.header.world_to_pixel(world_coordinate, unsqueeze_origin)
-
+    
+    def pixel_to_world(self, pixel_coordinate, internal_transpose = False):
+        return self.header.pixel_to_world(pixel_coordinate, internal_transpose = internal_transpose)
+
+    def world_to_pixel(self, world_coordinate):
+        return self.header.world_to_pixel(world_coordinate)
+    
     def pixel_to_world_delta(self, pixel_coordinate):
         return self.header.pixel_to_world_delta(pixel_coordinate)
 
     def world_to_pixel_delta(self, world_coordinate):
         return self.header.world_to_pixel_delta(world_coordinate)
-
+    
     @property
     def origin(self) -> torch.Tensor:
         """
         Returns the origin (bottom-left corner) of the image window.
 
         Returns:
             torch.Tensor: A 1D tensor of shape (2,) containing the (x, y) coordinates of the origin.
@@ -284,21 +281,17 @@
         return self
 
     def flatten(self, attribute: str = "data") -> np.ndarray:
         return getattr(self, attribute).reshape(-1)
 
     def get_coordinate_meshgrid(self):
         return self.header.get_coordinate_meshgrid()
-
     def get_coordinate_corner_meshgrid(self):
         return self.header.get_coordinate_corner_meshgrid()
 
-    def get_coordinate_simps_meshgrid(self):
-        return self.header.get_coordinate_simps_meshgrid()
-
     def reduce(self, scale: int, **kwargs):
         """This operation will downsample an image by the factor given. If
         scale = 2 then 2x2 blocks of pixels will be summed together to
         form individual larger pixels. A new image object will be
         returned with the appropriate pixelscale and data tensor. Note
         that the window does not change in this operation since the
         pixels are condensed, but the pixel size is increased
@@ -467,17 +460,15 @@
 
     def index(self, other):
         if isinstance(other, Image) and hasattr(other, "identity"):
             for i, self_image in enumerate(self.image_list):
                 if other.identity == self_image.identity:
                     return i
             else:
-                raise ValueError(
-                    "Could not find identity match between image list and input image"
-                )
+                raise ValueError("Could not find identity match between image list and input image")
         raise NotImplementedError(f"Image_List cannot get index for {type(other)}")
 
     def to(self, dtype=None, device=None):
         if dtype is not None:
             dtype = AP_config.ap_dtype
         if device is not None:
             device = AP_config.ap_device
@@ -485,23 +476,25 @@
             image.to(dtype=dtype, device=device)
         return self
 
     def crop(self, *pixels):
         raise NotImplementedError("Crop function not available for Image_List object")
 
     def get_coordinate_meshgrid(self):
-        return tuple(image.get_coordinate_meshgrid() for image in self.image_list)
-
+        return tuple(
+            image.get_coordinate_meshgrid() for image in self.image_list
+        )
     def get_coordinate_corner_meshgrid(self):
         return tuple(
             image.get_coordinate_corner_meshgrid() for image in self.image_list
         )
-
     def get_coordinate_simps_meshgrid(self):
-        return tuple(image.get_coordinate_simps_meshgrid() for image in self.image_list)
+        return tuple(
+            image.get_coordinate_simps_meshgrid() for image in self.image_list
+        )
 
     def flatten(self, attribute="data"):
         return torch.cat(tuple(image.flatten(attribute) for image in self.image_list))
 
     def reduce(self, scale):
         assert isinstance(scale, int) or scale.dtype is torch.int32
         if scale == 1:
```

## autophot/image/jacobian_image.py

```diff
@@ -73,21 +73,15 @@
             if other_identity in self.parameters:
                 other_loc = self.parameters.index(other_identity)
             else:
                 self.set_data(
                     torch.cat(
                         (
                             self.data,
-                            torch.zeros(
-                                self.data.shape[0],
-                                self.data.shape[1],
-                                1,
-                                dtype=AP_config.ap_dtype,
-                                device=AP_config.ap_device,
-                            ),
+                            torch.zeros(self.data.shape[0], self.data.shape[1], 1),
                         ),
                         dim=2,
                     ),
                     require_shape=False,
                 )
                 self.parameters.append(other_identity)
                 other_loc = -1
```

## autophot/image/model_image.py

```diff
@@ -34,17 +34,15 @@
     def clear_image(self):
         self.data = torch.zeros_like(self.data)
 
     def shift_origin(self, shift, is_prepadded=True):
         self.window.shift_origin(shift)
         pix_shift = self.world_to_pixel_delta(shift)
         if torch.any(torch.abs(pix_shift) > 1):
-            raise NotImplementedError(
-                "Shifts larger than 1 pixel are currently not handled"
-            )
+            raise NotImplementedError("Shifts larger than 1 pixel are currently not handled")
         self.data = shift_Lanczos_torch(
             self.data,
             pix_shift[0],
             pix_shift[1],
             min(min(self.data.shape), 10),
             dtype=AP_config.ap_dtype,
             device=AP_config.ap_device,
@@ -57,15 +55,15 @@
         )
 
     def reduce(self, scale, **kwargs):
         return super().reduce(scale, target_identity=self.target_identity, **kwargs)
 
     def replace(self, other, data=None):
         if isinstance(other, Image):
-            if self.window.overlap_frac(other.window) == 0.0:  # fixme control flow
+            if self.window.overlap_frac(other.window) == 0.: # fixme control flow
                 return
             other_indices = self.window.get_indices(other)
             self_indices = other.window.get_indices(self)
             if (
                 self.data[self_indices].nelement() == 0
                 or other.data[other_indices].nelement() == 0
             ):
```

## autophot/image/psf_image.py

```diff
@@ -18,15 +18,15 @@
     The point spread function characterizes the response of an imaging system to a point source or point object.
 
     The shape of the PSF data must be odd.
 
     Attributes:
         data (torch.Tensor): The image data of the PSF.
         psf_upscale (torch.Tensor): Upscaling factor of the PSF. Default is 1.
-        identity (str): The identity of the image. Default is None.
+        band (str): The band of the image. Default is None.
 
     Methods:
         psf_border_int: Calculates and returns the convolution border size of the PSF image in integer format.
         psf_border: Calculates and returns the convolution border size of the PSF image in the units of pixelscale.
         _save_image_list: Saves the image list to the PSF HDU header.
         reduce: Reduces the size of the image using a given scale factor.
     """
@@ -37,22 +37,26 @@
 
         Args:
             *args: Variable length argument list.
             **kwargs: Arbitrary keyword arguments.
                 psf_upscale (int, optional): Upscaling factor of the PSF. Default is 1.
                 band (str, optional): The band of the image. Default is None.
         """
-        self.psf_upscale = torch.as_tensor(
-            kwargs.get("psf_upscale", 1), dtype=torch.int32, device=AP_config.ap_device
-        )
         super().__init__(*args, **kwargs)
         assert torch.all(
             (torch.tensor(self.data.shape) % 2) == 1
         ), "psf must have odd shape"
 
+        self.psf_upscale = torch.as_tensor(
+            kwargs.get("psf_upscale", 1), dtype=torch.int32, device=AP_config.ap_device
+        )
+
+        # set the band
+        self.band = kwargs.get("band", None)
+
     @property
     def psf_border_int(self):
         """Calculates and returns the border size of the PSF image in integer
         format. This is the border used for padding before convolution.
 
         Returns:
             torch.Tensor: The border size of the PSF image in integer format.
@@ -80,17 +84,15 @@
         units of pixelscale. This is the border used for padding
         before convolution.
 
         Returns:
             torch.Tensor: The border size of the PSF image in the units of pixelscale.
 
         """
-        return self.window.world_to_cartesian(
-            self.pixel_to_world_delta(self.psf_border_int.to(dtype=AP_config.ap_dtype))
-        )
+        return self.window.world_to_cartesian(self.pixel_to_world_delta(self.psf_border_int.to(dtype=AP_config.ap_dtype)))
 
     def _save_image_list(self, image_list, psf_header):
         """Saves the image list to the PSF HDU header.
 
         Args:
             image_list (list): The list of images to be saved.
             psf_header (astropy.io.fits.Header): The header of the PSF HDU.
@@ -110,14 +112,15 @@
             **kwargs: Arbitrary keyword arguments.
 
         Returns:
             PSF_Image: A copy of the current PSF_Image instance.
         """
         return super().copy(
             psf_upscale=self.psf_upscale,
+            band=self.band,
         )
 
     def blank_copy(self, **kwargs):
         """Creates a blank copy of the PSF_Image instance.
 
         This method generates a blank copy of the PSF_Image object while maintaining the 'psf_upscale' and 'band' properties.
 
@@ -125,14 +128,15 @@
             **kwargs: Arbitrary keyword arguments.
 
         Returns:
             PSF_Image: A blank copy of the current PSF_Image instance with the same properties but no data.
         """
         return super().blank_copy(
             psf_upscale=self.psf_upscale,
+            band=self.band,
         )
 
     def get_window(self, **kwargs):
         """Returns the window of the PSF_Image instance.
 
         This method returns the window of the PSF_Image object while maintaining the 'psf_upscale' and 'band' properties.
 
@@ -140,14 +144,15 @@
             **kwargs: Arbitrary keyword arguments.
 
         Returns:
             Window: The window associated with the PSF_Image instance.
         """
         return super().get_window(
             psf_upscale=self.psf_upscale,
+            band=self.band,
         )
 
     def to(self, dtype=None, device=None):
         """Transfers the PSF_Image instance to the specified device and modifies its data type.
 
         This method changes the data type of the PSF_Image object and moves it to a specified device.
         If no device is provided, it defaults to 'AP_config.ap_device'.
@@ -170,11 +175,13 @@
         Args:
             scale (float): The scale factor by which the size of the PSF image needs to be reduced.
             **kwargs: Arbitrary keyword arguments. This can be used to pass additional parameters required by the method.
 
         Returns:
             PSF_Image: A new instance of PSF_Image class with the reduced image size.
         """
-        return super().reduce(scale, psf_upscale=self.psf_upscale / scale, **kwargs)
+        return super().reduce(
+            scale, psf_upscale=self.psf_upscale / scale, band=self.band, **kwargs
+        )
 
     def expand(self, padding):
         raise NotImplementedError("expand not available for PSF_Image")
```

## autophot/image/target_image.py

```diff
@@ -21,23 +21,36 @@
 
     """
 
     image_count = 0
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
+        # set the band
+        self.band = kwargs.get("band", None)
 
         if not self.has_variance:
             self.set_variance(kwargs.get("variance", None))
         if not self.has_mask:
             self.set_mask(kwargs.get("mask", None))
         if not self.has_psf:
             self.set_psf(kwargs.get("psf", None), kwargs.get("psf_upscale", 1))
 
     @property
+    def band(self):
+        if self._band is None:
+            self._band = str(Target_Image.image_count)
+            Target_Image.image_count += 1
+        return self._band
+
+    @band.setter
+    def band(self, val):
+        self._band = val
+
+    @property
     def variance(self):
         if self.has_variance:
             return self._variance
         return torch.ones_like(self.data)
 
     @variance.setter
     def variance(self, variance):
@@ -107,15 +120,15 @@
             self._psf = psf
             return
 
         self._psf = PSF_Image(
             psf,
             psf_upscale=psf_upscale,
             pixelscale=self.pixelscale / psf_upscale,
-            identity=self.identity,
+            band=self.band,
         )
 
     def set_mask(self, mask):
         if mask is None:
             self._mask = None
             return
         assert mask.shape == self.data.shape, "mask must have same shape as data"
```

## autophot/image/window_object.py

```diff
@@ -18,54 +18,38 @@
       shape: the length of the sides of the window in physical units [arcsec]
       projection: A det = 1 matrix which describes the projection of coordinates on the sky. If nothing is given then an identity matrix is assumed. If a wcs object is given then the projection is the pixel scale matrix normalized to determinant of 1. Essentially this matrix described the transformation from a simple cartesian grid onto the sky which may be flipped, rotated or streched. [unitless 2x2 matrix]
       center: Instead of providing the origin, one can provide the center position. This will just be used to update the origin. [arcsec]
       state: A dictionary containing the origin, shape, and orientation information. [dict]
       wcs: An astropy.wcs.wcs.WCS object which gives information about the origin and orientation of the window.
 
     """
-
-    def __init__(
-        self,
-        origin=None,
-        shape=None,
-        projection=None,
-        center=None,
-        state=None,
-        wcs=None,
-    ):
+    
+    def __init__(self, origin=None, shape=None, projection=None, center=None, state=None, wcs=None):
         # If loading from a previous state, simply update values and end init
         if state is not None:
             self.update_state(state)
             return
-
+        
         # Determine projection
         if wcs is not None:
             proj = wcs.pixel_scale_matrix
             proj /= np.abs(np.linalg.det(proj))
-            self.projection = torch.tensor(
-                proj, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-            )
+            self.projection = torch.tensor(proj, dtype=AP_config.ap_dtype, device=AP_config.ap_device)
         elif projection is None:
-            self.projection = torch.eye(
-                2, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-            )
+            self.projection = torch.eye(2, dtype=AP_config.ap_dtype, device=AP_config.ap_device)
         else:
             # ensure it is a tensor
-            projection = torch.as_tensor(
-                projection, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-            )
+            projection = torch.as_tensor(projection, dtype=AP_config.ap_dtype, device=AP_config.ap_device)
             # normalize determinant to area of 1
             self.projection = projection / torch.linalg.det(projection).abs().sqrt()
 
         # Determine origin and shape
         if wcs is not None:
             self.origin = torch.as_tensor(
-                wcs.pixel_to_world(-0.5, -0.5),
-                dtype=AP_config.ap_dtype,
-                device=AP_config.ap_device,
+                wcs.pixel_to_world(-0.5, -0.5), dtype=AP_config.ap_dtype, device=AP_config.ap_device
             )
             self.shape = torch.as_tensor(
                 shape, dtype=AP_config.ap_dtype, device=AP_config.ap_device
             )
         elif center is None and shape is not None and origin is not None:
             self.shape = torch.as_tensor(
                 shape, dtype=AP_config.ap_dtype, device=AP_config.ap_device
@@ -82,26 +66,25 @@
             raise ValueError(
                 "One of center or origin must be provided to create window"
             )
 
     @property
     def end(self):
         return self.cartesian_to_world(self.shape)
-
+    
     @property
     def center(self):
         return self.origin + self.end / 2
 
     @center.setter
     def center(self, c):
-        self.origin = (
-            torch.as_tensor(c, dtype=AP_config.ap_dtype, device=AP_config.ap_device)
-            - self.end / 2
-        )
-
+        self.origin = torch.as_tensor(
+            c, dtype=AP_config.ap_dtype, device=AP_config.ap_device
+        ) - self.end / 2
+        
     # @property
     # def plt_extent(self):
     #     return tuple(
     #         pe.detach().cpu().item()
     #         for pe in (
     #             self.origin[0],
     #             self.origin[0] + self.end[0],
@@ -113,56 +96,42 @@
     def world_to_cartesian(self, world_coordinate):
         """Projects a world coordinate which may be rotated, flipped, or
         sheered into a regular square cartesian grid for the purpose
         of comparisons and where arithmetic is more straightforward.
 
         """
         return torch.linalg.solve(self.projection, world_coordinate)
-
     def cartesian_to_world(self, cartesian_coordinate):
         return self.projection @ cartesian_coordinate
 
     def copy(self):
-        return self.__class__(
-            origin=torch.clone(self.origin),
-            shape=torch.clone(self.shape),
-            projection=torch.clone(self.projection),
-        )
+        return self.__class__(origin=torch.clone(self.origin), shape=torch.clone(self.shape), projection = torch.clone(self.projection))
 
     def to(self, dtype=None, device=None):
         if dtype is None:
             dtype = AP_config.ap_dtype
         if device is None:
             device = AP_config.ap_device
         self.shape = self.shape.to(dtype=dtype, device=device)
         self.origin = self.origin.to(dtype=dtype, device=device)
         self.projection = self.projection.to(dtype=dtype, device=device)
-
+        
     def get_shape(self, pixelscale):
         return (torch.round(torch.linalg.solve(pixelscale, self.end).abs())).int()
 
     def get_shape_flip(self, pixelscale):
         return torch.flip(self.get_shape(pixelscale), (0,))
 
-    @torch.no_grad()
     def _get_indices(self, obj_window, obj_pixelscale):
         """
         Return an index slicing tuple for obj corresponding to this window
         """
-        unclipped_start = torch.round(
-            torch.linalg.solve(obj_pixelscale, (self.origin - obj_window.origin))
-        ).int()
-        unclipped_end = torch.round(
-            torch.linalg.solve(
-                obj_pixelscale, (self.origin + self.end - obj_window.origin)
-            )
-        ).int()
-        clipping_end = torch.round(
-            torch.linalg.solve(obj_pixelscale, obj_window.end)
-        ).int()
+        unclipped_start = torch.round(torch.linalg.solve(obj_pixelscale, (self.origin - obj_window.origin))).int()
+        unclipped_end = torch.round(torch.linalg.solve(obj_pixelscale, (self.origin + self.end - obj_window.origin))).int()
+        clipping_end = torch.round(torch.linalg.solve(obj_pixelscale, obj_window.end)).int()
         return (
             slice(
                 torch.max(
                     torch.tensor(0, dtype=torch.int, device=AP_config.ap_device),
                     unclipped_start[1],
                 ),
                 torch.min(clipping_end[1], unclipped_end[1]),
@@ -220,26 +189,22 @@
         """Add to the size of the window. This operation preserves the window
         center and changes the size (shape) of the window by
         increasing the border.
 
         """
         if isinstance(other, (float, int, torch.dtype)):
             new_shape = self.shape + 2 * other
-            return self.__class__(
-                center=self.center, shape=new_shape, projection=self.projection
-            )
+            return self.__class__(center = self.center, shape = new_shape, projection = self.projection)
         elif isinstance(other, (tuple, torch.Tensor)) and len(other) == len(
             self.origin
         ):
             new_shape = self.shape + 2 * torch.as_tensor(
                 other, dtype=AP_config.ap_dtype, device=AP_config.ap_device
             )
-            return self.__class__(
-                center=self.center, shape=new_shape, projection=self.projection
-            )
+            return self.__class__(center = self.center, shape = new_shape, projection = self.projection)
         raise ValueError(f"Window object cannot be added with {type(other)}")
 
     @torch.no_grad()
     def __iadd__(self, other):
         if isinstance(other, (float, int, torch.dtype)):
             keep_center = self.center.clone()
             self.shape += 2 * other
@@ -253,19 +218,17 @@
                 other, dtype=AP_config.ap_dtype, device=AP_config.ap_device
             )
             self.center = keep_center
             return self
         elif isinstance(other, (tuple, torch.Tensor)) and len(other) == (
             2 * len(self.origin)
         ):
-            self.origin -= self.cartesian_to_world(
-                torch.as_tensor(
-                    other[::2], dtype=AP_config.ap_dtype, device=AP_config.ap_device
-                )
-            )
+            self.origin -= self.cartesian_to_world(torch.as_tensor(
+                other[::2], dtype=AP_config.ap_dtype, device=AP_config.ap_device
+            ))
             self.shape -= torch.as_tensor(
                 torch.sum(other.view(-1, 2), axis=0),
                 dtype=AP_config.ap_dtype,
                 device=AP_config.ap_device,
             )
             return self
         raise ValueError(f"Window object cannot be added with {type(other)}")
@@ -275,26 +238,22 @@
         """Reduce the size of the window. This operation preserves the window
         center and changes the size (shape) of the window by reducing
         the border.
 
         """
         if isinstance(other, (float, int, torch.dtype)):
             new_shape = self.shape - 2 * other
-            return self.__class__(
-                center=self.center, shape=new_shape, projection=self.projection
-            )
+            return self.__class__(center = self.center, shape = new_shape, projection=self.projection)
         elif isinstance(other, (tuple, torch.Tensor)) and len(other) == len(
             self.origin
         ):
             new_shape = self.shape - 2 * torch.as_tensor(
                 other, dtype=AP_config.ap_dtype, device=AP_config.ap_device
             )
-            return self.__class__(
-                center=self.center, shape=new_shape, projection=self.projection
-            )
+            return self.__class__(center = self.center, shape = new_shape, projection=self.projection)
         raise ValueError(f"Window object cannot be added with {type(other)}")
 
     @torch.no_grad()
     def __isub__(self, other):
         if isinstance(other, (float, int, torch.dtype)) or (
             isinstance(other, torch.Tensor) and other.numel() == 1
         ):
@@ -330,26 +289,22 @@
         """Add to the size of the window. This operation preserves the window
         center and changes the size (shape) of the window by
         multiplying the border.
 
         """
         if isinstance(other, (float, int, torch.dtype)):
             new_shape = self.shape * other
-            return self.__class__(
-                center=self.center, shape=new_shape, projection=self.projection
-            )
+            return self.__class__(center = self.center, shape = new_shape, projection=self.projection)
         elif isinstance(other, (tuple, torch.Tensor)) and len(other) == len(
             self.origin
         ):
             new_shape = self.shape * torch.as_tensor(
                 other, dtype=AP_config.ap_dtype, device=AP_config.ap_device
             )
-            return self.__class__(
-                center=self.center, shape=new_shape, projection=self.projection
-            )
+            return self.__class__(center = self.center, shape = new_shape, projection=self.projection)
         raise ValueError(f"Window object cannot be added with {type(other)}")
 
     @torch.no_grad()
     def __imul__(self, other):
         if isinstance(other, (float, int, torch.dtype)):
             keep_center = self.center.clone()
             self.shape *= other
@@ -371,26 +326,22 @@
         """Reduce the size of the window. This operation preserves the window
         center and changes the size (shape) of the window by
         dividing the border.
 
         """
         if isinstance(other, (float, int, torch.dtype)):
             new_shape = self.shape / other
-            return self.__class__(
-                center=self.center, shape=new_shape, projection=self.projection
-            )
+            return self.__class__(center = self.center, shape = new_shape, projection=self.projection)
         elif isinstance(other, (tuple, torch.Tensor)) and len(other) == len(
             self.origin
         ):
             new_shape = self.shape / torch.as_tensor(
                 other, dtype=AP_config.ap_dtype, device=AP_config.ap_device
             )
-            return self.__class__(
-                center=self.center, shape=new_shape, projection=self.projection
-            )
+            return self.__class__(center = self.center, shape = new_shape, projection=self.projection)
         raise ValueError(f"Window object cannot be added with {type(other)}")
 
     @torch.no_grad()
     def __itruediv__(self, other):
         if isinstance(other, (float, int, torch.dtype)):
             keep_center = self.center.clone()
             self.shape /= other
@@ -443,58 +394,50 @@
         )
 
     # Window interaction operators
     @torch.no_grad()
     def __or__(self, other):
         cart_self_origin = self.world_to_cartesian(self.origin)
         cart_other_origin = self.world_to_cartesian(other.origin)
-
+        
         new_origin = torch.minimum(cart_self_origin, cart_other_origin)
         new_end = torch.maximum(
             cart_self_origin + self.shape, cart_other_origin + other.shape
         )
-        return self.__class__(
-            origin=self.cartesian_to_world(new_origin),
-            shape=new_end - new_origin,
-            projection=self.projection,
-        )
+        return self.__class__(origin = self.cartesian_to_world(new_origin), shape = new_end - new_origin, projection = self.projection)
 
     @torch.no_grad()
     def __ior__(self, other):
         cart_self_origin = self.world_to_cartesian(self.origin)
         cart_other_origin = self.world_to_cartesian(other.origin)
-
+        
         new_origin = torch.minimum(cart_self_origin, cart_other_origin)
         new_end = torch.maximum(
             cart_self_origin + self.shape, cart_other_origin + other.shape
         )
         self.origin = self.cartesian_to_world(new_origin)
         self.shape = new_end - new_origin
         return self
 
     @torch.no_grad()
     def __and__(self, other):
         cart_self_origin = self.world_to_cartesian(self.origin)
         cart_other_origin = self.world_to_cartesian(other.origin)
-
+        
         new_origin = torch.maximum(cart_self_origin, cart_other_origin)
         new_end = torch.minimum(
             cart_self_origin + self.shape, cart_other_origin + other.shape
         )
-        return self.__class__(
-            self.cartesian_to_world(new_origin),
-            new_end - new_origin,
-            projection=self.projection,
-        )
+        return self.__class__(self.cartesian_to_world(new_origin), new_end - new_origin, projection = self.projection)
 
     @torch.no_grad()
     def __iand__(self, other):
         cart_self_origin = self.world_to_cartesian(self.origin)
         cart_other_origin = self.world_to_cartesian(other.origin)
-
+        
         new_origin = torch.maximum(cart_self_origin, cart_other_origin)
         new_end = torch.minimum(
             cart_self_origin + self.shape, cart_other_origin + other.shape
         )
         self.origin = self.cartesian_to_world(new_origin)
         self.shape = new_end - new_origin
         return self
@@ -543,26 +486,26 @@
     def update_state(self, state):
         self.window_list = list(Window(state=st) for st in state)
 
     # Window interaction operators
     @torch.no_grad()
     def __or__(self, other):
         new_windows = list((sw | ow) for sw, ow in zip(self, other))
-        return self.__class__(window_list=new_windows)
+        return self.__class__(window_list = new_windows)
 
     @torch.no_grad()
     def __ior__(self, other):
         for sw, ow in zip(self, other):
             sw |= ow
         return self
 
     @torch.no_grad()
     def __and__(self, other):
         new_windows = list((sw & ow) for sw, ow in zip(self, other))
-        return self.__class__(window_list=new_windows)
+        return self.__class__(window_list = new_windows)
 
     @torch.no_grad()
     def __iand__(self, other):
         for sw, ow in zip(self, other):
             sw &= ow
         return self
 
@@ -599,39 +542,39 @@
     # Window adjustment operators
     @torch.no_grad()
     def __add__(self, other):
         try:
             new_windows = list(sw + ow for sw, ow in zip(self, other))
         except TypeError:
             new_windows = list(sw + other for sw in self)
-        return self.__class__(window_list=new_windows)
+        return self.__class__(window_list = new_windows)
 
     @torch.no_grad()
     def __sub__(self, other):
         try:
             new_windows = list(sw - ow for sw, ow in zip(self, other))
         except TypeError:
             new_windows = list(sw - other for sw in self)
-        return self.__class__(window_list=new_windows)
+        return self.__class__(window_list = new_windows)
 
     @torch.no_grad()
     def __mul__(self, other):
         try:
             new_windows = list(sw * ow for sw, ow in zip(self, other))
         except TypeError:
             new_windows = list(sw * other for sw in self)
-        return self.__class__(window_list=new_windows)
+        return self.__class__(window_list = new_windows)
 
     @torch.no_grad()
     def __truediv__(self, other):
         try:
             new_windows = list(sw / ow for sw, ow in zip(self, other))
         except TypeError:
             new_windows = list(sw / other for sw in self)
-        return self.__class__(window_list=new_windows)
+        return self.__class__(window_list = new_windows)
 
     @torch.no_grad()
     def __iadd__(self, other):
         try:
             for sw, ow in zip(self, other):
                 sw += ow
         except TypeError:
@@ -672,10 +615,8 @@
     def __len__(self):
         return len(self.window_list)
 
     def __iter__(self):
         return (win for win in self.window_list)
 
     def __str__(self):
-        return "Window List: \n" + (
-            "\n".join(list(str(window) for window in self)) + "\n"
-        )
+        return "Window List: \n" + ("\n".join(list(str(window) for window in self)) + "\n")
```

## autophot/models/__init__.py

```diff
@@ -1,25 +1,22 @@
 from .core_model import *
 from .model_object import *
 from .parameter_object import *
-from .parameter_group import *
 from .galaxy_model_object import *
 from .ray_model import *
 from .sersic_model import *
 from .group_model_object import *
 from .sky_model_object import *
 from .flatsky_model import *
 from .planesky_model import *
 from .gaussian_model import *
 from .spline_model import *
-from .relspline_model import *
 from .star_model_object import *
 from .psf_model import *
 from .superellipse_model import *
 from .edgeon_model import *
 from .exponential_model import *
 from .foureirellipse_model import *
 from .wedge_model import *
 from .warp_model import *
 from .moffat_model import *
 from .nuker_model import *
-from .zernike_model import *
```

## autophot/models/_model_methods.py

```diff
@@ -1,42 +1,16 @@
-from typing import Optional, Union, Dict, Tuple, Any
-from copy import deepcopy
-
 import numpy as np
 import torch
-
+from typing import Optional, Union, Dict, Tuple, Any
+from copy import deepcopy
 from .parameter_object import Parameter
-from ..utils.decorators import ignore_numpy_warnings, default_internal
-from ..utils.interpolate import (
-    _shift_Lanczos_kernel_torch,
-    simpsons_kernel,
-    curvature_kernel,
-    interp2d,
-)
 from ..image import Model_Image, Target_Image, Window
-from ..utils.operations import (
-    fft_convolve_torch,
-    fft_convolve_multi_torch,
-    grid_integrate,
-)
 from .. import AP_config
 
 
-@default_internal
-def angular_metric(self, X, Y, image=None, parameters=None):
-    return torch.atan2(Y, X)
-
-
-@default_internal
-def radius_metric(self, X, Y, image=None, parameters=None):
-    return torch.sqrt(
-        (X) ** 2 + (Y) ** 2 + self.softening**2
-    )
-
-
 @classmethod
 def build_parameter_specs(cls, user_specs=None):
     parameter_specs = {}
     for base in cls.__bases__:
         try:
             parameter_specs.update(base.build_parameter_specs())
         except AttributeError:
@@ -66,160 +40,7 @@
         # If a parameter object is provided, simply use as-is
         if isinstance(self.parameter_specs[p], Parameter):
             self.parameters.add_parameter(self.parameter_specs[p].to())
         elif isinstance(self.parameter_specs[p], dict):
             self.parameters.add_parameter(Parameter(p, **self.parameter_specs[p]))
         else:
             raise ValueError(f"unrecognized parameter specification for {p}")
-
-
-def _sample_init(self, image, parameters, center):
-    if self.sampling_mode == "midpoint" and max(image.data.shape) >= 100:
-        Coords = image.get_coordinate_meshgrid()
-        X, Y = Coords - center[..., None, None]
-        mid = self.evaluate_model(X=X, Y=Y, image=image, parameters=parameters)
-        kernel = curvature_kernel(AP_config.ap_dtype, AP_config.ap_device)
-        # convolve curvature kernel to numericall compute second derivative
-        curvature = torch.nn.functional.pad(
-            torch.nn.functional.conv2d(
-                mid.view(1, 1, *mid.shape),
-                kernel.view(1, 1, *kernel.shape),
-                padding="valid",
-            ),
-            (1, 1, 1, 1),
-            mode="replicate",
-        ).squeeze()
-        return mid + curvature, mid
-    elif self.sampling_mode == "trapezoid" and max(image.data.shape) >= 100:
-        Coords = image.get_coordinate_corner_meshgrid()
-        X, Y = Coords - center[..., None, None]
-        dens = self.evaluate_model(X=X, Y=Y, image=image, parameters=parameters)
-        kernel = (
-            torch.ones(
-                (1, 1, 2, 2), dtype=AP_config.ap_dtype, device=AP_config.ap_device
-            )
-            / 4.0
-        )
-        trapz = torch.nn.functional.conv2d(
-            dens.view(1, 1, *dens.shape), kernel, padding="valid"
-        )
-        trapz = trapz.squeeze()
-        kernel = curvature_kernel(AP_config.ap_dtype, AP_config.ap_device)
-        curvature = torch.nn.functional.pad(
-            torch.nn.functional.conv2d(
-                trapz.view(1, 1, *trapz.shape),
-                kernel.view(1, 1, *kernel.shape),
-                padding="valid",
-            ),
-            (1, 1, 1, 1),
-            mode="replicate",
-        ).squeeze()
-        return trapz + curvature, trapz
-
-    Coords = image.get_coordinate_simps_meshgrid()
-    X, Y = Coords - center[..., None, None]
-    dens = self.evaluate_model(X=X, Y=Y, image=image, parameters=parameters)
-    kernel = simpsons_kernel(dtype=AP_config.ap_dtype, device=AP_config.ap_device)
-    # midpoint is just every other sample in the simpsons grid
-    mid = dens[1::2, 1::2]
-    simps = torch.nn.functional.conv2d(
-        dens.view(1, 1, *dens.shape), kernel, stride=2, padding="valid"
-    )
-    return mid.squeeze(), simps.squeeze()
-
-
-def _integrate_reference(self, image_data, image_header, parameters):
-    return torch.sum(image_data) / image_data.numel()
-
-
-def _sample_integrate(self, deep, reference, image, parameters, center):
-    if self.integrate_mode == "none":
-        pass
-    elif self.integrate_mode == "threshold":
-        Coords = image.get_coordinate_meshgrid()
-        X, Y = Coords - center[..., None, None]
-        ref = self._integrate_reference(
-            deep, image.header, parameters
-        )  # fixme, error can be over 100% on initial sampling reference is invalid
-        error = torch.abs((deep - reference))
-        select = error > (self.sampling_tolerance * ref)
-        intdeep = grid_integrate(
-            X=X[select],
-            Y=Y[select],
-            image_header=image.header,
-            eval_brightness=self.evaluate_model,
-            eval_parameters=parameters,
-            dtype=AP_config.ap_dtype,
-            device=AP_config.ap_device,
-            quad_level=self.integrate_quad_level,
-            gridding=self.integrate_gridding,
-            max_depth=self.integrate_max_depth,
-            reference=self.sampling_tolerance * ref,
-        )
-        deep[select] = intdeep
-    else:
-        raise ValueError(
-            f"{self.name} has unknown integration mode: {self.integrate_mode}"
-        )
-    return deep
-
-
-def _sample_convolve(self, image, shift, psf, shift_method="bilinear"):
-    """
-    image: Image object with image.data pixel matrix
-    shift: the amount of shifting to do in pixel units
-    psf: a PSF_Image object
-    """
-    if shift is not None:
-        if shift_method == "bilinear":
-            psf_data = torch.nn.functional.pad(psf.data, (1, 1, 1, 1))
-            X, Y = torch.meshgrid(
-                torch.arange(
-                    psf_data.shape[1],
-                    dtype=AP_config.ap_dtype,
-                    device=AP_config.ap_device,
-                )
-                - shift[0],
-                torch.arange(
-                    psf_data.shape[0],
-                    dtype=AP_config.ap_dtype,
-                    device=AP_config.ap_device,
-                )
-                - shift[1],
-                indexing="xy",
-            )
-            shift_psf = interp2d(psf_data, X.clone(), Y.clone())
-        elif "lanczos" in shift_method:
-            lanczos_order = int(shift_method[shift_method.find(":") + 1 :])
-            psf_data = torch.nn.functional.pad(
-                psf.data, (lanczos_order, lanczos_order, lanczos_order, lanczos_order)
-            )
-            LL = _shift_Lanczos_kernel_torch(
-                -shift[0],
-                -shift[1],
-                lanczos_order,
-                AP_config.ap_dtype,
-                AP_config.ap_device,
-            )
-            shift_psf = torch.nn.functional.conv2d(
-                psf_data.view(1, 1, *psf_data.shape),
-                LL.view(1, 1, *LL.shape),
-                padding="same",
-            ).squeeze()
-        else:
-            raise ValueError(f"unrecognized subpixel shift method: {shift_method}")
-    else:
-        shift_psf = psf.data
-    shift_psf = shift_psf / torch.sum(shift_psf)
-    if self.psf_convolve_mode == "fft":
-        image.data = fft_convolve_torch(image.data, shift_psf, img_prepadded=True)
-    elif self.psf_convolve_mode == "direct":
-        image.data = torch.nn.functional.conv2d(
-            image.data.view(1, 1, *image.data.shape),
-            torch.flip(
-                shift_psf.view(1, 1, *shift_psf.shape),
-                dims=(2, 3),
-            ),
-            padding="same",
-        ).squeeze()
-    else:
-        raise ValueError(f"unrecognized psf_convolve_mode: {self.psf_convolve_mode}")
```

## autophot/models/_shared_methods.py

```diff
@@ -21,23 +21,18 @@
     moffat_np,
     nuker_torch,
     nuker_np,
 )
 from ..utils.decorators import ignore_numpy_warnings, default_internal
 from ..utils.conversions.coordinates import Rotate_Cartesian
 from ..utils.conversions.functions import sersic_I0_to_flux_np, sersic_flux_to_I0_torch
-from ..image import (
-    Image_List,
-    Target_Image,
-    Model_Image_List,
-    Target_Image_List,
-    Window_List,
-)
+from ..image import Image_List, Target_Image, Model_Image_List, Target_Image_List, Window_List
 from .. import AP_config
 
+
 # Target Selector Decorator
 ######################################################################
 def select_target(func):
     @functools.wraps(func)
     def targeted(self, target=None, **kwargs):
         if target is None:
             send_target = self.target
@@ -101,28 +96,28 @@
             target_area.data.detach().cpu().numpy()[-1, :],
         )
     )
     edge_average = np.median(edge)
     edge_scatter = iqr(edge, rng=(16, 84)) / 2
     # Convert center coordinates to target area array indices
     icenter = target_area.world_to_pixel(parameters["center"].value)
-
+    
     # Collect isophotes for 1D fit
     iso_info = isophotes(
         target_area.data.detach().cpu().numpy() - edge_average,
         (icenter[1].item(), icenter[0].item()),
         threshold=3 * edge_scatter,
-        pa=(parameters["PA"].value - target.north).detach().cpu().item()
-        if "PA" in parameters
-        else 0.0,
+        pa=(parameters["PA"].value - target.north).detach().cpu().item() if "PA" in parameters else 0.0,
         q=parameters["q"].value.detach().cpu().item() if "q" in parameters else 1.0,
         n_isophotes=15,
     )
     R = np.array(list(iso["R"] for iso in iso_info)) * target.pixel_length.item()
-    flux = np.array(list(iso["flux"] for iso in iso_info)) / target.pixel_area.item()
+    flux = (
+        np.array(list(iso["flux"] for iso in iso_info)) / target.pixel_area.item()
+    )
     # Correct the flux if values are negative, so fit can be done in log space
     if np.sum(flux < 0) > 0:
         AP_config.ap_logger.debug("fixing flux")
         flux -= np.min(flux) - np.abs(np.min(flux) * 0.1)
     flux = np.log10(flux)
 
     x0 = list(x0_func(model, R, flux))
@@ -133,18 +128,14 @@
 
     def optim(x, r, f):
         residual = (f - np.log10(prof_func(r, *x))) ** 2
         N = np.argsort(residual)
         return np.mean(residual[:-3])
 
     res = minimize(optim, x0=x0, args=(R, flux), method="Nelder-Mead")
-    if not res.success and AP_config.ap_verbose >= 2:
-        AP_config.ap_logger.warn(
-            f"initialization fit not successful for {model.name}, falling back to defaults"
-        )
 
     if force_uncertainty is None:
         reses = []
         for i in range(10):
             N = np.random.randint(0, len(R), len(R))
             reses.append(
                 minimize(optim, x0=x0, args=(R[N], flux[N]), method="Nelder-Mead")
@@ -179,31 +170,31 @@
 ):
     if all(list(model[param].value is not None for param in params)):
         return
     # Get the sub-image area corresponding to the model image
     target_area = target[model.window]
     edge = np.concatenate(
         (
-            target_area.data[:, 0].detach().cpu().numpy(),
-            target_area.data[:, -1].detach().cpu().numpy(),
-            target_area.data[0, :].detach().cpu().numpy(),
-            target_area.data[-1, :].detach().cpu().numpy(),
+            target_area.data[:, 0],
+            target_area.data[:, -1],
+            target_area.data[0, :],
+            target_area.data[-1, :],
         )
     )
     edge_average = np.median(edge)
     edge_scatter = iqr(edge, rng=(16, 84)) / 2
     # Convert center coordinates to target area array indices
     icenter = target_area.world_to_pixel(model["center"].value)
-
+    
     iso_info = isophotes(
         target_area.data.detach().cpu().numpy() - edge_average,
         (icenter[1].item(), icenter[0].item()),
         threshold=3 * edge_scatter,
-        pa=(model["PA"].value - target.north).item() if "PA" in model else 0.0,
-        q=model["q"].value.item() if "q" in model else 1.0,
+        pa=(model["PA"].value - target.north).detach().cpu().item() if "PA" in model else 0.0,
+        q=model["q"].value.detach().cpu().item() if "q" in model else 1.0,
         n_isophotes=15,
         more=True,
     )
     R = np.array(list(iso["R"] for iso in iso_info)) * target.pixel_length.item()
     was_none = list(False for i in range(len(params)))
     for i, p in enumerate(params):
         if model[p].value is None:
@@ -211,30 +202,28 @@
             model[p].set_value(np.zeros(segments), override_locked=True)
             model[p].set_uncertainty(np.zeros(segments), override_locked=True)
     for r in range(segments):
         flux = []
         for iso in iso_info:
             modangles = (
                 iso["angles"]
-                - (
-                    (model["PA"].value - target.north).detach().cpu().item()
-                    + r * np.pi / segments
-                )
+                - ((model["PA"].value - target.north).detach().cpu().item() + r * np.pi / segments)
             ) % np.pi
             flux.append(
                 np.median(
                     iso["isovals"][
                         np.logical_or(
                             modangles < (0.5 * np.pi / segments),
                             modangles >= (np.pi * (1 - 0.5 / segments)),
                         )
                     ]
                 )
+                / target.pixel_area.item()
             )
-        flux = np.array(flux) / target.pixel_area.item()
+        flux = np.array(flux)
         if np.sum(flux < 0) >= 1:
             flux -= np.min(flux) - np.abs(np.min(flux) * 0.1)
         flux = np.log10(flux)
 
         x0 = list(x0_func(model, R, flux))
         for i, param in enumerate(params):
             x0[i] = (
@@ -278,114 +267,114 @@
 # Exponential
 ######################################################################
 @default_internal
 def exponential_radial_model(self, R, image=None, parameters=None):
     return exponential_torch(
         R,
         parameters["Re"].value,
-        image.pixel_area * 10 ** parameters["Ie"].value,
+        (10 ** parameters["Ie"].value) * image.pixel_area,
     )
 
 
 @default_internal
 def exponential_iradial_model(self, i, R, image=None, parameters=None):
     return exponential_torch(
         R,
         parameters["Re"].value[i],
-        image.pixel_area * 10 ** parameters["Ie"].value[i],
+        (10 ** parameters["Ie"].value[i]) * image.pixel_area,
     )
 
 
 # Sersic
 ######################################################################
 @default_internal
 def sersic_radial_model(self, R, image=None, parameters=None):
     return sersic_torch(
         R,
         parameters["n"].value,
         parameters["Re"].value,
-        image.pixel_area * 10 ** parameters["Ie"].value,
+        (10 ** parameters["Ie"].value) * image.pixel_area,
     )
 
 
 @default_internal
 def sersic_iradial_model(self, i, R, image=None, parameters=None):
     return sersic_torch(
         R,
         parameters["n"].value[i],
         parameters["Re"].value[i],
-        image.pixel_area * 10 ** parameters["Ie"].value[i],
+        (10 ** parameters["Ie"].value[i]) * image.pixel_area,
     )
 
 
 # Moffat
 ######################################################################
 @default_internal
 def moffat_radial_model(self, R, image=None, parameters=None):
     return moffat_torch(
         R,
         parameters["n"].value,
         parameters["Rd"].value,
-        image.pixel_area * 10 ** parameters["I0"].value,
+        (10 ** parameters["I0"].value) * image.pixel_area,
     )
 
 
 @default_internal
 def moffat_iradial_model(self, i, R, image=None, parameters=None):
     return moffat_torch(
         R,
         parameters["n"].value[i],
         parameters["Rd"].value[i],
-        image.pixel_area * 10 ** parameters["I0"].value[i],
+        (10 ** parameters["I0"].value[i]) * image.pixel_area,
     )
 
 
 # Nuker Profile
 ######################################################################
 @default_internal
 def nuker_radial_model(self, R, image=None, parameters=None):
     return nuker_torch(
         R,
         parameters["Rb"].value,
-        image.pixel_area * 10 ** parameters["Ib"].value,
+        (10 ** parameters["Ib"].value) * image.pixel_area,
         parameters["alpha"].value,
         parameters["beta"].value,
         parameters["gamma"].value,
     )
 
 
 @default_internal
 def nuker_iradial_model(self, i, R, image=None, parameters=None):
     return nuker_torch(
         R,
         parameters["Rb"].value[i],
-        image.pixel_area * 10 ** parameters["Ib"].value[i],
+        (10 ** parameters["Ib"].value[i]) * image.pixel_area,
         parameters["alpha"].value[i],
         parameters["beta"].value[i],
         parameters["gamma"].value[i],
     )
 
 
 # Gaussian
 ######################################################################
 @default_internal
 def gaussian_radial_model(self, R, image=None, parameters=None):
     return gaussian_torch(
         R,
         parameters["sigma"].value,
-        image.pixel_area * 10 ** parameters["flux"].value,
+        (10 ** parameters["flux"].value) * image.pixel_area,
     )
 
 
 @default_internal
 def gaussian_iradial_model(self, i, R, image=None, parameters=None):
     return gaussian_torch(
         R,
         parameters["sigma"].value[i],
-        image.pixel_area * 10 ** parameters["flux"].value[i],
+        (10 ** parameters["flux"].value[i]) * image.pixel_area,
     )
 
 
 # Spline
 ######################################################################
 @torch.no_grad()
 @ignore_numpy_warnings
@@ -398,33 +387,34 @@
         return
 
     # Create the I(R) profile radii if needed
     if parameters["I(R)"].prof is None:
         new_prof = [0, 2 * target.pixel_length]
         while new_prof[-1] < torch.max(self.window.shape / 2):
             new_prof.append(
-                new_prof[-1] + torch.max(2 * target.pixel_length, new_prof[-1] * 0.2)
+                new_prof[-1] + torch.max(2 * target.pixel_area, new_prof[-1] * 0.2)
             )
         new_prof.pop()
         new_prof.pop()
         new_prof.append(torch.sqrt(torch.sum((self.window.shape / 2) ** 2)))
         parameters["I(R)"].set_profile(new_prof)
 
     profR = parameters["I(R)"].prof.detach().cpu().numpy()
     target_area = target[self.window]
     Coords = target_area.get_coordinate_meshgrid()
-    X, Y = Coords - parameters["center"].value[..., None, None]
+    X, Y = Coords - parameters["center"].value[...,None, None]
     X, Y = self.transform_coordinates(X, Y, target, parameters)
     R = self.radius_metric(X, Y, target, parameters).detach().cpu().numpy()
     rad_bins = [profR[0]] + list((profR[:-1] + profR[1:]) / 2) + [profR[-1] * 100]
     raveldat = target_area.data.detach().cpu().numpy().ravel()
-
+    
     I = (
         binned_statistic(R.ravel(), raveldat, statistic="median", bins=rad_bins)[0]
-    ) / target.pixel_area.item()
+        / target_area.pixel_area.item()
+    )
     N = np.isfinite(I)
     if not np.all(N):
         I[np.logical_not(N)] = np.interp(profR[np.logical_not(N)], profR[N], I[N])
     if I[-1] >= I[-2]:
         I[-1] = I[-2] / 2
     S = binned_statistic(
         R.ravel(), raveldat, statistic=lambda d: iqr(d, rng=[16, 84]) / 2, bins=rad_bins
@@ -467,15 +457,15 @@
     )
     parameters["I(R)"].set_uncertainty(
         np.zeros((segments, len(parameters["I(R)"].prof))), override_locked=True
     )
     profR = parameters["I(R)"].prof.detach().cpu().numpy()
     target_area = target[self.window]
     Coords = target_area.get_coordinate_meshgrid()
-    X, Y = Coords - parameters["center"].value[..., None, None]
+    X, Y = Coords - parameters["center"].value[...,None, None]
     X, Y = self.transform_coordinates(X, Y, target, parameters)
     R = self.radius_metric(X, Y, target, parameters).detach().cpu().numpy()
     T = self.angular_metric(X, Y, target, parameters).detach().cpu().numpy()
     rad_bins = [profR[0]] + list((profR[:-1] + profR[1:]) / 2) + [profR[-1] * 100]
     raveldat = target_area.data.detach().cpu().numpy().ravel()
     for s in range(segments):
         if segments % 2 == 0 and symmetric:
@@ -507,15 +497,16 @@
                 angles < (2 * np.pi / segments), angles >= (np.pi * (2 - 1 / segments))
             )
         TCHOOSE = TCHOOSE.ravel()
         I = (
             binned_statistic(
                 R.ravel()[TCHOOSE], raveldat[TCHOOSE], statistic="median", bins=rad_bins
             )[0]
-        ) / target.pixel_area.item()
+            / target_area.pixel_area.item()
+        )
         N = np.isfinite(I)
         if not np.all(N):
             I[np.logical_not(N)] = np.interp(profR[np.logical_not(N)], profR[N], I[N])
         S = binned_statistic(
             R.ravel(),
             raveldat,
             statistic=lambda d: iqr(d, rng=[16, 84]) / 2,
@@ -528,100 +519,25 @@
         parameters["I(R)"].set_uncertainty(
             S / (np.abs(I) * np.log(10)), override_locked=True, index=s
         )
 
 
 @default_internal
 def spline_radial_model(self, R, image=None, parameters=None):
-    return (
-        spline_torch(
-            R,
-            parameters["I(R)"].prof,
-            parameters["I(R)"].value,
-            extend=self.extend_profile,
-        )
-        * image.pixel_area
+    return spline_torch(
+        R,
+        parameters["I(R)"].prof,
+        parameters["I(R)"].value,
+        image.pixel_area,
+        extend=self.extend_profile,
     )
 
 
 @default_internal
 def spline_iradial_model(self, i, R, image=None, parameters=None):
-    return (
-        spline_torch(
-            R,
-            parameters["I(R)"].prof,
-            parameters["I(R)"].value[i],
-            extend=self.extend_profile,
-        )
-        * image.pixel_area
-    )
-
-# RelSpline
-######################################################################
-@torch.no_grad()
-@ignore_numpy_warnings
-@select_target
-@default_internal
-def relspline_initialize(self, target=None, parameters=None, **kwargs):
-    super(self.__class__, self).initialize(target=target, parameters=parameters)
-
-    target_area = target[self.window]
-    if parameters["I0"].value is None:
-        center = target_area.world_to_pixel(parameters["center"].value)
-        flux = target_area.data[center[1].int().item(), center[0].int().item()]
-        parameters["I0"].set_value(torch.log10(torch.abs(flux) / target_area.pixel_area), override_locked = True)
-        parameters["I0"].set_uncertainty(0.01, override_locked = True)
-        
-    if parameters["dI(R)"].value is not None and parameters["dI(R)"].prof is not None:
-        return
-
-    # Create the I(R) profile radii if needed
-    if parameters["dI(R)"].prof is None:
-        new_prof = [2 * target.pixel_length]
-        while new_prof[-1] < torch.max(self.window.shape / 2):
-            new_prof.append(
-                new_prof[-1] + torch.max(2 * target.pixel_length, new_prof[-1] * 0.2)
-            )
-        new_prof.pop()
-        new_prof.pop()
-        new_prof.append(torch.sqrt(torch.sum((self.window.shape / 2) ** 2)))
-        parameters["dI(R)"].set_profile(new_prof)
-
-    profR = parameters["dI(R)"].prof.detach().cpu().numpy()
-        
-    Coords = target_area.get_coordinate_meshgrid()
-    X, Y = Coords - parameters["center"].value[..., None, None]
-    X, Y = self.transform_coordinates(X, Y, target, parameters)
-    R = self.radius_metric(X, Y, target, parameters).detach().cpu().numpy()
-    rad_bins = [profR[0]] + list((profR[:-1] + profR[1:]) / 2) + [profR[-1] * 100]
-    raveldat = target_area.data.detach().cpu().numpy().ravel()
-
-    I = (
-        binned_statistic(R.ravel(), raveldat, statistic="median", bins=rad_bins)[0]
-    ) / target.pixel_area.item()
-    N = np.isfinite(I)
-    if not np.all(N):
-        I[np.logical_not(N)] = np.interp(profR[np.logical_not(N)], profR[N], I[N])
-    if I[-1] >= I[-2]:
-        I[-1] = I[-2] / 2
-    S = binned_statistic(
-        R.ravel(), raveldat, statistic=lambda d: iqr(d, rng=[16, 84]) / 2, bins=rad_bins
-    )[0]
-    N = np.isfinite(S)
-    if not np.all(N):
-        S[np.logical_not(N)] = np.interp(profR[np.logical_not(N)], profR[N], S[N])
-    parameters["dI(R)"].set_value(np.log10(np.abs(I)) - parameters["I0"].value.item(), override_locked=True)
-    parameters["dI(R)"].set_uncertainty(
-        S / (np.abs(I) * np.log(10)), override_locked=True
-    )
-
-@default_internal
-def relspline_radial_model(self, R, image=None, parameters=None):
-    return (
-        spline_torch(
-            R,
-            torch.cat((torch.zeros_like(parameters["I0"].value).unsqueeze(-1),parameters["dI(R)"].prof)),
-            torch.cat((parameters["I0"].value.unsqueeze(-1), parameters["I0"].value + parameters["dI(R)"].value)),
-            extend=self.extend_profile,
-        )
-        * image.pixel_area
+    return spline_torch(
+        R,
+        parameters["I(R)"].prof,
+        parameters["I(R)"].value[i],
+        image.pixel_area,
+        extend=self.extend_profile,
     )
```

## autophot/models/core_model.py

```diff
@@ -9,15 +9,15 @@
 import matplotlib.pyplot as plt
 import yaml
 
 from ..utils.conversions.optimization import cyclic_difference_np
 from ..utils.conversions.dict_to_hdf5 import dict_to_hdf5
 from ..utils.optimization import reduced_chi_squared
 from ..utils.decorators import ignore_numpy_warnings, default_internal
-from ..image import Model_Image, Window, Target_Image, Target_Image_List
+from ..image import Model_Image, Window, Target_Image
 from .parameter_group import Parameter_Group
 from ._shared_methods import select_target, select_sample
 from .. import AP_config
 
 __all__ = ["AutoPhot_Model"]
 
 
@@ -73,34 +73,36 @@
     def __init__(self, name, *args, target=None, window=None, locked=False, **kwargs):
         assert (
             ":" not in name and "|" not in name
         ), "characters '|' and ':' are reserved for internal model operations please do not include these in a model name"
         self.name = name
         AP_config.ap_logger.debug("Creating model named: {self.name}")
         self.constraints = kwargs.get("constraints", None)
+        self.equality_constraints = []
         self.parameters = Parameter_Group(self.name)
+        self.requires_grad = kwargs.get("requires_grad", False)
         self.target = target
         self.window = window
         self._locked = locked
         self.mask = kwargs.get("mask", None)
 
     def add_equality_constraint(self, model, parameter):
         if isinstance(parameter, (tuple, list)):
             for P in parameter:
                 self.add_equality_constraint(model, P)
             return
-        if AP_config.ap_verbose >= 2:
-            AP_config.ap_logger.info(
-                f"adding equality constraint between {self.name} and {model.name} for parameter: {parameter}"
-            )
         del_param = self.parameters.get_name(parameter)
-        old_groups = del_param.groups
         use_param = model.parameters.get_name(parameter)
+        old_groups = del_param.groups
         for group in old_groups:
-            group.replace(del_param, use_param)
+            group.pop_id(del_param.identity)
+            group.add_parameter(use_param)
+
+        self.equality_constraints.append(parameter)
+        model.equality_constraints.append(parameter)
 
     @torch.no_grad()
     @ignore_numpy_warnings
     @select_target
     @default_internal
     def initialize(self, target=None, parameters=None, **kwargs):
         """When this function finishes, all parameters should have numerical
@@ -135,25 +137,18 @@
                 parameters, as_representation, parameters_identity
             )
 
         model = self.sample()
         data = self.target[self.window]
         variance = data.variance
         if self.target.has_mask:
-            if isinstance(data, Target_Image_List):
-                mask = tuple(torch.logical_not(submask) for submask in data.mask)
-                chi2 = sum(torch.sum(((mo - da).data ** 2 / va)[ma]) / 2.0 for mo, da, va, ma in zip(model, data, variance, mask))
-            else:
-                mask = torch.logical_not(data.mask)
-                chi2 = torch.sum(((model - data).data ** 2 / variance)[mask]) / 2.0
+            mask = torch.logical_not(data.mask)
+            chi2 = torch.sum(((model - data).data ** 2 / variance)[mask]) / 2.0
         else:
-            if isinstance(data, Target_Image_List):
-                chi2 = sum(torch.sum(((mo - da).data ** 2 / va)) / 2.0 for mo, da, va in zip(model, data, variance))
-            else:
-                chi2 = torch.sum(((model - data).data ** 2 / variance)) / 2.0
+            chi2 = torch.sum(((model - data).data ** 2 / variance)) / 2.0
 
         return chi2
 
     def jacobian(
         self,
         parameters=None,
         as_representation=False,
@@ -180,31 +175,25 @@
             return
 
         # If the window is given in proper format, simply use as-is
         if isinstance(window, Window):
             self._window = window
         elif len(window) == 2:
             self._window = Window(
-                origin=self.target.pixel_to_world(
-                    torch.tensor(
-                        (window[0][0] - 0.5, window[1][0] - 0.5),
-                        dtype=AP_config.ap_dtype,
-                        device=AP_config.ap_device,
-                    )
-                ),
-                shape=self.target.window.world_to_cartesian(
-                    self.target.pixel_to_world_delta(
-                        torch.tensor(
-                            (window[0][1] - window[0][0], window[1][1] - window[1][0]),
-                            dtype=AP_config.ap_dtype,
-                            device=AP_config.ap_device,
-                        )
-                    )
-                ),
-                projection=self.target.pixelscale,
+                origin=self.target.pixel_to_world(torch.tensor(
+                    (window[0][0]-0.5, window[1][0]-0.5),
+                    dtype=AP_config.ap_dtype,
+                    device=AP_config.ap_device,
+                )),
+                shape=self.target.window.world_to_cartesian(self.target.pixel_to_world_delta(torch.tensor(
+                    (window[0][1] - window[0][0], window[1][1] - window[1][0]),
+                    dtype=AP_config.ap_dtype,
+                    device=AP_config.ap_device,
+                ))),
+                projection = self.target.pixelscale,
             )
         elif len(window) == 4:
             origin = torch.tensor(
                 (window[0], window[1]),
                 dtype=AP_config.ap_dtype,
                 device=AP_config.ap_device,
             )
@@ -365,8 +354,9 @@
         elif isinstance(parameters, torch.Tensor):
             self.parameters.set_values(
                 parameters,
                 as_representation=as_representation,
                 parameters_identity=parameters_identity,
             )
             parameters = self.parameters
+
         return self.sample(image=image, window=window, parameters=parameters, **kwargs)
```

## autophot/models/edgeon_model.py

```diff
@@ -46,46 +46,38 @@
     ):
         super().initialize(target=target, parameters=parameters)
         if parameters["PA"].value is not None:
             return
         target_area = target[self.window]
         edge = np.concatenate(
             (
-                target_area.data[:, 0].detach().cpu().numpy(),
-                target_area.data[:, -1].detach().cpu().numpy(),
-                target_area.data[0, :].detach().cpu().numpy(),
-                target_area.data[-1, :].detach().cpu().numpy(),
+                target_area.data[:, 0],
+                target_area.data[:, -1],
+                target_area.data[0, :],
+                target_area.data[-1, :],
             )
         )
         edge_average = np.median(edge)
         edge_scatter = iqr(edge, rng=(16, 84)) / 2
         icenter = target_area.world_to_pixel(parameters["center"].value)
-
+        
         iso_info = isophotes(
             target_area.data.detach().cpu().numpy() - edge_average,
             (icenter[1].detach().cpu().item(), icenter[0].detach().cpu().item()),
             threshold=3 * edge_scatter,
             pa=0.0,
             q=1.0,
             n_isophotes=15,
         )
         parameters["PA"].set_value(
             (
-                -(
-                    (
-                        Angle_Average(
-                            list(
-                                iso["phase2"]
-                                for iso in iso_info[-int(len(iso_info) / 3) :]
-                            )
-                        )
-                        / 2
-                    )
-                    + target.north
+                -((Angle_Average(
+                    list(iso["phase2"] for iso in iso_info[-int(len(iso_info) / 3) :])
                 )
+                   / 2) + target.north)
             )
             % np.pi,
             override_locked=True,
         )
 
     @default_internal
     def transform_coordinates(self, X, Y, image=None, parameters=None):
@@ -98,15 +90,15 @@
         Y=None,
         image: "Image" = None,
         parameters: "Parameter_Group" = None,
         **kwargs,
     ):
         if X is None:
             Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[..., None, None]
+            X, Y = Coords - parameters["center"].value[...,None, None]
         XX, YY = self.transform_coordinates(X, Y, image=image, parameters=parameters)
 
         return self.brightness_model(
             torch.abs(XX), torch.abs(YY), image=image, parameters=parameters
         )
 
 
@@ -134,25 +126,25 @@
         super().initialize(target=target, parameters=parameters)
         if (parameters["I0"].value is not None) and (
             parameters["hs"].value is not None
         ):
             return
         target_area = target[self.window]
         icenter = target_area.world_to_pixel(parameters["center"].value)
-
+        
         if parameters["I0"].value is None:
             parameters["I0"].set_value(
                 torch.log10(
                     torch.mean(
                         target_area.data[
                             int(icenter[0]) - 2 : int(icenter[0]) + 2,
                             int(icenter[1]) - 2 : int(icenter[1]) + 2,
                         ]
                     )
-                    / target.pixel_area.item()
+                    / target.pixel_area
                 ),
                 override_locked=True,
             )
             parameters["I0"].set_uncertainty(
                 torch.std(
                     target_area.data[
                         int(icenter[0]) - 2 : int(icenter[0]) + 2,
@@ -167,17 +159,18 @@
                 torch.max(self.window.shape) * 0.1, override_locked=True
             )
             parameters["hs"].set_value(parameters["hs"].value / 2, override_locked=True)
 
     @default_internal
     def brightness_model(self, X, Y, image=None, parameters=None):
         return (
-            (image.pixel_area * 10 ** parameters["I0"].value)
+            (image.pixel_area)
+            * (10 ** parameters["I0"].value)
             * self.radial_model(X, image=image, parameters=parameters)
-            / (torch.cosh((Y + self.softening) / parameters["hs"].value) ** 2)
+            / (torch.cosh(Y / parameters["hs"].value) ** 2)
         )
 
 
 class Edgeon_Isothermal(Edgeon_Sech):
     """A self-gravitating locally-isothermal edgeon disk. This comes from
     van der Kruit & Searle 1981.
 
@@ -203,13 +196,13 @@
         parameters["rs"].set_value(
             torch.max(self.window.shape) * 0.4, override_locked=True
         )
         parameters["rs"].set_value(parameters["rs"].value / 2, override_locked=True)
 
     @default_internal
     def radial_model(self, R, image=None, parameters=None):
-        Rscaled = torch.abs((R + self.softening) / parameters["rs"].value)
+        Rscaled = torch.abs(R / parameters["rs"].value)
         return (
             Rscaled
             * torch.exp(-Rscaled)
             * torch.special.scaled_modified_bessel_k1(Rscaled)
         )
```

## autophot/models/exponential_model.py

```diff
@@ -18,15 +18,15 @@
     parametric_segment_initialize,
     select_target,
 )
 from ..utils.initialize import isophotes
 from ..utils.decorators import ignore_numpy_warnings, default_internal
 from ..utils.parametric_profiles import exponential_torch, exponential_np
 from ..utils.conversions.coordinates import Rotate_Cartesian
-
+    
 __all__ = [
     "Exponential_Galaxy",
     "Exponential_Star",
     "Exponential_SuperEllipse",
     "Exponential_SuperEllipse_Warp",
     "Exponential_Warp",
     "Exponential_Ray",
@@ -118,18 +118,18 @@
         parametric_initialize(
             self, parameters, target, _wrap_exp, ("Re", "Ie"), _x0_func
         )
 
     from ._shared_methods import exponential_radial_model as radial_model
 
     @default_internal
-    def evaluate_model(self, X=None, Y=None, image=None, parameters=None):
+    def evaluate_model(self, X = None, Y = None, image=None, parameters=None):
         if X is None:
             Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[..., None, None]
+            X, Y = Coords - parameters["center"].value[...,None, None]
         return self.radial_model(
             self.radius_metric(X, Y, image=image, parameters=parameters),
             image=image,
             parameters=parameters,
         )
```

## autophot/models/flatsky_model.py

```diff
@@ -29,30 +29,34 @@
     @ignore_numpy_warnings
     @select_target
     @default_internal
     def initialize(self, target=None, parameters=None, **kwargs):
         super().initialize(target=target, parameters=parameters)
 
         if parameters["sky"].value is None:
-            parameters["sky"].set_value(
-                torch.log10(torch.median(target[self.window].data) / target.pixel_area),
+            parameters["sky"].set_representation(
+                np.log10(
+                    torch.median(target[self.window].data) / target.pixel_area
+                ),
                 override_locked=True,
             )
         if parameters["sky"].uncertainty is None:
             parameters["sky"].set_uncertainty(
                 (
                     (
                         iqr(
                             target[self.window].data.detach().cpu().numpy(),
                             rng=(31.731 / 2, 100 - 31.731 / 2),
                         )
                         / (2.0 * target.pixel_area.item())
                     )
                     / np.sqrt(np.prod(self.window.shape.detach().cpu().numpy()))
                 )
-                / (10 ** parameters["sky"].value.item() * np.log(10)),
+                / (10 ** parameters["sky"].value * np.log(10)),
                 override_locked=True,
             )
 
     def evaluate_model(self, X=None, Y=None, image=None, parameters=None, **kwargs):
         ref = image.data if X is None else X
-        return torch.ones_like(ref) * (image.pixel_area * 10 ** parameters["sky"].value)
+        return torch.ones_like(ref) * (
+            (10 ** parameters["sky"].value) * image.pixel_area
+        )
```

## autophot/models/galaxy_model_object.py

```diff
@@ -57,54 +57,46 @@
     @ignore_numpy_warnings
     @select_target
     @default_internal
     def initialize(
         self, target=None, parameters: Optional["Parameter_Group"] = None, **kwargs
     ):
         super().initialize(target=target, parameters=parameters)
-
         if not (parameters["PA"].value is None or parameters["q"].value is None):
             return
         target_area = target[self.window]
-        target_area_data = target_area.data.detach().cpu().numpy()
         edge = np.concatenate(
             (
-                target_area_data[:, 0],
-                target_area_data[:, -1],
-                target_area_data[0, :],
-                target_area_data[-1, :],
+                target_area.data.detach().cpu().numpy()[:, 0],
+                target_area.data.detach().cpu().numpy()[:, -1],
+                target_area.data.detach().cpu().numpy()[0, :],
+                target_area.data.detach().cpu().numpy()[-1, :],
             )
         )
-        edge_average = np.nanmedian(edge)
-        edge_scatter = iqr(edge[np.isfinite(edge)], rng=(16, 84)) / 2
+        edge_average = np.median(edge)
+        edge_scatter = iqr(edge, rng=(16, 84)) / 2
         icenter = target_area.world_to_pixel(parameters["center"].value)
-
+        
         if parameters["PA"].value is None:
             iso_info = isophotes(
                 target_area.data.detach().cpu().numpy() - edge_average,
                 (icenter[1].detach().cpu().item(), icenter[0].detach().cpu().item()),
                 threshold=3 * edge_scatter,
                 pa=0.0,
                 q=1.0,
                 n_isophotes=15,
             )
             parameters["PA"].set_value(
                 (
-                    -(
-                        (
-                            Angle_Average(
-                                list(
-                                    iso["phase2"]
-                                    for iso in iso_info[-int(len(iso_info) / 3) :]
-                                )
-                            )
-                            / 2
+                    -((Angle_Average(
+                        list(
+                            iso["phase2"] for iso in iso_info[-int(len(iso_info) / 3) :]
                         )
-                        + target.north
                     )
+                       / 2) + target.north)
                 )
                 % np.pi,
                 override_locked=True,
             )
         if parameters["q"].value is None:
             q_samples = np.linspace(0.1, 0.9, 15)
             iso_info = isophotes(
@@ -116,27 +108,33 @@
             )
             parameters["q"].set_value(
                 q_samples[np.argmin(list(iso["amplitude2"] for iso in iso_info))],
                 override_locked=True,
             )
 
     @default_internal
+    def radius_metric(self, X, Y, image=None, parameters=None):
+        return torch.sqrt(
+            (torch.abs(X) + 1e-8) ** 2 + (torch.abs(Y) + 1e-8) ** 2
+        )  # epsilon added for numerical stability of gradient
+
+    @default_internal
     def transform_coordinates(self, X, Y, image=None, parameters=None):
         X, Y = Rotate_Cartesian(-(parameters["PA"].value - image.north), X, Y)
         return (
             X,
             Y / parameters["q"].value,
         )
 
     @default_internal
     def evaluate_model(
         self, X=None, Y=None, image=None, parameters: "Parameter_Group" = None, **kwargs
     ):
         if X is None or Y is None:
             Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[..., None, None]
+            X, Y = Coords - parameters["center"].value[...,None, None]
         XX, YY = self.transform_coordinates(X, Y, image, parameters)
         return self.radial_model(
             self.radius_metric(XX, YY, image, parameters),
             image=image,
             parameters=parameters,
         )
```

## autophot/models/gaussian_model.py

```diff
@@ -302,15 +302,15 @@
 
     from ._shared_methods import gaussian_radial_model as radial_model
 
     @default_internal
     def evaluate_model(self, X=None, Y=None, image=None, parameters=None):
         if X is None:
             Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[..., None, None]
+            X, Y = Coords - parameters["center"].value[...,None, None]
         return self.radial_model(torch.sqrt(X ** 2 + Y ** 2), image, parameters)
 
 
 class Gaussian_Ray(Ray_Galaxy):
     """ray galaxy model with a gaussian profile for the radial light
     model. The gaussian radial profile is defined as:
```

## autophot/models/group_model_object.py

```diff
@@ -75,14 +75,25 @@
                 f"{self.name} already has model with name {model.name}, every model must have a unique name."
             )
 
         self.models[model.name] = model
         self.parameters.add_group(model.parameters)
         self.update_window()
 
+    @property
+    def equality_constraints(self):
+        try:
+            return self._equality_constraints
+        except AttributeError:
+            return []
+
+    @equality_constraints.setter
+    def equality_constraints(self, val):
+        pass
+
     def update_window(self, include_locked: bool = False):
         """Makes a new window object which encloses all the windows of the
         sub models in this group model object.
 
         """
         if isinstance(
             self.target, Image_List
```

## autophot/models/model_object.py

```diff
@@ -4,27 +4,25 @@
 
 from torch.autograd.functional import jacobian
 import numpy as np
 import torch
 import matplotlib.pyplot as plt
 
 from .core_model import AutoPhot_Model
-from ..image import (
-    Model_Image,
-    Window,
-    PSF_Image,
-    Jacobian_Image,
-    Window_List,
-    Target_Image,
-    Target_Image_List,
-)
+from ..image import Model_Image, Window, PSF_Image, Jacobian_Image, Window_List
 from .parameter_object import Parameter
 from .parameter_group import Parameter_Group
 from ..utils.initialize import center_of_mass
 from ..utils.decorators import ignore_numpy_warnings, default_internal
+from ..utils.operations import (
+    fft_convolve_torch,
+    fft_convolve_multi_torch,
+    selective_integrate,
+)
+from ..utils.interpolate import _shift_Lanczos_kernel_torch
 from ._shared_methods import select_target
 from .. import AP_config
 
 __all__ = ["Component_Model"]
 
 
 class Component_Model(AutoPhot_Model):
@@ -36,21 +34,20 @@
     and model evaluation functions. This class also handles
     integration, PSF convolution, and computing the Jacobian matrix.
 
     Attributes:
       parameter_specs (dict): Specifications for the model parameters.
       _parameter_order (tuple): Fixed order of parameters.
       psf_mode (str): Technique and scope for PSF convolution.
-      sampling_mode (str): Method for initial sampling of model. Can be one of midpoint, trapezoid, simpson. Default: midpoint
-      sampling_tolerance (float): accuracy to which each pixel should be evaluated. Default: 1e-2
-      integrate_mode (str): Integration scope for the model. One of none, threshold, full where threshold will select which pixels to integrate while full (in development) will integrate all pixels. Default: threshold
-      integrate_max_depth (int): Maximum recursion depth when performing sub pixel integration.
-      integrate_gridding (int): Amount by which to subdivide pixels when doing recursive pixel integration.
-      integrate_quad_level (int): The initial quadrature level for sub pixel integration. Please always choose an odd number 3 or higher.
-      softening (float): Softening length used for numerical stability and integration stability to avoid discontinuities (near R=0). Effectively has units of arcsec. Default: 1e-5
+      psf_window_size (int): Size in pixels of the PSF convolution box.
+      integrate_mode (str): Integration scope for the model.
+      integrate_window_size (int): Size of the window in which to perform integration.
+      integrate_factor (int): Factor by which to upscale each dimension when integrating.
+      integrate_recursion_factor (int): Relative size of windows between recursion levels.
+      integrate_recursion_depth (int): Number of recursion cycles to apply when integrating.
       jacobian_chunksize (int): Maximum size of parameter list before jacobian will be broken into smaller chunks.
       special_kwargs (list): Parameters which are treated specially by the model object and should not be updated directly.
       useable (bool): Indicates if the model is useable.
 
     Methods:
       initialize: Determine initial values for the center coordinates.
       sample: Evaluate the model on the space covered by an image object.
@@ -64,64 +61,45 @@
     }
     # Fixed order of parameters for all methods that interact with the list of parameters
     _parameter_order = ("center",)
 
     # Scope for PSF convolution
     psf_mode = "none"  # none, full
     # Technique for PSF convolution
-    psf_convolve_mode = "fft"  # fft, direct
-    # Method to use when performing subpixel shifts. bilinear set by default for stability around pixel edges, though lanczos:3 is also fairly stable, and all are stable when away from pixel edges
-    psf_subpixel_shift = "bilinear"  # bilinear, lanczos:2, lanczos:3, lanczos:5, none
-
-    # Method for initial sampling of model
-    sampling_mode = "midpoint"  # midpoint, trapezoid, simpson
-
-    # Level to which each pixel should be evaluated
-    sampling_tolerance = 1e-2
-
+    psf_convolve_mode = "fft" # fft, direct
+    # method for initial sampling of grid before subpixel integration
+    sampling_mode = "midpoint" # midpoint, trapezoid, simpson 
     # Integration scope for model
     integrate_mode = "threshold"  # none, threshold, full*
 
-    # Maximum recursion depth when performing sub pixel integration
-    integrate_max_depth = 3
-
-    # Amount by which to subdivide pixels when doing recursive pixel integration
-    integrate_gridding = 5
-
-    # The initial quadrature level for sub pixel integration. Please always choose an odd number 3 or higher
-    integrate_quad_level = 3
+    # Number of recursion cycles to apply when integrating (threshold mode)
+    integrate_recursion_depth = 3
+    # Threshold for triggering pixel integration (threshold mode)
+    integrate_threshold = 1e-2
 
     # Maximum size of parameter list before jacobian will be broken into smaller chunks, this is helpful for limiting the memory requirements to build a model, lower jacobian_chunksize is slower but uses less memory
     jacobian_chunksize = 10
 
-    # Softening length used for numerical stability and/or integration stability to avoid discontinuities (near R=0)
-    softening = 1e-3
-
     # Parameters which are treated specially by the model object and should not be updated directly when initializing
     special_kwargs = ["parameters", "filename", "model_type"]
     track_attrs = [
         "psf_mode",
         "psf_convolve_mode",
         "sampling_mode",
-        "sampling_tolerance",
         "integrate_mode",
-        "integrate_max_depth",
-        "integrate_gridding",
-        "integrate_quad_level",
+        "integrate_recursion_depth",
+        "integrate_threshold",
         "jacobian_chunksize",
-        "softening",
     ]
     useable = False
 
     def __init__(self, name, *args, **kwargs):
-        self._target_identity = None
         super().__init__(name, *args, **kwargs)
 
         self.psf = None
-        self.psf_aux_image = None
 
         # Set any user defined attributes for the model
         for kwarg in kwargs:  # fixme move to core model?
             # Skip parameters with special behaviour
             if kwarg in self.special_kwargs:
                 continue
             # Set the model parameter
@@ -136,51 +114,33 @@
             kwargs.get("parameters", None)
         )
         with torch.no_grad():
             self.build_parameters()
             if isinstance(kwargs.get("parameters", None), torch.Tensor):
                 self.parameters.set_values(kwargs["parameters"])
 
-    def set_aux_psf(self, aux_psf, add_parameters=True):
-        """Set the PSF for this model as an auxiliary psf model. This psf
-        model will be resampled as part of the model sampling step to
-        track changes made during fitting.
-
-        Args:
-          aux_psf: The auxiliary psf model
-          add_parameters: if true, the parameters of the auxiliary psf model will become model parameters for this model as well.
-
-        """
-
-        self._psf = aux_psf
-
-        if add_parameters:
-            self.parameters.add_group(aux_psf.parameters)
-
     @property
     def psf(self):
         if self._psf is None:
-            try:
-                return self.target.psf
-            except AttributeError:
-                return None
+            return self.target.psf
         return self._psf
 
     @psf.setter
     def psf(self, val):
         if val is None:
             self._psf = None
         elif isinstance(val, PSF_Image):
             self._psf = val
-        elif isinstance(val, AutoPhot_Model):
-            self.set_aux_psf(val)
         else:
-            self._psf = PSF_Image(val, pixelscale = self.target.pixelscale, psf_upscale = 1)
-            AP_config.ap_logger.warn("Setting PSF with pixel matrix, assuming target pixelscale is the same as PSF pixelscale. To remove this warning, set PSFs as an ap.image.PSF_Image or ap.models.AutoPhot_Model object instead.")
-    
+            self._psf = PSF_Image(
+                val,
+                pixelscale=self.target.pixelscale,
+                band=self.target.band,
+            )
+
     # Initialization functions
     ######################################################################
     @torch.no_grad()
     @ignore_numpy_warnings
     @select_target
     @default_internal
     def initialize(
@@ -209,15 +169,15 @@
             return
 
         if parameters["center"].locked:
             return
 
         # Convert center coordinates to target area array indices
         init_icenter = target_area.world_to_pixel(parameters["center"].value)
-
+        
         # Compute center of mass in window
         COM = center_of_mass(
             (
                 init_icenter[1].detach().cpu().item(),
                 init_icenter[0].detach().cpu().item(),
             ),
             target_area.data.detach().cpu().numpy(),
@@ -225,17 +185,15 @@
         if np.any(np.array(COM) < 0) or np.any(
             np.array(COM) >= np.array(target_area.data.shape)
         ):
             AP_config.ap_logger.warning("center of mass failed, using center of window")
             return
         COM = (COM[1], COM[0])
         # Convert center of mass indices to coordinates
-        COM_center = target_area.pixel_to_world(
-            torch.tensor(COM, dtype=AP_config.ap_dtype, device=AP_config.ap_device)
-        )
+        COM_center = target_area.pixel_to_world(torch.tensor(COM, dtype=AP_config.ap_dtype, device=AP_config.ap_device))
 
         # Set the new coordinates as the model center
         parameters["center"].value = COM_center
 
     # Fit loop functions
     ######################################################################
     def evaluate_model(
@@ -252,15 +210,15 @@
 
         Args:
           image (Image): The image defining the set of pixels on which to evaluate the model
 
         """
         if X is None or Y is None:
             Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[..., None, None]
+            X, Y = Coords - parameters["center"].value[...,None, None]
         return torch.zeros_like(X)  # do nothing in base model
 
     def sample(
         self,
         image: Optional["Image"] = None,
         window: Optional[Window] = None,
         parameters: Optional[Parameter_Group] = None,
@@ -305,96 +263,122 @@
         if parameters is None:
             parameters = self.parameters
 
         if "window" in self.psf_mode:
             raise NotImplementedError("PSF convolution in sub-window not available yet")
 
         if "full" in self.psf_mode:
-            if isinstance(self.psf, AutoPhot_Model):
-                psf = self.psf.sample(
-                    image=self.psf_aux_image,
-                    parameters=parameters.groups[self.psf.name],
-                )
-                psf = PSF_Image(
-                    data=psf.data,
-                    pixelscale=psf.pixelscale,
-                    psf_upscale=torch.round(image.pixel_length / psf.pixel_length).int(),
-                )
-            else:
-                psf = self.psf
             # Add border for psf convolution edge effects, will be cropped out later
-            working_window += psf.psf_border
+            working_window += self.psf.psf_border
             # Determine the pixels scale at which to evalaute, this is smaller if the PSF is upscaled
-            working_pixelscale = image.pixelscale / psf.psf_upscale
+            working_pixelscale = image.pixelscale / self.psf.psf_upscale
             # Make the image object to which the samples will be tracked
             working_image = Model_Image(
                 pixelscale=working_pixelscale, window=working_window
-            )
+            )            
             # Sub pixel shift to align the model with the center of a pixel
-            if self.psf_subpixel_shift != "none":
-                pixel_center = working_image.world_to_pixel(parameters["center"].value)
-                center_shift = pixel_center - torch.round(pixel_center)
-                working_image.header.pixel_shift_origin(center_shift)
-            else:
-                center_shift = None
-
+            pixel_center = working_image.world_to_pixel(parameters["center"].value)
+            center_shift = pixel_center - torch.round(pixel_center)
+            working_image.header.pixel_shift_origin(center_shift)
             # Evaluate the model at the current resolution
-            reference, deep = self._sample_init(
-                image=working_image,
-                parameters=parameters,
-                center=parameters["center"].value,
+            working_image.data += self.evaluate_model(
+                image=working_image, parameters=parameters
             )
             # If needed, super-resolve the image in areas of high curvature so pixels are properly sampled
-            deep = self._sample_integrate(
-                deep, reference, working_image, parameters, parameters["center"].value
-            )
-
-            # update the image with the integrated pixels
-            working_image.data += deep
-
+            if self.integrate_mode == "none":
+                pass
+            elif self.integrate_mode == "threshold":
+                Coords = working_image.get_coordinate_meshgrid()
+                X, Y = Coords - parameters["center"].value[...,None, None]
+                selective_integrate(
+                    X=X,
+                    Y=Y,
+                    data=working_image.data,
+                    image_header=working_image.header,
+                    eval_brightness=self.evaluate_model,
+                    eval_parameters=parameters,
+                    max_depth=self.integrate_recursion_depth,
+                    integrate_threshold=self.integrate_threshold,
+                )
+            else:
+                raise ValueError(
+                    f"{self.name} has unknown integration mode: {self.integrate_mode}"
+                )
             # Convolve the PSF
-            self._sample_convolve(
-                working_image, center_shift, psf, self.psf_subpixel_shift
-            )
-
+            pix_center_shift = working_image.world_to_pixel_delta(center_shift)
+            LL = _shift_Lanczos_kernel_torch(
+                -pix_center_shift[0],
+                -pix_center_shift[1],
+                3,
+                AP_config.ap_dtype,
+                AP_config.ap_device,
+            )
+            shift_psf = torch.nn.functional.conv2d(
+                self.psf.data.view(1, 1, *self.psf.data.shape),
+                LL.view(1, 1, *LL.shape),
+                padding="same",
+            ).squeeze()
+            # Remove unphysical negative pixels from Lanczos interpolation
+            shift_psf[shift_psf < 0] = torch.tensor(0., dtype=AP_config.ap_dtype, device = AP_config.ap_device)
+            if self.psf_convolve_mode == "fft":
+                working_image.data = fft_convolve_torch(
+                    working_image.data, shift_psf / torch.sum(shift_psf), img_prepadded=True
+                )
+            elif self.psf_convolve_mode == "direct":
+                working_image.data = torch.nn.functional.conv2d(
+                    working_image.data.view(1, 1, *working_image.data.shape),
+                    torch.flip(shift_psf.view(1, 1, *shift_psf.shape) / torch.sum(shift_psf), dims = (2,3)),
+                    padding="same",
+                ).squeeze()
+            else:
+                raise ValueError(f"unrecognized psf_convolve_mode: {self.psf_convolve_mode}")
+                
             # Shift image back to align with original pixel grid
-            if self.psf_subpixel_shift != "none":
-                working_image.header.pixel_shift_origin(-center_shift)
+            working_image.header.shift_origin(-center_shift)
             # Add the sampled/integrated/convolved pixels to the requested image
-            working_image = working_image.reduce(psf.psf_upscale).crop(
-                psf.psf_border_int
+            working_image = working_image.reduce(self.psf.psf_upscale).crop(
+                self.psf.psf_border_int
             )
+            if self.mask is not None:
+                working_image.data = working_image.data * torch.logical_not(self.mask)
+            image += working_image
 
         else:
-
             # Create an image to store pixel samples
             working_image = Model_Image(
                 pixelscale=image.pixelscale, window=working_window
             )
             # Evaluate the model on the image
-            reference, deep = self._sample_init(
-                image=working_image,
-                parameters=parameters,
-                center=parameters["center"].value,
+            working_image.data += self.evaluate_model(
+                image=working_image, parameters=parameters
             )
             # Super-resolve and integrate where needed
-            deep = self._sample_integrate(
-                deep,
-                reference,
-                working_image,
-                parameters,
-                center=parameters["center"].value,
-            )
+            if self.integrate_mode == "none":
+                pass
+            elif self.integrate_mode == "threshold":
+                Coords = working_image.get_coordinate_meshgrid()
+                X, Y = Coords - parameters["center"].value[...,None, None]
+                selective_integrate(
+                    X=X,
+                    Y=Y,
+                    data=working_image.data,
+                    image_header=working_image.header,
+                    eval_brightness=self.evaluate_model,
+                    eval_parameters=parameters,
+                    max_depth=self.integrate_recursion_depth,
+                    integrate_threshold=self.integrate_threshold,
+                )
+            else:
+                raise ValueError(
+                    f"{self.name} has unknown integration mode: {self.integrate_mode}"
+                )
             # Add the sampled/integrated pixels to the requested image
-            working_image.data += deep
-
-        if self.mask is not None:
-            working_image.data = working_image.data * torch.logical_not(self.mask)
-
-        image += working_image
+            if self.mask is not None:
+                working_image.data = working_image.data * torch.logical_not(self.mask)
+            image += working_image
 
         return image
 
     @torch.no_grad()
     def jacobian(
         self,
         parameters: Optional[torch.Tensor] = None,
@@ -539,63 +523,32 @@
                 parameters_identity=pids[ichunk : ichunk + self.jacobian_chunksize],
                 window=window,
                 **kwargs,
             )
 
         return jac_img
 
-    @property
-    def target(self):
-        try:
-            return self._target
-        except AttributeError:
-            return None
-
-    @target.setter
-    def target(self, tar):
-        assert tar is None or isinstance(tar, Target_Image)
-
-        # If a target image list is assigned, pick out the target appropriate for this model
-        if isinstance(tar, Target_Image_List) and self._target_identity is not None:
-            for subtar in tar:
-                if subtar.identity == self._target_identity:
-                    usetar = subtar
-                    break
-            else:
-                raise KeyError(
-                    f"Could not find target in Target_Image_List with matching identity to {self.name}: {self._target_identity}"
-                )
-        else:
-            usetar = tar
-
-        self._target = usetar
-
-        # Remember the target identity to use
-        try:
-            self._target_identity = self._target.identity
-        except AttributeError:
-            pass
-
     def get_state(self):
         """Returns a dictionary with a record of the current state of the
         model.
 
         Specifically, the current parameter settings and the
         window for this model. From this information it is possible
         for the model to re-build itself lated when loading from
         disk. Note that the target image is not saved, this must be
         reset when loading the model.
 
         """
         state = super().get_state()
         state["window"] = self.window.get_state()
-        state["parameters"] = self.parameters.get_state(save_groups=False)
-        state["target_identity"] = self._target_identity
-        if isinstance(self.psf, AutoPhot_Model):
-            state["psf"] = self.psf.get_state()
+        state["parameter_order"] = list(self.parameter_order)
+        if "parameters" not in state:
+            state["parameters"] = {}
+        for P in self.parameters:
+            state["parameters"][P.name] = P.get_state()
         for key in self.track_attrs:
             if getattr(self, key) != getattr(self.__class__, key):
                 state[key] = getattr(self, key)
         return state
 
     def load(self, filename: Union[str, dict, io.TextIOBase] = "AutoPhot.yaml"):
         """Used to load the model from a saved state.
@@ -606,41 +559,21 @@
 
         Args:
           filename: The source from which to load the model parameters. Can be a string (the name of the file on disc), a dictionary (formatted as if from self.get_state), or an io.TextIOBase (a file stream to load the file from).
 
         """
         state = AutoPhot_Model.load(filename)
         self.name = state["name"]
-        # Use window saved state to initialize model window
         self.window = Window(**state["window"])
-        # reassign target in case a target list was given
-        self._target_identity = state["target_identity"]
-        self.target = self.target
-        # Set any attributes which were not default
         for key in self.track_attrs:
             if key in state:
                 setattr(self, key, state[key])
-        # Load the parameter group, this is handled by the parameter group object
-        self.parameters = Parameter_Group(self.name, state=state["parameters"])
-        # Move parameters to the appropriate device and dtype
+        self.parameters = Parameter_Group(self.name)
+        for P in state["parameter_order"]:
+            self.parameters.add_parameter(Parameter(**state["parameters"][P]))
         self.parameters.to(dtype=AP_config.ap_dtype, device=AP_config.ap_device)
-        # Re-create the aux PSF model if there was one
-        if "psf" in state:
-            self.set_aux_psf(
-                AutoPhot_Model(
-                    state["psf"]["name"],
-                    filename=state["psf"],
-                    target=self.target,
-                )
-            )
         return state
 
     # Extra background methods for the basemodel
     ######################################################################
-    from ._model_methods import radius_metric
-    from ._model_methods import angular_metric
-    from ._model_methods import _sample_init
-    from ._model_methods import _sample_integrate
-    from ._model_methods import _sample_convolve
-    from ._model_methods import _integrate_reference
     from ._model_methods import build_parameter_specs
     from ._model_methods import build_parameters
```

## autophot/models/moffat_model.py

```diff
@@ -2,15 +2,14 @@
 import numpy as np
 
 from .galaxy_model_object import Galaxy_Model
 from .star_model_object import Star_Model
 from ._shared_methods import parametric_initialize, select_target
 from ..utils.decorators import ignore_numpy_warnings, default_internal
 from ..utils.parametric_profiles import moffat_np
-from ..utils.conversions.functions import moffat_I0_to_flux, general_uncertainty_prop
 
 __all__ = ["Moffat_Galaxy", "Moffat_Star"]
 
 
 def _x0_func(model_params, R, F):
     return 2.0, R[4], F[0]
 
@@ -53,38 +52,14 @@
     def initialize(self, target=None, parameters=None, **kwargs):
         super().initialize(target=target, parameters=parameters)
 
         parametric_initialize(
             self, parameters, target, _wrap_moffat, ("n", "Rd", "I0"), _x0_func
         )
 
-    @default_internal
-    def total_flux(self, parameters=None):
-        return moffat_I0_to_flux(
-            10 ** parameters["I0"].value,
-            parameters["n"].value,
-            parameters["Rd"].value,
-            parameters["q"].value,
-        )
-    @default_internal
-    def total_flux_uncertainty(self, parameters=None):
-        return general_uncertainty_prop(
-            (10 ** parameters["I0"].value,
-             parameters["n"].value,
-             parameters["Rd"].value,
-             parameters["q"].value
-            ),
-            ((10 ** parameters["I0"].value) * parameters["I0"].uncertainty * torch.log(10*torch.ones_like(parameters["I0"].value)),
-             parameters["n"].uncertainty,
-             parameters["Rd"].uncertainty,
-             parameters["q"].uncertainty
-            ),
-            moffat_I0_to_flux
-        )
-    
     from ._shared_methods import moffat_radial_model as radial_model
 
 
 class Moffat_Star(Star_Model):
     """basic star model with a Moffat profile for the radial light
     profile. The functional form of the Moffat profile is defined as:
 
@@ -120,40 +95,16 @@
 
         parametric_initialize(
             self, parameters, target, _wrap_moffat, ("n", "Rd", "I0"), _x0_func
         )
 
     from ._shared_methods import moffat_radial_model as radial_model
 
-    @default_internal
-    def total_flux(self, parameters=None):
-        return moffat_I0_to_flux(
-            10 ** parameters["I0"].value,
-            parameters["n"].value,
-            parameters["Rd"].value,
-            torch.ones_like(parameters["n"].value),
-        )
-    @default_internal
-    def total_flux_uncertainty(self, parameters=None):
-        return general_uncertainty_prop(
-            (10 ** parameters["I0"].value,
-             parameters["n"].value,
-             parameters["Rd"].value,
-             torch.ones_like(parameters["n"].value)
-            ),
-            ((10 ** parameters["I0"].value) * parameters["I0"].uncertainty * torch.log(10*torch.ones_like(parameters["I0"].value)),
-             parameters["n"].uncertainty,
-             parameters["Rd"].uncertainty,
-             torch.zeros_like(parameters["n"].value)
-            ),
-            moffat_I0_to_flux
-        )
-    
     def evaluate_model(self, X=None, Y=None, image=None, parameters=None):
         if X is None:
             Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[..., None, None]
+            X, Y = Coords - parameters["center"].value[...,None, None]
         return self.radial_model(
             self.radius_metric(X, Y, image=image, parameters=parameters),
             image=image,
             parameters=parameters,
         )
```

## autophot/models/nuker_model.py

```diff
@@ -149,15 +149,15 @@
 
     from ._shared_methods import nuker_radial_model as radial_model
 
     @default_internal
     def evaluate_model(self, X=None, Y=None, image=None, parameters=None):
         if X is None:
             Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[..., None, None]
+            X, Y = Coords - parameters["center"].value[...,None, None]
         return self.radial_model(
             self.radius_metric(X, Y, image=image, parameters=parameters),
             image=image,
             parameters=parameters,
         )
```

## autophot/models/parameter_group.py

```diff
@@ -7,61 +7,49 @@
 from ..utils.conversions.optimization import (
     boundaries,
     inv_boundaries,
     d_boundaries_dval,
     d_inv_boundaries_dval,
     cyclic_boundaries,
 )
-from .parameter_object import Parameter
 from .. import AP_config
 
 __all__ = ["Parameter_Group"]
 
 
 class Parameter_Group(object):
-    def __init__(self, name, groups=None, parameters=None, state=None):
+    def __init__(self, name, groups=None, parameters=None):
         self.name = name
         self.groups = OrderedDict()
-        self.top_level_parameters = set()
-        self.parameters = OrderedDict()
-        if state is not None:
-            self.set_state(state)
-            return
         if groups is not None:
             for G in groups:
                 self.add_group(G)
+        self.parameters = OrderedDict()
         if parameters is not None:
             for P in parameters:
                 self.add_parameter(P)
 
     def copy(self):
         return Parameter_Group(
             name=self.name,
             groups=list(group.copy() for group in self.groups.values()),
             parameters=list(parameter.copy() for parameter in self.parameters.values()),
         )
 
     def add_group(self, group):
         self.groups[group.name] = group
         for P in group.parameters.values():
-            self.add_parameter(P, top_level=False)
+            self.add_parameter(P)
 
-    def add_parameter(self, parameter, top_level=True):
-        if top_level:
-            self.top_level_parameters.update([parameter.identity])
-
-        # Add the parameter identity to the list of parameters in this group
+    def add_parameter(self, parameter):
         self.parameters[parameter.identity] = parameter
-        # Add a link for the parameter back to this group
         parameter.groups.add(self)
-
-        # Redundant parameter setting, ensures that parameters with the same identity do not coexist (generally only applicable when loading a model)
         for group in self.groups.values():
             if parameter.identity in group.parameters:
-                group.add_parameter(parameter, top_level=False)
+                group.add_parameter(parameter)
 
     def get_identities(self):
         return sum((list(param.identities) for param in self), [])
 
     def order(self, parameters_identity=None):
         if parameters_identity is None:
             return list(param.identity for param in self)
@@ -132,46 +120,14 @@
             return parameters
 
         # If the full vector is requested, they are added in bulk
         for P in porder:
             parameters += list(self.get_id(P).identities)
         return parameters
 
-    def get_name_vector(self, parameters_identity=None):
-        PVL = self.vector_len(parameters_identity=parameters_identity)
-        parameters = list(None for i in range(int(sum(PVL))))
-        porder = self.order(parameters_identity=parameters_identity)
-
-        # If vector is requested by identity, they are individually updated
-        pindex = 0
-        if parameters_identity is not None:
-            for P in porder:
-                isarray = len(self.get_id(P).identities) > 1
-                for ind, pid in enumerate(self.get_id(P).identities):
-                    if pid in parameters_identity:
-                        if isarray:
-                            parameters[pindex] = f"{self.get_id(P).name}:{ind}"
-                        else:
-                            parameters[pindex] = self.get_id(P).name
-                        pindex += 1
-            return parameters
-
-        # If the full vector is requested, they are added in bulk
-        for P, V in zip(porder, PVL):
-            isarray = len(self.get_id(P).identities) > 1
-            if isarray:
-                for ind, pid in enumerate(self.get_id(P).identities):
-                    parameters[pindex] = f"{self.get_id(P).name}:{ind}"
-                    pindex += 1
-            else:
-                parameters[pindex] = self.get_id(P).name
-                pindex += 1
-        return parameters
-        
-    
     def transform(
         self, in_parameters, to_representation=True, parameters_identity=None
     ):
         PVL = self.vector_len(parameters_identity=parameters_identity)
         out_parameters = torch.zeros(
             int(np.sum(PVL)),
             dtype=AP_config.ap_dtype,
@@ -336,100 +292,63 @@
                 uncertainty[start : start + V].reshape(
                     self.get_id(P).representation.shape
                 ),
                 as_representation=as_representation,
             )
             start += V
 
-    def iter_all(self):
-        return self.parameters.values()
-
-    def iter_top_level(self):
-        return filter(
-            lambda p: p.identity in self.top_level_parameters, self.parameters.values()
-        )
-
     def __iter__(self):
         return filter(lambda p: not p.locked, self.parameters.values())
 
     def get_id(self, key):
         if ":" in key:
             return self.parameters[key[: key.find(":")]]
         else:
             return self.parameters[key]
 
     def get_name(self, key):
-        # The : character is used for nested parameter group names
         if ":" in key:
             return self.groups[key[: key.find(":")]].get_name(key[key.find(":") + 1 :])
-
-        # Attempt to find the parameter with that key name
-        for Pid in self.top_level_parameters:
-            if self.parameters[Pid].name == key:
-                return self.parameters[Pid]
-        # If the key cannot be found, raise an error
-        raise KeyError()
+        else:
+            for P in self.parameters.values():
+                if P.name == key:
+                    return P
+            else:
+                raise KeyError()
 
     def pop_id(self, key):
         try:
-            self.top_level_parameters.remove(key)
+            del self.parameters[key]
         except KeyError:
             pass
-        return self.parameters.pop(key)
+        for group in self.groups.values():
+            group.pop_id(key)
 
-    def replace(self, old_param, new_param):
-        was_top_level = old_param.identity in self.top_level_parameters
-        self.pop_id(old_param.identity)
-        self.add_parameter(new_param, top_level=was_top_level)
+    def pop_name(self, key):
+        try:
+            param = self.get_name(key)
+            del self.parameters[param.identity]
+        except KeyError:
+            pass
+        for group in self.groups.values():
+            try:
+                group.pop_id(key)
+            except KeyError:
+                pass
 
     def to(self, dtype=None, device=None):
-        if dtype is None:
-            dtype = AP_config.ap_dtype
-        if device is None:
-            device = AP_config.ap_device
         for P in self.parameters.values():
             P.to(dtype=dtype, device=device)
         return self
 
-    def get_state(self, save_groups=True):
-        state = {"name": self.name}
-        if len(self.parameters) > 0:
-            state["parameter_order"] = list(P.name for P in self.iter_top_level())
-            state["parameters"] = dict(
-                (P.name, P.get_state()) for P in self.iter_top_level()
-            )
-        if save_groups and len(self.groups) > 0:
-            state["groups"] = dict(
-                (G.name, G.get_state()) for G in self.groups.values()
-            )
-        return state
-
-    def set_state(self, state):
-        self.name = state["name"]
-        if "parameters" in state:
-            self.parameters = OrderedDict()
-            for param in state["parameter_order"]:
-                self.add_parameter(
-                    Parameter(**state["parameters"][param]), top_level=True
-                )
-        if "groups" in state:
-            self.groups = OrderedDict()
-            for group in state["groups"]:
-                self.add_group(Parameter_Group(group, state=state["groups"][group]))
-
     def __getitem__(self, key):
         return self.get_name(key)
 
     def __contains__(self, key):
         try:
             self.get_name(key)
             return True
         except KeyError:
             return False
 
     def __len__(self):
         return len(self.parameters)
-
-    def __str__(self):
-        return f"Parameter Group: {self.name}\n" + "\n".join(
-            list(str(P) for P in self.iter_all())
-        )
```

## autophot/models/parameter_object.py

```diff
@@ -57,15 +57,15 @@
         self.cyclic = kwargs.get("cyclic", False)
         self.locked = kwargs.get("locked", False)
         self._representation = None
         self.set_value(value, override_locked=True)
         self.units = kwargs.get("units", "none")
         self._uncertainty = None
         self.uncertainty = kwargs.get("uncertainty", None)
-        self.set_profile(kwargs.get("prof", None), override_locked=True)
+        self.set_profile(kwargs.get("prof", None))
         self.groups = kwargs.get("groups", set())
         self.to()
 
     @property
     def identity(self):
         return self._identity
 
@@ -452,15 +452,15 @@
                     save_lim.append(self.limits[i].item())
                 else:
                     save_lim.append(self.limits[i].detach().cpu().tolist())
             state["limits"] = tuple(save_lim)
         if self.cyclic:
             state["cyclic"] = self.cyclic
         if self.prof is not None:
-            state["prof"] = self.prof.detach().cpu().tolist()
+            state["prof"] = self.prof.detach().cpu().numpy().tolist()
 
         return state
 
     def set_profile(self, prof, override_locked=False):
         if self.locked and not override_locked:
             return
         if prof is None:
@@ -476,46 +476,25 @@
 
         """
         self.name = state["name"]
         self._identity = state["identity"]
         self.units = state.get("units", None)
         self.limits = state.get("limits", None)
         self.cyclic = state.get("cyclic", False)
-        self.set_uncertainty(state.get("uncertainty", None), override_locked = True)
-        self.set_value(state.get("value", None), override_locked = True)
-        self.set_profile(state.get("prof", None), override_locked = True)
+        self.uncertainty = state.get("uncertainty", None)
+        self.value = state.get("value", None)
         self.locked = state.get("locked", False)
+        self.prof = state.get("prof", None)
 
     def __str__(self):
         """String representation of the parameter which indicates it's value
         along with uncertainty, units, limits, etc.
 
         """
-
-        value = self.value.detach().cpu().tolist() if self.value is not None else "None"
-        uncertainty = (
-            self.uncertainty.detach().cpu().tolist()
-            if self.uncertainty is not None
-            else "None"
-        )
-        if self.limits is None:
-            limits = None
-        else:
-            limits0 = (
-                self.limits[0].detach().cpu().tolist()
-                if self.limits[0] is not None
-                else "None"
-            )
-            limits1 = (
-                self.limits[1].detach().cpu().tolist()
-                if self.limits[1] is not None
-                else "None"
-            )
-            limits = (limits0, limits1)
-        return f"{self.name}: {value} +- {uncertainty} [{self.units}{'' if self.locked is False else ', locked'}{'' if limits is None else (', ' + str(limits))}{'' if self.cyclic is False else ', cyclic'}]"
+        return f"{self.name}: {self.value} +- {self.uncertainty} [{self.units}{'' if self.locked is False else ', locked'}{'' if self.limits is None else (', ' + str(self.limits))}{'' if self.cyclic is False else ', cyclic'}]"
 
     def __iter__(self):
         """If the parameter has multiple values, it is posible to iterate over
         the values.
 
         """
         self.i = -1
```

## autophot/models/planesky_model.py

```diff
@@ -23,51 +23,47 @@
         delta: Tensor for slope of the sky brightness in each image dimension
 
     """
 
     model_type = f"plane {Sky_Model.model_type}"
     parameter_specs = {
         "sky": {"units": "flux/arcsec^2"},
-        "delta": {"units": "flux/arcsec"},
+        "delta": {"units": "sky/arcsec"},
     }
     _parameter_order = Sky_Model._parameter_order + ("sky", "delta")
     useable = True
 
     @torch.no_grad()
     @ignore_numpy_warnings
     @select_target
     @default_internal
     def initialize(self, target=None, parameters=None, **kwargs):
         super().initialize(target=target, parameters=parameters)
 
         if parameters["sky"].value is None:
             parameters["sky"].set_value(
-                np.median(target[self.window].data.detach().cpu().numpy())
-                / target.pixel_area.item(),
+                np.median(target[self.window].data) / target.pixel_area,
                 override_locked=True,
             )
         if parameters["sky"].uncertainty is None:
             parameters["sky"].set_uncertainty(
                 (
-                    iqr(
-                        target[self.window].data.detach().cpu().numpy(),
-                        rng=(31.731 / 2, 100 - 31.731 / 2),
-                    )
-                    / (2.0)
+                    iqr(target[self.window].data, rng=(31.731 / 2, 100 - 31.731 / 2))
+                    / (2.0 * target.pixel_area)
                 )
                 / np.sqrt(np.prod(self.window.shape.detach().cpu().numpy())),
                 override_locked=True,
             )
         if parameters["delta"].value is None:
             parameters["delta"].set_value([0.0, 0.0], override_locked=True)
             parameters["delta"].set_uncertainty([0.1, 0.1], override_locked=True)
 
     @default_internal
     def evaluate_model(self, X=None, Y=None, image=None, parameters=None, **kwargs):
         if X is None:
             Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[..., None, None]
+            X, Y = Coords - parameters["center"].value[...,None, None]
         return (
-            image.pixel_area * parameters["sky"].value
+            (parameters["sky"].value * image.pixel_area)
             + X * parameters["delta"].value[0]
             + Y * parameters["delta"].value[1]
         )
```

## autophot/models/psf_model.py

```diff
@@ -1,98 +1,99 @@
 import torch
 
 from .star_model_object import Star_Model
 from ..image import Model_Image
 from ..utils.decorators import ignore_numpy_warnings, default_internal
-from ..utils.interpolate import interp2d
+from ..utils.interpolate import _shift_Lanczos_kernel_torch
 from ._shared_methods import select_target
 from .. import AP_config
 
 __all__ = ["PSF_Star"]
 
 
 class PSF_Star(Star_Model):
     """Star model which uses an image of the PSF as it's representation
-    for stars. Using bilinear interpolation it will shift the PSF
+    for stars. Using Lanczos interpolation it will shift the PSF
     within a pixel to accurately represent the center location of a
     point source. There is no funcitonal form for this object type as
     any image can be supplied. Note that as an argument to the model
     at construction one can provide "psf" as an AutoPhot Model_Image
-    object. Since only bilinear interpolation is performed, it is
-    recommended to provide the PSF at a higher resolution than the
-    image if it is near the nyquist sampling limit. Bilinear
-    interpolation is very fast and accurate for smooth models, so this
-    way it is possible to do the expensive interpolation before
-    optimization and save time. Note that if you do this you must
-    provide the PSF as a Model_Image object with the correct PSF
-    (essentially just divide the PSF by the upsampling factor you
-    used).
+    object. If the supplied image is at a higher resolution than the
+    target image then the PSF will be upsampled at the time of
+    sampling after the shift has been performed. In this way it is
+    possible to get a more accurate representation of the PSF.
 
     Parameters:
         flux: the total flux of the star model, represented as the log of the total flux.
-
     """
 
     model_type = f"psf {Star_Model.model_type}"
     parameter_specs = {
         "flux": {"units": "log10(flux/arcsec^2)"},
     }
     _parameter_order = Star_Model._parameter_order + ("flux",)
     useable = True
 
+    lanczos_kernel_size = 5
+    clip_lanczos_kernel = True
+
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         # fixme, model already has PSF interface, those can just be merged
         if "psf" in kwargs:
             self.psf_model = kwargs["psf"]
         else:
             self.psf_model = Model_Image(
                 data=torch.clone(self.psf.data),
                 pixelscale=self.psf.pixelscale,
             )
-        self.psf_model.header.shift_origin(
-            self.psf_model.origin - self.psf_model.center
-        )
 
     @torch.no_grad()
     @ignore_numpy_warnings
     @select_target
     @default_internal
     def initialize(self, target=None, parameters=None, **kwargs):
         super().initialize(target=target, parameters=parameters)
         target_area = target[self.window]
         if parameters["flux"].value is None:
             parameters["flux"].set_value(
-                torch.log10(torch.abs(torch.sum(target_area.data)) / target.pixel_area),
+                torch.log10(
+                    torch.abs(torch.sum(target_area.data)) / target_area.pixel_area
+                ),
                 override_locked=True,
             )
         if parameters["flux"].uncertainty is None:
             parameters["flux"].set_uncertainty(
                 torch.abs(parameters["flux"].value) * 1e-2, override_locked=True
             )
 
     @default_internal
     def evaluate_model(self, X=None, Y=None, image=None, parameters=None, **kwargs):
-        if X is None:
-            Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[..., None, None]
-
-        # Convert coordinates into pixel locations in the psf image
-        pX, pY = self.psf_model.world_to_pixel(
-            torch.stack((X, Y)).view(2, -1), unsqueeze_origin=True
+        new_origin = parameters["center"].value - self.psf_model.window.end / 2
+        pixel_origin = image.pixel_to_world(torch.round(image.world_to_pixel(new_origin)))
+        pixel_shift = (
+            image.world_to_pixel(new_origin) - image.world_to_pixel(pixel_origin)
         )
-        pX = pX.reshape(X.shape)
-        pY = pY.reshape(Y.shape)
-
-        # Select only the pixels where the PSF image is defined
-        select = torch.logical_and(
-            torch.logical_and(pX > -0.5, pX < self.psf_model.data.shape[1]),
-            torch.logical_and(pY > -0.5, pY < self.psf_model.data.shape[0]),
+        LL = _shift_Lanczos_kernel_torch(
+            -pixel_shift[0],
+            -pixel_shift[1],
+            3,
+            AP_config.ap_dtype,
+            AP_config.ap_device,
+        )
+        psf = Model_Image(
+            data=torch.nn.functional.conv2d(
+                (
+                    torch.clone(self.psf_model.data)
+                    * ((10 ** parameters["flux"].value) * image.pixel_area)
+                ).view(1, 1, *self.psf_model.data.shape),
+                LL.view(1, 1, *LL.shape),
+                padding="same",
+            )[0][0],
+            origin=new_origin,
+            pixelscale=self.psf_model.pixelscale,
         )
 
-        # Zero everywhere outside the psf
-        result = torch.zeros_like(X)
-
-        # Use bilinear interpolation of the PSF at the requested coordinates
-        result[select] = interp2d(self.psf_model.data, pX[select], pY[select])
-
-        return result * (image.pixel_area * 10 ** parameters["flux"].value)
+        # fixme pick nearest neighbor for each X, Y? interpolate?
+        img = image.blank_copy()
+        img += psf
+        return img.data
```

## autophot/models/ray_model.py

```diff
@@ -40,14 +40,18 @@
 
     def __init__(self, *args, **kwargs):
         self.symmetric_rays = True
         super().__init__(*args, **kwargs)
         self.rays = kwargs.get("rays", Ray_Galaxy.rays)
 
     @default_internal
+    def angular_metric(self, X, Y, image=None, parameters=None):
+        return torch.atan2(Y, X)
+
+    @default_internal
     def polar_model(self, R, T, image=None, parameters=None):
         model = torch.zeros_like(R)
         if self.rays % 2 == 0 and self.symmetric_rays:
             for r in range(self.rays):
                 angles = (T - (r * np.pi / self.rays)) % np.pi
                 indices = torch.logical_or(
                     angles < (np.pi / self.rays),
@@ -90,15 +94,15 @@
                 weight = (torch.cos(angles[indices] * self.rays) + 1) / 2
                 model[indices] += weight * self.iradial_model(r, R[indices], image)
         return model
 
     def evaluate_model(self, X=None, Y=None, image=None, parameters=None, **kwargs):
         if X is None:
             Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[..., None, None]
+            X, Y = Coords - parameters["center"].value[...,None, None]
         XX, YY = self.transform_coordinates(X, Y, image, parameters)
 
         return self.polar_model(
             self.radius_metric(XX, YY, image=image, parameters=parameters),
             self.angular_metric(XX, YY, image=image, parameters=parameters),
             image=image,
             parameters=parameters,
```

## autophot/models/sersic_model.py

```diff
@@ -15,16 +15,14 @@
     parametric_segment_initialize,
     select_target,
 )
 from ..utils.decorators import ignore_numpy_warnings, default_internal
 from ..utils.initialize import isophotes
 from ..utils.parametric_profiles import sersic_torch, sersic_np
 from ..utils.conversions.coordinates import Rotate_Cartesian
-from ..utils.conversions.functions import sersic_Ie_to_flux_torch, general_uncertainty_prop
-
 
 __all__ = [
     "Sersic_Galaxy",
     "Sersic_Star",
     "Sersic_Warp",
     "Sersic_SuperEllipse",
     "Sersic_FourierEllipse",
@@ -78,52 +76,14 @@
     def initialize(self, target=None, parameters=None, **kwargs):
         super().initialize(target=target, parameters=parameters)
 
         parametric_initialize(
             self, parameters, target, _wrap_sersic, ("n", "Re", "Ie"), _x0_func
         )
 
-    @default_internal
-    def total_flux(self, parameters=None):
-        return sersic_Ie_to_flux_torch(
-            10 ** parameters["Ie"].value,
-            parameters["n"].value,
-            parameters["Re"].value,
-            parameters["q"].value,
-        )
-    @default_internal
-    def total_flux_uncertainty(self, parameters=None):
-        return general_uncertainty_prop(
-            (10 ** parameters["Ie"].value,
-             parameters["n"].value,
-             parameters["Re"].value,
-             parameters["q"].value
-            ),
-            ((10 ** parameters["Ie"].value) * parameters["Ie"].uncertainty * torch.log(10 * torch.ones_like(parameters["Ie"].value)),
-             parameters["n"].uncertainty,
-             parameters["Re"].uncertainty,
-             parameters["q"].uncertainty
-            ),
-            sersic_Ie_to_flux_torch
-        )
-        # return sersic_Ie_to_flux_uncertainty_torch(
-        #     10 ** parameters["Ie"].value,
-        #     parameters["n"].value,
-        #     parameters["Re"].value,
-        #     parameters["q"].value,
-        #     (10 ** parameters["Ie"].value) * parameters["Ie"].uncertainty * torch.log(10 * torch.ones_like(parameters["Ie"].value)),
-        #     parameters["n"].uncertainty,
-        #     parameters["Re"].uncertainty,
-        #     parameters["q"].uncertainty,
-        # )
-
-    def _integrate_reference(self, image_data, image_header, parameters):
-        tot = self.total_flux(parameters)
-        return tot / image_data.numel()
-
     from ._shared_methods import sersic_radial_model as radial_model
 
 
 class Sersic_Star(Star_Model):
     """basic star model with a sersic profile for the radial light
     profile. The functional form of the Sersic profile is defined as:
 
@@ -161,27 +121,18 @@
         parametric_initialize(
             self, parameters, target, _wrap_sersic, ("n", "Re", "Ie"), _x0_func
         )
 
     from ._shared_methods import sersic_radial_model as radial_model
 
     @default_internal
-    def total_flux(self, parameters=None):
-        return sersic_Ie_to_flux_torch(
-            10 ** parameters["Ie"].value,
-            parameters["n"].value,
-            parameters["Re"].value,
-            torch.ones_like(parameters["n"].value),
-        )
-
-    @default_internal
     def evaluate_model(self, X=None, Y=None, image=None, parameters=None):
         if X is None:
             Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[..., None, None]
+            X, Y = Coords - parameters["center"].value[...,None, None]
         return self.radial_model(
             self.radius_metric(X, Y, image=image, parameters=parameters),
             image=image,
             parameters=parameters,
         )
```

## autophot/models/spline_model.py

```diff
@@ -79,20 +79,14 @@
     useable = True
     extend_profile = True
 
     @default_internal
     def transform_coordinates(self, X=None, Y=None, image=None, parameters=None):
         return X, Y
 
-    @default_internal
-    def evaluate_model(self, X=None, Y=None, image=None, parameters=None):
-        return self.radial_model(
-            self.radius_metric(X, Y, image, parameters), image, parameters
-        )
-
     from ._shared_methods import spline_initialize as initialize
     from ._shared_methods import spline_radial_model as radial_model
 
 
 class Spline_Warp(Warp_Galaxy):
     """warped coordinate galaxy model with a spline light
     profile. The light profile is defined as a cubic spline
```

## autophot/models/star_model_object.py

```diff
@@ -17,14 +17,28 @@
     already been done when constructing the PSF.
 
     """
 
     model_type = f"star {Component_Model.model_type}"
     useable = False
 
+    @default_internal
+    def radius_metric(self, X, Y, image=None, parameters=None):
+        return torch.sqrt(
+            (torch.abs(X) + 1e-8) ** 2 + (torch.abs(Y) + 1e-8) ** 2
+        )  # epsilon added for numerical stability of gradient
+
     @property
     def psf_mode(self):
         return "none"
 
     @psf_mode.setter
     def psf_mode(self, val):
         pass
+
+    @property
+    def integrate_mode(self):
+        return "none"
+
+    @integrate_mode.setter
+    def integrate_mode(self, val):
+        pass
```

## autophot/models/superellipse_model.py

```diff
@@ -34,18 +34,18 @@
     }
     _parameter_order = Galaxy_Model._parameter_order + ("C0",)
     useable = False
 
     @default_internal
     def radius_metric(self, X, Y, image=None, parameters=None):
         return torch.pow(
-            torch.pow(torch.abs(X), parameters["C0"].value + 2.0)
-            + torch.pow(torch.abs(Y), parameters["C0"].value + 2.0),
+            torch.pow(torch.abs(X) + 1e-8, parameters["C0"].value + 2.0)
+            + torch.pow(torch.abs(Y) + 1e-8, parameters["C0"].value + 2.0),
             1.0 / (parameters["C0"].value + 2.0),
-        )
+        )  # epsilon added for numerical stability of gradient
 
 
 class SuperEllipse_Warp(Warp_Galaxy):
     """Expanded warp model which includes a superellipse transformation
     in its radius metric. This allows for the expression of "boxy" and
     "disky" isophotes instead of pure ellipses. This is a common
     extension of the standard elliptical representation, especially
@@ -71,11 +71,11 @@
     }
     _parameter_order = Warp_Galaxy._parameter_order + ("C0",)
     useable = False
 
     @default_internal
     def radius_metric(self, X, Y, image=None, parameters=None):
         return torch.pow(
-            torch.pow(torch.abs(X), parameters["C0"].value + 2.0)
-            + torch.pow(torch.abs(Y), parameters["C0"].value + 2.0),
+            torch.pow(torch.abs(X) + 1e-8, parameters["C0"].value + 2.0)
+            + torch.pow(torch.abs(Y) + 1e-8, parameters["C0"].value + 2.0),
             1.0 / (parameters["C0"].value + 2.0),
         )  # epsilon added for numerical stability of gradient
```

## autophot/models/warp_model.py

```diff
@@ -87,30 +87,27 @@
                     parameters[prof_param].set_profile(new_prof)
 
         if not (parameters["PA(R)"].value is None or parameters["q(R)"].value is None):
             return
 
         if parameters["PA(R)"].value is None:
             parameters["PA(R)"].set_value(
-                np.zeros(len(parameters["PA(R)"].prof)) + target.north,
-                override_locked=True,
+                np.zeros(len(parameters["PA(R)"].prof)) + target.north, override_locked=True
             )
 
         if parameters["q(R)"].value is None:
             parameters["q(R)"].set_value(
                 np.ones(len(parameters["q(R)"].prof)) * 0.9, override_locked=True
             )
 
     @default_internal
     def transform_coordinates(self, X, Y, image=None, parameters=None):
         X, Y = super().transform_coordinates(X, Y, image, parameters)
         R = self.radius_metric(X, Y, image, parameters)
         PA = cubic_spline_torch(
-            parameters["PA(R)"].prof,
-            -(parameters["PA(R)"].value - image.north),
-            R.view(-1),
+            parameters["PA(R)"].prof, -(parameters["PA(R)"].value - image.north), R.view(-1)
         ).view(*R.shape)
         q = cubic_spline_torch(
             parameters["q(R)"].prof, parameters["q(R)"].value, R.view(-1)
         ).view(*R.shape)
         X, Y = Rotate_Cartesian(PA, X, Y)
         return X, Y / q
```

## autophot/models/wedge_model.py

```diff
@@ -34,14 +34,18 @@
 
     def __init__(self, *args, **kwargs):
         self.symmetric_wedges = True
         super().__init__(*args, **kwargs)
         self.wedges = kwargs.get("wedges", 2)
 
     @default_internal
+    def angular_metric(self, X, Y, image=None, parameters=None):
+        return torch.atan2(Y, X)
+
+    @default_internal
     def polar_model(self, R, T, image=None, parameters=None):
         model = torch.zeros_like(R)
         if self.wedges % 2 == 0 and self.symmetric_wedges:
             for w in range(self.wedges):
                 angles = (T - (w * np.pi / self.wedges)) % np.pi
                 indices = torch.logical_or(
                     angles < (np.pi / (2 * self.wedges)),
@@ -72,15 +76,15 @@
                 model[indices] += self.iradial_model(w, R[indices], image, parameters)
         return model
 
     @default_internal
     def evaluate_model(self, X=None, Y=None, image=None, parameters=None, **kwargs):
         if X is None:
             Coords = image.get_coordinate_meshgrid()
-            X, Y = Coords - parameters["center"].value[..., None, None]
+            X, Y = Coords - parameters["center"].value[...,None, None]
         XX, YY = self.transform_coordinates(X, Y, image, parameters)
 
         return self.polar_model(
             self.radius_metric(XX, YY, image=image, parameters=parameters),
             self.angular_metric(XX, YY, image=image, parameters=parameters),
             image=image,
             parameters=parameters,
```

## autophot/parse_config/basic_config.py

```diff
@@ -87,15 +87,15 @@
     model_list = []
     for model in model_info_list:
         model_list.append(AutoPhot_Model(target=target, **model))
 
     MODEL = AutoPhot_Model(
         name="AutoPhot",
         model_type="group model",
-        models=model_list,
+        model_list=model_list,
         target=target,
     )
 
     # Parse Optimize
     ######################################################################
     AP_config.ap_logger.info("Running optimization")
     MODEL.initialize()
```

## autophot/plots/__init__.py

```diff
@@ -1,4 +1,3 @@
 from .profile import *
 from .image import *
 from .visuals import *
-from .diagnostic import *
```

## autophot/plots/image.py

```diff
@@ -13,77 +13,52 @@
 from .visuals import *
 
 
 __all__ = ["target_image", "model_image", "residual_image", "model_window"]
 
 
 def target_image(fig, ax, target, window=None, **kwargs):
-    """
-    This function is used to display a target image using the provided figure and axes.
-
-    Args:
-        fig (matplotlib.figure.Figure): The figure object in which the target image will be displayed.
-        ax (matplotlib.axes.Axes): The axes object on which the target image will be plotted.
-        target (Image or Image_List): The image or list of images to be displayed.
-        window (Window, optional): The window through which the image is viewed. If `None`, the window of the
-            provided `target` is used. Defaults to `None`.
-        **kwargs: Arbitrary keyword arguments.
-
-    Returns:
-        fig (matplotlib.figure.Figure): The figure object containing the displayed target image.
-        ax (matplotlib.axes.Axes): The axes object containing the displayed target image.
-
-    Note:
-        If the `target` is an `Image_List`, this function will recursively call itself for each image in the list.
-        The `window` parameter and `kwargs` are passed unchanged to each recursive call.
-    """
-
     # recursive call for target image list
     if isinstance(target, Image_List):
         for i in range(len(target.image_list)):
             target_image(fig, ax[i], target.image_list[i], window=window, **kwargs)
         return fig, ax
     if window is None:
         window = target.window
     target_area = target[window]
     dat = np.copy(target_area.data.detach().cpu().numpy())
     if target_area.has_mask:
         dat[target_area.mask.detach().cpu().numpy()] = np.nan
     X, Y = target_area.get_coordinate_corner_meshgrid()
-    X = X.detach().cpu().numpy()
-    Y = Y.detach().cpu().numpy()
+
     sky = np.nanmedian(dat)
     noise = iqr(dat[np.isfinite(dat)]) / 2
     vmin = sky - 5 * noise
     vmax = sky + 5 * noise
 
     im = ax.pcolormesh(
-        X,
-        Y,
-        dat,
+        X, Y, dat,
         cmap="Greys",
         norm=ImageNormalize(
             stretch=HistEqStretch(
                 dat[np.logical_and(dat <= (sky + 3 * noise), np.isfinite(dat))]
             ),
             clip=False,
             vmax=sky + 3 * noise,
             vmin=np.nanmin(dat),
         ),
     )
-
+    
     im = ax.pcolormesh(
-        X,
-        Y,
-        np.ma.masked_where(dat < (sky + 3 * noise), dat),
+        X, Y, np.ma.masked_where(dat < (sky + 3 * noise), dat),
         cmap=cmap_grad,
         norm=matplotlib.colors.LogNorm(),
         clim=[sky + 3 * noise, None],
     )
-
+    
     ax.axis("equal")
 
     return fig, ax
 
 
 @torch.no_grad()
 def model_image(
@@ -91,114 +66,56 @@
     ax,
     model,
     sample_image=None,
     window=None,
     target=None,
     showcbar=True,
     target_mask=False,
-    cmap_levels=None,
     **kwargs,
 ):
-    """
-    This function is used to generate a model image and display it using the provided figure and axes.
-
-    Args:
-        fig (matplotlib.figure.Figure): The figure object in which the image will be displayed.
-        ax (matplotlib.axes.Axes): The axes object on which the image will be plotted.
-        model (Model): The model object used to generate a model image if `sample_image` is not provided.
-        sample_image (Image or Image_List, optional): The image or list of images to be displayed.
-            If `None`, a model image is generated using the provided `model`. Defaults to `None`.
-        window (Window, optional): The window through which the image is viewed. If `None`, the window of the
-            provided `model` is used. Defaults to `None`.
-        target (Target, optional): The target or list of targets for the image or image list.
-            If `None`, the target of the `model` is used. Defaults to `None`.
-        showcbar (bool, optional): Whether to show the color bar. Defaults to `True`.
-        target_mask (bool, optional): Whether to apply the mask of the target. If `True` and if the target has a mask,
-            the mask is applied to the image. Defaults to `False`.
-        cmap_levels (int, optional): The number of discrete levels to convert the continuous color map to.
-            If not `None`, the color map is converted to a ListedColormap with the specified number of levels.
-            Defaults to `None`.
-        **kwargs: Arbitrary keyword arguments. These are used to override the default imshow_kwargs.
-
-    Returns:
-        fig (matplotlib.figure.Figure): The figure object containing the displayed image.
-        ax (matplotlib.axes.Axes): The axes object containing the displayed image.
-
-    Note:
-        If the `sample_image` is an `Image_List`, this function will recursively call itself for each image in the list,
-        with the corresponding target and window. The `showcbar` parameter and `kwargs` are passed unchanged to each recursive call.
-    """
-
     if sample_image is None:
         sample_image = model.make_model_image()
         sample_image = model(sample_image)
-
-    # Use model target if not given
     if target is None:
         target = model.target
-
-    # Use model window if not given
     if window is None:
         window = model.window
-
-    # Handle image lists
     if isinstance(sample_image, Image_List):
         for i, images in enumerate(zip(sample_image, target, window)):
             model_image(
                 fig,
                 ax[i],
                 model,
                 sample_image=images[0],
                 window=images[2],
                 target=images[1],
                 showcbar=showcbar,
                 **kwargs,
             )
         return fig, ax
 
-    # Evaluate the model image
     X, Y = sample_image.get_coordinate_corner_meshgrid()
-    X = X.detach().cpu().numpy()
-    Y = Y.detach().cpu().numpy()
     sample_image = sample_image.data.detach().cpu().numpy()
-
-    # Default kwargs for image
     imshow_kwargs = {
         "cmap": cmap_grad,
         "norm": matplotlib.colors.LogNorm(),  # "norm": ImageNormalize(stretch=LogStretch(), clip=False),
     }
-
-    # Update with user provided kwargs
     imshow_kwargs.update(kwargs)
-
-    # if requested, convert the continuous colourmap into discrete levels
-    if cmap_levels is not None:
-        imshow_kwargs["cmap"] = matplotlib.colors.ListedColormap(
-            list(imshow_kwargs["cmap"](c) for c in np.linspace(0.0, 1.0, cmap_levels))
-        )
-
-    # If zeropoint is available, convert to surface brightness units
     if target.zeropoint is not None:
         sample_image = flux_to_sb(
             sample_image, target.pixel_area.item(), target.zeropoint.item()
         )
         del imshow_kwargs["norm"]
         imshow_kwargs["cmap"] = imshow_kwargs["cmap"].reversed()
 
-    # Apply the mask if available
     if target_mask and target.has_mask:
         sample_image[target.mask.detach().cpu().numpy()] = np.nan
 
-    # Plot the image
     im = ax.pcolormesh(X, Y, sample_image, **imshow_kwargs)
-
-    # Enforce equal spacing on x y
     ax.axis("equal")
-
-    # Add a colourbar
     if showcbar:
         if target.zeropoint is not None:
             clb = fig.colorbar(im, ax=ax, label="Surface Brightness [mag/arcsec$^2$]")
             clb.ax.invert_yaxis()
         else:
             clb = fig.colorbar(im, ax=ax, label=f"log$_{{10}}$(flux)")
 
@@ -215,47 +132,14 @@
     showcbar=True,
     window=None,
     center_residuals=False,
     clb_label=None,
     normalize_residuals=False,
     **kwargs,
 ):
-    """
-    This function is used to calculate and display the residuals of a model image with respect to a target image.
-    The residuals are calculated as the difference between the target image and the sample image.
-
-    Args:
-        fig (matplotlib.figure.Figure): The figure object in which the residuals will be displayed.
-        ax (matplotlib.axes.Axes): The axes object on which the residuals will be plotted.
-        model (Model): The model object used to generate a model image if `sample_image` is not provided.
-        target (Target or Image_List, optional): The target or list of targets for the image or image list.
-            If `None`, the target of the `model` is used. Defaults to `None`.
-        sample_image (Image or Image_List, optional): The image or list of images from which residuals will be calculated.
-            If `None`, a model image is generated using the provided `model`. Defaults to `None`.
-        showcbar (bool, optional): Whether to show the color bar. Defaults to `True`.
-        window (Window or Window_List, optional): The window through which the image is viewed. If `None`, the window of the
-            provided `model` is used. Defaults to `None`.
-        center_residuals (bool, optional): Whether to subtract the median of the residuals. If `True`, the median is subtracted
-            from the residuals. Defaults to `False`.
-        clb_label (str, optional): The label for the colorbar. If `None`, a default label is used based on the normalization of the
-            residuals. Defaults to `None`.
-        normalize_residuals (bool, optional): Whether to normalize the residuals. If `True`, residuals are divided by the square root
-            of the variance of the target. Defaults to `False`.
-        **kwargs: Arbitrary keyword arguments. These are used to override the default imshow_kwargs.
-
-    Returns:
-        fig (matplotlib.figure.Figure): The figure object containing the displayed residuals.
-        ax (matplotlib.axes.Axes): The axes object containing the displayed residuals.
-
-    Note:
-        If the `window`, `target`, or `sample_image` are lists, this function will recursively call itself for each element in the list,
-        with the corresponding window, target, and sample image. The `showcbar`, `center_residuals`, and `kwargs` are passed unchanged to
-        each recursive call.
-    """
-
     if window is None:
         window = model.window
     if target is None:
         target = model.target
     if sample_image is None:
         sample_image = model.make_model_image()
         sample_image = model(sample_image)
@@ -271,16 +155,14 @@
                 showcbar=showcbar,
                 center_residuals=center_residuals,
                 **kwargs,
             )
         return fig, ax
 
     X, Y = sample_image[window].get_coordinate_corner_meshgrid()
-    X = X.detach().cpu().numpy()
-    Y = Y.detach().cpu().numpy()
     residuals = (target[window] - sample_image[window]).data
     if normalize_residuals:
         residuals = residuals / torch.sqrt(target[window].variance)
     residuals = residuals.detach().cpu().numpy()
 
     if target.has_mask:
         residuals[target[window].mask.detach().cpu().numpy()] = np.nan
@@ -308,89 +190,61 @@
             im, ax=ax, label=default_label if clb_label is None else clb_label
         )
         clb.ax.set_yticks([])
         clb.ax.set_yticklabels([])
     return fig, ax
 
 
-def model_window(fig, ax, model, target=None, rectangle_linewidth=2, **kwargs):
+def model_window(fig, ax, model, target = None, rectangle_linewidth=2, **kwargs):
     if isinstance(ax, np.ndarray):
         for i, axitem in enumerate(ax):
-            model_window(
-                fig, axitem, model, target=model.target.image_list[i], **kwargs
-            )
+            model_window(fig, axitem, model, target = model.target.image_list[i], **kwargs)
         return fig, ax
 
     if isinstance(model, Group_Model):
         for m in model.models.values():
             if isinstance(m.window, Window_List):
                 use_window = m.window.window_list[m.target.index(target)]
             else:
                 use_window = m.window
-
+                
             lowright = use_window.shape.clone()
-            lowright[1] = 0.0
+            lowright[1] = 0.
             lowright = use_window.origin + use_window.cartesian_to_world(lowright)
-            lowright = lowright.detach().cpu().numpy()
             upleft = use_window.shape.clone()
-            upleft[0] = 0.0
+            upleft[0] = 0.
             upleft = use_window.origin + use_window.cartesian_to_world(upleft)
-            upleft = upleft.detach().cpu().numpy()
             end = use_window.origin + use_window.end
-            end = end.detach().cpu().numpy()
-            x = [
-                use_window.origin[0].detach().cpu().numpy(),
-                lowright[0],
-                end[0],
-                upleft[0],
-            ]
-            y = [
-                use_window.origin[1].detach().cpu().numpy(),
-                lowright[1],
-                end[1],
-                upleft[1],
-            ]
+            x = [use_window.origin[0], lowright[0], end[0], upleft[0]]
+            y = [use_window.origin[1], lowright[1], end[1], upleft[1]]
             ax.add_patch(
                 Polygon(
-                    xy=list(zip(x, y)),
+                    xy=list(zip(x,y)),
                     fill=False,
                     linewidth=rectangle_linewidth,
                     edgecolor=main_pallet["secondary1"],
                 )
             )
     else:
         if isinstance(model.window, Window_List):
             use_window = model.window.window_list[model.target.index(target)]
         else:
             use_window = model.window
         lowright = use_window.shape.clone()
-        lowright[1] = 0.0
+        lowright[1] = 0.
         lowright = use_window.origin + use_window.cartesian_to_world(lowright)
-        lowright = lowright.detach().cpu().numpy()
         upleft = use_window.shape.clone()
-        upleft[0] = 0.0
+        upleft[0] = 0.
         upleft = use_window.origin + use_window.cartesian_to_world(upleft)
-        upleft = upleft.detach().cpu().numpy()
         end = use_window.origin + use_window.end
-        end = end.detach().cpu().numpy()
-        x = [
-            use_window.origin[0].detach().cpu().numpy(),
-            lowright[0],
-            end[0],
-            upleft[0],
-        ]
-        y = [
-            use_window.origin[1].detach().cpu().numpy(),
-            lowright[1],
-            end[1],
-            upleft[1],
-        ]
+        x = [use_window.origin[0], lowright[0], end[0], upleft[0]]
+        y = [use_window.origin[1], lowright[1], end[1], upleft[1]]
         ax.add_patch(
             Polygon(
-                xy=list(zip(x, y)),
+                xy=list(zip(x,y)),
                 fill=False,
                 linewidth=rectangle_linewidth,
                 edgecolor=main_pallet["secondary1"],
             )
         )
 
     return fig, ax
```

## autophot/plots/profile.py

```diff
@@ -104,15 +104,15 @@
     Rbins = [0.0]
     while Rbins[-1] < Rlast_pix:
         Rbins.append(Rbins[-1] + max(2, Rbins[-1] * 0.1))
     Rbins = np.array(Rbins)
 
     with torch.no_grad():
         image = model.target[model.window]
-        X, Y = image.get_coordinate_meshgrid() - model["center"].value[..., None, None]
+        X, Y = image.get_coordinate_meshgrid() - model["center"].value[...,None,None]
         X, Y = model.transform_coordinates(X, Y)
         R = model.radius_metric(X, Y)
         R = R.detach().cpu().numpy()
 
     count, bins, binnum = binned_statistic(
         R.ravel(),
         image.data.detach().cpu().numpy().ravel(),
```

## autophot/plots/visuals.py

```diff
@@ -332,14 +332,15 @@
         [cpoints[i], int(div_list[i][5:7], 16) / 256, int(div_list[i][5:7], 16) / 256]
     )
 cmap_div = LinearSegmentedColormap("cmap_div", div_cdict)
 
 # P = plt.cm.plasma_r
 # C = plt.cm.cividis
 # N = 3
+# print(np.concatenate((C(np.linspace(0,1,N)),np.array(((1,1,1,1),)),P(np.linspace(0,1,N)))))
 # cmap_div = ListedColormap(["#083D77", "#7E886B", "#B9AE65", "#FFFFFF", "#F1B555", "#EE964B", "#F95738"])
 
 # main_pallet = {
 #     "primary1": "g",
 #     "primary2": "r",
 #     "primary3": "b",
 #     "primary4": "ornnge",
```

## autophot/utils/interpolate.py

```diff
@@ -1,15 +1,12 @@
-from functools import lru_cache
-
 import numpy as np
 import torch
 import matplotlib.pyplot as plt
 from astropy.convolution import convolve, convolve_fft
 from torch.nn.functional import conv2d
-
 from .operations import fft_convolve_torch
 
 
 def _h_poly(t):
     """Helper function to compute the 'h' polynomial matrix used in the
     cubic spline.
 
@@ -294,85 +291,7 @@
     return np.array(flux)
 
 
 def interp1d_torch(x_in, y_in, x_out):
     indices = torch.searchsorted(x_in[:-1], x_out) - 1
     weights = (y_in[1:] - y_in[:-1]) / (x_in[1:] - x_in[:-1])
     return y_in[indices] + weights[indices] * (x_out - x_in[indices])
-
-
-def interp2d(
-    im: torch.Tensor,
-    x: torch.Tensor,
-    y: torch.Tensor,
-) -> torch.Tensor:
-    """
-    Interpolates a 2D image at specified coordinates.
-    Similar to `torch.nn.functional.grid_sample` with `align_corners=False`.
-
-    Args:
-        im (Tensor): A 2D tensor representing the image.
-        x (Tensor): A tensor of x coordinates (in pixel space) at which to interpolate.
-        y (Tensor): A tensor of y coordinates (in pixel space) at which to interpolate.
-
-    Returns:
-        Tensor: Tensor with the same shape as `x` and `y` containing the interpolated values.
-    """
-
-    # Convert coordinates to pixel indices
-    h, w = im.shape
-
-    # reshape for indexing purposes
-    start_shape = x.shape
-    x = x.view(-1)
-    y = y.view(-1)
-
-    x0 = x.floor().long()
-    y0 = y.floor().long()
-    x1 = x0 + 1
-    y1 = y0 + 1
-    x0 = x0.clamp(0, w - 2)
-    x1 = x1.clamp(1, w - 1)
-    y0 = y0.clamp(0, h - 2)
-    y1 = y1.clamp(1, h - 1)
-
-    fa = im[y0, x0]
-    fb = im[y1, x0]
-    fc = im[y0, x1]
-    fd = im[y1, x1]
-
-    wa = (x1 - x) * (y1 - y)
-    wb = (x1 - x) * (y - y0)
-    wc = (x - x0) * (y1 - y)
-    wd = (x - x0) * (y - y0)
-
-    result = fa * wa + fb * wb + fc * wc + fd * wd
-
-    return result.view(*start_shape)
-
-
-@lru_cache(maxsize=32)
-def curvature_kernel(dtype, device):
-    kernel = (
-        torch.tensor(
-            [
-                [0.0, 1.0, 0.0],
-                [1.0, -4, 1.0],
-                [0.0, 1.0, 0.0],
-            ],  # [[1., -2.0, 1.], [-2.0, 4, -2.0], [1.0, -2.0, 1.0]],
-            device=device,
-            dtype=dtype,
-        )
-    )
-    return kernel
-
-
-@lru_cache(maxsize=32)
-def simpsons_kernel(dtype, device):
-    kernel = torch.ones(1, 1, 3, 3, dtype=dtype, device=device)
-    kernel[0, 0, 1, 1] = 16.0
-    kernel[0, 0, 1, 0] = 4.0
-    kernel[0, 0, 0, 1] = 4.0
-    kernel[0, 0, 1, 2] = 4.0
-    kernel[0, 0, 2, 1] = 4.0
-    kernel = kernel / 36.0
-    return kernel
```

## autophot/utils/operations.py

```diff
@@ -1,16 +1,14 @@
-from functools import lru_cache
 from typing import Callable, Optional
 
 import torch
 import matplotlib.pyplot as plt
 import numpy as np
 from astropy.convolution import convolve, convolve_fft
 from scipy.fft import next_fast_len
-from scipy.special import roots_legendre
 
 
 def fft_convolve_torch(img, psf, psf_fft=False, img_prepadded=False):
     # Ensure everything is tensor
     img = torch.as_tensor(img)
     psf = torch.as_tensor(psf)
 
@@ -83,163 +81,108 @@
         -(N - 1) / (2 * N), (N - 1) / (2 * N), N, dtype=dtype, device=device
     )
 
 
 def displacement_grid(Nx, Ny, pixelscale=None, dtype=torch.float64, device="cpu"):
     px = displacement_spacing(Nx, dtype=dtype, device=device)
     py = displacement_spacing(Ny, dtype=dtype, device=device)
-    PX, PY = torch.meshgrid(px, py, indexing="xy")
-    return (pixelscale @ torch.stack((PX, PY)).view(2, -1)).reshape((2, *PX.shape))
+    PX, PY = torch.meshgrid(px, py, indexing = "xy")
+    return (pixelscale @ torch.stack((PX, PY)).view(2,-1)).reshape((2, *PX.shape))
 
 
-@lru_cache(maxsize=32)
-def quad_table(n, p, dtype, device):
-    """
-    from: https://pomax.github.io/bezierinfo/legendre-gauss.html
-    """
-    abscissa, weights = roots_legendre(n)
-
-    w = torch.tensor(weights, dtype=dtype, device=device)
-    a = torch.tensor(abscissa, dtype=dtype, device=device)
-    X, Y = torch.meshgrid(a, a, indexing="xy")
-
-    W = torch.outer(w, w) / 4.0
-
-    X, Y = p @ (torch.stack((X, Y)).view(2, -1) / 2.0)
-
-    return X, Y, W.reshape(-1)
-
-
-def single_quad_integrate(
-    X, Y, image_header, eval_brightness, eval_parameters, dtype, device, quad_level=3
+def selective_integrate(
+    X: torch.Tensor,
+    Y: torch.Tensor,
+    data: torch.Tensor,
+    image_header: "Image_Header",
+    eval_brightness: Callable,
+    eval_parameters: "Parameter_Group",
+    max_depth: int = 3,
+    _depth: int = 1,
+    _reference_brightness: Optional[float] = None,
+    integrate_threshold: float = 1e-2,
 ):
+    """Sample the model at higher resolution than the input image.
 
-    # collect gaussian quadrature weights
-    abscissaX, abscissaY, weight = quad_table(
-        quad_level, image_header.pixelscale, dtype, device
-    )
-    # Specify coordinates at which to evaluate function
-    Xs = torch.repeat_interleave(X[..., None], quad_level ** 2, -1) + abscissaX
-    Ys = torch.repeat_interleave(Y[..., None], quad_level ** 2, -1) + abscissaY
-
-    # Evaluate the model at the quadrature points
-    res = eval_brightness(
-        X=Xs,
-        Y=Ys,
-        image=image_header,
-        parameters=eval_parameters,
-    )
-
-    # Reference flux for pixel is simply the mean of the evaluations
-    ref = res[..., (quad_level**2) // 2] #res.mean(axis=-1) # # alternative, use midpoint
-    
-    # Apply the weights and reduce to original pixel space
-    res = (res * weight).sum(axis=-1)
-
-    return res, ref
-
-def grid_integrate(
-    X,
-    Y,
-    image_header,
-    eval_brightness,
-    eval_parameters,
-    dtype,
-    device,
-    quad_level=3,
-    gridding=5,
-    _current_depth=1,
-    max_depth=2,
-    reference=None,
-):
-    """The grid_integrate function performs adaptive quadrature
-    integration over a given pixel grid, offering precision control
-    where it is needed most.
+    This function selectively refines the integration of an input
+    image based on the local curvature of the image data.  It
+    recursively evaluates the model at higher resolutions in areas
+    where the curvature exceeds the specified threshold.  With
+    each level of recursion, the function refines the affected
+    areas using a 3x3 grid for super-resolution.
 
     Args:
-      X (torch.Tensor): A 2D tensor representing the x-coordinates of the grid on which the function will be integrated.
-      Y (torch.Tensor): A 2D tensor representing the y-coordinates of the grid on which the function will be integrated.
-      image_header (ImageHeader): An object containing meta-information about the image.
-      eval_brightness (callable): A function that evaluates the brightness at each grid point. This function should be compatible with PyTorch tensor operations.
-      eval_parameters (Parameter_Group): An object containing parameters that are passed to the eval_brightness function.
-      dtype (torch.dtype): The data type of the output tensor. The dtype argument should be a valid PyTorch data type.
-      device (torch.device): The device on which to perform the computations. The device argument should be a valid PyTorch device.
-      quad_level (int, optional): The initial level of quadrature used in the integration. Defaults to 3.
-      gridding (int, optional): The factor by which the grid is subdivided when the integration error for a pixel is above the allowed threshold. Defaults to 5.
-      _current_depth (int, optional): The current depth level of the grid subdivision. Used for recursive calls to the function. Defaults to 1.
-      max_depth (int, optional): The maximum depth level of grid subdivision. Once this level is reached, no further subdivision is performed. Defaults to 2.
-      reference (torch.Tensor or None, optional): A scalar value that represents the allowed threshold for the integration error. 
+      X (torch.tensor): A tensor representing the X coordinates of the input image.
+      Y (torch.tensor): A tensor representing the Y coordinates of the input image.
+      data (torch.tensor): A tensor containing the input image data.
+      image_header (Image_Header): An instance of the Image_Header class containing the image's header information.
+      eval_brightness (Callable): Function which evaluates the brightness at a given coordinate.
+      _depth (int, optional): The current recursion depth. Default is 1.
+      max_depth (int, optional): The maximum recursion depth allowed. Default is 3.
+      _reference_brightness (float or None, optional): The reference brightness value used to normalize the curvature
+                                                       values. If None, the maximum value of the input data divided by
+                                                       10 will be used. Default is None.
 
     Returns:
-      torch.Tensor: A tensor of the same shape as X and Y that represents the result of the integration on the grid.
-
-    This function operates by first performing a quadrature
-    integration over the given pixels. If the maximum depth level has
-    been reached, it simply returns the result. Otherwise, it
-    calculates the integration error for each pixel and selects those
-    that have an error above the allowed threshold. For pixels that
-    have low error, the result is set as computed. For those with high
-    error, it sets up a finer sampling grid and recursively evaluates
-    the quadrature integration on it. Finally, it integrates the
-    results from the finer sampling grid back to the current
-    resolution.
+        None. The function updates the input data tensor in-place with the selectively integrated values.
 
     """
-
-    # perform quadrature integration on the given pixels
-    res, ref = single_quad_integrate(
-        X,
-        Y,
-        image_header,
-        eval_brightness,
-        eval_parameters,
-        dtype,
-        device,
-        quad_level=quad_level,
+    # check recursion depth, exit if too deep
+    if _depth > max_depth:
+        return
+
+    with torch.no_grad():
+        if _reference_brightness is None:
+            _reference_brightness = torch.max(data) / 10
+        curvature_kernel = torch.tensor(
+            [[0, 1.0, 0], [1.0, -4, 1.0], [0, 1.0, 0]],
+            device=data.device,
+            dtype=data.dtype,
+        )
+        if _depth == 1:
+            curvature = torch.abs(fft_convolve_torch(data, curvature_kernel))
+            curvature[:, 0] = 0
+            curvature[:, -1] = 0
+            curvature[0, :] = 0
+            curvature[-1, :] = 0
+            curvature /= _reference_brightness
+            select = curvature > integrate_threshold
+        else:
+            curvature = (
+                torch.sum(data * curvature_kernel, axis=(1, 2)) / _reference_brightness
+            )
+            select = curvature > integrate_threshold
+            select = select.view(-1, 1, 1).repeat(1, 3, 3)
+
+        # compute the subpixel coordinate shifts for even integration within a pixel
+        shiftsx, shiftsy = displacement_grid(
+            3,
+            3,
+            pixelscale=image_header.pixelscale,
+            device=data.device,
+            dtype=data.dtype,
+        )
+
+    # Reshape coordinates to add two dimensions with the super-resolved coordiantes
+    Xs = X[select].view(-1, 1, 1).repeat(1, 3, 3) + shiftsx
+    Ys = Y[select].view(-1, 1, 1).repeat(1, 3, 3) + shiftsy
+    # evaluate the model on the new smaller coordinate grid in each pixel
+    res = eval_brightness(
+        image=image_header.super_resolve(3), parameters=eval_parameters, X=Xs, Y=Ys
     )
 
-    # if the max depth is reached, simply return the integrated pixels
-    if _current_depth >= max_depth:
-        return res
-
-    # Begin integral
-    integral = torch.zeros_like(X)
-
-    # Select pixels which have errors above the allowed threshold
-    select = torch.abs((res - ref)) > reference
-
-    # For pixels with low error, set the results as computed
-    integral[torch.logical_not(select)] = res[torch.logical_not(select)]
-
-    # Set up sub-gridding to super resolve problem pixels
-    stepx, stepy = displacement_grid(
-        gridding, gridding, image_header.pixelscale, dtype, device
-    )
-    # Write out the coordinates for the super resolved pixels
-    subgridX = torch.repeat_interleave(
-        X[select].unsqueeze(-1), gridding ** 2, -1
-    ) + stepx.reshape(-1)
-    subgridY = torch.repeat_interleave(
-        Y[select].unsqueeze(-1), gridding ** 2, -1
-    ) + stepy.reshape(-1)
-
-    # Recursively evaluate the quadrature integration on the finer sampling grid
-    subgridres = grid_integrate(
-        subgridX,
-        subgridY,
-        image_header.super_resolve(gridding),
-        eval_brightness,
-        eval_parameters,
-        dtype,
-        device,
-        quad_level=quad_level+2,
-        gridding=gridding,
-        _current_depth=_current_depth+1,
+    # Apply recursion to integrate any further pixels as needed
+    selective_integrate(
+        X=Xs,
+        Y=Ys,
+        data=res,
+        image_header=image_header.super_resolve(3),
+        eval_brightness=eval_brightness,
+        eval_parameters=eval_parameters,
+        _depth=_depth + 1,
         max_depth=max_depth,
-        reference=reference * gridding**2,        
+        _reference_brightness=_reference_brightness,
+        integrate_threshold=integrate_threshold,
     )
 
-    # Integrate the finer sampling grid back to current resolution
-    integral[select] = subgridres.sum(axis=(-1,))
-
-    return integral
-    
+    # Update the pixels with the new integrated values
+    data[select] = res.sum(axis=(1, 2))
```

## autophot/utils/optimization.py

```diff
@@ -1,9 +1,8 @@
 import torch
-
 from .. import AP_config
 
 
 def chi_squared(target, model, mask=None, variance=None):
     if mask is None:
         if variance is None:
             return torch.sum((target - model) ** 2)
```

## autophot/utils/parametric_profiles.py

```diff
@@ -11,15 +11,17 @@
     Parameters:
         R: Radii tensor at which to evaluate the sersic function
         n: sersic index restricted to n > 0.36
         Re: Effective radius in the same units as R
         Ie: Effective surface density
     """
     bn = sersic_n_to_b(n)
-    return Ie * torch.exp(-bn * (torch.pow(R / Re, 1 / n) - 1))
+    return Ie * torch.exp(
+        -bn * (torch.pow((R + 1e-8) / Re, 1 / n) - 1)
+    )  # epsilon added for numerical stability of gradient
 
 
 def sersic_np(R, n, Re, Ie):
     """Sersic 1d profile function, works more generally with numpy
     operations. In the event that impossible values are passed to the
     function it returns large values to guide optimizers away from
     such values.
@@ -125,19 +127,20 @@
         Ib: brightness at the scale length, represented as the log of the brightness divided by pixel scale squared.
         Rb: scale length radius
         alpha: sharpness of transition between power law slopes
         beta: outer power law slope
         gamma: inner power law slope
 
     """
+    Rplus = R + 1e-8  # added for numerical stability near R = 0
     return (
         Ib
         * (2 ** ((beta - gamma) / alpha))
-        * ((R / Rb) ** (-gamma))
-        * ((1 + (R / Rb) ** alpha) ** ((gamma - beta) / alpha))
+        * ((Rplus / Rb) ** (-gamma))
+        * ((1 + (Rplus / Rb) ** alpha) ** ((gamma - beta) / alpha))
     )
 
 
 def nuker_np(R, Rb, Ib, alpha, beta, gamma):
     """Nuker 1d profile function, works with numpy functions
 
     Parameters:
@@ -153,29 +156,30 @@
         Ib
         * (2 ** ((beta - gamma) / alpha))
         * ((R / Rb) ** (-gamma))
         * ((1 + (R / Rb) ** alpha) ** ((gamma - beta) / alpha))
     )
 
 
-def spline_torch(R, profR, profI, extend):
+def spline_torch(R, profR, profI, pixelscale2, extend):
     """Spline 1d profile function, cubic spline between points up
     to second last point beyond which is linear, specifically designed
     for pytorch.
 
     Parameters:
         R: Radii tensor at which to evaluate the sersic function
         profR: radius values for the surface density profile in the same units as R
         profI: surface density values for the surface density profile
+        pixelscale2: squared pixelscale. Just use 1 if not needed
     """
     I = cubic_spline_torch(profR, profI, R.view(-1), extend="none").view(*R.shape)
     res = torch.zeros_like(I)
     res[R <= profR[-1]] = 10 ** (I[R <= profR[-1]])
     if extend:
         res[R > profR[-1]] = 10 ** (
             profI[-2]
             + (R[R > profR[-1]] - profR[-2])
             * ((profI[-1] - profI[-2]) / (profR[-1] - profR[-2]))
         )
     else:
         res[R > profR[-1]] = 0
-    return res
+    return res * pixelscale2
```

## autophot/utils/conversions/functions.py

```diff
@@ -59,14 +59,15 @@
         * R ** 2
         * q
         * n
         * (torch.exp(bn) * bn ** (-2 * n))
         * torch.exp(gammaln(2 * n))
     )
 
+
 def sersic_flux_to_Ie_torch(flux, n, R, q):
     bn = sersic_n_to_b(n)
     return flux / (
         2
         * np.pi
         * R ** 2
         * q
@@ -75,30 +76,7 @@
         * torch.exp(gammaln(2 * n))
     )
 
 
 def sersic_inv_torch(I, n, Re, Ie):
     bn = sersic_n_to_b(n)
     return Re * ((1 - (1 / bn) * torch.log(I / Ie)) ** (n))
-
-def moffat_I0_to_flux(I0, n, rd, q):
-    return I0 * np.pi * rd**2 * q / (n - 1)
-
-def general_uncertainty_prop(
-        param_tuple, #tuple of parameter values
-        param_err_tuple, # tuple of parameter uncertainties
-        forward # forward function through which to get uncertainty
-):
-    # Make a new set of parameters which track uncertainty
-    new_params = []
-    for p in param_tuple:
-        newp = p.detach()
-        newp.requires_grad = True
-        new_params.append(newp)
-    # propogate forward and compute derivatives
-    f = forward(*new_params)
-    f.backward()
-    # Add all the error contributions in quadrature
-    x = torch.zeros_like(f)
-    for i in range(len(new_params)):
-        x = x + (new_params[i].grad * param_err_tuple[i])**2
-    return x.sqrt()
```

## autophot/utils/initialize/initialize.py

```diff
@@ -19,16 +19,16 @@
 
     if q is None:
         q = 1.0
 
     if R is None:
         # Determine basic threshold if none given
         if threshold is None:
-            threshold = np.nanmedian(image) + 3 * iqr(image[np.isfinite(image)], rng=(16, 84)) / 2
-            
+            threshold = np.median(image) + 3 * iqr(image, rng=(16, 84)) / 2
+
         # Sample growing isophotes until threshold is reached
         ellipse_radii = [1.0]
         while ellipse_radii[-1] < (max(image.shape) / 2):
             ellipse_radii.append(ellipse_radii[-1] * (1 + 0.2))
             isovals = _iso_extract(
                 image,
                 ellipse_radii[-1],
@@ -43,28 +43,28 @@
             )
             if len(isovals) < 3:
                 continue
             # Stop when at 3 time background noise
             if (np.quantile(isovals, 0.8) < threshold) and len(ellipse_radii) > 4:
                 break
         R = ellipse_radii[-1]
-        
+
     # Determine which radii to sample based on input R, pa, and q
     if isinstance(pa, float) and isinstance(q, float) and isinstance(R, float):
         if n_isophotes == 1:
             isophote_radii = [R]
         else:
             isophote_radii = np.linspace(0, R, n_isophotes)
     elif hasattr(R, "__len__"):
         isophote_radii = R
     elif hasattr(pa, "__len__"):
         isophote_radii = np.ones(len(pa)) * R
     elif hasattr(q, "__len__"):
         isophote_radii = np.ones(len(q)) * R
-        
+
     # Sample the requested isophotes and record desired info
     iso_info = []
     for i, r in enumerate(isophote_radii):
         iso_info.append({"R": r})
         isovals = _iso_extract(
             image,
             r,
```

## autophot/utils/initialize/segmentation_map.py

```diff
@@ -49,15 +49,15 @@
         else:
             raise ValueError(
                 f"unrecognized file type, should be one of: fits, npy\n{image}"
             )
 
     centroids = {}
 
-    XX, YY = np.meshgrid(np.arange(seg_map.shape[1]), np.arange(seg_map.shape[0]))
+    XX, YY = np.meshgrid(np.arange(seg_map.shape[0]), np.arange(seg_map.shape[1]))
 
     for index in np.unique(seg_map):
         if index is None or index in skip_index:
             continue
         N = seg_map == index
         xcentroid = np.sum(XX[N] * image[N]) / np.sum(image[N])
         ycentroid = np.sum(YY[N] * image[N]) / np.sum(image[N])
```

## Comparing `autophot-0.10.4.dist-info/LICENSE` & `autophot-0.9.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `autophot-0.10.4.dist-info/METADATA` & `autophot-0.9.0.dist-info/METADATA`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: autophot
-Version: 0.10.4
+Version: 0.9.0
 Summary: A fast, flexible, differentiable, and automated astronomical image modelling tool for precise parallel multi-wavelength photometry
 Home-page: https://github.com/Autostronomy/AutoPhot
 Author: Connor Stone
 Author-email: connorstone628@gmail.com
 License: GPL-3.0 license
 Classifier: Development Status :: 1 - Planning
 Classifier: Intended Audience :: Science/Research
@@ -27,15 +27,15 @@
   <source media="(prefers-color-scheme: dark)" srcset="media/AP_logo_white.png">
   <source media="(prefers-color-scheme: light)" srcset="media/AP_logo.png">
   <img alt="AutoPhot logo" src="media/AP_logo.png" width="70%">
 </picture>
 
 
 [![unittests](https://github.com/Autostronomy/AutoPhot/actions/workflows/testing.yaml/badge.svg?branch=main)](https://github.com/Autostronomy/AutoPhot/actions/workflows/testing.yaml)
-[![docs](https://github.com/Autostronomy/AutoPhot/actions/workflows/documentation.yaml/badge.svg?branch=main)](https://autostronomy.github.io/AutoPhot/)
+[![docs](https://github.com/Autostronomy/AutoPhot/actions/workflows/documentation.yaml/badge.svg?branch=main)](https://connorstoneastro.github.io/AutoPhot/)
 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
 [![pypi](https://img.shields.io/pypi/v/autophot.svg?logo=pypi&logoColor=white&label=PyPI)](https://pypi.org/project/autophot/)
 [![downloads](https://img.shields.io/pypi/dm/autophot?label=PyPI%20Downloads)](https://libraries.io/pypi/autophot)
 [![codecov](https://img.shields.io/codecov/c/github/Autostronomy/AutoPhot?logo=codecov)](https://app.codecov.io/gh/Autostronomy/AutoPhot?search=&displayType=list)
 
 AutoPhot is a fast, flexible, and automated astronomical image modelling tool for precise parallel multi-wavelength photometry. It is a python based package that uses PyTorch to quickly and efficiently perform analysis tasks. Written by [Connor Stone](https://connorjstone.com/) for tasks such as LSB imaging, handling crowded fields, multi-band photometry, and analyzing massive data from future telescopes. AutoPhot is flexible and fast for any astronomical image modelling task. While it uses PyTorch (originally developed for Machine Learning) it is NOT a machine learning based tool.
 
@@ -47,16 +47,16 @@
 pip install autophot
 ```
 
 If PyTorch gives you any trouble on your system, just follow the instructions on the [pytorch website](https://pytorch.org/) to install a version for your system.
 
 Also note that AutoPhot is only available for python3.
 
-See [the documentation](https://autostronomy.github.io/AutoPhot/) for more details.
+See [the documentation](https://connorstoneastro.github.io/AutoPhot/) for more details.
 
 ## Documentation
 
-You can find the documentation at the [GitHub Pages site connected with the AutoPhot project](https://autostronomy.github.io/AutoPhot/) which covers many of the main use cases for AutoPhot. It is still in development, but lots of useful information is there. Feel free to contact the author, [Connor Stone](https://connorjstone.com/), for any questions not answered by the documentation or tutorials.
+You can find the documentation at the [GitHub Pages site connected with the AutoPhot project](https://connorstoneastro.github.io/AutoPhot/) which covers many of the main use cases for AutoPhot. It is still in development, but lots of useful information is there. Feel free to contact the author, [Connor Stone](https://connorjstone.com/), for any questions not answered by the documentation or tutorials.
 
 ## Credit / Citation
 
-If you use AutoPhot in your research, please follow the [citation instructions here](https://autostronomy.github.io/AutoPhot/citation.html). A new paper for the updated AutoPhot code is in the works.
+If you use AutoPhot in your research, please follow the [citation instructions here](https://connorstoneastro.github.io/AutoPhot/citation.html). A new paper for the updated AutoPhot code is in the works.
```

## Comparing `autophot-0.10.4.dist-info/RECORD` & `autophot-0.9.0.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,87 +1,83 @@
-autophot/AP_config.py,sha256=kHnBU3kbJ8EVggNPqRhlOmx9n5SXtr7cUcSvEopHvY4,3186
-autophot/__init__.py,sha256=q68HcSDHv2TM9PB3NIR7MOSp5g8IKz8i6qMdWefgLzs,5720
+autophot/AP_config.py,sha256=554fT-07-OZFFxoi7-EDEvY90JSbC7zE7eos5Ilyf_Y,3171
+autophot/__init__.py,sha256=ychDtM1pOWWXxJEh3EPjokIkTwngdq6FwjinaT00Pk0,5719
 autophot/__main__.py,sha256=Ce_QoW2_-x3o3ZXbErfBFzJ_M20JzoaB3dUvwzAhXx4,281
-autophot/fit/__init__.py,sha256=boNrIHYF2zcQMmaC-kySRItUdeO_Q1xtZbLP8j_Etf8,1210
+autophot/fit/__init__.py,sha256=lIizX8FXOMdBIRzmnL9dCbJZcOdnHSYw6e8Hgfay9Sk,1092
 autophot/fit/base.py,sha256=rluDW6eyCKhHLWSyHULizd74yPXd6LqoXrGUASMvQgs,6406
 autophot/fit/gp.py,sha256=PvMC6LeAIYWwDteVVo3AY7lb_TkQOY2-C5yWfK4CpUY,30
 autophot/fit/gradient.py,sha256=DyfzqK6I5LUV2LcdFHFDrr3C77WmMtxg9Pzf--2bZV0,6961
 autophot/fit/hmc.py,sha256=5uOtmmV6lz0ue06v6PTLVFALCbyfUlGR-dC1RCmcgX8,6939
-autophot/fit/iterative.py,sha256=PPWLFrKlZRIyaoHUVyaa0P8h2JrTmcdxvh5m2A1hA-A,13710
-autophot/fit/lm.py,sha256=MPb_UsYklNV8gvB1yuVWmr906n4W1Bn9Jvp2_r5T0Q0,18661
+autophot/fit/iterative.py,sha256=vtbJYCXBkz9pfaud-kUIVIOlGZEWO09GStLGmoCyl44,13401
+autophot/fit/lm.py,sha256=T3pMHayI7FyEZ9frEmRP2rtnpcrRyxKKVA_PqiNKQeA,31419
 autophot/fit/mhmcmc.py,sha256=qOrvDuHQF2oteCXGy2yYhHRr3xqkvBSq6D2jWItTfWE,4381
-autophot/fit/nuts.py,sha256=VE-oId_3qrjL-AYz1DFXngN_SxIL9L8tht2rGR9Puxo,7150
-autophot/fit/oldlm.py,sha256=johFavfi5ycfSKiYGrD1SYmIzNcgvJXFrN20GZo2ZoY,28502
+autophot/fit/nuts.py,sha256=o-i4mbfe1wGYFA-Zl79ORhMhrqJ9RWwvIB9pjMameZM,7150
 autophot/image/__init__.py,sha256=UPRYMGXs_WUIEkyoDh0467vU495wES6JOLkL9kFKMP8,1150
-autophot/image/image_header.py,sha256=Xley8mwPEPp0nwqBMQdnt6uu1zrtYIxdI8L9RRHZ5pA,18011
-autophot/image/image_object.py,sha256=o4Ivrd2CCxMtlW-HTcaZfHOH_Q0wp-MZppjwBcJQqfc,20710
-autophot/image/jacobian_image.py,sha256=ZNJMOEV0IK-1m6BQPI1cp8q6mrsUXpiESbYmPQg9IEA,5514
-autophot/image/model_image.py,sha256=vFeOMBoAsPO1lfGMnruzWz0lgZA0e0Kpn2cdMX6Yps8,6411
-autophot/image/psf_image.py,sha256=TUysUYtt7BE7abijJn32YKxnsYPYfmsFGzpNO32H3jo,6629
-autophot/image/target_image.py,sha256=_k6h2Pi2cw4UvddxObQ-mGzX5EI5xnxqw11wkfoA4Jk,14446
-autophot/image/window_object.py,sha256=N--CVJshSA7J5t9DXfTz-UUJ_DxER7_XBxQadT_HDVs,24450
-autophot/models/__init__.py,sha256=jHHdx1joNcv-JlXVqZnP9es6vh0DxBCkNV51cEa7jqE,745
-autophot/models/_model_methods.py,sha256=Jt6NmTQKwHiCRwzjEM19kmEZY6cRlW6Ca9SD4c6sqio,8292
-autophot/models/_shared_methods.py,sha256=KX62Reg5ZVwUy2TcHV63TI-8XgFWmHGjb00dRS-12SI,22590
-autophot/models/core_model.py,sha256=XKpa__w6PaSXqM_-GbQcWc4WdwIV-1u176qt6Ubp5Os,13125
-autophot/models/edgeon_model.py,sha256=bqi0EmE-sGeBuY0BUSzChCXA1AygBjfpMwijhMOjecU,7177
-autophot/models/exponential_model.py,sha256=fvBWjZeBGcZTEg6Zx63TIcr79bt7Y-cVGjkl3RARCFg,14412
-autophot/models/flatsky_model.py,sha256=BqV7GynKwnTXg1-In7ywQc62sew5Jh3fKBaMS5Jm7M4,2047
+autophot/image/image_header.py,sha256=Khpcf203ERINVe4CETkdbkHg-agCFsb0cjD5HGJh5Uw,16790
+autophot/image/image_object.py,sha256=TlyOtyNBqJD0H6MIjvS6PvMS5AgXBUfyxgGjpy0RAxs,20545
+autophot/image/jacobian_image.py,sha256=1s3eAq3xB353i_srThMZVaJ7Wp-aXu0POLfvZ1JQSgg,5269
+autophot/image/model_image.py,sha256=geyEf-h3-8JHZY0ndT_iukaEpp5aS9uzacuKBKASa1M,6379
+autophot/image/psf_image.py,sha256=4fyUQ6PX7FVCs0vqf8kavu4I6SK4Rcp1d4Zenf26ltM,6791
+autophot/image/target_image.py,sha256=RR--6U6VULnGB3RnDEQuRuA39i-ECr-lOaJ_XLewO9M,14763
+autophot/image/window_object.py,sha256=FGMijIWaUW2bn67lcDZ_GVZfyHFfFt_ZgGexUIueGrs,23813
+autophot/models/__init__.py,sha256=V5ziTIawNay5GoqHIz-x1rnIOZqARe5VmMyFPCWLOR8,654
+autophot/models/_model_methods.py,sha256=ishhSsylSqw0YElOPDAiN8x-eL1-9_0RSxV_-M-irk0,1742
+autophot/models/_shared_methods.py,sha256=3OoOaBR2k4HBg6kto0ExfZMZk8B0XYfGSzztD0rV98Y,19240
+autophot/models/core_model.py,sha256=wBfrXDscC7GJYbc523ENyBlK6viLynY8Uih-snXQwG0,12502
+autophot/models/edgeon_model.py,sha256=0gssPwcdvQX_XaYzJej7-6eVyqQnj0Yq1e7foK5IMX8,6850
+autophot/models/exponential_model.py,sha256=H0aX1p-6VrYNNiUPbuBVmcjkzfm4ycTcJvZQdV3KpOs,14419
+autophot/models/flatsky_model.py,sha256=07MDrRaPY693XbwL93wv_9UVakEH1O3c0FAOf5Cta8g,2108
 autophot/models/foureirellipse_model.py,sha256=3EIwbBnOA4pEk3XuTMbtcWM73pPc0VbPnvonZk8Lhmk,10650
-autophot/models/galaxy_model_object.py,sha256=LWzaa9hrd5YEklRWGJNfeS7v4dRAofKqGbBZYNcDqZ8,5249
-autophot/models/gaussian_model.py,sha256=NF4FNcls6O-4qSAlvQ5L05t3U4BC41RSgZfeaD1P-lc,13247
-autophot/models/group_model_object.py,sha256=uw9QO0qFonHC8at4gQLblU4yrp1sF1hIZxdUneU0Qkw,10888
-autophot/models/model_object.py,sha256=ms-4bWAVAvfZzPadFUoBudAMhPlha6fd6nCuQZGQiqU,25903
-autophot/models/moffat_model.py,sha256=fPUT8YE2tMesqtXVDWUuxLFJy_y8S0ThR6jyi-lldco,5755
-autophot/models/nuker_model.py,sha256=HwHdbpYL8UpIpvFq9Dd5F2ZmGhHtvqir17AUHovvdaQ,18952
-autophot/models/parameter_group.py,sha256=epTRcuL0rTj553A2lcnzqzUX_7doqGvDl-zX-lTkf7U,16039
-autophot/models/parameter_object.py,sha256=iOQYddscllmLNqQctQjokilkls-TZ2srlMEP1uMFgTs,18866
-autophot/models/planesky_model.py,sha256=G2W0-e1JRRunWKLbZGBFkS2L2iIivefPJxvHP1etYsg,2570
-autophot/models/psf_model.py,sha256=eoTZkbxIwHHd1fR2UF16SwFnBb1Hx9IhQq0ImtQyOlk,3889
-autophot/models/ray_model.py,sha256=wxey5Q85bHOKxH-d8k_eCO3hLlAzJfPr8jpTcdp5f4Q,4878
-autophot/models/relspline_model.py,sha256=cT0i6sRx4lIwja7syD8mNsDA-hyDq-Oxu-0iCF85Xow,3178
-autophot/models/sersic_model.py,sha256=Ym7zFjcitANeP75Qi0cWGsOj0VTnrwSqpgSPiohkVT8,18049
+autophot/models/galaxy_model_object.py,sha256=PbykT7MWQJ7zYqA3jLDMWuaHeOiDoi8WAcZ_TyR0lXs,5306
+autophot/models/gaussian_model.py,sha256=w7ZwwNRJyUSK-n13cCO_ZFcrLj64Tb_2E0_90DqYogg,13246
+autophot/models/group_model_object.py,sha256=t5hhDwM-29I5DNPxEQ1nHeMdjDf54DC4qeMpU6uQxXU,11139
+autophot/models/model_object.py,sha256=iB425DUNRj2OJRCL4JlM8sx9uoQYqGu8PSGjCZQpAzM,23870
+autophot/models/moffat_model.py,sha256=NCHcx5MmHUdj2k4koAgvYE62MEO-DbKLhBwHNHbDb2E,3904
+autophot/models/nuker_model.py,sha256=vKNpY5rbSG7u27iC2zsVN3e07Z453ljT1NeadL8zDvE,18951
+autophot/models/parameter_group.py,sha256=ctOQfhfUUiACOimNynmweJW4ZoLVmnjdVsyLerPJkUc,12454
+autophot/models/parameter_object.py,sha256=3bxOeP7MqMkh27_fBz9SMxMQD5dA9ET-V457cdu3xLc,18097
+autophot/models/planesky_model.py,sha256=lEDwDru7LUJaGPMbqcZwoEcD4S5_nRJGsy9pzJkpyKA,2450
+autophot/models/psf_model.py,sha256=HmL0qOdKqcgc_DZHB0Y1X8agQhe6bPx1efrpfJSC3iw,3719
+autophot/models/ray_model.py,sha256=tJCf9J8mHLH48natMp-fIBictTLUYm8UJAwrzsla5WE,4998
+autophot/models/sersic_model.py,sha256=RkdYJtzzz47cuyGv7YTtujeUjKb3a3O1C2J2KvOmQkU,16161
 autophot/models/sky_model_object.py,sha256=WD450x05pnwMDOzDo5yhQfqYXAHgE8QPkJLVyKJeFGQ,875
-autophot/models/spline_model.py,sha256=7rrOir-0p4liIbH6S_M9i_WEpYXoA8S-JozkQtNqyNQ,10912
-autophot/models/star_model_object.py,sha256=-39H17zPOEQV3zMsv-qp7p4Ixk2sz-ihuRFXpf59Avg,838
-autophot/models/superellipse_model.py,sha256=VHNSliFyvErksbMDYVSHg4chPIlvEyxse3bGDFicNRs,3178
-autophot/models/warp_model.py,sha256=xaCbSyGi2vkEc58VF87XePFxW3k6EYAsybQoSNb_TGo,4783
-autophot/models/wedge_model.py,sha256=m6ieDtGbLy5oXvlTMlbuVACHJzFqkxQG8yqXQQnW67s,3861
-autophot/models/zernike_model.py,sha256=PDp-QAfxX3_K_hnGEjsKBGPydzNJG31XqWtzQxXa4ho,4102
+autophot/models/spline_model.py,sha256=oqhe_37BVZh3EAmsMFMtuTt5A8FR8AXuMomPE1tx-S8,10695
+autophot/models/star_model_object.py,sha256=6KSF_AZ-Ahj3JnWPC7Flihp0VAiZj9TMD_qd9I555Gs,1226
+autophot/models/superellipse_model.py,sha256=z5tDajtolns9PQR2lrH_zg3hgHtrNuneTZny7IzpwGQ,3259
+autophot/models/warp_model.py,sha256=V0LYcbhlv3EK6DnmpGxfj8Dpb_cQzEAv6f8rfHDY4Ts,4741
+autophot/models/wedge_model.py,sha256=9YYusEhdM5I5M4VruJV2HHZNMl4ve3pylxVMexycG8w,3981
 autophot/parse_config/__init__.py,sha256=CT6gEcILfcEdz3I5nbgqKeno8tyGZsMtHmfXQUWejUA,57
-autophot/parse_config/basic_config.py,sha256=DpjP989i6iLpOYkTWilcgM5PQ6ivKq5ooZ7R-ms5vws,4143
+autophot/parse_config/basic_config.py,sha256=fwFhrdMxAeXYK_z0_gkjeiK6xKjetcsm0bV78o7-W6w,4147
 autophot/parse_config/galfit_config.py,sha256=D3MHk-doLnKTE_NMc1CgGEWnFGBM-vLF6m4ozVJGKls,4590
 autophot/parse_config/shared_methods.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-autophot/plots/__init__.py,sha256=h_6hXhmoiSdwiH62GLsNTfnL5z5kAI6NgcBLSqqtrSQ,93
-autophot/plots/diagnostic.py,sha256=IZy4Ix5pnT9ylmtsKoHQjVFfryp8NWmJmJBP8SP1xGU,3174
-autophot/plots/image.py,sha256=_l6ohqEadFfu_LmUWjIPOuD9lelKAJJZ8YX3AZGsbmo,15086
-autophot/plots/profile.py,sha256=DfGzTTSVnD9Jubxg2pJT8hnStEBloIUYaQrt7NOzrS4,7558
+autophot/plots/__init__.py,sha256=p8lf0eFHXeZ7-3s0SHRb3zIH1pR6x_jnMxIPUHaEPls,67
+autophot/plots/image.py,sha256=lU7-dd2FoPtIRJEBnc0Eo3VfIQIRoACjbsQhx0KlNBc,8074
+autophot/plots/profile.py,sha256=Ux35_Rg5RNb4qeZ5ADxJM60LBWw5OK1FmnLHxVtb0Ww,7556
 autophot/plots/shared_elements.py,sha256=-9cuoMXOSZh7uB32Fkn2R5wCFecNU3WIGvVrjmP8ESY,3048
-autophot/plots/visuals.py,sha256=Qi7mUkhnH-3MkMPHqTZVbuDQ-JRDzFB4tc2vu-JKJak,20928
+autophot/plots/visuals.py,sha256=hZyvNt3wY8rbZOXEtFM6nSkGPili7Mm-4RSd1fnZqMw,21022
 autophot/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autophot/utils/angle_operations.py,sha256=oJp9g9v5v7SGzuDRXILg7B0d2CcXygT3GXAzoUbtPGk,800
 autophot/utils/decorators.py,sha256=Jr7PTg_RO2IMTDy7foPjCNXMoDsWPTReO_VAlO76_GA,976
-autophot/utils/interpolate.py,sha256=9snjQXHDNHOFqQiQc_TegkImJikb-pwhd7JtE1xq6vE,11781
-autophot/utils/operations.py,sha256=QFeIAcLZQoinhFOmuytfhMNOErrdDTVLF2yh5TlGLNI,8668
-autophot/utils/optimization.py,sha256=sCQ7DDH7KNeO71QgiehI2IVAsBGF1BqT7GtW86gyW18,964
-autophot/utils/parametric_profiles.py,sha256=EYMKuapPXAX2VS5p5NIQB7KFn5vyWplMisT2VS5r54A,5748
+autophot/utils/interpolate.py,sha256=pKldYZuyTnCHWMVkQqWAYSlmz92_Iv4myOKEncVa8MI,9815
+autophot/utils/operations.py,sha256=ulu8Tlms4S5UjfMBX8nZ1yQdjhhtHoAOSCf6C9_oUeU,6728
+autophot/utils/optimization.py,sha256=VkAusKnyc4zyztbceazKADVy85g6VV7qqYIw6V7cQos,963
+autophot/utils/parametric_profiles.py,sha256=JY6reeVkATmuPkhKY7cWPgrnpZaM--simb-8mS2xh1A,5990
 autophot/utils/conversions/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autophot/utils/conversions/coordinates.py,sha256=X-5K8_3uYj5MFVK9KsuYFJ18iwA-AGCcmqsaojMnO_c,1840
 autophot/utils/conversions/dict_to_hdf5.py,sha256=XIy_JvX8Nrp-9N3sk5F3gIY2YLX5I88HxEYWbPGbfsU,1095
-autophot/utils/conversions/functions.py,sha256=5cZK4QA0LMiK5hT_0CV7CJvpyPuh-iNfmZWcjKlxjsM,2609
+autophot/utils/conversions/functions.py,sha256=YEKL5WAZvBQkR_P0DK255SIYtJoRzQmnbcEoCbweHIs,1829
 autophot/utils/conversions/optimization.py,sha256=SpxOQyC1uoaoMhn4U1V9nOrdAdyUWR59LptQ77uVpEY,3260
 autophot/utils/conversions/units.py,sha256=hqJMpEnoE6PqKNaas5qazTkzsNsGUZgg_P3X0lp-xKQ,2539
 autophot/utils/initialize/__init__.py,sha256=3loeT7k8s7wtWy_RZvGgTyxnlLKCe3AzpPUBI995Qz0,281
 autophot/utils/initialize/center.py,sha256=PiL0-oXOzY-J3IUoK-bJy_w1Fv7LZZwdEcN7-kRJcZM,3103
 autophot/utils/initialize/construct_psf.py,sha256=B79DqCOfW0rkhHlmNZ9P-lC0oH3JjMsn-A88yNYbw4c,4542
-autophot/utils/initialize/initialize.py,sha256=F1dzoZSji3kQmNreuXqyrJrlf2OLN-eeCNMc4QR3gFo,3972
-autophot/utils/initialize/segmentation_map.py,sha256=b5lRHsCH46pjYDpX_j6txD6egGuB1OLWe6eQQcBZQ_M,7036
+autophot/utils/initialize/initialize.py,sha256=V1Epy8vudmquOxSB5S-moCcS7bvjdr5qze6IcqihsSs,3921
+autophot/utils/initialize/segmentation_map.py,sha256=6fG5U4y5P6MDnX4_6ROBdRhUq6TxgbwVG1V3MaLf7-c,7036
 autophot/utils/isophote/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autophot/utils/isophote/ellipse.py,sha256=p2SGzU067jBIBrKjhYKpnJQF7MSThMcnLajsaXeJ23k,1085
 autophot/utils/isophote/extract.py,sha256=ousarHmkj7GrgAZ6mO3G-QY0tXjhd8bBh6lHzb--d6U,8531
 autophot/utils/isophote/integrate.py,sha256=jNOCbSYC1dZO1pasEMcRUqZCpt43DEPVME4Hsa37DKQ,7012
-autophot-0.10.4.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
-autophot-0.10.4.dist-info/METADATA,sha256=j41bX9ZJqF00vjOGUiFsgFo0S9Ou4MOJbvLL36ot8t0,3557
-autophot-0.10.4.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
-autophot-0.10.4.dist-info/entry_points.txt,sha256=Rqzg1Bj83DI-80fEIqW6zkfKdKTCVN2Y5RbCoHQDU9A,56
-autophot-0.10.4.dist-info/top_level.txt,sha256=Ls8-8keele8x-yodnyQ9iPEF46A_7jDzX0pCF8yDu7o,9
-autophot-0.10.4.dist-info/RECORD,,
+autophot-0.9.0.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
+autophot-0.9.0.dist-info/METADATA,sha256=KA_ErnIVWdrKbblvnk8wfxc4k3-bpTu3kEaEXkR0C4o,3572
+autophot-0.9.0.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+autophot-0.9.0.dist-info/entry_points.txt,sha256=Rqzg1Bj83DI-80fEIqW6zkfKdKTCVN2Y5RbCoHQDU9A,56
+autophot-0.9.0.dist-info/top_level.txt,sha256=Ls8-8keele8x-yodnyQ9iPEF46A_7jDzX0pCF8yDu7o,9
+autophot-0.9.0.dist-info/RECORD,,
```

