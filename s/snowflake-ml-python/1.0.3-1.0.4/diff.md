# Comparing `tmp/snowflake_ml_python-1.0.3-py3-none-any.whl.zip` & `tmp/snowflake_ml_python-1.0.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,261 +1,277 @@
-Zip file size: 1957662 bytes, number of entries: 259
--rw-r--r--  2.0 unx      161 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/env.py
--rw-r--r--  2.0 unx    14175 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/env_utils.py
--rw-r--r--  2.0 unx     7502 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/file_utils.py
--rw-r--r--  2.0 unx     2696 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/init_utils.py
--rw-r--r--  2.0 unx    20145 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/telemetry.py
--rw-r--r--  2.0 unx     2168 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/type_utils.py
--rw-r--r--  2.0 unx     3678 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/formatting.py
--rw-r--r--  2.0 unx     7703 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/identifier.py
--rw-r--r--  2.0 unx     2068 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/import_utils.py
--rw-r--r--  2.0 unx     4550 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/parallelize.py
--rw-r--r--  2.0 unx     3722 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/pkg_version_utils.py
--rw-r--r--  2.0 unx    12205 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/query_result_checker.py
--rw-r--r--  2.0 unx     1400 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/temp_file_utils.py
--rw-r--r--  2.0 unx     2117 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/uri.py
--rw-r--r--  2.0 unx    26746 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/fileset.py
--rw-r--r--  2.0 unx     1040 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/fileset_errors.py
--rw-r--r--  2.0 unx     5915 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/parquet_parser.py
--rw-r--r--  2.0 unx    11536 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/sfcfs.py
--rw-r--r--  2.0 unx    14859 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/stage_fs.py
--rw-r--r--  2.0 unx     3462 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/tf_dataset.py
--rw-r--r--  2.0 unx     2386 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/torch_datapipe.py
--r-xr-xr-x  2.0 unx      197 b- defN 23-Jul-14 18:32 snowflake/ml/model/_core_requirements.py
--rw-r--r--  2.0 unx      345 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/image_builds/base_image_builder.py
--rw-r--r--  2.0 unx    11239 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py
--rw-r--r--  2.0 unx     3998 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/image_builds/docker_context.py
--rw-r--r--  2.0 unx      887 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/image_builds/gunicorn_run.sh
--rw-r--r--  2.0 unx     4894 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py
--rw-r--r--  2.0 unx     1264 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template
--rw-r--r--  2.0 unx     8602 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/snowservice/deploy.py
--rw-r--r--  2.0 unx     3801 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/snowservice/deploy_options.py
--rw-r--r--  2.0 unx      579 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template
--rw-r--r--  2.0 unx     1470 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/utils/constants.py
--rw-r--r--  2.0 unx     7494 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/utils/snowservice_client.py
--rw-r--r--  2.0 unx     9160 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/warehouse/deploy.py
--rw-r--r--  2.0 unx     2489 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/warehouse/infer_template.py
--rw-r--r--  2.0 unx     8855 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deployer.py
--rw-r--r--  2.0 unx     4560 b- defN 23-Jul-14 18:30 snowflake/ml/model/_env.py
--rw-r--r--  2.0 unx     2299 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/_base.py
--rw-r--r--  2.0 unx     6423 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/custom.py
--rw-r--r--  2.0 unx     7166 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/pytorch.py
--rw-r--r--  2.0 unx     7843 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/sklearn.py
--rw-r--r--  2.0 unx     7993 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/snowmlmodel.py
--rw-r--r--  2.0 unx     7287 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/torchscript.py
--rw-r--r--  2.0 unx     7603 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/xgboost.py
--rw-r--r--  2.0 unx    26469 b- defN 23-Jul-14 18:30 snowflake/ml/model/_model.py
--rw-r--r--  2.0 unx     2101 b- defN 23-Jul-14 18:30 snowflake/ml/model/_model_handler.py
--rw-r--r--  2.0 unx    17945 b- defN 23-Jul-14 18:30 snowflake/ml/model/_model_meta.py
--rw-r--r--  2.0 unx     8016 b- defN 23-Jul-14 18:30 snowflake/ml/model/custom_model.py
--rw-r--r--  2.0 unx    60533 b- defN 23-Jul-14 18:30 snowflake/ml/model/model_signature.py
--rw-r--r--  2.0 unx     5187 b- defN 23-Jul-14 18:30 snowflake/ml/model/type_hints.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/calibration/__init__.py
--r-xr-xr-x  2.0 unx    55506 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/calibration/calibrated_classifier_cv.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/__init__.py
--r-xr-xr-x  2.0 unx    53433 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/affinity_propagation.py
--r-xr-xr-x  2.0 unx    55446 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/agglomerative_clustering.py
--r-xr-xr-x  2.0 unx    53271 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/birch.py
--r-xr-xr-x  2.0 unx    55653 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/bisecting_k_means.py
--r-xr-xr-x  2.0 unx    53612 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/dbscan.py
--r-xr-xr-x  2.0 unx    55986 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/feature_agglomeration.py
--r-xr-xr-x  2.0 unx    55240 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/k_means.py
--r-xr-xr-x  2.0 unx    53814 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/mean_shift.py
--r-xr-xr-x  2.0 unx    56515 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/mini_batch_k_means.py
--r-xr-xr-x  2.0 unx    56946 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/optics.py
--r-xr-xr-x  2.0 unx    54004 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/spectral_biclustering.py
--r-xr-xr-x  2.0 unx    56942 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/spectral_clustering.py
--r-xr-xr-x  2.0 unx    53134 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/spectral_coclustering.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/compose/__init__.py
--r-xr-xr-x  2.0 unx    55717 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/compose/column_transformer.py
--r-xr-xr-x  2.0 unx    53302 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/compose/transformed_target_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/__init__.py
--r-xr-xr-x  2.0 unx    53274 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/elliptic_envelope.py
--r-xr-xr-x  2.0 unx    51550 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/empirical_covariance.py
--r-xr-xr-x  2.0 unx    52824 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/graphical_lasso.py
--r-xr-xr-x  2.0 unx    54288 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/graphical_lasso_cv.py
--r-xr-xr-x  2.0 unx    51752 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/ledoit_wolf.py
--r-xr-xr-x  2.0 unx    52515 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/min_cov_det.py
--r-xr-xr-x  2.0 unx    51441 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/oas.py
--r-xr-xr-x  2.0 unx    51727 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/shrunk_covariance.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/__init__.py
--r-xr-xr-x  2.0 unx    56542 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/dictionary_learning.py
--r-xr-xr-x  2.0 unx    53914 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/factor_analysis.py
--r-xr-xr-x  2.0 unx    54376 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/fast_ica.py
--r-xr-xr-x  2.0 unx    52711 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/incremental_pca.py
--r-xr-xr-x  2.0 unx    56742 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/kernel_pca.py
--r-xr-xr-x  2.0 unx    57725 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py
--r-xr-xr-x  2.0 unx    55042 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py
--r-xr-xr-x  2.0 unx    55586 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/pca.py
--r-xr-xr-x  2.0 unx    53907 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/sparse_pca.py
--r-xr-xr-x  2.0 unx    53479 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/truncated_svd.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/discriminant_analysis/__init__.py
--r-xr-xr-x  2.0 unx    55727 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py
--r-xr-xr-x  2.0 unx    53792 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/__init__.py
--r-xr-xr-x  2.0 unx    54745 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/ada_boost_classifier.py
--r-xr-xr-x  2.0 unx    53644 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/ada_boost_regressor.py
--r-xr-xr-x  2.0 unx    55669 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/bagging_classifier.py
--r-xr-xr-x  2.0 unx    54913 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/bagging_regressor.py
--r-xr-xr-x  2.0 unx    60455 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/extra_trees_classifier.py
--r-xr-xr-x  2.0 unx    59066 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/extra_trees_regressor.py
--r-xr-xr-x  2.0 unx    62064 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py
--r-xr-xr-x  2.0 unx    61648 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py
--r-xr-xr-x  2.0 unx    61714 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py
--r-xr-xr-x  2.0 unx    60036 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py
--r-xr-xr-x  2.0 unx    54690 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/isolation_forest.py
--r-xr-xr-x  2.0 unx    60410 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/random_forest_classifier.py
--r-xr-xr-x  2.0 unx    59009 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/random_forest_regressor.py
--r-xr-xr-x  2.0 unx    54597 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/stacking_regressor.py
--r-xr-xr-x  2.0 unx    54172 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/voting_classifier.py
--r-xr-xr-x  2.0 unx    52707 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/voting_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/__init__.py
--r-xr-xr-x  2.0 unx    52220 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/generic_univariate_select.py
--r-xr-xr-x  2.0 unx    51918 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/select_fdr.py
--r-xr-xr-x  2.0 unx    51912 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/select_fpr.py
--r-xr-xr-x  2.0 unx    51920 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/select_fwe.py
--r-xr-xr-x  2.0 unx    51997 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/select_k_best.py
--r-xr-xr-x  2.0 unx    52017 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/select_percentile.py
--r-xr-xr-x  2.0 unx    54670 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/sequential_feature_selector.py
--r-xr-xr-x  2.0 unx    51649 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/variance_threshold.py
--rw-r--r--  2.0 unx     9110 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/framework/_utils.py
--rw-r--r--  2.0 unx    21900 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/framework/base.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/gaussian_process/__init__.py
--r-xr-xr-x  2.0 unx    57215 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py
--r-xr-xr-x  2.0 unx    55907 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py
--rw-r--r--  2.0 unx      298 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/impute/__init__.py
--r-xr-xr-x  2.0 unx    57770 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/impute/iterative_imputer.py
--r-xr-xr-x  2.0 unx    53992 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/impute/knn_imputer.py
--r-xr-xr-x  2.0 unx    52789 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/impute/missing_indicator.py
--rw-r--r--  2.0 unx    18118 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/impute/simple_imputer.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_approximation/__init__.py
--r-xr-xr-x  2.0 unx    51733 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py
--r-xr-xr-x  2.0 unx    53606 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_approximation/nystroem.py
--r-xr-xr-x  2.0 unx    52760 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py
--r-xr-xr-x  2.0 unx    52189 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_approximation/rbf_sampler.py
--r-xr-xr-x  2.0 unx    52188 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_ridge/__init__.py
--r-xr-xr-x  2.0 unx    53706 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_ridge/kernel_ridge.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/lightgbm/__init__.py
--r-xr-xr-x  2.0 unx    53228 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/lightgbm/lgbm_classifier.py
--r-xr-xr-x  2.0 unx    52739 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/lightgbm/lgbm_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/__init__.py
--r-xr-xr-x  2.0 unx    53454 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/ard_regression.py
--r-xr-xr-x  2.0 unx    53767 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/bayesian_ridge.py
--r-xr-xr-x  2.0 unx    54651 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/elastic_net.py
--r-xr-xr-x  2.0 unx    55909 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/elastic_net_cv.py
--r-xr-xr-x  2.0 unx    53707 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/gamma_regressor.py
--r-xr-xr-x  2.0 unx    52895 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/huber_regressor.py
--r-xr-xr-x  2.0 unx    54192 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lars.py
--r-xr-xr-x  2.0 unx    54399 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lars_cv.py
--r-xr-xr-x  2.0 unx    54291 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lasso.py
--r-xr-xr-x  2.0 unx    55066 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lasso_cv.py
--r-xr-xr-x  2.0 unx    55295 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lasso_lars.py
--r-xr-xr-x  2.0 unx    55241 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lasso_lars_cv.py
--r-xr-xr-x  2.0 unx    54586 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lasso_lars_ic.py
--r-xr-xr-x  2.0 unx    52421 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/linear_regression.py
--r-xr-xr-x  2.0 unx    58672 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/logistic_regression.py
--r-xr-xr-x  2.0 unx    59692 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/logistic_regression_cv.py
--r-xr-xr-x  2.0 unx    53877 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/multi_task_elastic_net.py
--r-xr-xr-x  2.0 unx    55505 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py
--r-xr-xr-x  2.0 unx    53459 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/multi_task_lasso.py
--r-xr-xr-x  2.0 unx    54711 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py
--r-xr-xr-x  2.0 unx    52986 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py
--r-xr-xr-x  2.0 unx    56337 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py
--r-xr-xr-x  2.0 unx    55412 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py
--r-xr-xr-x  2.0 unx    55842 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/perceptron.py
--r-xr-xr-x  2.0 unx    53738 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/poisson_regressor.py
--r-xr-xr-x  2.0 unx    57212 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/ransac_regressor.py
--r-xr-xr-x  2.0 unx    55272 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/ridge.py
--r-xr-xr-x  2.0 unx    55590 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/ridge_classifier.py
--r-xr-xr-x  2.0 unx    54129 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/ridge_classifier_cv.py
--r-xr-xr-x  2.0 unx    54905 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/ridge_cv.py
--r-xr-xr-x  2.0 unx    61258 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/sgd_classifier.py
--r-xr-xr-x  2.0 unx    55872 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/sgd_one_class_svm.py
--r-xr-xr-x  2.0 unx    58727 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/sgd_regressor.py
--r-xr-xr-x  2.0 unx    54160 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/theil_sen_regressor.py
--r-xr-xr-x  2.0 unx    55131 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/tweedie_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/manifold/__init__.py
--r-xr-xr-x  2.0 unx    54526 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/manifold/isomap.py
--r-xr-xr-x  2.0 unx    53744 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/manifold/mds.py
--r-xr-xr-x  2.0 unx    54515 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/manifold/spectral_embedding.py
--r-xr-xr-x  2.0 unx    57787 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/manifold/tsne.py
--rw-r--r--  2.0 unx      304 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/__init__.py
--rw-r--r--  2.0 unx    40077 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/classification.py
--rw-r--r--  2.0 unx     4921 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/correlation.py
--rw-r--r--  2.0 unx     4757 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/covariance.py
--rw-r--r--  2.0 unx    12037 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/metrics_utils.py
--rw-r--r--  2.0 unx    15397 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/ranking.py
--rw-r--r--  2.0 unx    23144 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/regression.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/mixture/__init__.py
--r-xr-xr-x  2.0 unx    58431 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py
--r-xr-xr-x  2.0 unx    56433 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/mixture/gaussian_mixture.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/model_selection/__init__.py
--r-xr-xr-x  2.0 unx    58982 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/model_selection/grid_search_cv.py
--r-xr-xr-x  2.0 unx    59826 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/model_selection/randomized_search_cv.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/multiclass/__init__.py
--r-xr-xr-x  2.0 unx    52414 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/multiclass/one_vs_one_classifier.py
--r-xr-xr-x  2.0 unx    53342 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py
--r-xr-xr-x  2.0 unx    52672 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/multiclass/output_code_classifier.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/naive_bayes/__init__.py
--r-xr-xr-x  2.0 unx    52999 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/naive_bayes/bernoulli_nb.py
--r-xr-xr-x  2.0 unx    53320 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/naive_bayes/categorical_nb.py
--r-xr-xr-x  2.0 unx    53007 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/naive_bayes/complement_nb.py
--r-xr-xr-x  2.0 unx    52147 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/naive_bayes/gaussian_nb.py
--r-xr-xr-x  2.0 unx    52764 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/naive_bayes/multinomial_nb.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/__init__.py
--r-xr-xr-x  2.0 unx    55551 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/k_neighbors_classifier.py
--r-xr-xr-x  2.0 unx    55033 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/k_neighbors_regressor.py
--r-xr-xr-x  2.0 unx    53510 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/kernel_density.py
--r-xr-xr-x  2.0 unx    55791 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/local_outlier_factor.py
--r-xr-xr-x  2.0 unx    52314 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/nearest_centroid.py
--r-xr-xr-x  2.0 unx    54223 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/nearest_neighbors.py
--r-xr-xr-x  2.0 unx    55699 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py
--r-xr-xr-x  2.0 unx    56180 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py
--r-xr-xr-x  2.0 unx    55066 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neural_network/__init__.py
--r-xr-xr-x  2.0 unx    52715 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neural_network/bernoulli_rbm.py
--r-xr-xr-x  2.0 unx    60213 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neural_network/mlp_classifier.py
--r-xr-xr-x  2.0 unx    59490 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neural_network/mlp_regressor.py
--rw-r--r--  2.0 unx      298 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/pipeline/__init__.py
--rw-r--r--  2.0 unx    23381 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/pipeline/pipeline.py
--rw-r--r--  2.0 unx      298 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/__init__.py
--rw-r--r--  2.0 unx     6092 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/binarizer.py
--rw-r--r--  2.0 unx    20422 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py
--rw-r--r--  2.0 unx     6285 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/label_encoder.py
--rw-r--r--  2.0 unx     8491 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/max_abs_scaler.py
--rw-r--r--  2.0 unx    10716 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/min_max_scaler.py
--rw-r--r--  2.0 unx     5951 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/normalizer.py
--rw-r--r--  2.0 unx    66998 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/one_hot_encoder.py
--rw-r--r--  2.0 unx    27956 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/ordinal_encoder.py
--r-xr-xr-x  2.0 unx    52849 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/preprocessing/polynomial_features.py
--rw-r--r--  2.0 unx    11981 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/robust_scaler.py
--rw-r--r--  2.0 unx    10672 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/standard_scaler.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/semi_supervised/__init__.py
--r-xr-xr-x  2.0 unx    53186 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/semi_supervised/label_propagation.py
--r-xr-xr-x  2.0 unx    53550 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/semi_supervised/label_spreading.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/__init__.py
--r-xr-xr-x  2.0 unx    55728 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/linear_svc.py
--r-xr-xr-x  2.0 unx    54143 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/linear_svr.py
--r-xr-xr-x  2.0 unx    56442 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/nu_svc.py
--r-xr-xr-x  2.0 unx    53518 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/nu_svr.py
--r-xr-xr-x  2.0 unx    56605 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/svc.py
--r-xr-xr-x  2.0 unx    53721 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/svr.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/tree/__init__.py
--r-xr-xr-x  2.0 unx    58804 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/tree/decision_tree_classifier.py
--r-xr-xr-x  2.0 unx    57500 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/tree/decision_tree_regressor.py
--r-xr-xr-x  2.0 unx    58167 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/tree/extra_tree_classifier.py
--r-xr-xr-x  2.0 unx    56872 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/tree/extra_tree_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/xgboost/__init__.py
--r-xr-xr-x  2.0 unx    62574 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/xgboost/xgb_classifier.py
--r-xr-xr-x  2.0 unx    62080 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/xgboost/xgb_regressor.py
--r-xr-xr-x  2.0 unx    62738 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/xgboost/xgbrf_classifier.py
--r-xr-xr-x  2.0 unx    62271 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/xgboost/xgbrf_regressor.py
--rw-r--r--  2.0 unx     1381 b- defN 23-Jul-14 18:30 snowflake/ml/registry/_schema.py
--rw-r--r--  2.0 unx    85709 b- defN 23-Jul-14 18:30 snowflake/ml/registry/model_registry.py
--rw-r--r--  2.0 unx     6138 b- defN 23-Jul-14 18:30 snowflake/ml/utils/connection_params.py
--rw-r--r--  2.0 unx     3893 b- defN 23-Jul-14 18:30 snowflake/ml/utils/sparse.py
--r-xr-xr-x  2.0 unx       16 b- defN 23-Jul-14 18:31 snowflake/ml/version.py
-?rw-------  2.0 unx       91 b- defN 23-Jul-14 18:32 snowflake_ml_python-1.0.3.dist-info/WHEEL
-?rw-------  2.0 unx    13340 b- defN 23-Jul-14 18:32 snowflake_ml_python-1.0.3.dist-info/METADATA
-?rw-------  2.0 unx    27677 b- defN 23-Jul-14 18:32 snowflake_ml_python-1.0.3.dist-info/RECORD
-259 files, 9322812 bytes uncompressed, 1912302 bytes compressed:  79.5%
+Zip file size: 1973197 bytes, number of entries: 275
+-rw-r--r--  2.0 unx      161 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/env.py
+-rw-r--r--  2.0 unx    13555 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/env_utils.py
+-rw-r--r--  2.0 unx     3200 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/exceptions/error_codes.py
+-rw-r--r--  2.0 unx       47 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/exceptions/error_messages.py
+-rw-r--r--  2.0 unx     1423 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/exceptions/exceptions.py
+-rw-r--r--  2.0 unx      419 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/exceptions/fileset_error_messages.py
+-rw-r--r--  2.0 unx     1040 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/exceptions/fileset_errors.py
+-rw-r--r--  2.0 unx      466 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/exceptions/modeling_error_messages.py
+-rw-r--r--  2.0 unx     7682 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/file_utils.py
+-rw-r--r--  2.0 unx     2696 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/init_utils.py
+-rw-r--r--  2.0 unx    20711 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/telemetry.py
+-rw-r--r--  2.0 unx     2168 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/type_utils.py
+-rw-r--r--  2.0 unx     3678 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/utils/formatting.py
+-rw-r--r--  2.0 unx     7703 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/utils/identifier.py
+-rw-r--r--  2.0 unx     2068 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/utils/import_utils.py
+-rw-r--r--  2.0 unx     4534 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/utils/parallelize.py
+-rw-r--r--  2.0 unx     3722 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/utils/pkg_version_utils.py
+-rw-r--r--  2.0 unx    10240 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/utils/query_result_checker.py
+-rw-r--r--  2.0 unx     1400 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/utils/temp_file_utils.py
+-rw-r--r--  2.0 unx     2117 b- defN 23-Jul-28 03:12 snowflake/ml/_internal/utils/uri.py
+-rw-r--r--  2.0 unx    28300 b- defN 23-Jul-28 03:12 snowflake/ml/fileset/fileset.py
+-rw-r--r--  2.0 unx     5915 b- defN 23-Jul-28 03:12 snowflake/ml/fileset/parquet_parser.py
+-rw-r--r--  2.0 unx    11536 b- defN 23-Jul-28 03:12 snowflake/ml/fileset/sfcfs.py
+-rw-r--r--  2.0 unx    15902 b- defN 23-Jul-28 03:12 snowflake/ml/fileset/stage_fs.py
+-rw-r--r--  2.0 unx     3462 b- defN 23-Jul-28 03:12 snowflake/ml/fileset/tf_dataset.py
+-rw-r--r--  2.0 unx     2386 b- defN 23-Jul-28 03:12 snowflake/ml/fileset/torch_datapipe.py
+-r-xr-xr-x  2.0 unx      197 b- defN 23-Jul-28 03:17 snowflake/ml/model/_core_requirements.py
+-rw-r--r--  2.0 unx      345 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/image_builds/base_image_builder.py
+-rw-r--r--  2.0 unx    10125 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py
+-rw-r--r--  2.0 unx     3525 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/image_builds/docker_context.py
+-rw-r--r--  2.0 unx      887 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/image_builds/gunicorn_run.sh
+-rw-r--r--  2.0 unx     6166 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py
+-rw-r--r--  2.0 unx     1143 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template
+-rw-r--r--  2.0 unx    12367 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/snowservice/deploy.py
+-rw-r--r--  2.0 unx     3205 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/snowservice/deploy_options.py
+-rw-r--r--  2.0 unx      591 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template
+-rw-r--r--  2.0 unx     1672 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/utils/constants.py
+-rw-r--r--  2.0 unx     7858 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/utils/snowservice_client.py
+-rw-r--r--  2.0 unx     8189 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/warehouse/deploy.py
+-rw-r--r--  2.0 unx     2489 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deploy_client/warehouse/infer_template.py
+-rw-r--r--  2.0 unx    12334 b- defN 23-Jul-28 03:12 snowflake/ml/model/_deployer.py
+-rw-r--r--  2.0 unx     5059 b- defN 23-Jul-28 03:12 snowflake/ml/model/_env.py
+-rw-r--r--  2.0 unx     2303 b- defN 23-Jul-28 03:12 snowflake/ml/model/_handlers/_base.py
+-rw-r--r--  2.0 unx     6423 b- defN 23-Jul-28 03:12 snowflake/ml/model/_handlers/custom.py
+-rw-r--r--  2.0 unx    11844 b- defN 23-Jul-28 03:12 snowflake/ml/model/_handlers/mlflow.py
+-rw-r--r--  2.0 unx     7271 b- defN 23-Jul-28 03:12 snowflake/ml/model/_handlers/pytorch.py
+-rw-r--r--  2.0 unx     7934 b- defN 23-Jul-28 03:12 snowflake/ml/model/_handlers/sklearn.py
+-rw-r--r--  2.0 unx     8084 b- defN 23-Jul-28 03:12 snowflake/ml/model/_handlers/snowmlmodel.py
+-rw-r--r--  2.0 unx     7698 b- defN 23-Jul-28 03:12 snowflake/ml/model/_handlers/tensorflow.py
+-rw-r--r--  2.0 unx     7392 b- defN 23-Jul-28 03:12 snowflake/ml/model/_handlers/torchscript.py
+-rw-r--r--  2.0 unx     7694 b- defN 23-Jul-28 03:12 snowflake/ml/model/_handlers/xgboost.py
+-rw-r--r--  2.0 unx    26762 b- defN 23-Jul-28 03:12 snowflake/ml/model/_model.py
+-rw-r--r--  2.0 unx     2101 b- defN 23-Jul-28 03:12 snowflake/ml/model/_model_handler.py
+-rw-r--r--  2.0 unx    19501 b- defN 23-Jul-28 03:12 snowflake/ml/model/_model_meta.py
+-rw-r--r--  2.0 unx     1304 b- defN 23-Jul-28 03:12 snowflake/ml/model/_signatures/base_handler.py
+-rw-r--r--  2.0 unx     1872 b- defN 23-Jul-28 03:12 snowflake/ml/model/_signatures/builtins_handler.py
+-rw-r--r--  2.0 unx    16266 b- defN 23-Jul-28 03:12 snowflake/ml/model/_signatures/core.py
+-rw-r--r--  2.0 unx     5432 b- defN 23-Jul-28 03:12 snowflake/ml/model/_signatures/numpy_handler.py
+-rw-r--r--  2.0 unx     6503 b- defN 23-Jul-28 03:12 snowflake/ml/model/_signatures/pandas_handler.py
+-rw-r--r--  2.0 unx     3948 b- defN 23-Jul-28 03:12 snowflake/ml/model/_signatures/pytorch_handler.py
+-rw-r--r--  2.0 unx     6298 b- defN 23-Jul-28 03:12 snowflake/ml/model/_signatures/snowpark_handler.py
+-rw-r--r--  2.0 unx     5152 b- defN 23-Jul-28 03:12 snowflake/ml/model/_signatures/tensorflow_handler.py
+-rw-r--r--  2.0 unx     3386 b- defN 23-Jul-28 03:12 snowflake/ml/model/_signatures/utils.py
+-rw-r--r--  2.0 unx     8016 b- defN 23-Jul-28 03:12 snowflake/ml/model/custom_model.py
+-rw-r--r--  2.0 unx    15071 b- defN 23-Jul-28 03:12 snowflake/ml/model/model_signature.py
+-rw-r--r--  2.0 unx     7456 b- defN 23-Jul-28 03:12 snowflake/ml/model/type_hints.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/calibration/__init__.py
+-r-xr-xr-x  2.0 unx    55539 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/calibration/calibrated_classifier_cv.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/__init__.py
+-r-xr-xr-x  2.0 unx    53466 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/affinity_propagation.py
+-r-xr-xr-x  2.0 unx    55479 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/agglomerative_clustering.py
+-r-xr-xr-x  2.0 unx    53304 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/birch.py
+-r-xr-xr-x  2.0 unx    55686 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/bisecting_k_means.py
+-r-xr-xr-x  2.0 unx    53645 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/dbscan.py
+-r-xr-xr-x  2.0 unx    56019 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/feature_agglomeration.py
+-r-xr-xr-x  2.0 unx    55273 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/k_means.py
+-r-xr-xr-x  2.0 unx    53847 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/mean_shift.py
+-r-xr-xr-x  2.0 unx    56548 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/mini_batch_k_means.py
+-r-xr-xr-x  2.0 unx    56979 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/optics.py
+-r-xr-xr-x  2.0 unx    54037 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/spectral_biclustering.py
+-r-xr-xr-x  2.0 unx    56975 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/spectral_clustering.py
+-r-xr-xr-x  2.0 unx    53167 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/cluster/spectral_coclustering.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/compose/__init__.py
+-r-xr-xr-x  2.0 unx    55750 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/compose/column_transformer.py
+-r-xr-xr-x  2.0 unx    53335 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/compose/transformed_target_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/covariance/__init__.py
+-r-xr-xr-x  2.0 unx    53307 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/covariance/elliptic_envelope.py
+-r-xr-xr-x  2.0 unx    51583 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/covariance/empirical_covariance.py
+-r-xr-xr-x  2.0 unx    52857 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/covariance/graphical_lasso.py
+-r-xr-xr-x  2.0 unx    54321 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/covariance/graphical_lasso_cv.py
+-r-xr-xr-x  2.0 unx    51785 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/covariance/ledoit_wolf.py
+-r-xr-xr-x  2.0 unx    52548 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/covariance/min_cov_det.py
+-r-xr-xr-x  2.0 unx    51474 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/covariance/oas.py
+-r-xr-xr-x  2.0 unx    51760 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/covariance/shrunk_covariance.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/decomposition/__init__.py
+-r-xr-xr-x  2.0 unx    56575 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/decomposition/dictionary_learning.py
+-r-xr-xr-x  2.0 unx    53947 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/decomposition/factor_analysis.py
+-r-xr-xr-x  2.0 unx    54409 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/decomposition/fast_ica.py
+-r-xr-xr-x  2.0 unx    52744 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/decomposition/incremental_pca.py
+-r-xr-xr-x  2.0 unx    56775 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/decomposition/kernel_pca.py
+-r-xr-xr-x  2.0 unx    57758 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py
+-r-xr-xr-x  2.0 unx    55075 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py
+-r-xr-xr-x  2.0 unx    55619 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/decomposition/pca.py
+-r-xr-xr-x  2.0 unx    53940 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/decomposition/sparse_pca.py
+-r-xr-xr-x  2.0 unx    53512 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/decomposition/truncated_svd.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/discriminant_analysis/__init__.py
+-r-xr-xr-x  2.0 unx    55760 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py
+-r-xr-xr-x  2.0 unx    53825 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/__init__.py
+-r-xr-xr-x  2.0 unx    54778 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/ada_boost_classifier.py
+-r-xr-xr-x  2.0 unx    53677 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/ada_boost_regressor.py
+-r-xr-xr-x  2.0 unx    55702 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/bagging_classifier.py
+-r-xr-xr-x  2.0 unx    54946 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/bagging_regressor.py
+-r-xr-xr-x  2.0 unx    60488 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/extra_trees_classifier.py
+-r-xr-xr-x  2.0 unx    59099 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/extra_trees_regressor.py
+-r-xr-xr-x  2.0 unx    62097 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py
+-r-xr-xr-x  2.0 unx    61681 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py
+-r-xr-xr-x  2.0 unx    61747 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py
+-r-xr-xr-x  2.0 unx    60069 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py
+-r-xr-xr-x  2.0 unx    54723 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/isolation_forest.py
+-r-xr-xr-x  2.0 unx    60443 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/random_forest_classifier.py
+-r-xr-xr-x  2.0 unx    59042 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/random_forest_regressor.py
+-r-xr-xr-x  2.0 unx    54630 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/stacking_regressor.py
+-r-xr-xr-x  2.0 unx    54205 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/voting_classifier.py
+-r-xr-xr-x  2.0 unx    52740 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/ensemble/voting_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/feature_selection/__init__.py
+-r-xr-xr-x  2.0 unx    52253 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/feature_selection/generic_univariate_select.py
+-r-xr-xr-x  2.0 unx    51951 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/feature_selection/select_fdr.py
+-r-xr-xr-x  2.0 unx    51945 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/feature_selection/select_fpr.py
+-r-xr-xr-x  2.0 unx    51953 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/feature_selection/select_fwe.py
+-r-xr-xr-x  2.0 unx    52030 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/feature_selection/select_k_best.py
+-r-xr-xr-x  2.0 unx    52050 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/feature_selection/select_percentile.py
+-r-xr-xr-x  2.0 unx    54703 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/feature_selection/sequential_feature_selector.py
+-r-xr-xr-x  2.0 unx    51682 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/feature_selection/variance_threshold.py
+-rw-r--r--  2.0 unx     9903 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/framework/_utils.py
+-rw-r--r--  2.0 unx    24514 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/framework/base.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/gaussian_process/__init__.py
+-r-xr-xr-x  2.0 unx    57248 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py
+-r-xr-xr-x  2.0 unx    55940 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py
+-rw-r--r--  2.0 unx      298 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/impute/__init__.py
+-r-xr-xr-x  2.0 unx    57803 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/impute/iterative_imputer.py
+-r-xr-xr-x  2.0 unx    54025 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/impute/knn_imputer.py
+-r-xr-xr-x  2.0 unx    52822 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/impute/missing_indicator.py
+-rw-r--r--  2.0 unx    18458 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/impute/simple_imputer.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/kernel_approximation/__init__.py
+-r-xr-xr-x  2.0 unx    51766 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py
+-r-xr-xr-x  2.0 unx    53639 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/kernel_approximation/nystroem.py
+-r-xr-xr-x  2.0 unx    52793 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py
+-r-xr-xr-x  2.0 unx    52222 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/kernel_approximation/rbf_sampler.py
+-r-xr-xr-x  2.0 unx    52221 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/kernel_ridge/__init__.py
+-r-xr-xr-x  2.0 unx    53739 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/kernel_ridge/kernel_ridge.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/lightgbm/__init__.py
+-r-xr-xr-x  2.0 unx    53261 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/lightgbm/lgbm_classifier.py
+-r-xr-xr-x  2.0 unx    52772 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/lightgbm/lgbm_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/__init__.py
+-r-xr-xr-x  2.0 unx    53487 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/ard_regression.py
+-r-xr-xr-x  2.0 unx    53800 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/bayesian_ridge.py
+-r-xr-xr-x  2.0 unx    54684 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/elastic_net.py
+-r-xr-xr-x  2.0 unx    55942 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/elastic_net_cv.py
+-r-xr-xr-x  2.0 unx    53740 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/gamma_regressor.py
+-r-xr-xr-x  2.0 unx    52928 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/huber_regressor.py
+-r-xr-xr-x  2.0 unx    54225 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/lars.py
+-r-xr-xr-x  2.0 unx    54432 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/lars_cv.py
+-r-xr-xr-x  2.0 unx    54324 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/lasso.py
+-r-xr-xr-x  2.0 unx    55099 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/lasso_cv.py
+-r-xr-xr-x  2.0 unx    55328 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/lasso_lars.py
+-r-xr-xr-x  2.0 unx    55274 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/lasso_lars_cv.py
+-r-xr-xr-x  2.0 unx    54619 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/lasso_lars_ic.py
+-r-xr-xr-x  2.0 unx    52454 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/linear_regression.py
+-r-xr-xr-x  2.0 unx    58705 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/logistic_regression.py
+-r-xr-xr-x  2.0 unx    59725 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/logistic_regression_cv.py
+-r-xr-xr-x  2.0 unx    53910 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/multi_task_elastic_net.py
+-r-xr-xr-x  2.0 unx    55538 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py
+-r-xr-xr-x  2.0 unx    53492 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/multi_task_lasso.py
+-r-xr-xr-x  2.0 unx    54744 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py
+-r-xr-xr-x  2.0 unx    53019 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py
+-r-xr-xr-x  2.0 unx    56370 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py
+-r-xr-xr-x  2.0 unx    55445 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py
+-r-xr-xr-x  2.0 unx    55875 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/perceptron.py
+-r-xr-xr-x  2.0 unx    53771 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/poisson_regressor.py
+-r-xr-xr-x  2.0 unx    57245 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/ransac_regressor.py
+-r-xr-xr-x  2.0 unx    55305 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/ridge.py
+-r-xr-xr-x  2.0 unx    55623 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/ridge_classifier.py
+-r-xr-xr-x  2.0 unx    54162 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/ridge_classifier_cv.py
+-r-xr-xr-x  2.0 unx    54938 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/ridge_cv.py
+-r-xr-xr-x  2.0 unx    61291 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/sgd_classifier.py
+-r-xr-xr-x  2.0 unx    55905 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/sgd_one_class_svm.py
+-r-xr-xr-x  2.0 unx    58760 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/sgd_regressor.py
+-r-xr-xr-x  2.0 unx    54193 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/theil_sen_regressor.py
+-r-xr-xr-x  2.0 unx    55164 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/linear_model/tweedie_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/manifold/__init__.py
+-r-xr-xr-x  2.0 unx    54559 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/manifold/isomap.py
+-r-xr-xr-x  2.0 unx    53777 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/manifold/mds.py
+-r-xr-xr-x  2.0 unx    54548 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/manifold/spectral_embedding.py
+-r-xr-xr-x  2.0 unx    57820 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/manifold/tsne.py
+-rw-r--r--  2.0 unx      304 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/metrics/__init__.py
+-rw-r--r--  2.0 unx    40159 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/metrics/classification.py
+-rw-r--r--  2.0 unx     4869 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/metrics/correlation.py
+-rw-r--r--  2.0 unx     4705 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/metrics/covariance.py
+-rw-r--r--  2.0 unx    11807 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/metrics/metrics_utils.py
+-rw-r--r--  2.0 unx    15748 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/metrics/ranking.py
+-rw-r--r--  2.0 unx    23742 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/metrics/regression.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/mixture/__init__.py
+-r-xr-xr-x  2.0 unx    58464 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py
+-r-xr-xr-x  2.0 unx    56466 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/mixture/gaussian_mixture.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/model_selection/__init__.py
+-r-xr-xr-x  2.0 unx    59015 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/model_selection/grid_search_cv.py
+-r-xr-xr-x  2.0 unx    59859 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/model_selection/randomized_search_cv.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/multiclass/__init__.py
+-r-xr-xr-x  2.0 unx    52447 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/multiclass/one_vs_one_classifier.py
+-r-xr-xr-x  2.0 unx    53375 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py
+-r-xr-xr-x  2.0 unx    52705 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/multiclass/output_code_classifier.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/naive_bayes/__init__.py
+-r-xr-xr-x  2.0 unx    53032 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/naive_bayes/bernoulli_nb.py
+-r-xr-xr-x  2.0 unx    53353 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/naive_bayes/categorical_nb.py
+-r-xr-xr-x  2.0 unx    53040 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/naive_bayes/complement_nb.py
+-r-xr-xr-x  2.0 unx    52180 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/naive_bayes/gaussian_nb.py
+-r-xr-xr-x  2.0 unx    52797 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/naive_bayes/multinomial_nb.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neighbors/__init__.py
+-r-xr-xr-x  2.0 unx    55584 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neighbors/k_neighbors_classifier.py
+-r-xr-xr-x  2.0 unx    55066 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neighbors/k_neighbors_regressor.py
+-r-xr-xr-x  2.0 unx    53543 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neighbors/kernel_density.py
+-r-xr-xr-x  2.0 unx    55824 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neighbors/local_outlier_factor.py
+-r-xr-xr-x  2.0 unx    52347 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neighbors/nearest_centroid.py
+-r-xr-xr-x  2.0 unx    54256 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neighbors/nearest_neighbors.py
+-r-xr-xr-x  2.0 unx    55732 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py
+-r-xr-xr-x  2.0 unx    56213 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py
+-r-xr-xr-x  2.0 unx    55099 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neural_network/__init__.py
+-r-xr-xr-x  2.0 unx    52748 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neural_network/bernoulli_rbm.py
+-r-xr-xr-x  2.0 unx    60246 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neural_network/mlp_classifier.py
+-r-xr-xr-x  2.0 unx    59523 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/neural_network/mlp_regressor.py
+-rw-r--r--  2.0 unx      298 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/pipeline/__init__.py
+-rw-r--r--  2.0 unx    24044 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/pipeline/pipeline.py
+-rw-r--r--  2.0 unx      298 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/preprocessing/__init__.py
+-rw-r--r--  2.0 unx     5995 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/preprocessing/binarizer.py
+-rw-r--r--  2.0 unx    20608 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py
+-rw-r--r--  2.0 unx     6166 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/preprocessing/label_encoder.py
+-rw-r--r--  2.0 unx     7748 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/preprocessing/max_abs_scaler.py
+-rw-r--r--  2.0 unx     9921 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/preprocessing/min_max_scaler.py
+-rw-r--r--  2.0 unx     6022 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/preprocessing/normalizer.py
+-rw-r--r--  2.0 unx    68331 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/preprocessing/one_hot_encoder.py
+-rw-r--r--  2.0 unx    27501 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/preprocessing/ordinal_encoder.py
+-r-xr-xr-x  2.0 unx    52882 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/preprocessing/polynomial_features.py
+-rw-r--r--  2.0 unx    11228 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/preprocessing/robust_scaler.py
+-rw-r--r--  2.0 unx     9928 b- defN 23-Jul-28 03:12 snowflake/ml/modeling/preprocessing/standard_scaler.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/semi_supervised/__init__.py
+-r-xr-xr-x  2.0 unx    53219 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/semi_supervised/label_propagation.py
+-r-xr-xr-x  2.0 unx    53583 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/semi_supervised/label_spreading.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/svm/__init__.py
+-r-xr-xr-x  2.0 unx    55761 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/svm/linear_svc.py
+-r-xr-xr-x  2.0 unx    54176 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/svm/linear_svr.py
+-r-xr-xr-x  2.0 unx    56475 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/svm/nu_svc.py
+-r-xr-xr-x  2.0 unx    53551 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/svm/nu_svr.py
+-r-xr-xr-x  2.0 unx    56638 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/svm/svc.py
+-r-xr-xr-x  2.0 unx    53754 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/svm/svr.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/tree/__init__.py
+-r-xr-xr-x  2.0 unx    58837 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/tree/decision_tree_classifier.py
+-r-xr-xr-x  2.0 unx    57533 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/tree/decision_tree_regressor.py
+-r-xr-xr-x  2.0 unx    58200 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/tree/extra_tree_classifier.py
+-r-xr-xr-x  2.0 unx    56905 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/tree/extra_tree_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/xgboost/__init__.py
+-r-xr-xr-x  2.0 unx    62607 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/xgboost/xgb_classifier.py
+-r-xr-xr-x  2.0 unx    62113 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/xgboost/xgb_regressor.py
+-r-xr-xr-x  2.0 unx    62771 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/xgboost/xgbrf_classifier.py
+-r-xr-xr-x  2.0 unx    62304 b- defN 23-Jul-28 03:17 snowflake/ml/modeling/xgboost/xgbrf_regressor.py
+-rw-r--r--  2.0 unx     1381 b- defN 23-Jul-28 03:12 snowflake/ml/registry/_schema.py
+-rw-r--r--  2.0 unx    87636 b- defN 23-Jul-28 03:12 snowflake/ml/registry/model_registry.py
+-rw-r--r--  2.0 unx     6138 b- defN 23-Jul-28 03:12 snowflake/ml/utils/connection_params.py
+-rw-r--r--  2.0 unx     3893 b- defN 23-Jul-28 03:12 snowflake/ml/utils/sparse.py
+-r-xr-xr-x  2.0 unx       16 b- defN 23-Jul-28 03:17 snowflake/ml/version.py
+?rw-------  2.0 unx       91 b- defN 23-Jul-28 03:17 snowflake_ml_python-1.0.4.dist-info/WHEEL
+?rw-------  2.0 unx    13989 b- defN 23-Jul-28 03:17 snowflake_ml_python-1.0.4.dist-info/METADATA
+?rw-------  2.0 unx    29364 b- defN 23-Jul-28 03:17 snowflake_ml_python-1.0.4.dist-info/RECORD
+275 files, 9376576 bytes uncompressed, 1925067 bytes compressed:  79.5%
```

## zipnote {}

```diff
@@ -1,13 +1,31 @@
 Filename: snowflake/ml/_internal/env.py
 Comment: 
 
 Filename: snowflake/ml/_internal/env_utils.py
 Comment: 
 
+Filename: snowflake/ml/_internal/exceptions/error_codes.py
+Comment: 
+
+Filename: snowflake/ml/_internal/exceptions/error_messages.py
+Comment: 
+
+Filename: snowflake/ml/_internal/exceptions/exceptions.py
+Comment: 
+
+Filename: snowflake/ml/_internal/exceptions/fileset_error_messages.py
+Comment: 
+
+Filename: snowflake/ml/_internal/exceptions/fileset_errors.py
+Comment: 
+
+Filename: snowflake/ml/_internal/exceptions/modeling_error_messages.py
+Comment: 
+
 Filename: snowflake/ml/_internal/file_utils.py
 Comment: 
 
 Filename: snowflake/ml/_internal/init_utils.py
 Comment: 
 
 Filename: snowflake/ml/_internal/telemetry.py
@@ -39,17 +57,14 @@
 
 Filename: snowflake/ml/_internal/utils/uri.py
 Comment: 
 
 Filename: snowflake/ml/fileset/fileset.py
 Comment: 
 
-Filename: snowflake/ml/fileset/fileset_errors.py
-Comment: 
-
 Filename: snowflake/ml/fileset/parquet_parser.py
 Comment: 
 
 Filename: snowflake/ml/fileset/sfcfs.py
 Comment: 
 
 Filename: snowflake/ml/fileset/stage_fs.py
@@ -111,23 +126,29 @@
 
 Filename: snowflake/ml/model/_handlers/_base.py
 Comment: 
 
 Filename: snowflake/ml/model/_handlers/custom.py
 Comment: 
 
+Filename: snowflake/ml/model/_handlers/mlflow.py
+Comment: 
+
 Filename: snowflake/ml/model/_handlers/pytorch.py
 Comment: 
 
 Filename: snowflake/ml/model/_handlers/sklearn.py
 Comment: 
 
 Filename: snowflake/ml/model/_handlers/snowmlmodel.py
 Comment: 
 
+Filename: snowflake/ml/model/_handlers/tensorflow.py
+Comment: 
+
 Filename: snowflake/ml/model/_handlers/torchscript.py
 Comment: 
 
 Filename: snowflake/ml/model/_handlers/xgboost.py
 Comment: 
 
 Filename: snowflake/ml/model/_model.py
@@ -135,14 +156,41 @@
 
 Filename: snowflake/ml/model/_model_handler.py
 Comment: 
 
 Filename: snowflake/ml/model/_model_meta.py
 Comment: 
 
+Filename: snowflake/ml/model/_signatures/base_handler.py
+Comment: 
+
+Filename: snowflake/ml/model/_signatures/builtins_handler.py
+Comment: 
+
+Filename: snowflake/ml/model/_signatures/core.py
+Comment: 
+
+Filename: snowflake/ml/model/_signatures/numpy_handler.py
+Comment: 
+
+Filename: snowflake/ml/model/_signatures/pandas_handler.py
+Comment: 
+
+Filename: snowflake/ml/model/_signatures/pytorch_handler.py
+Comment: 
+
+Filename: snowflake/ml/model/_signatures/snowpark_handler.py
+Comment: 
+
+Filename: snowflake/ml/model/_signatures/tensorflow_handler.py
+Comment: 
+
+Filename: snowflake/ml/model/_signatures/utils.py
+Comment: 
+
 Filename: snowflake/ml/model/custom_model.py
 Comment: 
 
 Filename: snowflake/ml/model/model_signature.py
 Comment: 
 
 Filename: snowflake/ml/model/type_hints.py
@@ -762,17 +810,17 @@
 
 Filename: snowflake/ml/utils/sparse.py
 Comment: 
 
 Filename: snowflake/ml/version.py
 Comment: 
 
-Filename: snowflake_ml_python-1.0.3.dist-info/WHEEL
+Filename: snowflake_ml_python-1.0.4.dist-info/WHEEL
 Comment: 
 
-Filename: snowflake_ml_python-1.0.3.dist-info/METADATA
+Filename: snowflake_ml_python-1.0.4.dist-info/METADATA
 Comment: 
 
-Filename: snowflake_ml_python-1.0.3.dist-info/RECORD
+Filename: snowflake_ml_python-1.0.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## snowflake/ml/_internal/env_utils.py

```diff
@@ -1,9 +1,10 @@
 import collections
 import copy
+import re
 import textwrap
 import warnings
 from importlib import metadata as importlib_metadata
 from typing import DefaultDict, Dict, List, Optional, Tuple
 
 from packaging import requirements, specifiers, utils as packaging_utils, version
 
@@ -12,14 +13,16 @@
 from snowflake.ml._internal.utils import query_result_checker
 from snowflake.snowpark import session
 
 _SNOWML_PKG_NAME = "snowflake-ml-python"
 _INFO_SCHEMA_PACKAGES_HAS_RUNTIME_VERSION: Optional[bool] = None
 _SNOWFLAKE_CONDA_PACKAGE_CACHE: Dict[str, List[version.Version]] = {}
 
+DEFAULT_CHANNEL_NAME = ""
+
 
 def _validate_pip_requirement_string(req_str: str) -> requirements.Requirement:
     """Validate the input pip requirement string according to PEP 508.
 
     Args:
         req_str: The string contains the pip requirement specification.
 
@@ -221,50 +224,14 @@
         A new requirement object without version specifier while others kept.
     """
     new_req = copy.deepcopy(req)
     new_req.specifier = specifiers.SpecifierSet()
     return new_req
 
 
-def resolve_conda_environment(
-    packages: List[requirements.Requirement], channels: List[str], python_version: str
-) -> Optional[List[str]]:
-    """Use conda api to check if given packages are resolvable in given channels. Only work when conda is
-        locally installed.
-
-    Args:
-        packages: Packages to be installed.
-        channels: Anaconda channels (name or url) where conda should search into.
-        python_version: A string of python version where model is run.
-
-    Returns:
-        List of frozen dependencies represented in PEP 508 form if resolvable, None otherwise.
-    """
-    from conda import exceptions as conda_exceptions
-    from conda_libmamba_solver import solver
-
-    package_names = list(map(lambda x: x.name, packages))
-    specs = list(map(str, packages)) + [f"python=={python_version}"]
-
-    conda_solver = solver.LibMambaSolver("snow-env", channels=channels, specs_to_add=specs)
-    try:
-        solve_result = conda_solver.solve_final_state()
-    except (
-        conda_exceptions.ResolvePackageNotFound,
-        conda_exceptions.UnsatisfiableError,
-        conda_exceptions.PackagesNotFoundError,
-        solver.LibMambaUnsatisfiableError,
-    ):
-        return None
-
-    return sorted(
-        f"{pkg_record.name}=={pkg_record.version}" for pkg_record in solve_result if pkg_record.name in package_names
-    )
-
-
 def _check_runtime_version_column_existence(session: session.Session) -> bool:
     sql = textwrap.dedent(
         """
         SHOW COLUMNS
         LIKE 'runtime_version'
         IN TABLE information_schema.packages;
         """
@@ -347,7 +314,28 @@
             raise ValueError("At most 1 version specifier using == operator is supported without local conda resolver.")
         available_versions = list(req.specifier.filter(set(_SNOWFLAKE_CONDA_PACKAGE_CACHE.get(req.name, []))))
         if not available_versions:
             return None
         else:
             ret_list.append(str(req))
     return sorted(ret_list)
+
+
+# We have to use re to support MLFlow generated python string, which use = rather than ==
+PYTHON_VERSION_PATTERN = re.compile(r"python(?:(?P<op>=|==|>|<|>=|<=|~=|===)(?P<ver>\d(?:\.\d+)+))?")
+
+
+def parse_python_version_string(dep: str) -> Optional[str]:
+    if dep.startswith("python"):
+        m = PYTHON_VERSION_PATTERN.search(dep)
+        if m is None:
+            return None
+        op = m.group("op")
+        if op and (op != "=" and op != "=="):
+            raise ValueError("Unsupported operator for python version specifier.")
+        ver = m.group("ver")
+        if ver:
+            return ver
+        else:
+            # "python" only, no specifier
+            return ""
+    return None
```

## snowflake/ml/_internal/file_utils.py

```diff
@@ -149,19 +149,23 @@
             elif path.is_dir():
                 hash = _update_hash_from_dir(path, hash)
         return hash
 
     return _update_hash_from_dir(directory, hashlib.sha1()).hexdigest()
 
 
-def get_all_modules(dirname: str, prefix: str = "") -> List[pkgutil.ModuleInfo]:
+def get_all_modules(dirname: str, prefix: str = "") -> List[str]:
+    modules = [mod.name for mod in pkgutil.iter_modules([dirname], prefix=prefix)]
     subdirs = [f.path for f in os.scandir(dirname) if f.is_dir()]
-    modules = list(pkgutil.iter_modules(subdirs, prefix=prefix))
-    for dirname in subdirs:
-        modules.extend(get_all_modules(dirname, prefix=f"{prefix}.{dirname}" if prefix else dirname))
+    for sub_dirname in subdirs:
+        basename = os.path.basename(sub_dirname)
+        sub_dir_namespace = f"{prefix}{basename}"
+        if sub_dir_namespace not in modules:
+            modules.append(sub_dir_namespace)
+        modules.extend(get_all_modules(sub_dirname, prefix=f"{sub_dir_namespace}."))
     return modules
 
 
 def _able_ascii_encode(s: str) -> bool:
     try:
         s.encode("ascii", errors="strict")
         return True
```

## snowflake/ml/_internal/telemetry.py

```diff
@@ -21,15 +21,19 @@
 )
 
 from typing_extensions import ParamSpec
 
 from snowflake import connector
 from snowflake.connector import telemetry as connector_telemetry, time_util
 from snowflake.ml._internal import env
-from snowflake.snowpark import dataframe, exceptions, session
+from snowflake.ml._internal.exceptions import (
+    error_codes,
+    exceptions as snowml_exceptions,
+)
+from snowflake.snowpark import dataframe, exceptions as snowpark_exceptions, session
 from snowflake.snowpark._internal import utils
 
 _log_counter = 0
 _FLUSH_SIZE = 10
 
 _Args = ParamSpec("_Args")
 _ReturnValue = TypeVar("_ReturnValue")
@@ -43,14 +47,15 @@
     TYPE_FUNCTION_USAGE = "function_usage"
     # message keys for telemetry
     KEY_PROJECT = "project"
     KEY_SUBPROJECT = "subproject"
     KEY_FUNC_NAME = "func_name"
     KEY_FUNC_PARAMS = "func_params"
     KEY_ERROR_INFO = "error_info"
+    KEY_ERROR_CODE = "error_code"
     KEY_VERSION = "version"
     KEY_PYTHON_VERSION = "python_version"
     KEY_OS = "operating_system"
     KEY_DATA = "data"
     KEY_CATEGORY = "category"
     KEY_API_CALLS = "api_calls"
     KEY_SFQIDS = "sfqids"
@@ -256,15 +261,15 @@
                 if not isinstance(conn, connector.SnowflakeConnection):
                     raise TypeError(f"Expected a conn object of type SnowflakeConnection, but got {type(conn)}")
             # get an active session
             else:
                 try:
                     active_session = next(iter(session._get_active_sessions()))
                 # server no default session
-                except exceptions.SnowparkSessionException:
+                except snowpark_exceptions.SnowparkSessionException:
                     try:
                         return func(*args, **kwargs)
                     except Exception as e:
                         # suppress SnowparkSessionException from telemetry in the stack trace
                         raise e from None
                 conn = active_session._conn._conn
                 if conn is None:
@@ -295,17 +300,19 @@
                 api_calls=api_calls,
                 sfqids=sfqids,
                 custom_tags=custom_tags,
             )
             try:
                 res = func(*args, **kwargs)
             except Exception as e:
-                error = repr(e)
-                telemetry_args["error"] = error
-                raise
+                if not isinstance(e, snowml_exceptions.SnowflakeMLException):
+                    e = snowml_exceptions.SnowflakeMLException(error_code=error_codes.UNDEFINED, original_exception=e)
+                telemetry_args["error"] = repr(e)
+                telemetry_args["error_code"] = e.error_code
+                raise e.original_exception
             else:
                 return res
             finally:
                 telemetry.send_function_usage_telemetry(**telemetry_args)
                 global _log_counter
                 _log_counter += 1
                 if _log_counter >= _FLUSH_SIZE or "error" in telemetry_args:
@@ -521,26 +528,28 @@
         func_name: str,
         function_category: str,
         func_params: Optional[Dict[str, Any]] = None,
         api_calls: Optional[List[Dict[str, Any]]] = None,
         sfqids: Optional[List[Any]] = None,
         custom_tags: Optional[Dict[str, Union[bool, int, str, float]]] = None,
         error: Optional[str] = None,
+        error_code: Optional[str] = None,
     ) -> None:
         """
         Send function usage telemetry message.
 
         Args:
             func_name: Function name.
             function_category: Function category.
             func_params: Function parameters.
             api_calls: API calls.
             sfqids: Snowflake query IDs.
             custom_tags: Custom tags.
             error: Error.
+            error_code: Error code.
         """
         data: Dict[str, Any] = {
             TelemetryField.KEY_FUNC_NAME.value: func_name,
             TelemetryField.KEY_CATEGORY.value: function_category,
         }
         if func_params:
             data[TelemetryField.KEY_FUNC_PARAMS.value] = func_params
@@ -555,14 +564,15 @@
         message: Dict[str, Any] = {
             **self._create_basic_telemetry_data(telemetry_type),
             TelemetryField.KEY_DATA.value: data,
         }
 
         if error:
             message[TelemetryField.KEY_ERROR_INFO.value] = error
+            message[TelemetryField.KEY_ERROR_CODE.value] = error_code
 
         self._send(message)
 
     @suppress_exceptions
     def send_batch(self) -> None:
         """Send the telemetry data batch immediately."""
         if self._telemetry:
```

## snowflake/ml/_internal/utils/parallelize.py

```diff
@@ -76,15 +76,15 @@
             n_output_cols = len(mapped_df.columns)
 
         if partition_id == n_partitions - 1:
             last_partition_df = mapped_df
         else:
             if n_output_cols != len(mapped_df.columns):
                 raise Exception("All partitions must contain the same number of columns.")
-            mapped_df = mapped_df.with_column(partition_id_col, F.lit(partition_id))  # type: ignore
+            mapped_df = mapped_df.with_column(partition_id_col, F.lit(partition_id))
             unioned_df = mapped_df if unioned_df is None else unioned_df.union_all(mapped_df)
 
     # Store results in a list of size |n_partitions| x |n_rows| x |n_output_cols|
     all_results: List[List[List[Any]]] = [[] for _ in range(n_partitions - 1)]
 
     # Collect the results of the first n-1 partitions, removing the partition_id column
     unioned_result = unioned_df.collect(statement_params=statement_params) if unioned_df is not None else []
```

## snowflake/ml/_internal/utils/query_result_checker.py

```diff
@@ -115,45 +115,17 @@
                 [{row_idx}][{expected_col_name}]. Actual value at position [{row_idx}][{expected_col_name}] was
                 '{result[row_idx][expected_col_name]}'.{_query_log(sql)}"""
             )
         )
     return True
 
 
-def cell_value_partial_matcher(
-    row_idx: int, col_idx: int, expected_value: Any, result: list[snowpark.Row], sql: str | None = None
-) -> bool:
-    """Returns true if `expected_value` is found in `result[row_idx, col_idx]` cell. Raise exception otherwise."""
-    if len(result) <= row_idx or len(result[row_idx]) <= col_idx:
-        raise connector.DataError(
-            formatting.unwrap(
-                f"""Query Result did not have required number of rows x col [{row_idx}][{col_idx}]. Result from
-                    operation was: {result}.{_query_log(sql)}"""
-            )
-        )
-    validated = False
-    if isinstance(expected_value, str):
-        validated = expected_value in result[row_idx][col_idx]
-    else:
-        validated = expected_value == result[row_idx][col_idx]
-    if not validated:
-        raise connector.DataError(
-            formatting.unwrap(
-                f"""Query Result did not have the expected value '{expected_value}' at expected position
-                [{row_idx}][{col_idx}]. Actual value at position [{row_idx}][{col_idx}] was
-                '{result[row_idx][col_idx]}'.{_query_log(sql)}"""
-            )
-        )
-    return True
-
-
 _DEFAULT_MATCHERS = [
     partial(result_dimension_matcher, 1, 1),
     partial(column_name_matcher, "status"),
-    partial(cell_value_partial_matcher, 0, 0, "successfully"),
 ]
 
 
 class ResultValidator:
     """Convenience wrapper for validation SnowPark DataFrames that are returned as the result of session operations.
 
     Usage Example:
@@ -195,29 +167,14 @@
 
         Returns:
             ResultValidator object (self)
         """
         self._success_matchers.append(partial(column_name_matcher, expected_col_name))
         return self
 
-    def has_value_match(self, row_idx: int, col_idx: int, expected_value: Any) -> ResultValidator:
-        """Validate that the a column with the name `expected_column_name` exists in the result.
-
-        Args:
-            row_idx: Row index of the cell that needs to match.
-            col_idx: Column index of the cell that needs to match.
-            expected_value: Value that the cell needs to match. For strings it is treated as a substring match, all
-                other types will expect an exact match.
-
-        Returns:
-            ResultValidator object (self)
-        """
-        self._success_matchers.append(partial(cell_value_partial_matcher, row_idx, col_idx, expected_value))
-        return self
-
     def has_named_value_match(self, row_idx: int, col_name: str, expected_value: Any) -> ResultValidator:
         """Validate that the column `col_name` in row `row_idx` of ther results exists and matches `expected_value`.
 
         Args:
             row_idx: Row index of the cell that needs to match.
             col_name: Column name of the cell that needs to match.
             expected_value: Value that the cell needs to match. For strings it is treated as a substring match, all
```

## snowflake/ml/fileset/fileset.py

```diff
@@ -2,17 +2,23 @@
 import inspect
 import logging
 from typing import Any, Callable, List, Optional
 
 from snowflake import snowpark
 from snowflake.connector import connection
 from snowflake.ml._internal import telemetry
+from snowflake.ml._internal.exceptions import (
+    error_codes,
+    exceptions as snowml_exceptions,
+    fileset_error_messages,
+    fileset_errors,
+)
 from snowflake.ml._internal.utils import identifier, import_utils
-from snowflake.ml.fileset import fileset_errors, sfcfs
-from snowflake.snowpark import exceptions, functions, types
+from snowflake.ml.fileset import sfcfs
+from snowflake.snowpark import exceptions as snowpark_exceptions, functions, types
 
 # The max file size for data loading.
 TARGET_FILE_SIZE = 32 * 2**20
 
 # Expected type of a stage where a FileSet can be located.
 # The type is the value of the 'type' column of a `show stages` query.
 _FILESET_STAGE_TYPE = "INTERNAL NO CSE"
@@ -22,23 +28,27 @@
 
 def _raise_if_deleted(func: Callable[..., Any]) -> Callable[..., Any]:
     """A function decorator where an error will be raised when the fileset has been deleted."""
 
     @functools.wraps(func)
     def raise_if_deleted_helper(self: Any, *args: Any, **kwargs: Any) -> Any:
         if self._is_deleted:
-            raise fileset_errors.FileSetAlreadyDeletedError("The FileSet has already been deleted.")
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.SNOWML_DELETE_FAILED,
+                original_exception=fileset_errors.FileSetAlreadyDeletedError("The FileSet has already been deleted."),
+            )
         return func(self, *args, **kwargs)
 
     return raise_if_deleted_helper
 
 
 class FileSet:
     """A FileSet represents an immutable snapshot of the result of a query in the form of files."""
 
+    @telemetry.send_api_usage_telemetry(project=_PROJECT)
     def __init__(
         self,
         *,
         target_stage_loc: str,
         name: str,
         sf_connection: Optional[connection.SnowflakeConnection] = None,
         snowpark_session: Optional[snowpark.Session] = None,
@@ -76,17 +86,17 @@
                                          target_stage_loc="@mydb.mychema.mystage/mydir",
                                          name="helloworld")
         >>> my_fileset.files()
         ----
         ['sfc://@mydb.myschema.mystage/mydir/helloworld/data_0_0_0.snappy.parquet']
         """
         if sf_connection and snowpark_session:
-            raise ValueError("sf_connection and snowpark_session cannot be specified at the same time.")
+            raise ValueError(fileset_error_messages.BOTH_SF_CONNECTION_AND_SNOWPARK_SESSION_SPECIFIED)
         if not sf_connection and not snowpark_session:
-            raise ValueError("sf_connection or snowpark_session must be provided")
+            raise ValueError(fileset_error_messages.NO_SF_CONNECTION_OR_SNOWPARK_SESSION)
         self._snowpark_session = (
             snowpark_session
             if snowpark_session
             else snowpark.Session.builder.config("connection", sf_connection).create()
         )
         self._target_stage_loc = target_stage_loc
         _validate_target_stage_loc(self._snowpark_session, self._target_stage_loc)
@@ -134,14 +144,16 @@
             A FileSet object.
 
         Raises:
             ValueError: An error occured when not exactly one of sf_connection and snowpark_session is given.
             FileSetExistError: An error occured whern a FileSet with the same name exists in the given path.
             FileSetError: An error occured when the SQL query/dataframe is not able to get materialized.
 
+        # noqa: DAR401
+
         Note: During the generation of stage files, data casting will occur. The casting rules are as follows::
             - Data casting:
                 - DecimalType(NUMBER):
                     - If its scale is zero, cast to BIGINT
                     - If its scale is non-zero, cast to FLOAT
                 - DoubleType(DOUBLE): Cast to FLOAT.
                 - ByteType(TINYINT): Cast to SMALLINT.
@@ -184,33 +196,36 @@
         >>>     snowpark_dataframe=df,
         >>> )
         >>> my_fileset.files()
         ----
         ['sfc://@mydb.myschema.mystage/helloworld/data_0_0_0.snappy.parquet']
         """
         if snowpark_dataframe and sf_connection:
-            raise ValueError("sf_connection and snowpark_session cannot be specified at the same time.")
+            raise ValueError(fileset_error_messages.BOTH_SF_CONNECTION_AND_SNOWPARK_SESSION_SPECIFIED)
 
         if not snowpark_dataframe:
             if not sf_connection:
-                raise ValueError("Either snowpark_dataframe or sf_connection should be non-empty.")
+                raise ValueError(fileset_error_messages.NO_SF_CONNECTION_OR_SNOWPARK_DATAFRAME)
             if not query:
                 raise ValueError("Please use non-empty query to generate meaningful result.")
             snowpark_session = snowpark.Session.builder.config("connection", sf_connection).create()
             snowpark_dataframe = snowpark_session.sql(query)
 
         assert snowpark_dataframe is not None
         assert snowpark_dataframe._session is not None
         snowpark_session = snowpark_dataframe._session
         casted_df = _cast_snowpark_dataframe(snowpark_dataframe)
 
-        _validate_target_stage_loc(snowpark_session, target_stage_loc)
+        try:
+            _validate_target_stage_loc(snowpark_session, target_stage_loc)
+        except snowml_exceptions.SnowflakeMLException as e:
+            raise e.original_exception
         target_stage_exists = snowpark_session.sql(f"List {_fileset_absoluate_path(target_stage_loc, name)}").collect()
         if target_stage_exists:
-            raise fileset_errors.FileSetExistError(f"FileSet with name {name} has already existed.")
+            raise fileset_errors.FileSetExistError(fileset_error_messages.FILESET_ALREADY_EXISTS.format(name))
 
         if shuffle:
             casted_df = casted_df.order_by(functions.random())
 
         try:
             # partition_by helps generate more uniform sharding among files.
             # As a side effect, the sizes of generate files might exceed max_file_size.
@@ -228,28 +243,39 @@
                     project=_PROJECT,
                     function_name=telemetry.get_statement_params_full_func_name(
                         inspect.currentframe(), cls.__class__.__name__
                     ),
                     api_calls=[snowpark.DataFrameWriter.copy_into_location],
                 ),
             )
-        except exceptions.SnowparkClientException as e:
+        except snowpark_exceptions.SnowparkClientException as e:
             # Snowpark wraps the Python Connector error code in the head of the error message.
             if e.message.startswith(fileset_errors.ERRNO_FILE_EXIST_IN_STAGE):
-                raise fileset_errors.FileSetExistError(f"FileSet with name {name} has already existed.")
+                raise fileset_errors.FileSetExistError(fileset_error_messages.FILESET_ALREADY_EXISTS.format(name))
             else:
                 raise fileset_errors.FileSetError(str(e))
 
         return cls(target_stage_loc=target_stage_loc, name=name, snowpark_session=snowpark_session)
 
     @property
     def name(self) -> str:
         """Get the name of the FileSet."""
         return self._name
 
+    def _list_files(self) -> List[str]:
+        """Private helper function that lists all files in this fileset and caches the results for subsequent use."""
+        if self._files:
+            return self._files
+        loc = self._fileset_absolute_path()
+
+        # TODO(zzhu)[SNOW-703491]: We could use manifest file to speed up file listing
+        files = self._fs.ls(loc)
+        self._files = [f"sfc://{file}" for file in files]
+        return self._files
+
     def _fileset_absolute_path(self) -> str:
         """Get the Snowflake absoluate path to this FileSet directory."""
         return _fileset_absoluate_path(self._target_stage_loc, self.name)
 
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
     )
@@ -266,22 +292,15 @@
         Example:
         >>> my_fileset = FileSet(sf_connection=conn, target_stage_loc="@mydb.mychema.mystage", name="test")
         >>> my_fileset.files()
         ----
         ["sfc://@mydb.myschema.mystage/test/hello_world_0_0_0.snappy.parquet",
          "sfc://@mydb.myschema.mystage/test/hello_world_0_0_1.snappy.parquet"]
         """
-        if self._files:
-            return self._files
-        loc = self._fileset_absolute_path()
-
-        # TODO(zzhu)[SNOW-703491]: We could use manifest file to speed up file listing
-        files = self._fs.ls(loc)
-        self._files = [f"sfc://{file}" for file in files]
-        return self._files
+        return self._list_files()
 
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
     )
     @snowpark._internal.utils.private_preview(version="0.2.0")
     @_raise_if_deleted
     def fileset_stage_location(self) -> str:
@@ -333,17 +352,17 @@
         >>>     print(data)
         ----
         {'_COL_1':[10]}
         """
         IterableWrapper, _ = import_utils.import_or_get_dummy("torchdata.datapipes.iter.IterableWrapper")
         torch_datapipe_module, _ = import_utils.import_or_get_dummy("snowflake.ml.fileset.torch_datapipe")
 
-        self._fs.optimize_read(self.files())
+        self._fs.optimize_read(self._list_files())
 
-        input_dp = IterableWrapper(self.files())
+        input_dp = IterableWrapper(self._list_files())
         return torch_datapipe_module.ReadAndParseParquet(input_dp, self._fs, batch_size, shuffle, drop_last_batch)
 
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         func_params_to_log=["batch_size", "shuffle", "drop_last_batch"],
     )
     @snowpark._internal.utils.private_preview(version="0.2.0")
@@ -372,17 +391,19 @@
         >>> for data in dp:
         >>>     print(data)
         ----
         {'_COL_1': <tf.Tensor: shape=(1,), dtype=int64, numpy=[10]>}
         """
         tf_dataset_module, _ = import_utils.import_or_get_dummy("snowflake.ml.fileset.tf_dataset")
 
-        self._fs.optimize_read(self.files())
+        self._fs.optimize_read(self._list_files())
 
-        return tf_dataset_module.read_and_parse_parquet(self.files(), self._fs, batch_size, shuffle, drop_last_batch)
+        return tf_dataset_module.read_and_parse_parquet(
+            self._list_files(), self._fs, batch_size, shuffle, drop_last_batch
+        )
 
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
     )
     @snowpark._internal.utils.private_preview(version="0.2.0")
     @_raise_if_deleted
     def to_snowpark_dataframe(self) -> snowpark.DataFrame:
@@ -395,15 +416,15 @@
             A Snowpark dataframe that contains the data of this FileSet.
 
         Note: The dataframe generated by this method might not have the same schema as the original one. Specifically,
             - NUMBER type with scale != 0 will become float.
             - Unsupported types (see comments of :func:`~FileSet.fileset.make`) will not have any guarantee.
                 For example, an OBJECT column may be scanned back as a STRING column.
         """
-        query_id = _get_fileset_query_id_or_raise(self.files(), self._fileset_absolute_path())
+        query_id = _get_fileset_query_id_or_raise(self._list_files(), self._fileset_absolute_path())
         file_path_pattern = f".*data_{query_id}.*[.]parquet"
         df = self._snowpark_session.read.option("pattern", file_path_pattern).parquet(self._fileset_absolute_path())
         assert isinstance(df, snowpark.DataFrame)
         return df
 
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
@@ -412,30 +433,33 @@
     @_raise_if_deleted
     def delete(self) -> None:
         """Delete the FileSet directory and all the stage files in it.
 
         If not called, the FileSet and all its stage files will stay in Snowflake stage.
 
         Raises:
-            FileSetCannotDeleteError: An error occured when the FileSet cannot get deleted.
+            SnowflakeMLException: An error occured when the FileSet cannot get deleted.
         """
         delete_sql = f"remove {self._fileset_absolute_path()}"
         try:
             self._snowpark_session.sql(delete_sql).collect(
                 statement_params=telemetry.get_function_usage_statement_params(
                     project=_PROJECT,
                     function_name=telemetry.get_statement_params_full_func_name(
                         inspect.currentframe(), self.__class__.__name__
                     ),
                 ),
             )
             self._files = []
             self._is_deleted = True
-        except exceptions.SnowparkClientException as e:
-            raise fileset_errors.FileSetCannotDeleteError(e)
+        except snowpark_exceptions.SnowparkClientException as e:
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.SNOWML_DELETE_FAILED,
+                original_exception=fileset_errors.FileSetCannotDeleteError(str(e)),
+            )
         return
 
 
 def _get_fileset_query_id_or_raise(files: List[str], fileset_absolute_path: str) -> Optional[str]:
     """Obtain the query ID used to generate the FileSet stage files.
 
     If the input stage files are not generated by the same query, an error will be raised.
@@ -444,15 +468,15 @@
         files: A list of stage file paths follows sfc protocol
         fileset_absolute_path: the Snowflake absoluate path to this FileSet directory
 
     Returns:
         The query id of the sql query which is used to generate the stage files.
 
     Raises:
-        MoreThanOneQuerySourceError: If the input files are not generated by the same query.
+        SnowflakeMLException: If the input files are not generated by the same query.
     """
     if not files:
         return None
 
     valid = True
     query_id = None
     common_prefix = f"sfc://{fileset_absolute_path}data_"
@@ -467,15 +491,20 @@
                 valid = False
                 break
         else:
             idx = truncatred_filename.find("_")
             query_id = truncatred_filename[:idx]
 
     if not valid:
-        raise fileset_errors.MoreThanOneQuerySourceError("This FileSet contains files generated by the other queries.")
+        raise snowml_exceptions.SnowflakeMLException(
+            error_code=error_codes.SNOWML_INVALID_QUERY,
+            original_exception=fileset_errors.MoreThanOneQuerySourceError(
+                "This FileSet contains files generated by the other queries."
+            ),
+        )
     return query_id
 
 
 def _validate_target_stage_loc(snowpark_session: snowpark.Session, target_stage_loc: str) -> bool:
     """Validate the input stage location is in the right format and the target stage is an internal SSE stage.
 
     A valid format for the input stage location should be '@<database>.<schema>.<stage>/<optional_directories>/',
@@ -486,31 +515,42 @@
         target_stage_loc: Path to the target location. Should be in the form of
             '@<database>.<schema>.<stage>/<optional_directories>/'
 
     Returns:
         A Boolean value about whether the input target stage location is a valid path in an internal SSE stage.
 
     Raises:
-        FileSetLocationError: An error occured when the input stage path is invalid.
+        SnowflakeMLException: The input stage path does not start with '@'.
+        SnowflakeMLException: No valid stages found.
+        SnowflakeMLException: An error occured when the input stage path is invalid.
     """
     if not target_stage_loc.startswith("@"):
-        raise fileset_errors.FileSetLocationError('FileSet location should start with "@".')
+        raise snowml_exceptions.SnowflakeMLException(
+            error_code=error_codes.SNOWML_INVALID_STAGE,
+            original_exception=fileset_errors.FileSetLocationError('FileSet location should start with "@".'),
+        )
     try:
         db, schema, stage, _ = identifier.parse_schema_level_object_identifier(target_stage_loc[1:])
         df_stages = snowpark_session.sql(f"Show stages like '{stage}' in SCHEMA {db}.{schema}")
-        df_stages = df_stages.filter(functions.col('"type"').like(f"%{_FILESET_STAGE_TYPE}%"))  # type:ignore[arg-type]
+        df_stages = df_stages.filter(functions.col('"type"').like(f"%{_FILESET_STAGE_TYPE}%"))
         valid_stage = df_stages.collect()
         if not valid_stage:
-            raise fileset_errors.FileSetLocationError(
-                "A FileSet requires its location to be in an existing server-side-encrypted internal stage."
-                "See https://docs.snowflake.com/en/sql-reference/sql/create-stage#internal-stage-parameters-internalstageparams "  # noqa: E501
-                "on how to create such a stage."
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.SNOWML_NOT_FOUND,
+                original_exception=fileset_errors.FileSetLocationError(
+                    "A FileSet requires its location to be in an existing server-side-encrypted internal stage."
+                    "See https://docs.snowflake.com/en/sql-reference/sql/create-stage#internal-stage-parameters-internalstageparams "  # noqa: E501
+                    "on how to create such a stage."
+                ),
             )
     except ValueError as e:
-        raise fileset_errors.FileSetLocationError(e)
+        raise snowml_exceptions.SnowflakeMLException(
+            error_code=error_codes.SNOWML_INVALID_STAGE,
+            original_exception=fileset_errors.FileSetLocationError(str(e)),
+        )
     return True
 
 
 def _fileset_absoluate_path(target_stage_loc: str, fileset_name: str) -> str:
     """Get the Snowflake absoluate path to a FileSet.
 
     Args:
@@ -548,27 +588,27 @@
     for field in fields:
         src = field.column_identifier.quoted_name
         if isinstance(field.datatype, types.DecimalType):
             if field.datatype.scale:
                 dest: types.DataType = types.FloatType()
             else:
                 dest = types.LongType()
-            selected_cols.append(functions.cast(functions.col(src), dest).alias(src))  # type:ignore[arg-type]
+            selected_cols.append(functions.cast(functions.col(src), dest).alias(src))
         elif isinstance(field.datatype, types.DoubleType):
             dest = types.FloatType()
-            selected_cols.append(functions.cast(functions.col(src), dest).alias(src))  # type:ignore[arg-type]
+            selected_cols.append(functions.cast(functions.col(src), dest).alias(src))
         elif isinstance(field.datatype, types.ByteType):
             # Snowpark maps ByteType to BYTEINT, which will not do the casting job when unloading to parquet files.
             # We will use SMALLINT instead until this issue got fixed.
             # Investigate JIRA filed: SNOW-725041
             dest = types.ShortType()
-            selected_cols.append(functions.cast(functions.col(src), dest).alias(src))  # type:ignore[arg-type]
+            selected_cols.append(functions.cast(functions.col(src), dest).alias(src))
         elif field.datatype in (types.ShortType(), types.IntegerType(), types.LongType()):
             dest = field.datatype
-            selected_cols.append(functions.cast(functions.col(src), dest).alias(src))  # type:ignore[arg-type]
+            selected_cols.append(functions.cast(functions.col(src), dest).alias(src))
         else:
             if field.datatype in (types.DateType(), types.TimestampType(), types.TimeType()):
                 logging.warning(
                     "A Column with DATE or TIMESTAMP data type detected. "
                     "It might not be able to get converted to tensors. "
                     "Please consider handle it in feature engineering."
                 )
```

## snowflake/ml/fileset/stage_fs.py

```diff
@@ -6,16 +6,21 @@
 
 import fsspec
 from fsspec.implementations import http as httpfs
 
 from snowflake import snowpark
 from snowflake.connector import connection
 from snowflake.ml._internal import telemetry
-from snowflake.ml.fileset import fileset_errors
-from snowflake.snowpark import exceptions
+from snowflake.ml._internal.exceptions import (
+    error_codes,
+    exceptions as snowml_exceptions,
+    fileset_error_messages,
+    fileset_errors,
+)
+from snowflake.snowpark import exceptions as snowpark_exceptions
 
 # The default length of how long a presigned url stays active in seconds.
 # Presigned url here is used to fetch file objects from Snowflake when SFStageFileSystem.open() is called.
 _PRESIGNED_URL_LIFETIME_SEC = 14400
 
 # The threshold of when the presigned url should get refreshed before its expiration.
 _PRESIGNED_URL_HEADROOM_SEC = 3600
@@ -93,17 +98,17 @@
                 - cache_type, cache_options, block_size: Configure file buffering.
                 See more information in https://filesystem-spec.readthedocs.io/en/latest/features.html
 
         Raises:
             ValueError: An error occured when not exactly one of sf_connection and snowpark_session is given.
         """
         if sf_connection and snowpark_session:
-            raise ValueError("sf_connection and snowpark_session cannot be specified at the same time.")
+            raise ValueError(fileset_error_messages.BOTH_SF_CONNECTION_AND_SNOWPARK_SESSION_SPECIFIED)
         if not sf_connection and not snowpark_session:
-            raise ValueError("sf_connection or snowpark_session must be provided")
+            raise ValueError(fileset_error_messages.NO_SF_CONNECTION_OR_SNOWPARK_SESSION)
         if sf_connection:
             self._session = snowpark.Session.builder.config("connection", sf_connection).create()
         else:
             self._session = snowpark_session
 
         logging.debug(f"Creating new stage file system on @{db}.{schema}.{stage}")
         self._db = db
@@ -144,25 +149,33 @@
                 returned. If set to be True, each list item in the result is a dict, whose keys are "name", "size",
                 "type", "md5" and "last_modified".
 
         Returns:
             A list of filename if `detail` is false, or a list of dict if `detail` is true.
 
         Raises:
-            StageNotFoundError: An error occured when the given path points to a stage that cannot be found.
-            FileSetError: An error occured when Snowflake cannot list files in the given stage path.
+            SnowflakeMLException: An error occured when the given path points to a stage that cannot be found.
+            SnowflakeMLException: An error occured when Snowflake cannot list files in the given stage path.
         """
         try:
             loc = self.stage_name
             objects = self._session.sql(f"LIST {loc}/{path}").collect()
-        except exceptions.SnowparkClientException as e:
+        except snowpark_exceptions.SnowparkClientException as e:
             if e.message.startswith(fileset_errors.ERRNO_DOMAIN_NOT_EXIST):
-                raise fileset_errors.StageNotFoundError(f"Stage {loc} does not exist or is not authorized.")
+                raise snowml_exceptions.SnowflakeMLException(
+                    error_code=error_codes.SNOWML_NOT_FOUND,
+                    original_exception=fileset_errors.StageNotFoundError(
+                        f"Stage {loc} does not exist or is not authorized."
+                    ),
+                )
             else:
-                raise fileset_errors.FileSetError(str(e))
+                raise snowml_exceptions.SnowflakeMLException(
+                    error_code=error_codes.INTERNAL_SNOWML_ERROR,
+                    original_exception=fileset_errors.FileSetError(str(e)),
+                )
         files = self._parse_list_result(objects, path)
         if detail:
             return files
         else:
             return [f["name"] for f in files]
 
     @telemetry.send_api_usage_telemetry(
@@ -207,15 +220,15 @@
             **kwargs: Extra options that supported by fsspec. See more in
                 https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.open
 
         Returns:
             A fsspec file-like object.
 
         Raises:
-            StageFileNotFoundError: An error occured when the given path points to a file that cannot be found.
+            SnowflakeMLException: An error occured when the given path points to a file that cannot be found.
         """
         path = path.lstrip("/")
         cached_presigned_url = self._url_cache.get(path, None)
         if not cached_presigned_url:
             res = self._fetch_presigned_urls([path])
             url = res[0][1]
             expire_at = time.time() + _PRESIGNED_URL_LIFETIME_SEC
@@ -225,15 +238,18 @@
         elif cached_presigned_url.is_expiring():
             self.optimize_read()
             cached_presigned_url = self._url_cache[path]
         url = cached_presigned_url.url
         try:
             return self._fs._open(url, mode=mode, **kwargs)
         except FileNotFoundError:
-            raise fileset_errors.StageFileNotFoundError(f"Stage file {path} doesn't exist.")
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.SNOWML_NOT_FOUND,
+                original_exception=fileset_errors.StageFileNotFoundError(f"Stage file {path} doesn't exist."),
+            )
 
     def _parse_list_result(
         self, list_result: List[Tuple[str, int, str, str]], search_path: str
     ) -> List[Dict[str, Any]]:
         """Convert the result from LIST query to the expected format of fsspec ls() method.
 
         Note that Snowflake LIST query has different behavior with ls(). LIST query will return all the stage files
@@ -324,13 +340,21 @@
                     project=_PROJECT,
                     function_name=telemetry.get_statement_params_full_func_name(
                         inspect.currentframe(), self.__class__.__name__
                     ),
                     api_calls=[snowpark.DataFrame.collect],
                 ),
             )
-        except exceptions.SnowparkClientException as e:
+        except snowpark_exceptions.SnowparkClientException as e:
             if e.message.startswith(fileset_errors.ERRNO_STAGE_NOT_EXIST):
-                raise fileset_errors.StageNotFoundError(f"Stage {self.stage_name} does not exist or is not authorized.")
+                raise snowml_exceptions.SnowflakeMLException(
+                    error_code=error_codes.SNOWML_NOT_FOUND,
+                    original_exception=fileset_errors.StageNotFoundError(
+                        f"Stage {self.stage_name} does not exist or is not authorized."
+                    ),
+                )
             else:
-                raise fileset_errors.FileSetError(str(e))
+                raise snowml_exceptions.SnowflakeMLException(
+                    error_code=error_codes.INTERNAL_SNOWML_ERROR,
+                    original_exception=fileset_errors.FileSetError(str(e)),
+                )
         return presigned_urls
```

## snowflake/ml/model/_core_requirements.py

```diff
@@ -1 +1 @@
-REQUIREMENTS=['anyio>=3.5.0,<4', 'cloudpickle', 'numpy>=1.23,<2', 'packaging>=20.9,<24', 'pandas>=1.0.0,<2', 'pyyaml>=6.0,<7', 'snowflake-snowpark-python>=1.4.0,<2', 'typing-extensions>=4.1.0,<5']
+REQUIREMENTS=['anyio>=3.5.0,<4', 'cloudpickle', 'numpy>=1.23,<2', 'packaging>=20.9,<24', 'pandas>=1.0.0,<2', 'pyyaml>=6.0,<7', 'snowflake-snowpark-python>=1.5.1,<2', 'typing-extensions>=4.1.0,<5']
```

## snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py

```diff
@@ -1,27 +1,22 @@
 import base64
 import json
 import logging
 import os
-import posixpath
 import subprocess
 import tempfile
-import zipfile
 from enum import Enum
 from typing import List
 
-import yaml
-
 from snowflake import snowpark
 from snowflake.ml._internal.utils import query_result_checker
 from snowflake.ml.model._deploy_client.image_builds import (
     base_image_builder,
     docker_context,
 )
-from snowflake.ml.model._deploy_client.utils import constants
 
 
 class Platform(Enum):
     LINUX_AMD64 = "linux/amd64"
 
 
 class ClientImageBuilder(base_image_builder.ImageBuilder):
@@ -32,28 +27,28 @@
     Requires prior installation and running of Docker with BuildKit. See installation instructions in
         https://docs.docker.com/engine/install/
 
 
     """
 
     def __init__(
-        self, *, id: str, image_repo: str, model_zip_stage_path: str, session: snowpark.Session, use_gpu: bool = False
+        self, *, id: str, image_repo: str, model_dir: str, session: snowpark.Session, use_gpu: bool = False
     ) -> None:
         """Initialization
 
         Args:
             id: A hexadecimal string used for naming the image tag.
             image_repo: Path to image repository.
-            model_zip_stage_path: Path to model zip file in stage.
+            model_dir: Local model directory, downloaded form stage and extracted.
             use_gpu: Boolean flag for generating the CPU or GPU base image.
             session: Snowpark session
         """
         self.image_tag = "/".join([image_repo.rstrip("/"), id]) + ":latest"
         self.image_repo = image_repo
-        self.model_zip_stage_path = model_zip_stage_path
+        self.model_dir = model_dir
         self.use_gpu = use_gpu
         self.session = session
 
     def build_and_upload_image(self) -> str:
         """
         Builds and uploads an image to the model registry.
         """
@@ -72,14 +67,28 @@
             credentials = f"0sessiontoken:{json.dumps(token_obj)}"
             encoded_credentials = base64.b64encode(credentials.encode("utf-8")).decode("utf-8")
             content = {"auths": {self.image_tag: {"auth": encoded_credentials}}}
             config_path = os.path.join(docker_config_dir, "config.json")
             with open(config_path, "w", encoding="utf-8") as file:
                 json.dump(content, file)
 
+        def _cleanup_local_image() -> None:
+            try:
+                image_exist_command = f"docker image inspect {self.image_tag}"
+                subprocess.check_call(
+                    image_exist_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True
+                )
+            except subprocess.CalledProcessError:
+                # Image does not exist, probably due to failed build step
+                pass
+            else:
+                commands = ["docker", "--config", config_dir, "rmi", self.image_tag]
+                logging.info(f"Removing local image: {self.image_tag}")
+                self._run_docker_commands(commands)
+
         self.validate_docker_client_env()
 
         query_result = (
             query_result_checker.SqlResultValidator(
                 self.session,
                 query="SHOW PARAMETERS LIKE 'PYTHON_CONNECTOR_QUERY_RESULT_FORMAT' IN SESSION",
             )
@@ -91,20 +100,25 @@
         with tempfile.TemporaryDirectory() as config_dir:
             try:
                 # Workaround for SNOW-841699: Fail to authenticate to image registry with session token generated from
                 # Snowpark. Need to temporarily set the json query format in order to process GS token response.
                 self.session.sql("ALTER SESSION SET PYTHON_CONNECTOR_QUERY_RESULT_FORMAT = 'json'").collect()
                 _setup_docker_config(config_dir)
                 self._build(config_dir)
-                self._upload(config_dir)
+            except Exception as e:
+                raise RuntimeError(f"Failed to build docker image: {str(e)}")
+            else:
+                try:
+                    self._upload(config_dir)
+                except Exception as e:
+                    raise RuntimeError(f"Failed to upload docker image to registry: {str(e)}")
+                finally:
+                    _cleanup_local_image()
             finally:
                 self.session.sql(f"ALTER SESSION SET PYTHON_CONNECTOR_QUERY_RESULT_FORMAT = '{prev_format}'").collect()
-                commands = ["docker", "--config", config_dir, "rmi", self.image_tag]
-                logging.info(f"Removing local image: {self.image_tag}")
-                self._run_docker_commands(commands)
         return self.image_tag
 
     def validate_docker_client_env(self) -> None:
         """Ensure docker client is running and BuildKit is enabled. Note that Buildx always uses BuildKit.
         - Ensure docker daemon is running through the "docker info" command on shell. When docker daemon is running,
         return code will be 0, else return code will be 1.
         - Ensure BuildKit is enabled by checking "docker buildx version".
@@ -125,65 +139,23 @@
             subprocess.check_call(buildx_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True)
         except subprocess.CalledProcessError:
             raise ConnectionError(
                 "Please ensured Docker is installed with BuildKit by following "
                 "https://docs.docker.com/build/buildkit/#getting-started"
             )
 
-    def _extract_model_zip(self, context_dir: str) -> str:
-        """Extract a zip file into the specified directory.
-
-        Args:
-            context_dir: Directory to extract the zip to.
-
-        Returns:
-            The extracted model directory.
-        """
-
-        local_model_zip_path = os.path.join(context_dir, posixpath.basename(self.model_zip_stage_path))
-        if zipfile.is_zipfile(local_model_zip_path):
-            extracted_model_dir = os.path.join(context_dir, constants.MODEL_DIR)
-            with zipfile.ZipFile(local_model_zip_path, "r") as model_zip:
-                if len(model_zip.namelist()) > 1:
-                    model_zip.extractall(extracted_model_dir)
-            conda_path = os.path.join(extracted_model_dir, "env", "conda.yaml")
-
-            def remove_snowml_from_conda() -> None:
-                with open(conda_path, encoding="utf-8") as file:
-                    conda_yaml = yaml.safe_load(file)
-
-                dependencies = conda_yaml["dependencies"]
-                dependencies = [dep for dep in dependencies if not dep.startswith("snowflake-ml-python")]
-
-                conda_yaml["dependencies"] = dependencies
-
-                with open(conda_path, "w", encoding="utf-8") as file:
-                    yaml.dump(conda_yaml, file)
-
-            # TODO(shchen): Remove once SNOW-840411 is landed.
-            remove_snowml_from_conda()
-        return extracted_model_dir
-
     def _build(self, docker_config_dir: str) -> None:
         """Constructs the Docker context directory and then builds a Docker image based on that context.
 
         Args:
             docker_config_dir: Path to docker configuration directory, which stores the temporary session token.
         """
 
         with tempfile.TemporaryDirectory() as context_dir:
-            # Download the model zip file that is already uploaded to stage during model registry log_model step.
-            # This is needed in order to obtain the conda and requirement file inside the model zip.
-            self.session.file.get(self.model_zip_stage_path, context_dir)
-
-            extracted_model_dir = self._extract_model_zip(context_dir)
-
-            dc = docker_context.DockerContext(
-                context_dir=context_dir, model_dir=extracted_model_dir, use_gpu=self.use_gpu
-            )
+            dc = docker_context.DockerContext(context_dir=context_dir, model_dir=self.model_dir, use_gpu=self.use_gpu)
             dc.build()
             self._build_image_from_context(context_dir=context_dir, docker_config_dir=docker_config_dir)
 
     def _run_docker_commands(self, commands: List[str]) -> None:
         """Run docker commands in a new child process.
 
         Args:
@@ -197,15 +169,15 @@
 
         if proc.stdout:
             for line in iter(proc.stdout.readline, ""):
                 output_lines.append(line)
                 logging.info(line)
 
         if proc.wait():
-            raise RuntimeError(f"Docker build failed: {''.join(output_lines)}")
+            raise RuntimeError(f"Docker commands failed: \n {''.join(output_lines)}")
 
     def _build_image_from_context(
         self, context_dir: str, docker_config_dir: str, *, platform: Platform = Platform.LINUX_AMD64
     ) -> None:
         """Builds a Docker image based on provided context.
 
         Args:
```

## snowflake/ml/model/_deploy_client/image_builds/docker_context.py

```diff
@@ -1,8 +1,7 @@
-import importlib
 import os
 import shutil
 import string
 from abc import ABC
 
 from snowflake.ml.model._deploy_client.utils import constants
 
@@ -18,47 +17,37 @@
         Args:
             context_dir: Path to context directory.
             model_dir: Path to local model directory.
             use_gpu: Boolean flag for generating the CPU or GPU base image.
         """
         self.context_dir = context_dir
         self.model_dir = model_dir
-        # TODO(shchen): SNOW-825995, Define dockerfile template used for model deployment. use_gpu will be used.
         self.use_gpu = use_gpu
 
     def build(self) -> None:
         """
         Generates and/or moves resources into the Docker context directory.Rename the random model directory name to
         constant "model_dir" instead for better readability.
         """
         self._generate_inference_code()
         self._copy_entrypoint_script_to_docker_context()
-        self._copy_snowml_source_code_to_docker_context()
+        self._copy_model_env_dependency_to_docker_context()
         self._generate_docker_file()
 
-    def _copy_snowml_source_code_to_docker_context(self) -> None:
-        """Copy the entire snowflake/ml source code to docker context. This will be particularly useful for CI tests
-        against latest changes.
-
-        Note that we exclude the experimental directory mainly for development scenario; as experimental directory won't
-        be included in the release.
-        """
-        snow_ml_source_dir = list(importlib.import_module("snowflake.ml").__path__)[0]
-        shutil.copytree(
-            snow_ml_source_dir,
-            os.path.join(self.context_dir, "snowflake", "ml"),
-            ignore=shutil.ignore_patterns("*.pyc", "experimental"),
-        )
-
     def _copy_entrypoint_script_to_docker_context(self) -> None:
         """Copy gunicorn_run.sh entrypoint to docker context directory."""
         path = os.path.join(os.path.dirname(__file__), constants.ENTRYPOINT_SCRIPT)
         assert os.path.exists(path), f"Run script file missing at path: {path}"
         shutil.copy(path, os.path.join(self.context_dir, constants.ENTRYPOINT_SCRIPT))
 
+    def _copy_model_env_dependency_to_docker_context(self) -> None:
+        path = os.path.join(self.model_dir, constants.MODEL_ENV_FOLDER)
+        assert os.path.exists(path), f"Model env folder missing at path: {path}"
+        shutil.copytree(path, os.path.join(self.context_dir, constants.MODEL_ENV_FOLDER))
+
     def _generate_docker_file(self) -> None:
         """
         Generates dockerfile based on dockerfile template.
         """
         docker_file_path = os.path.join(self.context_dir, "Dockerfile")
         docker_file_template = os.path.join(os.path.dirname(__file__), "templates/dockerfile_template")
 
@@ -67,15 +56,15 @@
         ) as template:
             dockerfile_content = string.Template(template.read()).safe_substitute(
                 {
                     # TODO(shchen): SNOW-835411, Support overwriting base image
                     "base_image": "mambaorg/micromamba:focal-cuda-11.7.1"
                     if self.use_gpu
                     else "mambaorg/micromamba:1.4.3",
-                    "model_dir": constants.MODEL_DIR,
+                    "model_env_folder": constants.MODEL_ENV_FOLDER,
                     "inference_server_dir": constants.INFERENCE_SERVER_DIR,
                     "entrypoint_script": constants.ENTRYPOINT_SCRIPT,
                 }
             )
             dockerfile.write(dockerfile_content)
 
     def _generate_inference_code(self) -> None:
```

## snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py

```diff
@@ -1,29 +1,33 @@
 import logging
 import os
+import sys
 import tempfile
 import zipfile
+from typing import List, cast
 
 import pandas as pd
 from starlette import applications, requests, responses, routing
 
 logger = logging.getLogger(__name__)
-loaded_model = None
+_LOADED_MODEL = None
+_LOADED_META = None
+TARGET_METHOD = "predict"
+MODEL_CODE_DIR = "code"
 
 
 def _run_setup() -> None:
     """Set up logging and load model into memory."""
     # Align the application logger's handler with Gunicorn's to capture logs from all processes.
     gunicorn_logger = logging.getLogger("gunicorn.error")
     logger.handlers = gunicorn_logger.handlers
     logger.setLevel(gunicorn_logger.level)
 
-    from snowflake.ml.model import _model as model_api
-
-    global loaded_model
+    global _LOADED_MODEL
+    global _LOADED_META
 
     MODEL_ZIP_STAGE_PATH = os.getenv("MODEL_ZIP_STAGE_PATH")
     assert MODEL_ZIP_STAGE_PATH, "Missing environment variable MODEL_ZIP_STAGE_PATH"
     root_path = os.path.abspath(os.sep)
     model_zip_stage_path = os.path.join(root_path, MODEL_ZIP_STAGE_PATH)
 
     with tempfile.TemporaryDirectory() as tmp_dir:
@@ -32,15 +36,19 @@
             logger.info(f"Extracting model zip from {model_zip_stage_path} to {extracted_dir}")
             with zipfile.ZipFile(model_zip_stage_path, "r") as model_zip:
                 if len(model_zip.namelist()) > 1:
                     model_zip.extractall(extracted_dir)
         else:
             raise RuntimeError(f"No model zip found at stage path: {model_zip_stage_path}")
         logger.info(f"Loading model from {extracted_dir} into memory")
-        loaded_model, _ = model_api._load_model_for_deploy(model_dir_path=extracted_dir)
+
+        sys.path.insert(0, os.path.join(extracted_dir, MODEL_CODE_DIR))
+        from snowflake.ml.model import _model as model_api
+
+        _LOADED_MODEL, _LOADED_META = model_api._load_model_for_deploy(model_dir_path=extracted_dir)
         logger.info("Successfully loaded model into memory")
 
 
 async def ready(request: requests.Request) -> responses.JSONResponse:
     """Endpoint to check if the application is ready."""
     return responses.JSONResponse({"status": "ready"})
 
@@ -48,44 +56,62 @@
 async def predict(request: requests.Request) -> responses.JSONResponse:
     """Endpoint to make predictions based on input data.
 
     Args:
         request: The input data is expected to be in the following JSON format:
             {
                 "data": [
-                    [0, 5.1, 3.5, 4.2, 1.3],
-                    [1, 4.7, 3.2, 4.1, 4.2]
+                    [0, {'_ID': 0, 'input_feature_0': 0.0, 'input_feature_1': 1.0}],
+                    [1, {'_ID': 1, 'input_feature_0': 2.0, 'input_feature_1': 3.0}],
             }
             Each row is represented as a list, where the first element denotes the index of the row.
 
     Returns:
         Two possible responses:
-        For success, return a JSON response {"data": [[0, 1], [1, 2]]}, where the first element of each resulting list
-            denotes the index of the row, and the rest of the elements represent the prediction results for that row.
+        For success, return a JSON response
+            {
+                "data": [
+                    [0, {'_ID': 0, 'output': 1}],
+                    [1, {'_ID': 1, 'output': 2}]
+                ]
+            },
+            The first element of each resulting list denotes the index of the row, and the rest of the elements
+            represent the prediction results for that row.
         For an error, return {"error": error_message, "status_code": http_response_status_code}.
     """
+    assert _LOADED_MODEL, "model is not loaded"
+    assert _LOADED_META, "model metadata is not loaded"
+    from snowflake.ml.model.model_signature import FeatureSpec
+
     try:
         input = await request.json()
+        features = cast(List[FeatureSpec], _LOADED_META.signatures[TARGET_METHOD].inputs)
+        dtype_map = {feature.name: feature.as_dtype() for feature in features}
+        input_cols = [spec.name for spec in features]
+        output_cols = [spec.name for spec in _LOADED_META.signatures[TARGET_METHOD].outputs]
         assert "data" in input, "missing data field in the request input"
         # The expression x[1:] is used to exclude the index of the data row.
-        input_data = [x[1:] for x in input.get("data")]
-        x = pd.DataFrame(input_data)
+        input_data = [x[1] for x in input.get("data")]
+        df = pd.json_normalize(input_data).astype(dtype=dtype_map)
+        x = df[input_cols]
         assert len(input_data) != 0 and not all(not row for row in input_data), "empty data"
     except Exception as e:
         error_message = f"Input data malformed: {str(e)}"
         return responses.JSONResponse({"error": error_message}, status_code=400)
 
-    assert loaded_model
-
     try:
         # TODO(shchen): SNOW-835369, Support target method in inference server (Multi-task model).
         # Mypy ignore will be fixed along with the above ticket.
-        predictions = loaded_model.predict(x)  # type: ignore[attr-defined]
-        result = predictions.to_records(index=True).tolist()
-        response = {"data": result}
+        predictions_df = _LOADED_MODEL.predict(x)  # type: ignore[attr-defined]
+        predictions_df.columns = output_cols
+        # Use _ID to keep the order of prediction result and associated features.
+        _KEEP_ORDER_COL_NAME = "_ID"
+        if _KEEP_ORDER_COL_NAME in df.columns:
+            predictions_df[_KEEP_ORDER_COL_NAME] = df[_KEEP_ORDER_COL_NAME]
+        response = {"data": [[i, row] for i, row in enumerate(predictions_df.to_dict(orient="records"))]}
         return responses.JSONResponse(response)
     except Exception as e:
         error_message = f"Prediction failed: {str(e)}"
         return responses.JSONResponse({"error": error_message}, status_code=400)
 
 
 def _in_test_mode() -> bool:
```

## snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template

```diff
@@ -1,18 +1,18 @@
 FROM $base_image as build
 
-COPY $model_dir/env/conda.yaml conda.yaml
-COPY $model_dir/env/requirements.txt requirements.txt
+COPY $model_env_folder/conda.yaml conda.yaml
+COPY $model_env_folder/requirements.txt requirements.txt
 
 # Set MAMBA_DOCKERFILE_ACTIVATE=1 to activate the conda environment during build time.
 ARG MAMBA_DOCKERFILE_ACTIVATE=1
 
 # The micromamba image comes with an empty environment named base.
 RUN --mount=type=cache,target=/opt/conda/pkgs micromamba install -y -n base -f conda.yaml && \
-	python -m pip install "uvicorn[standard]" gunicorn starlette && \
+	python -m pip install "uvicorn[standard]" gunicorn starlette==0.30.0 && \
 	python -m pip install -r requirements.txt
 
 FROM debian:buster-slim AS runtime
 
 ENV USER nonrootuser
 ENV UID 1000
 ENV HOME /home/$USER
@@ -21,17 +21,14 @@
 	--uid $UID \
 	--home $HOME \
 	$USER
 
 COPY $inference_server_dir ./$inference_server_dir
 COPY $entrypoint_script ./$entrypoint_script
 RUN chmod +x /$entrypoint_script
- # Copy Snowflake/ml source code
-# TODO: not needed as source code is either in model, or pulled from conda
-COPY snowflake ./snowflake
 
 # The mamba root prefix by default is set to /opt/conda, in which the base conda environment is built at.
 COPY --from=build /opt/conda /opt/conda
 
 # Expose the port on which the Starlette app will run.
 EXPOSE 5000
```

## snowflake/ml/model/_deploy_client/snowservice/deploy.py

```diff
@@ -1,48 +1,52 @@
 import logging
 import os
 import posixpath
 import string
 import tempfile
 from abc import ABC
-from typing import Any, Dict, cast
+from typing import Any, Dict, Optional, cast
 
 from typing_extensions import Unpack
 
-from snowflake.ml.model._deploy_client.image_builds import (
-    base_image_builder,
-    client_image_builder,
-)
+from snowflake.ml._internal import file_utils
+from snowflake.ml.model import _model, _model_meta, type_hints
+from snowflake.ml.model._deploy_client.image_builds import client_image_builder
 from snowflake.ml.model._deploy_client.snowservice import deploy_options
 from snowflake.ml.model._deploy_client.utils import constants, snowservice_client
-from snowflake.snowpark import Session
+from snowflake.snowpark import FileOperation, Session
 
 
 def _deploy(
     session: Session,
     *,
     model_id: str,
     service_func_name: str,
     model_zip_stage_path: str,
-    **kwargs: Unpack[deploy_options.SnowServiceDeployOptionsTypedHint],
-) -> None:
+    deployment_stage_path: str,
+    **kwargs: Unpack[type_hints.SnowparkContainerServiceDeployOptions],
+) -> _model_meta.ModelMetadata:
     """Entrypoint for model deployment to SnowService. This function will trigger a docker image build followed by
     workflow deployment to SnowService.
 
     Args:
         session: Snowpark session
         model_id: Unique hex string of length 32, provided by model registry.
         service_func_name: The service function name in SnowService associated with the created service.
         model_zip_stage_path: Path to model zip file in stage. Note that this path has a "@" prefix.
+        deployment_stage_path: Path to stage containing deployment artifacts.
         **kwargs: various SnowService deployment options.
 
     Raises:
         ValueError: Raised when model_id is empty.
         ValueError: Raised when service_func_name is empty.
         ValueError: Raised when model_stage_file_path is empty.
+
+    Returns:
+        The metadata of the model that has been deployed.
     """
     snowpark_logger = logging.getLogger("snowflake.snowpark")
     snowflake_connector_logger = logging.getLogger("snowflake.connector")
     snowpark_log_level = snowpark_logger.level
     snowflake_connector_log_level = snowflake_connector_logger.level
     try:
         # Setting appropriate log level to prevent console from being polluted by vast amount of snowpark and snowflake
@@ -53,90 +57,157 @@
             raise ValueError('Must provide a non-empty string for "model_id" when deploying to SnowService')
         if not service_func_name:
             raise ValueError('Must provide a non-empty string for "service_func_name" when deploying to SnowService')
         if not model_zip_stage_path:
             raise ValueError(
                 'Must provide a non-empty string for "model_stage_file_path" when deploying to SnowService'
             )
+        if not deployment_stage_path:
+            raise ValueError(
+                'Must provide a non-empty string for "deployment_stage_path" when deploying to SnowService'
+            )
+
+        # Remove full qualified name to avoid double quotes corrupting the service spec
+        model_zip_stage_path = model_zip_stage_path.replace('"', "")
+        deployment_stage_path = deployment_stage_path.replace('"', "")
+
         assert model_zip_stage_path.startswith("@"), f"stage path should start with @, actual: {model_zip_stage_path}"
+        assert deployment_stage_path.startswith("@"), f"stage path should start with @, actual: {deployment_stage_path}"
         options = deploy_options.SnowServiceDeployOptions.from_dict(cast(Dict[str, Any], kwargs))
-        image_builder = client_image_builder.ClientImageBuilder(
-            id=model_id, image_repo=options.image_repo, model_zip_stage_path=model_zip_stage_path, session=session
-        )
-        ss_deployment = SnowServiceDeployment(
-            session=session,
-            model_id=model_id,
-            service_func_name=service_func_name,
-            model_zip_stage_path=model_zip_stage_path,
-            image_builder=image_builder,
-            options=options,
-        )
-        ss_deployment.deploy()
+
+        # TODO[shchen]: SNOW-863701, Explore ways to prevent entire model zip being downloaded during deploy step
+        #  (for both warehouse and snowservice deployment)
+        # One alternative is for model registry to duplicate the model metadata and env dependency storage from model
+        # zip so that we don't have to pull down the entire model zip.
+        fo = FileOperation(session=session)
+        zf = fo.get_stream(model_zip_stage_path)
+        with file_utils.unzip_stream_in_temp_dir(stream=zf) as temp_local_model_dir_path:
+            # Download the model zip file that is already uploaded to stage during model registry log_model step.
+            # This is needed in order to obtain the conda and requirement file inside the model zip, as well as to
+            # return the model object needed for deployment info tracking.
+            ss_deployment = SnowServiceDeployment(
+                session=session,
+                model_id=model_id,
+                service_func_name=service_func_name,
+                model_zip_stage_path=model_zip_stage_path,  # Pass down model_zip_stage_path for service spec file
+                deployment_stage_path=deployment_stage_path,
+                model_dir=temp_local_model_dir_path,
+                options=options,
+            )
+            ss_deployment.deploy()
+            meta = _model.load_model(model_dir_path=temp_local_model_dir_path, meta_only=True)
+            return meta
     finally:
         # Preserve the original logging level.
         snowpark_logger.setLevel(snowpark_log_level)
         snowflake_connector_logger.setLevel(snowflake_connector_log_level)
 
 
+def _get_or_create_image_repo(session: Session, *, image_repo: Optional[str]) -> str:
+    def _sanitize_dns_url(url: str) -> str:
+        # Align with existing SnowService image registry url standard.
+        return url.lower()
+
+    if image_repo:
+        return _sanitize_dns_url(image_repo)
+
+    try:
+        conn = session._conn._conn
+        org = conn.host.split(".")[1]
+        account = conn.account
+        db = conn._database
+        schema = conn._schema
+        subdomain = constants.PROD_IMAGE_REGISTRY_SUBDOMAIN
+        sanitized_url = _sanitize_dns_url(
+            f"{org}-{account}.{subdomain}.{constants.PROD_IMAGE_REGISTRY_DOMAIN}/{db}/"
+            f"{schema}/{constants.SNOWML_IMAGE_REPO}"
+        )
+        client = snowservice_client.SnowServiceClient(session)
+        client.create_image_repo(constants.SNOWML_IMAGE_REPO)
+        return sanitized_url
+    except Exception:
+        raise RuntimeError(
+            "Failed to construct image repo URL, please ensure the following connections"
+            "parameters are set in your session: ['host', 'account', 'database', 'schema']"
+        )
+
+
 class SnowServiceDeployment(ABC):
     """
     Class implementation that encapsulates image build and workflow deployment to SnowService
-
-    #TODO[shchen], SNOW-830093 GPU support on model deployment to SnowService
     """
 
     def __init__(
         self,
         session: Session,
         model_id: str,
         service_func_name: str,
+        model_dir: str,
         model_zip_stage_path: str,
-        image_builder: base_image_builder.ImageBuilder,
+        deployment_stage_path: str,
         options: deploy_options.SnowServiceDeployOptions,
     ) -> None:
         """Initialization
 
         Args:
             session: Snowpark session
             model_id: Unique hex string of length 32, provided by model registry; if not provided, auto-generate one for
                         resource naming.The model_id serves as an idempotent key throughout the deployment workflow.
             service_func_name: The service function name in SnowService associated with the created service.
+            model_dir: Local model directory, downloaded form stage and extracted.
             model_zip_stage_path: Path to model zip file in stage.
-            image_builder: InferenceImageBuilder instance that handles image build and upload to image registry.
+            deployment_stage_path: Path to stage containing deployment artifacts.
             options: A SnowServiceDeployOptions object containing deployment options.
         """
 
         self.session = session
         self.id = model_id
         self.service_func_name = service_func_name
         self.model_zip_stage_path = model_zip_stage_path
-        self.image_builder = image_builder
+        self.model_dir = model_dir
         self.options = options
         self._service_name = f"service_{model_id}"
         # Spec file and future deployment related artifacts will be stored under {stage}/models/{model_id}
-        self._model_artifact_stage_location = posixpath.join(options.stage, "models", self.id)
+        self._model_artifact_stage_location = posixpath.join(deployment_stage_path, "models", self.id)
 
     def deploy(self) -> None:
         """
         This function triggers image build followed by workflow deployment to SnowService.
         """
         if self.options.prebuilt_snowflake_image:
             image = self.options.prebuilt_snowflake_image
-            logging.info(f"Skipped image build. Use Snowflake prebuilt image: {self.options.prebuilt_snowflake_image}")
+            logging.warning(f"Skipped image build. Use prebuilt image: {self.options.prebuilt_snowflake_image}")
         else:
+            logging.warning(
+                "Building the Docker image and deploying to Snowpark Container Service. "
+                "This process may take a few minutes."
+            )
             image = self._build_and_upload_image()
+
+            logging.warning(
+                f"Image successfully built! To prevent the need for rebuilding the Docker image in future deployments, "
+                f"simply specify 'prebuilt_snowflake_image': '{image}' in the options field of the deploy() function"
+            )
         self._deploy_workflow(image)
 
     def _build_and_upload_image(self) -> str:
         """This function handles image build and upload to image registry.
 
         Returns:
             Path to the image in the remote image repository.
         """
-        return self.image_builder.build_and_upload_image()
+        image_repo = _get_or_create_image_repo(self.session, image_repo=self.options.image_repo)
+        image_builder = client_image_builder.ClientImageBuilder(
+            id=self.id,
+            image_repo=image_repo,
+            model_dir=self.model_dir,
+            session=self.session,
+            use_gpu=True if self.options.use_gpu else False,
+        )
+        return image_builder.build_and_upload_image()
 
     def _prepare_and_upload_artifacts_to_stage(self, image: str) -> None:
         """Constructs and upload service spec to stage.
 
         Args:
             image: Name of the image to create SnowService container from.
         """
@@ -148,15 +219,15 @@
             with open(spec_template_path, encoding="utf-8") as template, open(
                 spec_file_path, "w", encoding="utf-8"
             ) as spec_file:
                 content = string.Template(template.read()).substitute(
                     {
                         "image": image,
                         "predict_endpoint_name": constants.PREDICT,
-                        "stage": self.options.stage,
+                        "model_stage": self.model_zip_stage_path[1:].split("/")[0],  # Reserve only the stage name
                         "model_zip_stage_path": self.model_zip_stage_path[1:],  # Remove the @ prefix
                         "inference_server_container_name": constants.INFERENCE_SERVER_CONTAINER,
                     }
                 )
                 spec_file.write(content)
                 logging.info(f"Create service spec: \n {content}")
```

## snowflake/ml/model/_deploy_client/snowservice/deploy_options.py

```diff
@@ -1,88 +1,67 @@
-from typing import Any, Dict, Optional, TypedDict
-
-from typing_extensions import NotRequired
+import inspect
+from typing import Any, Dict, Optional
 
 from snowflake.ml.model._deploy_client.utils import constants
 
 
-class SnowServiceDeployOptionsTypedHint(TypedDict):
-    """Deployment options for deploying to SnowService.
-
-    stage: the name of the stage for uploading artifacts.
-    compute_pool: SnowService compute pool name.
-    image_repo: SnowService image repo path. e.g. "<image_registry>/<db>/<schema>/<repo>"
-    min_instances: Minimum number of service replicas.
-    max_instances: Maximum number of service replicas.
-    endpoint: The specific name of the endpoint that the service function will communicate with. Default to
-                "predict". This option is useful when service has multiple endpoints.
-    overridden_base_image: When provided, it will override the base image.
-    """
-
-    stage: str
-    compute_pool: str
-    image_repo: str
-    min_instances: NotRequired[int]
-    max_instances: NotRequired[int]
-    endpoint: NotRequired[str]
-    overridden_base_image: NotRequired[str]
-
-
 class SnowServiceDeployOptions:
     def __init__(
         self,
-        stage: str,
         compute_pool: str,
-        image_repo: str,
         *,
-        min_instances: int = 1,
-        max_instances: int = 1,
-        endpoint: str = constants.PREDICT,
-        overridden_base_image: Optional[str] = None,
+        image_repo: Optional[str] = None,
+        min_instances: Optional[int] = 1,
+        max_instances: Optional[int] = 1,
+        endpoint: Optional[str] = constants.PREDICT,
         prebuilt_snowflake_image: Optional[str] = None,
+        use_gpu: Optional[bool] = False,
     ) -> None:
         """Initialization
 
+        When updated, please ensure the type hint is updated accordingly at: //snowflake/ml/model/type_hints
+
         Args:
-            stage: the name of the stage for uploading artifacts.
-            compute_pool: SnowService compute pool name.
-            image_repo: SnowService image repo path. e.g. "<image_registry>/<db>/<schema>/<repo>"
-            min_instances: Minimum number of service replicas.
-            max_instances: Maximum number of service replicas.
-            endpoint: The specific name of the endpoint that the service function will communicate with. Default to
-                        "predict". This option is useful when service has multiple endpoints.
-            overridden_base_image: When provided, it will override the base image.
-            prebuilt_snowflake_image: When provided, the image building step is skipped, and the pre-built image from
+            compute_pool: SnowService compute pool name. Please refer to official doc for how to create a
+                compute pool: https://docs.snowflake.com/LIMITEDACCESS/snowpark-containers/reference/compute-pool
+            image_repo: SnowService image repo path. e.g. "<image_registry>/<db>/<schema>/<repo>". Default to auto
+                inferred based on session information.
+            min_instances: Minimum number of service replicas. Default to 1.
+            max_instances: Maximum number of service replicas. Default to 1.
+            endpoint: The specific name of the endpoint that the service function will communicate with. This option is
+                useful when the service has multiple endpoints. Default to predict.
+            prebuilt_snowflake_image: When provided, the image-building step is skipped, and the pre-built image from
                 Snowflake is used as is. This option is for users who consistently use the same image for multiple use
                 cases, allowing faster deployment. The snowflake image used for deployment is logged to the console for
-                future use.
+                future use. Default to None.
+            use_gpu: When set to True, a CUDA-enabled Docker image will be used to provide a runtime CUDA environment.
+                Default to False.
         """
 
-        self.stage = stage
         self.compute_pool = compute_pool
         self.image_repo = image_repo
         self.min_instances = min_instances
         self.max_instances = max_instances
         self.endpoint = endpoint
-        self.overridden_base_image = overridden_base_image
         self.prebuilt_snowflake_image = prebuilt_snowflake_image
+        self.use_gpu = use_gpu
 
     @classmethod
     def from_dict(cls, options_dict: Dict[str, Any]) -> "SnowServiceDeployOptions":
         """Construct SnowServiceDeployOptions instance based from an option dictionary.
 
         Args:
             options_dict: The dict containing various deployment options.
 
         Raises:
             ValueError: When required option is missing.
 
         Returns:
             A SnowServiceDeployOptions object
         """
-        required_options = [constants.STAGE, constants.COMPUTE_POOL, constants.IMAGE_REPO]
+        required_options = [constants.COMPUTE_POOL]
         missing_keys = [key for key in required_options if options_dict.get(key) is None]
         if missing_keys:
             raise ValueError(f"Must provide options when deploying to SnowService: {', '.join(missing_keys)}")
-        # SnowService image repo cannot handle upper case repo name.
-        options_dict[constants.IMAGE_REPO] = options_dict[constants.IMAGE_REPO].lower()
-        return cls(**options_dict)
+        supported_options_keys = inspect.signature(cls.__init__).parameters.keys()
+        filtered_options = {k: v for k, v in options_dict.items() if k in supported_options_keys}
+        return cls(**filtered_options)
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template

```diff
@@ -15,23 +15,23 @@
 000000e0: 6c75 6d65 4d6f 756e 7473 3a0a 2020 2020  lumeMounts:.    
 000000f0: 2020 2020 2d20 6e61 6d65 3a20 766f 6c31      - name: vol1
 00000100: 0a20 2020 2020 2020 2020 206d 6f75 6e74  .          mount
 00000110: 5061 7468 3a20 2f6c 6f63 616c 2f75 7365  Path: /local/use
 00000120: 722f 766f 6c31 0a20 2020 2020 2020 202d  r/vol1.        -
 00000130: 206e 616d 653a 2073 7461 6765 0a20 2020   name: stage.   
 00000140: 2020 2020 2020 206d 6f75 6e74 5061 7468         mountPath
-00000150: 3a20 247b 7374 6167 657d 0a20 2065 6e64  : ${stage}.  end
-00000160: 706f 696e 743a 0a20 2020 202d 206e 616d  point:.    - nam
-00000170: 653a 2024 7b70 7265 6469 6374 5f65 6e64  e: ${predict_end
-00000180: 706f 696e 745f 6e61 6d65 7d0a 2020 2020  point_name}.    
-00000190: 2020 706f 7274 3a20 3530 3030 0a20 2076    port: 5000.  v
-000001a0: 6f6c 756d 653a 0a20 2020 202d 206e 616d  olume:.    - nam
-000001b0: 653a 2076 6f6c 310a 2020 2020 2020 736f  e: vol1.      so
-000001c0: 7572 6365 3a20 6c6f 6361 6c20 2023 206f  urce: local  # o
-000001d0: 6e6c 7920 6c6f 6361 6c20 656d 7074 7944  nly local emptyD
-000001e0: 6972 2076 6f6c 756d 6520 6973 2073 7570  ir volume is sup
-000001f0: 706f 7274 6564 0a20 2020 202d 206e 616d  ported.    - nam
-00000200: 653a 2073 7461 6765 0a20 2020 2020 2073  e: stage.      s
-00000210: 6f75 7263 653a 2022 4024 7b73 7461 6765  ource: "@${stage
-00000220: 7d22 0a20 2020 2020 2075 6964 3a20 3130  }".      uid: 10
-00000230: 3030 0a20 2020 2020 2067 6964 3a20 3130  00.      gid: 10
-00000240: 3030 0a                                  00.
+00000150: 3a20 247b 6d6f 6465 6c5f 7374 6167 657d  : ${model_stage}
+00000160: 0a20 2065 6e64 706f 696e 743a 0a20 2020  .  endpoint:.   
+00000170: 202d 206e 616d 653a 2024 7b70 7265 6469   - name: ${predi
+00000180: 6374 5f65 6e64 706f 696e 745f 6e61 6d65  ct_endpoint_name
+00000190: 7d0a 2020 2020 2020 706f 7274 3a20 3530  }.      port: 50
+000001a0: 3030 0a20 2076 6f6c 756d 653a 0a20 2020  00.  volume:.   
+000001b0: 202d 206e 616d 653a 2076 6f6c 310a 2020   - name: vol1.  
+000001c0: 2020 2020 736f 7572 6365 3a20 6c6f 6361      source: loca
+000001d0: 6c20 2023 206f 6e6c 7920 6c6f 6361 6c20  l  # only local 
+000001e0: 656d 7074 7944 6972 2076 6f6c 756d 6520  emptyDir volume 
+000001f0: 6973 2073 7570 706f 7274 6564 0a20 2020  is supported.   
+00000200: 202d 206e 616d 653a 2073 7461 6765 0a20   - name: stage. 
+00000210: 2020 2020 2073 6f75 7263 653a 2022 4024       source: "@$
+00000220: 7b6d 6f64 656c 5f73 7461 6765 7d22 0a20  {model_stage}". 
+00000230: 2020 2020 2075 6964 3a20 3130 3030 0a20       uid: 1000. 
+00000240: 2020 2020 2067 6964 3a20 3130 3030 0a         gid: 1000.
```

## snowflake/ml/model/_deploy_client/utils/constants.py

```diff
@@ -28,20 +28,25 @@
     ResourceType.SERVICE: "SYSTEM$GET_SNOWSERVICE_STATUS",
     ResourceType.JOB: "SYSTEM$GET_JOB_STATUS",
 }
 
 PREDICT = "predict"
 STAGE = "stage"
 COMPUTE_POOL = "compute_pool"
-IMAGE_REPO = "image_repo"
 MIN_INSTANCES = "min_instances"
 MAX_INSTANCES = "max_instances"
 GPU_COUNT = "gpu"
 OVERRIDDEN_BASE_IMAGE = "image"
 ENDPOINT = "endpoint"
 SERVICE_SPEC = "service_spec"
 INFERENCE_SERVER_CONTAINER = "inference-server"
 
 """Image build related constants"""
+SNOWML_IMAGE_REPO = "snowml_repo"
 MODEL_DIR = "model_dir"
 INFERENCE_SERVER_DIR = "inference_server"
 ENTRYPOINT_SCRIPT = "gunicorn_run.sh"
+PROD_IMAGE_REGISTRY_DOMAIN = "snowflakecomputing.com"
+PROD_IMAGE_REGISTRY_SUBDOMAIN = "registry"
+DEV_IMAGE_REGISTRY_SUBDOMAIN = "registry-dev"
+MODEL_ENV_FOLDER = "env"
+CONDA_FILE = "conda.yaml"
```

## snowflake/ml/model/_deploy_client/utils/snowservice_client.py

```diff
@@ -16,41 +16,45 @@
         """Initialization
 
         Args:
             session: Snowpark session
         """
         self.session = session
 
+    def create_image_repo(self, repo_name: str) -> None:
+        self.session.sql(f"CREATE OR REPLACE IMAGE REPOSITORY {repo_name}").collect()
+
     def create_or_replace_service(
         self,
         service_name: str,
         compute_pool: str,
         spec_stage_location: str,
         *,
-        min_instances: int = 1,
-        max_instances: int = 1,
+        min_instances: Optional[int] = 1,
+        max_instances: Optional[int] = 1,
     ) -> None:
         """Create or replace service. Since SnowService doesn't support the CREATE OR REPLACE service syntax, we will
         first attempt to drop the service if it exists, and then create the service. Please note that this approach may
         have side effects due to the lack of transaction support.
 
         Args:
             service_name: Name of the service.
             min_instances: Minimum number of service replicas.
             max_instances: Maximum number of service replicas.
             compute_pool: Name of the compute pool.
             spec_stage_location: Stage path for the service spec.
         """
+        assert spec_stage_location.startswith("@"), f"stage path should start with @, actual: {spec_stage_location}"
         self._drop_service_if_exists(service_name)
         sql = f"""
              CREATE SERVICE {service_name}
                  MIN_INSTANCES={min_instances}
                  MAX_INSTANCES={max_instances}
                  COMPUTE_POOL={compute_pool}
-                 SPEC=@{spec_stage_location}
+                 SPEC={spec_stage_location}
          """
         logging.info(f"Create service with SQL: \n {sql}")
         self.session.sql(sql).collect()
 
     def _drop_service_if_exists(self, service_name: str) -> None:
         """Drop service if it already exists.
 
@@ -83,14 +87,15 @@
                 RETURNS OBJECT
                 SERVICE={service_name}
                 ENDPOINT={endpoint_name}
                 AS '/{path_at_service_endpoint}'
             """
         logging.info(f"Create service function with SQL: \n {sql}")
         self.session.sql(sql).collect()
+        logging.info(f"Successfully created service function: {service_func_name}")
 
     def block_until_resource_is_ready(
         self,
         resource_name: str,
         resource_type: constants.ResourceType,
         *,
         max_retries: int = 60,
```

## snowflake/ml/model/_deploy_client/warehouse/deploy.py

```diff
@@ -1,24 +1,18 @@
 import os
 import posixpath
 import tempfile
-import warnings
 from types import ModuleType
 from typing import IO, List, Optional, Tuple, TypedDict, Union
 
 from typing_extensions import Unpack
 
 from snowflake.ml._internal import env_utils, file_utils
 from snowflake.ml._internal.utils import identifier
-from snowflake.ml.model import (
-    _env as model_env,
-    _model,
-    _model_meta,
-    type_hints as model_types,
-)
+from snowflake.ml.model import _model, _model_meta, type_hints as model_types
 from snowflake.ml.model._deploy_client.warehouse import infer_template
 from snowflake.snowpark import session as snowpark_session, types as st
 
 
 def _deploy_to_warehouse(
     session: snowpark_session.Session,
     *,
@@ -64,22 +58,18 @@
         extract_model_code = infer_template._EXTRACT_STAGE_MODEL_CODE.format(
             model_stage_file_name=model_stage_file_name
         )
         meta = _model.load_model(session=session, model_stage_file_path=model_stage_file_path, meta_only=True)
 
     relax_version = kwargs.get("relax_version", False)
 
-    disable_local_conda_resolver = kwargs.get("disable_local_conda_resolver", False)
-
     if target_method not in meta.signatures.keys():
         raise ValueError(f"Target method {target_method} does not exist in model.")
 
-    final_packages = _get_model_final_packages(
-        meta, session, relax_version=relax_version, disable_local_conda_resolver=disable_local_conda_resolver
-    )
+    final_packages = _get_model_final_packages(meta, session, relax_version=relax_version)
 
     stage_location = kwargs.get("permanent_udf_stage_location", None)
     if stage_location:
         stage_location = posixpath.normpath(stage_location.strip())
         if not stage_location.startswith("@"):
             raise ValueError(f"Invalid stage location {stage_location}.")
 
@@ -148,74 +138,60 @@
     f.flush()
 
 
 def _get_model_final_packages(
     meta: _model_meta.ModelMetadata,
     session: snowpark_session.Session,
     relax_version: Optional[bool] = False,
-    disable_local_conda_resolver: Optional[bool] = False,
 ) -> List[str]:
     """Generate final packages list of dependency of a model to be deployed to warehouse.
 
     Args:
         meta: Model metadata to get dependency information.
         session: Snowpark connection session.
         relax_version: Whether or not relax the version restriction when fail to resolve dependencies.
             Defaults to False.
-        disable_local_conda_resolver: Set to disable use local conda resolver to do pre-check on environment and rely on
-            the information schema only. Defaults to False.
 
     Raises:
         RuntimeError: Raised when PIP requirements and dependencies from non-Snowflake anaconda channel found.
         RuntimeError: Raised when not all packages are available in snowflake conda channel.
 
     Returns:
         List of final packages string that is accepted by Snowpark register UDF call.
     """
     final_packages = None
     if (
-        any(channel.lower() not in ["", "snowflake"] for channel in meta._conda_dependencies.keys())
+        any(
+            channel.lower() not in [env_utils.DEFAULT_CHANNEL_NAME, "snowflake"]
+            for channel in meta._conda_dependencies.keys()
+        )
         or meta.pip_requirements
     ):
         raise RuntimeError("PIP requirements and dependencies from non-Snowflake anaconda channel is not supported.")
 
-    deps = meta._conda_dependencies[""]
+    deps = meta._conda_dependencies[env_utils.DEFAULT_CHANNEL_NAME]
 
-    try:
-        if disable_local_conda_resolver:
-            raise ImportError("Raise to disable local conda resolver. Should be captured.")
-        final_packages = env_utils.resolve_conda_environment(
-            deps, [model_env._SNOWFLAKE_CONDA_CHANNEL_URL], python_version=meta.python_version
-        )
-        if final_packages is None and relax_version:
-            final_packages = env_utils.resolve_conda_environment(
-                list(map(env_utils.relax_requirement_version, deps)),
-                [model_env._SNOWFLAKE_CONDA_CHANNEL_URL],
-                python_version=meta.python_version,
-            )
-    except ImportError:
-        warnings.warn(
-            "Cannot find conda resolver, use Snowflake information schema for best-effort dependency pre-check.",
-            category=RuntimeWarning,
-        )
+    final_packages = env_utils.validate_requirements_in_snowflake_conda_channel(
+        session=session,
+        reqs=deps,
+        python_version=meta.python_version,
+    )
+    if final_packages is None and relax_version:
         final_packages = env_utils.validate_requirements_in_snowflake_conda_channel(
             session=session,
-            reqs=deps,
+            reqs=list(map(env_utils.relax_requirement_version, deps)),
             python_version=meta.python_version,
         )
-        if final_packages is None and relax_version:
-            final_packages = env_utils.validate_requirements_in_snowflake_conda_channel(
-                session=session,
-                reqs=list(map(env_utils.relax_requirement_version, deps)),
-                python_version=meta.python_version,
-            )
 
-    finally:
+    if final_packages is None:
+        relax_version_info_str = "" if relax_version else "Try to set relax_version as True in the options. "
+        required_deps = list(map(env_utils.relax_requirement_version, deps)) if relax_version else deps
         if final_packages is None:
             raise RuntimeError(
                 "The model's dependency cannot fit into Snowflake Warehouse. "
-                + "Trying to set relax_version as True in the options. Required packages are:\n"
-                + '"'
-                + " ".join(map(str, meta._conda_dependencies[""]))
-                + '"'
+                + relax_version_info_str
+                + "Required packages are:\n"
+                + " ".join(map(lambda x: f'"{x}"', required_deps))
+                + "\n Required Python version is: "
+                + meta.python_version
             )
     return final_packages
```

## snowflake/ml/model/_deployer.py

```diff
@@ -1,25 +1,41 @@
 import traceback
 from enum import Enum
-from typing import Optional, TypedDict, Union, overload
+from typing import Optional, TypedDict, Union, cast, overload
 
 import pandas as pd
 from typing_extensions import Required
 
 from snowflake.ml._internal.utils import identifier
 from snowflake.ml.model import model_signature, type_hints as model_types
+from snowflake.ml.model._deploy_client.snowservice import deploy as snowservice_deploy
+from snowflake.ml.model._deploy_client.utils import constants as snowservice_constants
 from snowflake.ml.model._deploy_client.warehouse import (
     deploy as warehouse_deploy,
     infer_template,
 )
+from snowflake.ml.model._signatures import snowpark_handler
 from snowflake.snowpark import DataFrame as SnowparkDataFrame, Session, functions as F
 
 
 class TargetPlatform(Enum):
     WAREHOUSE = "warehouse"
+    SNOWPARK_CONTAINER_SERVICE = "snowpark_container_service"
+
+    def __repr__(self) -> str:
+        """Construct a string format that works with the "ModelReference" in model_registry.py. Fundamentally,
+        ModelReference uses the TargetPlatform enum type when constructing the "deploy" function through exec().
+        Since "exec" in Python takes input as a string, we need to dynamically construct a full path so that the
+        enum can be loaded successfully.
+
+        Returns:
+            A enum string representation.
+        """
+
+        return f"{__name__.split('.')[-1]}.{self.__class__.__name__}.{self.name}"
 
 
 class Deployment(TypedDict):
     """Deployment information.
 
     Attributes:
         name: Name of the deployment.
@@ -78,34 +94,66 @@
         model_stage_file_path: Model file in the stage to be deployed. Must be a file with .zip extension.
         options: Additional options when deploying the model.
             Each target platform will have their own specifications of options.
     """
     ...
 
 
+@overload
+def deploy(
+    session: Session,
+    *,
+    model_id: str,
+    name: str,
+    platform: TargetPlatform,
+    target_method: str,
+    model_stage_file_path: str,
+    deployment_stage_path: str,
+    options: Optional[model_types.DeployOptions],
+) -> Optional[Deployment]:
+    """Create a deployment from a model in a local directory and deploy it to remote platform.
+
+    Args:
+        session: Snowpark Connection Session.
+        model_id: Internal model ID string.
+        name: Name of the deployment for the model.
+        platform: Target platform to deploy the model.
+        target_method: The name of the target method to be deployed.
+        model_stage_file_path: Model file in the stage to be deployed. Must be a file with .zip extension.
+        deployment_stage_path: Path to stage containing snowpark container service deployment artifacts.
+        options: Additional options when deploying the model.
+            Each target platform will have their own specifications of options.
+    """
+    ...
+
+
 def deploy(
     session: Session,
     *,
     name: str,
     platform: TargetPlatform,
     target_method: str,
     model_dir_path: Optional[str] = None,
     model_stage_file_path: Optional[str] = None,
+    deployment_stage_path: Optional[str] = None,
+    model_id: Optional[str] = None,
     options: Optional[model_types.DeployOptions],
 ) -> Optional[Deployment]:
     """Create a deployment from a model and deploy it to remote platform.
 
     Args:
         session: Snowpark Connection Session.
+        model_id: Internal model ID string.
         name: Name of the deployment for the model.
         platform: Target platform to deploy the model.
         target_method: The name of the target method to be deployed.
         model_dir_path: Directory of the model. Exclusive with `model_stage_dir_path`.
         model_stage_file_path: Model file in the stage to be deployed. Exclusive with `model_dir_path`.
             Must be a file with .zip extension.
+        deployment_stage_path: Path to stage containing deployment artifacts.
         options: Additional options when deploying the model.
             Each target platform will have their own specifications of options.
 
     Raises:
         ValueError: Raised when target platform is unsupported.
         RuntimeError: Raised when running into errors when deploying to the warehouse.
         ValueError: Raised when target method does not exist in model.
@@ -132,16 +180,36 @@
                 model_stage_file_path=model_stage_file_path,
                 udf_name=name,
                 target_method=target_method,
                 **options,
             )
         except Exception:
             raise RuntimeError("Error happened when deploying to the warehouse: " + traceback.format_exc())
+
+    elif platform == TargetPlatform.SNOWPARK_CONTAINER_SERVICE:
+        options = cast(model_types.SnowparkContainerServiceDeployOptions, options)
+        assert model_id, "Require 'model_id' for Snowpark container service deployment"
+        assert model_stage_file_path, "Require 'model_stage_file_path' for Snowpark container service deployment"
+        assert deployment_stage_path, "Require 'deployment_stage_path' for Snowpark container service deployment"
+        if snowservice_constants.COMPUTE_POOL not in options:
+            raise ValueError("Missing 'compute_pool' in options field for Snowpark container service deployment")
+        try:
+            meta = snowservice_deploy._deploy(
+                session=session,
+                model_id=model_id,
+                service_func_name=name,
+                model_zip_stage_path=model_stage_file_path,
+                deployment_stage_path=deployment_stage_path,
+                **options,
+            )
+        except Exception:
+            raise RuntimeError(f"Failed to deploy to Snowpark Container Service: {traceback.format_exc()}")
+
     else:
-        raise ValueError("Unsupported target Platform.")
+        raise ValueError(f"Unsupported target Platform: {platform}")
     signature = meta.signatures.get(target_method, None)
     if not signature:
         raise ValueError(f"Target method {target_method} does not exist in model.")
     info = Deployment(name=name, platform=platform, signature=signature, options=options)
     return info
 
 
@@ -188,19 +256,20 @@
     """
 
     # Get options
     INTERMEDIATE_OBJ_NAME = "tmp_result"
     sig = deployment["signature"]
     keep_order = deployment["options"].get("keep_order", True)
     output_with_input_features = deployment["options"].get("output_with_input_features", False)
+    platform = deployment["platform"]
 
     # Validate and prepare input
     if not isinstance(X, SnowparkDataFrame):
         df = model_signature._convert_and_validate_local_data(X, sig.inputs)
-        s_df = model_signature._SnowparkDataFrameHandler.convert_from_df(session, df, keep_order=keep_order)
+        s_df = snowpark_handler.SnowparkDataFrameHandler.convert_from_df(session, df, keep_order=keep_order)
     else:
         model_signature._validate_snowpark_data(X, sig.inputs)
         s_df = X
 
         if keep_order:
             # ID is UINT64 type, this we should limit.
             if s_df.count() > 2**64:
@@ -212,21 +281,28 @@
 
     # Infer and get intermediate result
     input_cols = []
     for col_name in s_df.columns:
         literal_col_name = identifier.get_unescaped_names(col_name)
         input_cols.extend(
             [
-                F.lit(literal_col_name),  # type:ignore[arg-type]
+                F.lit(literal_col_name),
                 F.col(col_name),
             ]
         )
-    output_obj = F.call_udf(
-        identifier.get_inferred_name(deployment["name"]), F.object_construct(*input_cols)  # type:ignore[arg-type]
+
+    # TODO[shchen]: SNOW-870032, For SnowService, external function name cannot be double quoted, else it results in
+    # external function no found.
+    udf_name = (
+        deployment["name"]
+        if platform == TargetPlatform.SNOWPARK_CONTAINER_SERVICE
+        else identifier.get_inferred_name(deployment["name"])
     )
+    output_obj = F.call_udf(udf_name, F.object_construct(*input_cols))
+
     if output_with_input_features:
         df_res = s_df.with_column(INTERMEDIATE_OBJ_NAME, output_obj)
     else:
         df_res = s_df.select(output_obj.alias(INTERMEDIATE_OBJ_NAME))
 
     if keep_order:
         df_res = df_res.order_by(
@@ -244,10 +320,10 @@
     df_res = df_res.with_columns(
         [identifier.get_inferred_name(output_feature.name) for output_feature in sig.outputs],
         output_cols,
     ).drop(INTERMEDIATE_OBJ_NAME)
 
     # Get final result
     if not isinstance(X, SnowparkDataFrame):
-        return model_signature._SnowparkDataFrameHandler.convert_to_df(df_res, features=sig.outputs)
+        return snowpark_handler.SnowparkDataFrameHandler.convert_to_df(df_res, features=sig.outputs)
     else:
         return df_res
```

## snowflake/ml/model/_env.py

```diff
@@ -6,14 +6,15 @@
 from packaging import requirements, version
 
 from snowflake.ml._internal import env as snowml_env, env_utils
 from snowflake.ml._internal.utils import formatting
 
 _CONDA_ENV_FILE_NAME = "conda.yaml"
 _SNOWFLAKE_CONDA_CHANNEL_URL = "https://repo.anaconda.com/pkgs/snowflake"
+_NODEFAULTS = "nodefaults"
 _REQUIREMENTS_FILE_NAME = "requirements.txt"
 
 
 def save_conda_env_file(
     dir_path: str,
     deps: DefaultDict[str, List[requirements.Requirement]],
     python_version: Optional[str] = snowml_env.PYTHON_VERSION,
@@ -27,15 +28,19 @@
 
     Returns:
         The path to conda env file.
     """
     path = os.path.join(dir_path, _CONDA_ENV_FILE_NAME)
     env: Dict[str, Any] = dict()
     env["name"] = "snow-env"
-    env["channels"] = [_SNOWFLAKE_CONDA_CHANNEL_URL, "nodefaults"]
+    env["channels"] = (
+        [_SNOWFLAKE_CONDA_CHANNEL_URL]
+        + [channel_name for channel_name, channel_deps in deps.items() if len(channel_deps) == 0]
+        + [_NODEFAULTS]
+    )
     env["dependencies"] = [f"python=={python_version}"]
     for chan, reqs in deps.items():
         env["dependencies"].extend([f"{chan}::{str(req)}" if chan else str(req) for req in reqs])
 
     with open(path, "w", encoding="utf-8") as f:
         yaml.safe_dump(env, stream=f, default_flow_style=False)
 
@@ -74,24 +79,35 @@
 
     assert isinstance(env, dict)
 
     deps = []
 
     python_version = None
 
+    channels = env["channels"]
+    channels.remove(_SNOWFLAKE_CONDA_CHANNEL_URL)
+    channels.remove(_NODEFAULTS)
+
     for dep in env["dependencies"]:
         if isinstance(dep, str):
-            if dep.startswith("python=="):
-                hd, _, ver = dep.partition("==")
-                assert hd == "python"
-                python_version = ver
+            ver = env_utils.parse_python_version_string(dep)
+            # ver is None: not python, ver is "": python w/o specifier, ver is str: python w/ specifier
+            if ver is not None:
+                if ver:
+                    python_version = ver
             else:
                 deps.append(dep)
 
-    return env_utils.validate_conda_dependency_string_list(deps), python_version
+    conda_dep_dict = env_utils.validate_conda_dependency_string_list(deps)
+
+    if len(channels) > 0:
+        for channel in channels:
+            conda_dep_dict[channel] = []
+
+    return conda_dep_dict, python_version
 
 
 def load_requirements_file(path: str) -> List[requirements.Requirement]:
     """Load Python requirements.txt file from the given directory path.
 
     Args:
         path: Path to the requirements.txt file.
```

## snowflake/ml/model/_handlers/_base.py

```diff
@@ -39,15 +39,15 @@
     def _save_model(
         name: str,
         model: model_types._ModelType,
         model_meta: _model_meta.ModelMetadata,
         model_blobs_dir_path: str,
         sample_input: Optional[model_types.SupportedDataType] = None,
         is_sub_model: Optional[bool] = False,
-        **kwargs: Unpack[model_types.ModelSaveOption],
+        **kwargs: Unpack[model_types.BaseModelSaveOption],
     ) -> None:
         """Save the model.
 
         Args:
             name: Name of the model.
             model: The model object.
             model_meta: The model metadata.
```

## snowflake/ml/model/_handlers/pytorch.py

```diff
@@ -10,14 +10,18 @@
 from snowflake.ml.model import (
     _model_meta as model_meta_api,
     custom_model,
     model_signature,
     type_hints as model_types,
 )
 from snowflake.ml.model._handlers import _base
+from snowflake.ml.model._signatures import (
+    pytorch_handler,
+    utils as model_signature_utils,
+)
 
 if TYPE_CHECKING:
     import torch
 
 
 class _PyTorchHandler(_base._ModelHandler["torch.nn.Module"]):
     """Handler for PyTorch based model.
@@ -67,16 +71,16 @@
                 target_methods=kwargs.pop("target_methods", None),
                 default_target_methods=_PyTorchHandler.DEFAULT_TARGET_METHODS,
             )
 
             def get_prediction(
                 target_method_name: str, sample_input: "model_types.SupportedLocalDataType"
             ) -> model_types.SupportedLocalDataType:
-                if not model_signature._SeqOfPyTorchTensorHandler.can_handle(sample_input):
-                    sample_input = model_signature._SeqOfPyTorchTensorHandler.convert_from_df(
+                if not pytorch_handler.SeqOfPyTorchTensorHandler.can_handle(sample_input):
+                    sample_input = pytorch_handler.SeqOfPyTorchTensorHandler.convert_from_df(
                         model_signature._convert_local_data_to_df(sample_input)
                     )
 
                 model.eval()
                 target_method = getattr(model, target_method_name, None)
                 assert callable(target_method)
                 with torch.no_grad():
@@ -153,20 +157,20 @@
             ) -> Callable[[custom_model.CustomModel, pd.DataFrame], pd.DataFrame]:
                 @custom_model.inference_api
                 def fn(self: custom_model.CustomModel, X: pd.DataFrame) -> pd.DataFrame:
                     if X.isnull().any(axis=None):
                         raise ValueError("Tensor cannot handle null values.")
 
                     raw_model.eval()
-                    t = model_signature._SeqOfPyTorchTensorHandler.convert_from_df(X, signature.inputs)
+                    t = pytorch_handler.SeqOfPyTorchTensorHandler.convert_from_df(X, signature.inputs)
 
                     with torch.no_grad():
                         res = getattr(raw_model, target_method)(t)
-                    return model_signature._rename_pandas_df(
-                        data=model_signature._SeqOfPyTorchTensorHandler.convert_to_df(res), features=signature.outputs
+                    return model_signature_utils.rename_pandas_df(
+                        data=pytorch_handler.SeqOfPyTorchTensorHandler.convert_to_df(res), features=signature.outputs
                     )
 
                 return fn
 
             type_method_dict = {}
             for target_method_name, sig in model_meta.signatures.items():
                 type_method_dict[target_method_name] = fn_factory(raw_model, sig, target_method_name)
```

## snowflake/ml/model/_handlers/sklearn.py

```diff
@@ -10,14 +10,15 @@
 from snowflake.ml.model import (
     _model_meta as model_meta_api,
     custom_model,
     model_signature,
     type_hints as model_types,
 )
 from snowflake.ml.model._handlers import _base
+from snowflake.ml.model._signatures import numpy_handler, utils as model_signature_utils
 
 if TYPE_CHECKING:
     import sklearn.base
     import sklearn.pipeline
 
 
 class _SKLModelHandler(_base._ModelHandler[Union["sklearn.base.BaseEstimator", "sklearn.pipeline.Pipeline"]]):
@@ -157,19 +158,19 @@
                 @custom_model.inference_api
                 def fn(self: custom_model.CustomModel, X: pd.DataFrame) -> pd.DataFrame:
                     res = getattr(raw_model, target_method)(X)
 
                     if isinstance(res, list) and len(res) > 0 and isinstance(res[0], np.ndarray):
                         # In case of multi-output estimators, predict_proba(), decision_function(), etc., functions
                         # return a list of ndarrays. We need to deal them seperately
-                        df = model_signature._SeqOfNumpyArrayHandler.convert_to_df(res)
+                        df = numpy_handler.SeqOfNumpyArrayHandler.convert_to_df(res)
                     else:
                         df = pd.DataFrame(res)
 
-                    return model_signature._rename_pandas_df(df, signature.outputs)
+                    return model_signature_utils.rename_pandas_df(df, signature.outputs)
 
                 return fn
 
             type_method_dict = {}
             for target_method_name, sig in model_meta.signatures.items():
                 type_method_dict[target_method_name] = fn_factory(raw_model, sig, target_method_name)
```

## snowflake/ml/model/_handlers/snowmlmodel.py

```diff
@@ -10,14 +10,15 @@
 from snowflake.ml.model import (
     _model_meta as model_meta_api,
     custom_model,
     model_signature,
     type_hints as model_types,
 )
 from snowflake.ml.model._handlers import _base
+from snowflake.ml.model._signatures import numpy_handler, utils as model_signature_utils
 
 if TYPE_CHECKING:
     from snowflake.ml.modeling.framework.base import BaseEstimator
 
 
 class _SnowMLModelHandler(_base._ModelHandler["BaseEstimator"]):
     """Handler for SnowML based model.
@@ -160,19 +161,19 @@
                 @custom_model.inference_api
                 def fn(self: custom_model.CustomModel, X: pd.DataFrame) -> pd.DataFrame:
                     res = getattr(raw_model, target_method)(X)
 
                     if isinstance(res, list) and len(res) > 0 and isinstance(res[0], np.ndarray):
                         # In case of multi-output estimators, predict_proba(), decision_function(), etc., functions
                         # return a list of ndarrays. We need to deal them seperately
-                        df = model_signature._SeqOfNumpyArrayHandler.convert_to_df(res)
+                        df = numpy_handler.SeqOfNumpyArrayHandler.convert_to_df(res)
                     else:
                         df = pd.DataFrame(res)
 
-                    return model_signature._rename_pandas_df(df, signature.outputs)
+                    return model_signature_utils.rename_pandas_df(df, signature.outputs)
 
                 return fn
 
             type_method_dict = {}
             for target_method_name, sig in model_meta.signatures.items():
                 type_method_dict[target_method_name] = fn_factory(raw_model, sig, target_method_name)
```

## snowflake/ml/model/_handlers/torchscript.py

```diff
@@ -8,14 +8,18 @@
 from snowflake.ml.model import (
     _model_meta as model_meta_api,
     custom_model,
     model_signature,
     type_hints as model_types,
 )
 from snowflake.ml.model._handlers import _base
+from snowflake.ml.model._signatures import (
+    pytorch_handler,
+    utils as model_signature_utils,
+)
 
 if TYPE_CHECKING:
     import torch
 
 
 class _TorchScriptHandler(_base._ModelHandler["torch.jit.ScriptModule"]):  # type:ignore[name-defined]
     """Handler for PyTorch JIT based model.
@@ -63,16 +67,16 @@
                 target_methods=kwargs.pop("target_methods", None),
                 default_target_methods=_TorchScriptHandler.DEFAULT_TARGET_METHODS,
             )
 
             def get_prediction(
                 target_method_name: str, sample_input: "model_types.SupportedLocalDataType"
             ) -> model_types.SupportedLocalDataType:
-                if not model_signature._SeqOfPyTorchTensorHandler.can_handle(sample_input):
-                    sample_input = model_signature._SeqOfPyTorchTensorHandler.convert_from_df(
+                if not pytorch_handler.SeqOfPyTorchTensorHandler.can_handle(sample_input):
+                    sample_input = pytorch_handler.SeqOfPyTorchTensorHandler.convert_from_df(
                         model_signature._convert_local_data_to_df(sample_input)
                     )
 
                 model.eval()
                 target_method = getattr(model, target_method_name, None)
                 assert callable(target_method)
                 with torch.no_grad():
@@ -147,20 +151,20 @@
                     if X.isnull().any(axis=None):
                         raise ValueError("Tensor cannot handle null values.")
 
                     import torch
 
                     raw_model.eval()
 
-                    t = model_signature._SeqOfPyTorchTensorHandler.convert_from_df(X, signature.inputs)
+                    t = pytorch_handler.SeqOfPyTorchTensorHandler.convert_from_df(X, signature.inputs)
 
                     with torch.no_grad():
                         res = getattr(raw_model, target_method)(t)
-                    return model_signature._rename_pandas_df(
-                        data=model_signature._SeqOfPyTorchTensorHandler.convert_to_df(res), features=signature.outputs
+                    return model_signature_utils.rename_pandas_df(
+                        data=pytorch_handler.SeqOfPyTorchTensorHandler.convert_to_df(res), features=signature.outputs
                     )
 
                 return fn
 
             type_method_dict = {}
             for target_method_name, sig in model_meta.signatures.items():
                 type_method_dict[target_method_name] = fn_factory(raw_model, sig, target_method_name)
```

## snowflake/ml/model/_handlers/xgboost.py

```diff
@@ -10,14 +10,15 @@
 from snowflake.ml.model import (
     _model_meta as model_meta_api,
     custom_model,
     model_signature,
     type_hints as model_types,
 )
 from snowflake.ml.model._handlers import _base
+from snowflake.ml.model._signatures import numpy_handler, utils as model_signature_utils
 
 if TYPE_CHECKING:
     import xgboost
 
 
 class _XGBModelHandler(_base._ModelHandler[Union["xgboost.Booster", "xgboost.XGBModel"]]):
     """Handler for XGBoost based model.
@@ -158,19 +159,19 @@
                 @custom_model.inference_api
                 def fn(self: custom_model.CustomModel, X: pd.DataFrame) -> pd.DataFrame:
                     res = getattr(raw_model, target_method)(X)
 
                     if isinstance(res, list) and len(res) > 0 and isinstance(res[0], np.ndarray):
                         # In case of multi-output estimators, predict_proba(), decision_function(), etc., functions
                         # return a list of ndarrays. We need to deal them seperately
-                        df = model_signature._SeqOfNumpyArrayHandler.convert_to_df(res)
+                        df = numpy_handler.SeqOfNumpyArrayHandler.convert_to_df(res)
                     else:
                         df = pd.DataFrame(res)
 
-                    return model_signature._rename_pandas_df(df, signature.outputs)
+                    return model_signature_utils.rename_pandas_df(df, signature.outputs)
 
                 return fn
 
             type_method_dict = {}
             for target_method_name, sig in model_meta.signatures.items():
                 type_method_dict[target_method_name] = fn_factory(raw_model, sig, target_method_name)
```

## snowflake/ml/model/_model.py

```diff
@@ -1,50 +1,47 @@
 import os
 import posixpath
 import tempfile
 import warnings
 from types import ModuleType
-from typing import TYPE_CHECKING, Dict, List, Literal, Optional, Tuple, Union, overload
+from typing import Dict, List, Literal, Optional, Tuple, Union, overload
 
 from snowflake.ml._internal import file_utils, type_utils
 from snowflake.ml.model import (
     _env,
     _model_handler,
     _model_meta,
     custom_model,
     model_signature,
     type_hints as model_types,
 )
 from snowflake.snowpark import FileOperation, Session
 
-if TYPE_CHECKING:
-    from snowflake.ml.modeling.framework import base
-
 MODEL_BLOBS_DIR = "models"
 
 
 @overload
 def save_model(
     *,
     name: str,
-    model: "base.BaseEstimator",
+    model: model_types.SupportedNoSignatureRequirementsModelType,
     model_dir_path: str,
     metadata: Optional[Dict[str, str]] = None,
     conda_dependencies: Optional[List[str]] = None,
     pip_requirements: Optional[List[str]] = None,
     python_version: Optional[str] = None,
     ext_modules: Optional[List[ModuleType]] = None,
     code_paths: Optional[List[str]] = None,
     options: Optional[model_types.ModelSaveOption] = None,
 ) -> _model_meta.ModelMetadata:
-    """Save a SnowML modeling model under `dir_path`.
+    """Save a model that does not require a signature under `dir_path`.
 
     Args:
         name: Name of the model.
-        model: SnowML modeling model object.
+        model: Model object.
         model_dir_path: Directory to save the model.
         metadata: Model metadata.
         conda_dependencies: List of Conda package specs. Use "[channel::]package [operator version]" syntax to specify
             a dependency. It is a recommended way to specify your dependencies using conda. When channel is not
             specified, defaults channel will be used. When deploying to Snowflake Warehouse, defaults channel would be
             replaced with the Snowflake Anaconda channel.
         pip_requirements: List of PIP package specs. Model will not be able to deploy to the warehouse if there is pip
@@ -58,26 +55,26 @@
     ...
 
 
 @overload
 def save_model(
     *,
     name: str,
-    model: model_types.SupportedLocalModelType,
+    model: model_types.SupportedRequireSignatureModelType,
     model_dir_path: str,
     signatures: Dict[str, model_signature.ModelSignature],
     metadata: Optional[Dict[str, str]] = None,
     conda_dependencies: Optional[List[str]] = None,
     pip_requirements: Optional[List[str]] = None,
     python_version: Optional[str] = None,
     ext_modules: Optional[List[ModuleType]] = None,
     code_paths: Optional[List[str]] = None,
     options: Optional[model_types.ModelSaveOption] = None,
 ) -> _model_meta.ModelMetadata:
-    """Save a local model with user provided signatures under `dir_path`.
+    """Save a model that requires a external signature with user provided signatures under `dir_path`.
 
     Args:
         name: Name of the model.
         model: Model object.
         model_dir_path: Directory to save the model.
         signatures: Model data signatures for inputs and output for every target methods.
         metadata: Model metadata.
@@ -96,26 +93,27 @@
     ...
 
 
 @overload
 def save_model(
     *,
     name: str,
-    model: model_types.SupportedLocalModelType,
+    model: model_types.SupportedRequireSignatureModelType,
     model_dir_path: str,
     sample_input: model_types.SupportedDataType,
     metadata: Optional[Dict[str, str]] = None,
     conda_dependencies: Optional[List[str]] = None,
     pip_requirements: Optional[List[str]] = None,
     python_version: Optional[str] = None,
     ext_modules: Optional[List[ModuleType]] = None,
     code_paths: Optional[List[str]] = None,
     options: Optional[model_types.ModelSaveOption] = None,
 ) -> _model_meta.ModelMetadata:
-    """Save a local model under `dir_path` with signature inferred from a sample_input_data.
+    """Save a model that requires a external signature under `dir_path` with signature
+        inferred from a sample_input_data.
 
     Args:
         name: Name of the model.
         model: Model object.
         model_dir_path: Directory to save the model.
         sample_input: Sample input data to infer the model signatures from.
         metadata: Model metadata.
@@ -134,30 +132,30 @@
     ...
 
 
 @overload
 def save_model(
     *,
     name: str,
-    model: "base.BaseEstimator",
+    model: model_types.SupportedNoSignatureRequirementsModelType,
     session: Session,
     model_stage_file_path: str,
     metadata: Optional[Dict[str, str]] = None,
     conda_dependencies: Optional[List[str]] = None,
     pip_requirements: Optional[List[str]] = None,
     python_version: Optional[str] = None,
     ext_modules: Optional[List[ModuleType]] = None,
     code_paths: Optional[List[str]] = None,
     options: Optional[model_types.ModelSaveOption] = None,
 ) -> _model_meta.ModelMetadata:
-    """Save a SnowML modeling model to a zip file whose path is the provided stage file path.
+    """Save a model that does not require a signature to a zip file whose path is the provided stage file path.
 
     Args:
         name: Name of the model.
-        model: SnowML modeling model object.
+        model: Model object.
         session: Snowpark connection session.
         model_stage_file_path: Path to the file in Snowflake stage where the function should put the saved model.
             Must be a file with .zip extension.
         metadata: Model metadata.
         conda_dependencies: List of Conda package specs. Use "[channel::]package [operator version]" syntax to specify
             a dependency. It is a recommended way to specify your dependencies using conda. When channel is not
             specified, defaults channel will be used. When deploying to Snowflake Warehouse, defaults channel would be
@@ -173,27 +171,28 @@
     ...
 
 
 @overload
 def save_model(
     *,
     name: str,
-    model: model_types.SupportedLocalModelType,
+    model: model_types.SupportedRequireSignatureModelType,
     session: Session,
     model_stage_file_path: str,
     signatures: Dict[str, model_signature.ModelSignature],
     metadata: Optional[Dict[str, str]] = None,
     conda_dependencies: Optional[List[str]] = None,
     pip_requirements: Optional[List[str]] = None,
     python_version: Optional[str] = None,
     ext_modules: Optional[List[ModuleType]] = None,
     code_paths: Optional[List[str]] = None,
     options: Optional[model_types.ModelSaveOption] = None,
 ) -> _model_meta.ModelMetadata:
-    """Save a local model with user provided signatures to a zip file whose path is the provided stage file path.
+    """Save a model that requires a external signature with user provided signatures
+         to a zip file whose path is the provided stage file path.
 
     Args:
         name: Name of the model.
         model: Model object.
         session: Snowpark connection session.
         model_stage_file_path: Path to the file in Snowflake stage where the function should put the saved model.
             Must be a file with .zip extension.
@@ -214,28 +213,28 @@
     ...
 
 
 @overload
 def save_model(
     *,
     name: str,
-    model: model_types.SupportedLocalModelType,
+    model: model_types.SupportedRequireSignatureModelType,
     session: Session,
     model_stage_file_path: str,
     sample_input: model_types.SupportedDataType,
     metadata: Optional[Dict[str, str]] = None,
     conda_dependencies: Optional[List[str]] = None,
     pip_requirements: Optional[List[str]] = None,
     python_version: Optional[str] = None,
     ext_modules: Optional[List[ModuleType]] = None,
     code_paths: Optional[List[str]] = None,
     options: Optional[model_types.ModelSaveOption] = None,
 ) -> _model_meta.ModelMetadata:
-    """Save a local model to a zip file whose path is the provided stage file path with signature inferred from a
-        sample_input_data.
+    """Save a model that requires a external signature to a zip file whose path is the
+    provided stage file path with signature inferred from a sample_input_data.
 
     Args:
         name: Name of the model.
         model: Model object.
         session: Snowpark connection session.
         model_stage_file_path: Path to the file in Snowflake stage where the function should put the saved model.
             Must be a file with .zip extension.
@@ -324,23 +323,26 @@
             "model_dir_path and model_stage_file_path both cannot be "
             + f"{'None' if model_stage_file_path is None else 'specified'} at the same time."
         )
 
     if (
         (signatures is None)
         and (sample_input is None)
-        and not type_utils.LazyType("snowflake.ml.modeling.framework.base.BaseEstimator").isinstance(model)
+        and not (
+            type_utils.LazyType("snowflake.ml.modeling.framework.base.BaseEstimator").isinstance(model)
+            or type_utils.LazyType("mlflow.pyfunc.PyFuncModel").isinstance(model)
+        )
     ) or ((signatures is not None) and (sample_input is not None)):
         raise ValueError(
             "Signatures and sample_input both cannot be "
             + f"{'None for local model' if signatures is None else 'specified'} at the same time."
         )
 
     if not options:
-        options = {}
+        options = model_types.BaseModelSaveOption()
 
     if model_dir_path:
         if os.path.exists(model_dir_path):
             if not os.path.isdir(model_dir_path):
                 raise ValueError(f"Provided model directory {model_dir_path} is not a directory.")
             if os.listdir(model_dir_path):
                 warnings.warn(
```

## snowflake/ml/model/_model_meta.py

```diff
@@ -16,18 +16,21 @@
 from snowflake.ml._internal import env as snowml_env, env_utils, file_utils
 from snowflake.ml.model import (
     _core_requirements,
     _env,
     model_signature,
     type_hints as model_types,
 )
+from snowflake.ml.model._signatures import snowpark_handler
 from snowflake.snowpark import DataFrame as SnowparkDataFrame
 
 MODEL_METADATA_VERSION = 1
 _BASIC_DEPENDENCIES = _core_requirements.REQUIREMENTS
+_SNOWFLAKE_PKG_NAME = "snowflake"
+_SNOWFLAKE_ML_PKG_NAME = f"{_SNOWFLAKE_PKG_NAME}.ml"
 
 Dependency = namedtuple("Dependency", ["conda_name", "pip_name"])
 
 
 @dataclasses.dataclass
 class _ModelBlobMetadata:
     """Dataclass to store metadata of an individual model blob (sub-model) in the packed model.
@@ -77,46 +80,57 @@
         ext_modules: List of names of modules that need to be pickled with the model. Defaults to None.
         conda_dependencies: List of conda requirements for running the model. Defaults to None.
         pip_requirements: List of pip Python packages requirements for running the model. Defaults to None.
         python_version: A string of python version where model is run. Used for user override. If specified as None,
             current version would be captured. Defaults to None.
         **kwargs: Dict of attributes and values of the metadata. Used when loading from file.
 
+    Raises:
+        ValueError: Raised when the code path contains reserved file or directory.
+
     Yields:
         A model metadata object.
     """
     model_dir_path = os.path.normpath(model_dir_path)
     embed_local_ml_library = kwargs.pop("embed_local_ml_library", False)
+    # Use the last one which is loaded first, that is mean, it is loaded from site-packages.
+    # We could make sure that user does not overwrite our library with their code follow the same naming.
+    snowml_path = list(importlib.import_module(_SNOWFLAKE_ML_PKG_NAME).__path__)[-1]
     if embed_local_ml_library:
-        snowml_path = list(importlib.import_module("snowflake.ml").__path__)[0]
         kwargs["local_ml_library_version"] = f"{snowml_env.VERSION}+{file_utils.hash_directory(snowml_path)}"
 
     model_meta = ModelMetadata(
         name=name,
         metadata=metadata,
         model_type=model_type,
         conda_dependencies=conda_dependencies,
         pip_requirements=pip_requirements,
         python_version=python_version,
         signatures=signatures,
         **kwargs,
     )
-    if code_paths:
-        code_dir_path = os.path.join(model_dir_path, ModelMetadata.MODEL_CODE_DIR)
+
+    code_dir_path = os.path.join(model_dir_path, ModelMetadata.MODEL_CODE_DIR)
+    if embed_local_ml_library or code_paths:
         os.makedirs(code_dir_path, exist_ok=True)
-        for code_path in code_paths:
-            file_utils.copy_file_or_tree(code_path, code_dir_path)
 
     if embed_local_ml_library:
-        code_dir_path = os.path.join(model_dir_path, ModelMetadata.MODEL_CODE_DIR)
-        snowml_path = list(importlib.import_module("snowflake.ml").__path__)[0]
-        snowml_path_in_code = os.path.join(code_dir_path, "snowflake")
+        snowml_path_in_code = os.path.join(code_dir_path, _SNOWFLAKE_PKG_NAME)
         os.makedirs(snowml_path_in_code, exist_ok=True)
         file_utils.copy_file_or_tree(snowml_path, snowml_path_in_code)
 
+    if code_paths:
+        for code_path in code_paths:
+            # This part is to prevent users from providing code following our naming and overwrite our code.
+            if (
+                os.path.isfile(code_path) and os.path.splitext(os.path.basename(code_path))[0] == _SNOWFLAKE_PKG_NAME
+            ) or (os.path.isdir(code_path) and os.path.basename(code_path) == _SNOWFLAKE_PKG_NAME):
+                raise ValueError("`snowflake` is a reserved name and you cannot contain that into code path.")
+            file_utils.copy_file_or_tree(code_path, code_dir_path)
+
     try:
         imported_modules = []
         if ext_modules:
             registered_modules = cloudpickle.list_registry_pickle_by_value()
             for mod in ext_modules:
                 if mod.__name__ not in registered_modules:
                     cloudpickle.register_pickle_by_value(mod)
@@ -142,17 +156,31 @@
 
     meta = ModelMetadata.load_model_metadata(model_dir_path)
     code_path = os.path.join(model_dir_path, ModelMetadata.MODEL_CODE_DIR)
     if os.path.exists(code_path):
         if code_path in sys.path:
             sys.path.remove(code_path)
         sys.path.insert(0, code_path)
-        modules = file_utils.get_all_modules(code_path)
-        for module in modules:
-            sys.modules.pop(module.name, None)
+        module_names = file_utils.get_all_modules(code_path)
+        # If the module_name starts with snowflake, then do not replace it.
+        # When deploying, we would add them beforehand.
+        # When in the local, they should not be added. We already prevent user from overwriting us.
+        module_names = [
+            module_name
+            for module_name in module_names
+            if not (module_name.startswith(f"{_SNOWFLAKE_PKG_NAME}.") or module_name == _SNOWFLAKE_PKG_NAME)
+        ]
+        for module_name in module_names:
+            actual_module = sys.modules.pop(module_name, None)
+            if actual_module is not None:
+                sys.modules[module_name] = importlib.import_module(module_name)
+
+        assert code_path in sys.path
+        sys.path.remove(code_path)
+
     return meta
 
 
 class ModelMetadata:
     """Model metadata for Snowflake native model packaged model.
 
     Attributes:
@@ -240,28 +268,30 @@
         )
 
     def _include_if_absent(self, pkgs: List[Dependency]) -> None:
         conda_reqs_str, pip_reqs_str = tuple(zip(*pkgs))
         pip_reqs = env_utils.validate_pip_requirement_string_list(list(pip_reqs_str))
         conda_reqs = env_utils.validate_conda_dependency_string_list(list(conda_reqs_str))
 
-        for conda_req, pip_req in zip(conda_reqs[""], pip_reqs):
+        for conda_req, pip_req in zip(conda_reqs[env_utils.DEFAULT_CHANNEL_NAME], pip_reqs):
             req_to_add = env_utils.get_local_installed_version_of_pip_package(pip_req)
             req_to_add.name = conda_req.name
             for added_pip_req in self._pip_requirements:
                 if added_pip_req.name == pip_req.name:
                     warnings.warn(
                         (
                             f"Basic dependency {conda_req} specified from PIP requirements."
                             + " This may prevent model deploying to Snowflake Warehouse."
                         ),
                         category=UserWarning,
                     )
             try:
-                env_utils.append_conda_dependency(self._conda_dependencies, ("", req_to_add))
+                env_utils.append_conda_dependency(
+                    self._conda_dependencies, (env_utils.DEFAULT_CHANNEL_NAME, req_to_add)
+                )
             except env_utils.DuplicateDependencyError:
                 pass
             except env_utils.DuplicateDependencyInMultipleChannelsError:
                 warnings.warn(
                     (
                         f"Basic dependency {conda_req.name} specified from non-Snowflake channel."
                         + " This may prevent model deploying to Snowflake Warehouse."
@@ -369,20 +399,20 @@
         )
         if python_version:
             meta.python_version = python_version
         meta._pip_requirements = _env.load_requirements_file(os.path.join(env_dir_path, _env._REQUIREMENTS_FILE_NAME))
         return meta
 
 
-def _is_callable(model: model_types.SupportedLocalModelType, method_name: str) -> bool:
+def _is_callable(model: model_types.SupportedModelType, method_name: str) -> bool:
     return callable(getattr(model, method_name, None))
 
 
 def _validate_signature(
-    model: model_types.SupportedLocalModelType,
+    model: model_types.SupportedRequireSignatureModelType,
     model_meta: ModelMetadata,
     target_methods: Sequence[str],
     sample_input: Optional[model_types.SupportedDataType],
     get_prediction_fn: Callable[[str, model_types.SupportedLocalDataType], model_types.SupportedLocalDataType],
 ) -> ModelMetadata:
     if model_meta._signatures is not None:
         _validate_target_methods(model, list(model_meta.signatures.keys()))
@@ -393,26 +423,26 @@
         sample_input is not None
     ), "Model signature and sample input are None at the same time. This should not happen with local model."
     model_meta._signatures = {}
     trunc_sample_input = model_signature._truncate_data(sample_input)
     if isinstance(sample_input, SnowparkDataFrame):
         # Added because of Any from missing stubs.
         trunc_sample_input = cast(SnowparkDataFrame, trunc_sample_input)
-        local_sample_input = model_signature._SnowparkDataFrameHandler.convert_to_df(trunc_sample_input)
+        local_sample_input = snowpark_handler.SnowparkDataFrameHandler.convert_to_df(trunc_sample_input)
     else:
         local_sample_input = trunc_sample_input
     for target_method in target_methods:
         predictions_df = get_prediction_fn(target_method, local_sample_input)
         sig = model_signature.infer_signature(sample_input, predictions_df)
         model_meta._signatures[target_method] = sig
     return model_meta
 
 
 def _get_target_methods(
-    model: model_types.SupportedLocalModelType,
+    model: model_types.SupportedModelType,
     target_methods: Optional[Sequence[str]],
     default_target_methods: Sequence[str],
 ) -> Sequence[str]:
     if target_methods is None:
         target_methods = [method_name for method_name in default_target_methods if _is_callable(model, method_name)]
 
     _validate_target_methods(model, target_methods)
```

## snowflake/ml/model/model_signature.py

```diff
@@ -1,1092 +1,45 @@
-import json
-import textwrap
 import warnings
-from abc import ABC, abstractmethod
-from enum import Enum
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    Dict,
-    Final,
-    Generic,
-    List,
-    Literal,
-    Optional,
-    Sequence,
-    Tuple,
-    Type,
-    Union,
-    cast,
-    final,
-)
+from typing import Any, List, Literal, Optional, Sequence, Type
 
 import numpy as np
-import numpy.typing as npt
 import pandas as pd
-from typing_extensions import TypeGuard
 
 import snowflake.snowpark
 import snowflake.snowpark.types as spt
-from snowflake.ml._internal import type_utils
 from snowflake.ml._internal.utils import formatting, identifier
 from snowflake.ml.model import type_hints as model_types
-from snowflake.ml.model._deploy_client.warehouse import infer_template
-
-if TYPE_CHECKING:
-    import tensorflow
-    import torch
-
-
-class DataType(Enum):
-    def __init__(self, value: str, snowpark_type: Type[spt.DataType], numpy_type: npt.DTypeLike) -> None:
-        self._value = value
-        self._snowpark_type = snowpark_type
-        self._numpy_type = numpy_type
-
-    INT8 = ("int8", spt.ByteType, np.int8)
-    INT16 = ("int16", spt.ShortType, np.int16)
-    INT32 = ("int32", spt.IntegerType, np.int32)
-    INT64 = ("int64", spt.LongType, np.int64)
-
-    FLOAT = ("float", spt.FloatType, np.float32)
-    DOUBLE = ("double", spt.DoubleType, np.float64)
-
-    UINT8 = ("uint8", spt.ByteType, np.uint8)
-    UINT16 = ("uint16", spt.ShortType, np.uint16)
-    UINT32 = ("uint32", spt.IntegerType, np.uint32)
-    UINT64 = ("uint64", spt.LongType, np.uint64)
-
-    BOOL = ("bool", spt.BooleanType, np.bool_)
-    STRING = ("string", spt.StringType, np.str_)
-    BYTES = ("bytes", spt.BinaryType, np.bytes_)
-
-    def as_snowpark_type(self) -> spt.DataType:
-        """Convert to corresponding Snowpark Type.
-
-        Returns:
-            A Snowpark type.
-        """
-        return self._snowpark_type()
-
-    def __repr__(self) -> str:
-        return f"DataType.{self.name}"
-
-    @classmethod
-    def from_numpy_type(cls, np_type: npt.DTypeLike) -> "DataType":
-        """Translate numpy dtype to DataType for signature definition.
-
-        Args:
-            np_type: The numpy dtype.
-
-        Raises:
-            NotImplementedError: Raised when the given numpy type is not supported.
-
-        Returns:
-            Corresponding DataType.
-        """
-        np_to_snowml_type_mapping = {i._numpy_type: i for i in DataType}
-        for potential_type in np_to_snowml_type_mapping.keys():
-            if np.can_cast(np_type, potential_type, casting="no"):
-                # This is used since the same dtype might represented in different ways.
-                return np_to_snowml_type_mapping[potential_type]
-        raise NotImplementedError(f"Type {np_type} is not supported as a DataType.")
-
-    @classmethod
-    def from_torch_type(cls, torch_type: "torch.dtype") -> "DataType":
-        import torch
-
-        """Translate torch dtype to DataType for signature definition.
-
-        Args:
-            torch_type: The torch dtype.
-
-        Returns:
-            Corresponding DataType.
-        """
-        torch_dtype_to_numpy_dtype_mapping = {
-            torch.uint8: np.uint8,
-            torch.int8: np.int8,
-            torch.int16: np.int16,
-            torch.int32: np.int32,
-            torch.int64: np.int64,
-            torch.float32: np.float32,
-            torch.float64: np.float64,
-            torch.bool: np.bool_,
-        }
-        return cls.from_numpy_type(torch_dtype_to_numpy_dtype_mapping[torch_type])
-
-    @classmethod
-    def from_snowpark_type(cls, snowpark_type: spt.DataType) -> "DataType":
-        """Translate snowpark type to DataType for signature definition.
-
-        Args:
-            snowpark_type: The snowpark type.
-
-        Raises:
-            NotImplementedError: Raised when the given numpy type is not supported.
-
-        Returns:
-            Corresponding DataType.
-        """
-        if isinstance(snowpark_type, spt.ArrayType):
-            actual_sp_type = snowpark_type.element_type
-        else:
-            actual_sp_type = snowpark_type
-
-        snowpark_to_snowml_type_mapping: Dict[Type[spt.DataType], DataType] = {
-            i._snowpark_type: i
-            for i in DataType
-            # We by default infer as signed integer.
-            if i not in [DataType.UINT8, DataType.UINT16, DataType.UINT32, DataType.UINT64]
-        }
-        for potential_type in snowpark_to_snowml_type_mapping.keys():
-            if isinstance(actual_sp_type, potential_type):
-                return snowpark_to_snowml_type_mapping[potential_type]
-        # Fallback for decimal type.
-        if isinstance(snowpark_type, spt.DecimalType):
-            if snowpark_type.scale == 0:
-                return DataType.INT64
-        raise NotImplementedError(f"Type {snowpark_type} is not supported as a DataType.")
-
-    def is_same_snowpark_type(self, incoming_snowpark_type: spt.DataType) -> bool:
-        """Check if provided snowpark type is the same as Data Type.
-
-        Args:
-            incoming_snowpark_type: The snowpark type.
-
-        Raises:
-            NotImplementedError: Raised when the given numpy type is not supported.
-
-        Returns:
-            If the provided snowpark type is the same as the DataType.
-        """
-        # Special handle for Decimal Type.
-        if isinstance(incoming_snowpark_type, spt.DecimalType):
-            if incoming_snowpark_type.scale == 0:
-                return self == DataType.INT64 or self == DataType.UINT64
-            raise NotImplementedError(f"Type {incoming_snowpark_type} is not supported as a DataType.")
-
-        return isinstance(incoming_snowpark_type, self._snowpark_type)
-
-
-class BaseFeatureSpec(ABC):
-    """Abstract Class for specification of a feature."""
-
-    def __init__(self, name: str) -> None:
-        self._name = name
-
-    @final
-    @property
-    def name(self) -> str:
-        """Name of the feature."""
-        return self._name
-
-    @abstractmethod
-    def as_snowpark_type(self) -> spt.DataType:
-        """Convert to corresponding Snowpark Type."""
-        pass
-
-    @abstractmethod
-    def to_dict(self) -> Dict[str, Any]:
-        """Serialization"""
-        pass
-
-    @classmethod
-    @abstractmethod
-    def from_dict(self, input_dict: Dict[str, Any]) -> "BaseFeatureSpec":
-        """Deserialization"""
-        pass
-
-
-class FeatureSpec(BaseFeatureSpec):
-    """Specification of a feature in Snowflake native model packaging."""
-
-    def __init__(
-        self,
-        name: str,
-        dtype: DataType,
-        shape: Optional[Tuple[int, ...]] = None,
-    ) -> None:
-        """Initialize a feature.
-
-        Args:
-            name: Name of the feature.
-            dtype: Type of the elements in the feature.
-            shape: Used to represent scalar feature, 1-d feature list or n-d tensor.
-                -1 is used to represent variable length.Defaults to None.
-
-                E.g.
-                None: scalar
-                (2,): 1d list with fixed len of 2.
-                (-1,): 1d list with variable length. Used for ragged tensor representation.
-                (d1, d2, d3): 3d tensor.
-
-        Raises:
-            TypeError: Raised when the dtype input type is incorrect.
-            TypeError: Raised when the shape input type is incorrect.
-        """
-        super().__init__(name=name)
-
-        if not isinstance(dtype, DataType):
-            raise TypeError("dtype should be a model signature datatype.")
-        self._dtype = dtype
-
-        if shape and not isinstance(shape, tuple):
-            raise TypeError("Shape should be a tuple if presented.")
-        self._shape = shape
-
-    def as_snowpark_type(self) -> spt.DataType:
-        result_type = self._dtype.as_snowpark_type()
-        if not self._shape:
-            return result_type
-        for _ in range(len(self._shape)):
-            result_type = spt.ArrayType(result_type)
-        return result_type
-
-    def as_dtype(self) -> npt.DTypeLike:
-        """Convert to corresponding local Type."""
-        if not self._shape:
-            return self._dtype._numpy_type
-        return np.object_
-
-    def __eq__(self, other: object) -> bool:
-        if isinstance(other, FeatureSpec):
-            return self._name == other._name and self._dtype == other._dtype and self._shape == other._shape
-        else:
-            return False
-
-    def __repr__(self) -> str:
-        shape_str = f", shape={repr(self._shape)}" if self._shape else ""
-        return f"FeatureSpec(dtype={repr(self._dtype)}, name={repr(self._name)}{shape_str})"
-
-    def to_dict(self) -> Dict[str, Any]:
-        """Serialize the feature group into a dict.
-
-        Returns:
-            A dict that serializes the feature group.
-        """
-        base_dict: Dict[str, Any] = {
-            "type": self._dtype.name,
-            "name": self._name,
-        }
-        if self._shape is not None:
-            base_dict["shape"] = self._shape
-        return base_dict
-
-    @classmethod
-    def from_dict(cls, input_dict: Dict[str, Any]) -> "FeatureSpec":
-        """Deserialize the feature specification from a dict.
-
-        Args:
-            input_dict: The dict containing information of the feature specification.
-
-        Returns:
-            A feature specification instance deserialized and created from the dict.
-        """
-        name = input_dict["name"]
-        shape = input_dict.get("shape", None)
-        if shape:
-            shape = tuple(shape)
-        type = DataType[input_dict["type"]]
-        return FeatureSpec(name=name, dtype=type, shape=shape)
-
-
-class FeatureGroupSpec(BaseFeatureSpec):
-    """Specification of a group of features in Snowflake native model packaging."""
-
-    def __init__(self, name: str, specs: List[FeatureSpec]) -> None:
-        """Initialize a feature group.
-
-        Args:
-            name: Name of the feature group.
-            specs: A list of feature specifications that composes the group. All children feature specs have to have
-                name. And all of them should have the same type.
-        """
-        super().__init__(name=name)
-        self._specs = specs
-        self._validate()
-
-    def _validate(self) -> None:
-        if len(self._specs) == 0:
-            raise ValueError("No children feature specs.")
-        # each has to have name, and same type
-        if not all(s._name is not None for s in self._specs):
-            raise ValueError("All children feature specs have to have name.")
-        if not (all(s._shape is None for s in self._specs) or all(s._shape is not None for s in self._specs)):
-            raise ValueError("All children feature specs have to have same shape.")
-        first_type = self._specs[0]._dtype
-        if not all(s._dtype == first_type for s in self._specs):
-            raise ValueError("All children feature specs have to have same type.")
-
-    def as_snowpark_type(self) -> spt.DataType:
-        first_type = self._specs[0].as_snowpark_type()
-        return spt.MapType(spt.StringType(), first_type)
-
-    def __eq__(self, other: object) -> bool:
-        if isinstance(other, FeatureGroupSpec):
-            return self._specs == other._specs
-        else:
-            return False
-
-    def __repr__(self) -> str:
-        spec_strs = ",\n\t\t".join(repr(spec) for spec in self._specs)
-        return textwrap.dedent(
-            f"""FeatureGroupSpec(
-                name={repr(self._name)},
-                specs=[
-                    {spec_strs}
-                ]
-            )
-            """
-        )
-
-    def to_dict(self) -> Dict[str, Any]:
-        """Serialize the feature group into a dict.
-
-        Returns:
-            A dict that serializes the feature group.
-        """
-        return {"feature_group": {"name": self._name, "specs": [s.to_dict() for s in self._specs]}}
-
-    @classmethod
-    def from_dict(cls, input_dict: Dict[str, Any]) -> "FeatureGroupSpec":
-        """Deserialize the feature group from a dict.
-
-        Args:
-            input_dict: The dict containing information of the feature group.
-
-        Returns:
-            A feature group instance deserialized and created from the dict.
-        """
-        specs = []
-        for e in input_dict["feature_group"]["specs"]:
-            spec = FeatureSpec.from_dict(e)
-            specs.append(spec)
-        return FeatureGroupSpec(name=input_dict["feature_group"]["name"], specs=specs)
-
-
-class ModelSignature:
-    """Signature of a model that specifies the input and output of a model."""
-
-    def __init__(self, inputs: Sequence[BaseFeatureSpec], outputs: Sequence[BaseFeatureSpec]) -> None:
-        """Initialize a model signature
-
-        Args:
-            inputs: A sequence of feature specifications and feature group specifications that will compose the
-                input of the model.
-            outputs: A sequence of feature specifications and feature group specifications that will compose the
-                output of the model.
-        """
-        self._inputs = inputs
-        self._outputs = outputs
-
-    @property
-    def inputs(self) -> Sequence[BaseFeatureSpec]:
-        """Inputs of the model, containing a sequence of feature specifications and feature group specifications."""
-        return self._inputs
-
-    @property
-    def outputs(self) -> Sequence[BaseFeatureSpec]:
-        """Outputs of the model, containing a sequence of feature specifications and feature group specifications."""
-        return self._outputs
-
-    def __eq__(self, other: object) -> bool:
-        if isinstance(other, ModelSignature):
-            return self._inputs == other._inputs and self._outputs == other._outputs
-        else:
-            return False
-
-    def to_dict(self) -> Dict[str, Any]:
-        """Generate a dict to represent the whole signature.
-
-        Returns:
-            A dict that serializes the signature.
-        """
-
-        return {
-            "inputs": [spec.to_dict() for spec in self._inputs],
-            "outputs": [spec.to_dict() for spec in self._outputs],
-        }
-
-    @classmethod
-    def from_dict(cls, loaded: Dict[str, Any]) -> "ModelSignature":
-        """Create a signature given the dict containing specifications of children features and feature groups.
-
-        Args:
-            loaded: The dict to be deserialized.
-
-        Returns:
-            A signature deserialized and created from the dict.
-        """
-        sig_outs = loaded["outputs"]
-        sig_inputs = loaded["inputs"]
-
-        deserialize_spec: Callable[[Dict[str, Any]], BaseFeatureSpec] = (
-            lambda sig_spec: FeatureGroupSpec.from_dict(sig_spec)
-            if "feature_group" in sig_spec
-            else FeatureSpec.from_dict(sig_spec)
-        )
-
-        return ModelSignature(
-            inputs=[deserialize_spec(s) for s in sig_inputs], outputs=[deserialize_spec(s) for s in sig_outs]
-        )
-
-    def __repr__(self) -> str:
-        inputs_spec_strs = ",\n\t\t".join(repr(spec) for spec in self._inputs)
-        outputs_spec_strs = ",\n\t\t".join(repr(spec) for spec in self._outputs)
-        return textwrap.dedent(
-            f"""ModelSignature(
-                    inputs=[
-                        {inputs_spec_strs}
-                    ],
-                    outputs=[
-                        {outputs_spec_strs}
-                    ]
-                )"""
-        )
-
-
-class _BaseDataHandler(ABC, Generic[model_types._DataType]):
-    FEATURE_PREFIX: Final[str] = "feature"
-    INPUT_PREFIX: Final[str] = "input"
-    OUTPUT_PREFIX: Final[str] = "output"
-    SIG_INFER_ROWS_COUNT_LIMIT: Final[int] = 10
-
-    @staticmethod
-    @abstractmethod
-    def can_handle(data: model_types.SupportedDataType) -> TypeGuard[model_types._DataType]:
-        ...
-
-    @staticmethod
-    @abstractmethod
-    def count(data: model_types._DataType) -> int:
-        ...
-
-    @staticmethod
-    @abstractmethod
-    def truncate(data: model_types._DataType) -> model_types._DataType:
-        ...
-
-    @staticmethod
-    @abstractmethod
-    def validate(data: model_types._DataType) -> None:
-        ...
-
-    @staticmethod
-    @abstractmethod
-    def infer_signature(data: model_types._DataType, role: Literal["input", "output"]) -> Sequence[BaseFeatureSpec]:
-        ...
-
-    @staticmethod
-    @abstractmethod
-    def convert_to_df(data: model_types._DataType, ensure_serializable: bool = True) -> pd.DataFrame:
-        ...
-
-
-class _PandasDataFrameHandler(_BaseDataHandler[pd.DataFrame]):
-    @staticmethod
-    def can_handle(data: model_types.SupportedDataType) -> TypeGuard[pd.DataFrame]:
-        return isinstance(data, pd.DataFrame)
-
-    @staticmethod
-    def count(data: pd.DataFrame) -> int:
-        return len(data.index)
-
-    @staticmethod
-    def truncate(data: pd.DataFrame) -> pd.DataFrame:
-        return data.head(min(_PandasDataFrameHandler.count(data), _PandasDataFrameHandler.SIG_INFER_ROWS_COUNT_LIMIT))
-
-    @staticmethod
-    def validate(data: pd.DataFrame) -> None:
-        df_cols = data.columns
-
-        if df_cols.has_duplicates:  # Rule out categorical index with duplicates
-            raise ValueError("Data Validation Error: Duplicate column index is found.")
-
-        assert all(hasattr(data[col], "dtype") for col in data.columns), f"Unknown column confronted in {data}"
-
-        if len(df_cols) == 0:
-            raise ValueError("Data Validation Error: Empty data is found.")
-
-        if df_cols.dtype not in [
-            np.int64,
-            np.uint64,
-            np.float64,
-            np.object_,
-        ]:  # To keep compatibility with Pandas 2.x and 1.x
-            raise ValueError("Data Validation Error: Unsupported column index type is found.")
-
-        df_col_dtypes = [data[col].dtype for col in data.columns]
-        for df_col, df_col_dtype in zip(df_cols, df_col_dtypes):
-            if df_col_dtype == np.dtype("O"):
-                # Check if all objects have the same type
-                if not all(isinstance(data_row, type(data[df_col][0])) for data_row in data[df_col]):
-                    raise ValueError(
-                        f"Data Validation Error: Inconsistent type of object found in column data {data[df_col]}."
-                    )
-
-                if isinstance(data[df_col][0], list):
-                    arr = _convert_list_to_ndarray(data[df_col][0])
-                    arr_dtype = DataType.from_numpy_type(arr.dtype)
-
-                    converted_data_list = [_convert_list_to_ndarray(data_row) for data_row in data[df_col]]
-
-                    if not all(
-                        DataType.from_numpy_type(converted_data.dtype) == arr_dtype
-                        for converted_data in converted_data_list
-                    ):
-                        raise ValueError(
-                            "Data Validation Error: "
-                            + f"Inconsistent type of element in object found in column data {data[df_col]}."
-                        )
-
-                elif isinstance(data[df_col][0], np.ndarray):
-                    arr_dtype = DataType.from_numpy_type(data[df_col][0].dtype)
-
-                    if not all(DataType.from_numpy_type(data_row.dtype) == arr_dtype for data_row in data[df_col]):
-                        raise ValueError(
-                            "Data Validation Error: "
-                            + f"Inconsistent type of element in object found in column data {data[df_col]}."
-                        )
-                elif not isinstance(data[df_col][0], (str, bytes)):
-                    raise ValueError(f"Data Validation Error: Unsupported type confronted in {data[df_col]}")
-
-    @staticmethod
-    def infer_signature(data: pd.DataFrame, role: Literal["input", "output"]) -> Sequence[BaseFeatureSpec]:
-        feature_prefix = f"{_PandasDataFrameHandler.FEATURE_PREFIX}_"
-        df_cols = data.columns
-        role_prefix = (
-            _PandasDataFrameHandler.INPUT_PREFIX if role == "input" else _PandasDataFrameHandler.OUTPUT_PREFIX
-        ) + "_"
-        if df_cols.dtype in [np.int64, np.uint64, np.float64]:
-            ft_names = [f"{role_prefix}{feature_prefix}{i}" for i in df_cols]
-        else:
-            ft_names = list(map(str, data.columns.to_list()))
-
-        df_col_dtypes = [data[col].dtype for col in data.columns]
-
-        specs = []
-        for df_col, df_col_dtype, ft_name in zip(df_cols, df_col_dtypes, ft_names):
-            if df_col_dtype == np.dtype("O"):
-                if isinstance(data[df_col][0], list):
-                    arr = _convert_list_to_ndarray(data[df_col][0])
-                    arr_dtype = DataType.from_numpy_type(arr.dtype)
-                    ft_shape = np.shape(data[df_col][0])
-
-                    converted_data_list = [_convert_list_to_ndarray(data_row) for data_row in data[df_col]]
-
-                    if not all(np.shape(converted_data) == ft_shape for converted_data in converted_data_list):
-                        ft_shape = (-1,)
-
-                    specs.append(FeatureSpec(dtype=arr_dtype, name=ft_name, shape=ft_shape))
-                elif isinstance(data[df_col][0], np.ndarray):
-                    arr_dtype = DataType.from_numpy_type(data[df_col][0].dtype)
-                    ft_shape = np.shape(data[df_col][0])
-
-                    if not all(np.shape(data_row) == ft_shape for data_row in data[df_col]):
-                        ft_shape = (-1,)
-
-                    specs.append(FeatureSpec(dtype=arr_dtype, name=ft_name, shape=ft_shape))
-                elif isinstance(data[df_col][0], str):
-                    specs.append(FeatureSpec(dtype=DataType.STRING, name=ft_name))
-                elif isinstance(data[df_col][0], bytes):
-                    specs.append(FeatureSpec(dtype=DataType.BYTES, name=ft_name))
-            else:
-                specs.append(FeatureSpec(dtype=DataType.from_numpy_type(df_col_dtype), name=ft_name))
-        return specs
-
-    @staticmethod
-    def convert_to_df(data: pd.DataFrame, ensure_serializable: bool = True) -> pd.DataFrame:
-        if not ensure_serializable:
-            return data
-        # This convert is necessary since numpy dataframe cannot be correctly handled when provided as an element of
-        # a list when creating Snowpark Dataframe.
-        df_cols = data.columns
-        df_col_dtypes = [data[col].dtype for col in data.columns]
-        for df_col, df_col_dtype in zip(df_cols, df_col_dtypes):
-            if df_col_dtype == np.dtype("O"):
-                if isinstance(data[df_col][0], np.ndarray):
-                    data[df_col] = data[df_col].map(np.ndarray.tolist)
-        return data
-
-
-class _NumpyArrayHandler(_BaseDataHandler[model_types._SupportedNumpyArray]):
-    @staticmethod
-    def can_handle(data: model_types.SupportedDataType) -> TypeGuard[model_types._SupportedNumpyArray]:
-        return isinstance(data, np.ndarray)
-
-    @staticmethod
-    def count(data: model_types._SupportedNumpyArray) -> int:
-        return data.shape[0]
-
-    @staticmethod
-    def truncate(data: model_types._SupportedNumpyArray) -> model_types._SupportedNumpyArray:
-        return data[: min(_NumpyArrayHandler.count(data), _NumpyArrayHandler.SIG_INFER_ROWS_COUNT_LIMIT)]
-
-    @staticmethod
-    def validate(data: model_types._SupportedNumpyArray) -> None:
-        if data.shape == (0,):
-            # Empty array
-            raise ValueError("Data Validation Error: Empty data is found.")
-
-        if data.shape == ():
-            # scalar
-            raise ValueError("Data Validation Error: Scalar data is found.")
-
-    @staticmethod
-    def infer_signature(
-        data: model_types._SupportedNumpyArray, role: Literal["input", "output"]
-    ) -> Sequence[BaseFeatureSpec]:
-        feature_prefix = f"{_NumpyArrayHandler.FEATURE_PREFIX}_"
-        dtype = DataType.from_numpy_type(data.dtype)
-        role_prefix = (_NumpyArrayHandler.INPUT_PREFIX if role == "input" else _NumpyArrayHandler.OUTPUT_PREFIX) + "_"
-        if len(data.shape) == 1:
-            return [FeatureSpec(dtype=dtype, name=f"{role_prefix}{feature_prefix}0")]
-        else:
-            # For high-dimension array, 0-axis is for batch, 1-axis is for column, further more is details of columns.
-            features = []
-            n_cols = data.shape[1]
-            ft_names = [f"{role_prefix}{feature_prefix}{i}" for i in range(n_cols)]
-            for col_data, ft_name in zip(data[0], ft_names):
-                if isinstance(col_data, np.ndarray):
-                    ft_shape = np.shape(col_data)
-                    features.append(FeatureSpec(dtype=dtype, name=ft_name, shape=ft_shape))
-                else:
-                    features.append(FeatureSpec(dtype=dtype, name=ft_name))
-            return features
-
-    @staticmethod
-    def convert_to_df(data: model_types._SupportedNumpyArray, ensure_serializable: bool = True) -> pd.DataFrame:
-        if len(data.shape) == 1:
-            data = np.expand_dims(data, axis=1)
-        n_cols = data.shape[1]
-        if len(data.shape) == 2:
-            return pd.DataFrame(data)
-        else:
-            n_rows = data.shape[0]
-            if ensure_serializable:
-                return pd.DataFrame(data={i: [data[k, i].tolist() for k in range(n_rows)] for i in range(n_cols)})
-            return pd.DataFrame(data={i: [list(data[k, i]) for k in range(n_rows)] for i in range(n_cols)})
-
-
-class _SeqOfNumpyArrayHandler(_BaseDataHandler[Sequence[model_types._SupportedNumpyArray]]):
-    @staticmethod
-    def can_handle(data: model_types.SupportedDataType) -> TypeGuard[Sequence[model_types._SupportedNumpyArray]]:
-        if not isinstance(data, list):
-            return False
-        if len(data) == 0:
-            return False
-        if isinstance(data[0], np.ndarray):
-            return all(isinstance(data_col, np.ndarray) for data_col in data)
-        return False
-
-    @staticmethod
-    def count(data: Sequence[model_types._SupportedNumpyArray]) -> int:
-        return min(_NumpyArrayHandler.count(data_col) for data_col in data)
-
-    @staticmethod
-    def truncate(data: Sequence[model_types._SupportedNumpyArray]) -> Sequence[model_types._SupportedNumpyArray]:
-        return [
-            data_col[: min(_SeqOfNumpyArrayHandler.count(data), _SeqOfNumpyArrayHandler.SIG_INFER_ROWS_COUNT_LIMIT)]
-            for data_col in data
-        ]
-
-    @staticmethod
-    def validate(data: Sequence[model_types._SupportedNumpyArray]) -> None:
-        for data_col in data:
-            _NumpyArrayHandler.validate(data_col)
-
-    @staticmethod
-    def infer_signature(
-        data: Sequence[model_types._SupportedNumpyArray], role: Literal["input", "output"]
-    ) -> Sequence[BaseFeatureSpec]:
-        feature_prefix = f"{_SeqOfNumpyArrayHandler.FEATURE_PREFIX}_"
-        features: List[BaseFeatureSpec] = []
-        role_prefix = (
-            _SeqOfNumpyArrayHandler.INPUT_PREFIX if role == "input" else _SeqOfNumpyArrayHandler.OUTPUT_PREFIX
-        ) + "_"
-
-        for i, data_col in enumerate(data):
-            dtype = DataType.from_numpy_type(data_col.dtype)
-            ft_name = f"{role_prefix}{feature_prefix}{i}"
-            if len(data_col.shape) == 1:
-                features.append(FeatureSpec(dtype=dtype, name=ft_name))
-            else:
-                ft_shape = tuple(data_col.shape[1:])
-                features.append(FeatureSpec(dtype=dtype, name=ft_name, shape=ft_shape))
-        return features
-
-    @staticmethod
-    def convert_to_df(
-        data: Sequence[model_types._SupportedNumpyArray], ensure_serializable: bool = True
-    ) -> pd.DataFrame:
-        if ensure_serializable:
-            return pd.DataFrame(data={i: data_col.tolist() for i, data_col in enumerate(data)})
-        return pd.DataFrame(data={i: list(data_col) for i, data_col in enumerate(data)})
-
-
-class _SeqOfPyTorchTensorHandler(_BaseDataHandler[Sequence["torch.Tensor"]]):
-    @staticmethod
-    def can_handle(data: model_types.SupportedDataType) -> TypeGuard[Sequence["torch.Tensor"]]:
-        if not isinstance(data, list):
-            return False
-        if len(data) == 0:
-            return False
-        if type_utils.LazyType("torch.Tensor").isinstance(data[0]):
-            return all(type_utils.LazyType("torch.Tensor").isinstance(data_col) for data_col in data)
-        return False
-
-    @staticmethod
-    def count(data: Sequence["torch.Tensor"]) -> int:
-        return min(data_col.shape[0] for data_col in data)
-
-    @staticmethod
-    def truncate(data: Sequence["torch.Tensor"]) -> Sequence["torch.Tensor"]:
-        return [
-            data_col[
-                : min(_SeqOfPyTorchTensorHandler.count(data), _SeqOfPyTorchTensorHandler.SIG_INFER_ROWS_COUNT_LIMIT)
-            ]
-            for data_col in data
-        ]
-
-    @staticmethod
-    def validate(data: Sequence["torch.Tensor"]) -> None:
-        import torch
-
-        for data_col in data:
-            if data_col.shape == torch.Size([0]):
-                # Empty array
-                raise ValueError("Data Validation Error: Empty data is found.")
-
-            if data_col.shape == torch.Size([1]):
-                # scalar
-                raise ValueError("Data Validation Error: Scalar data is found.")
-
-    @staticmethod
-    def infer_signature(data: Sequence["torch.Tensor"], role: Literal["input", "output"]) -> Sequence[BaseFeatureSpec]:
-        feature_prefix = f"{_SeqOfPyTorchTensorHandler.FEATURE_PREFIX}_"
-        features: List[BaseFeatureSpec] = []
-        role_prefix = (
-            _SeqOfPyTorchTensorHandler.INPUT_PREFIX if role == "input" else _SeqOfPyTorchTensorHandler.OUTPUT_PREFIX
-        ) + "_"
-
-        for i, data_col in enumerate(data):
-            dtype = DataType.from_torch_type(data_col.dtype)
-            ft_name = f"{role_prefix}{feature_prefix}{i}"
-            if len(data_col.shape) == 1:
-                features.append(FeatureSpec(dtype=dtype, name=ft_name))
-            else:
-                ft_shape = tuple(data_col.shape[1:])
-                features.append(FeatureSpec(dtype=dtype, name=ft_name, shape=ft_shape))
-        return features
-
-    @staticmethod
-    def convert_to_df(data: Sequence["torch.Tensor"], ensure_serializable: bool = True) -> pd.DataFrame:
-        # Use list(...) instead of .tolist() to ensure that
-        # the content is still numpy array so that the type could be preserved.
-        # But that would not serializable and cannot use as UDF input and output.
-        if ensure_serializable:
-            return pd.DataFrame({i: data_col.detach().to("cpu").numpy().tolist() for i, data_col in enumerate(data)})
-        return pd.DataFrame({i: list(data_col.detach().to("cpu").numpy()) for i, data_col in enumerate(data)})
-
-    @staticmethod
-    def convert_from_df(
-        df: pd.DataFrame, features: Optional[Sequence[BaseFeatureSpec]] = None
-    ) -> Sequence["torch.Tensor"]:
-        import torch
-
-        res = []
-        if features:
-            for feature in features:
-                if isinstance(feature, FeatureGroupSpec):
-                    raise NotImplementedError("FeatureGroupSpec is not supported.")
-                assert isinstance(feature, FeatureSpec), "Invalid feature kind."
-                res.append(torch.from_numpy(np.stack(df[feature.name].to_numpy()).astype(feature._dtype._numpy_type)))
-            return res
-        return [torch.from_numpy(np.stack(df[col].to_numpy())) for col in df]
-
-
-class _SeqOfTensorflowTensorHandler(_BaseDataHandler[Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]]):
-    @staticmethod
-    def can_handle(
-        data: model_types.SupportedDataType,
-    ) -> TypeGuard[Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]]:
-        if not isinstance(data, list):
-            return False
-        if len(data) == 0:
-            return False
-        if type_utils.LazyType("tensorflow.Tensor").isinstance(data[0]) or type_utils.LazyType(
-            "tensorflow.Variable"
-        ).isinstance(data[0]):
-            return all(
-                type_utils.LazyType("tensorflow.Tensor").isinstance(data_col)
-                or type_utils.LazyType("tensorflow.Variable").isinstance(data_col)
-                for data_col in data
-            )
-        return False
-
-    @staticmethod
-    def count(data: Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]) -> int:
-        import tensorflow as tf
-
-        rows = []
-        for data_col in data:
-            shapes = data_col.shape.as_list()
-            if data_col.shape == tf.TensorShape(None) or (not shapes) or (shapes[0] is None):
-                # Unknown shape array
-                raise ValueError("Data Validation Error: Unknown shape data is found.")
-            # Make mypy happy
-            assert isinstance(shapes[0], int)
-
-            rows.append(shapes[0])
-
-        return min(rows)
-
-    @staticmethod
-    def truncate(
-        data: Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]
-    ) -> Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]:
-        return [
-            data_col[
-                : min(
-                    _SeqOfTensorflowTensorHandler.count(data), _SeqOfTensorflowTensorHandler.SIG_INFER_ROWS_COUNT_LIMIT
-                )
-            ]
-            for data_col in data
-        ]
-
-    @staticmethod
-    def validate(data: Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]) -> None:
-        import tensorflow as tf
-
-        for data_col in data:
-            if data_col.shape == tf.TensorShape(None) or any(dim is None for dim in data_col.shape.as_list()):
-                # Unknown shape array
-                raise ValueError("Data Validation Error: Unknown shape data is found.")
-
-            if data_col.shape == tf.TensorShape([0]):
-                # Empty array
-                raise ValueError("Data Validation Error: Empty data is found.")
-
-            if data_col.shape == tf.TensorShape([1]) or data_col.shape == tf.TensorShape([]):
-                # scalar
-                raise ValueError("Data Validation Error: Scalar data is found.")
-
-    @staticmethod
-    def infer_signature(
-        data: Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]], role: Literal["input", "output"]
-    ) -> Sequence[BaseFeatureSpec]:
-        feature_prefix = f"{_SeqOfTensorflowTensorHandler.FEATURE_PREFIX}_"
-        features: List[BaseFeatureSpec] = []
-        role_prefix = (
-            _SeqOfTensorflowTensorHandler.INPUT_PREFIX
-            if role == "input"
-            else _SeqOfTensorflowTensorHandler.OUTPUT_PREFIX
-        ) + "_"
-
-        for i, data_col in enumerate(data):
-            dtype = DataType.from_numpy_type(data_col.dtype.as_numpy_dtype)
-            ft_name = f"{role_prefix}{feature_prefix}{i}"
-            if len(data_col.shape) == 1:
-                features.append(FeatureSpec(dtype=dtype, name=ft_name))
-            else:
-                ft_shape = tuple(data_col.shape[1:])
-                features.append(FeatureSpec(dtype=dtype, name=ft_name, shape=ft_shape))
-        return features
-
-    @staticmethod
-    def convert_to_df(
-        data: Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]], ensure_serializable: bool = True
-    ) -> pd.DataFrame:
-        if ensure_serializable:
-            return pd.DataFrame({i: data_col.numpy().tolist() for i, data_col in enumerate(iterable=data)})
-        return pd.DataFrame({i: list(data_col.numpy()) for i, data_col in enumerate(iterable=data)})
-
-    @staticmethod
-    def convert_from_df(
-        df: pd.DataFrame, features: Optional[Sequence[BaseFeatureSpec]] = None
-    ) -> Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]:
-        import tensorflow as tf
-
-        res = []
-        if features:
-            for feature in features:
-                if isinstance(feature, FeatureGroupSpec):
-                    raise NotImplementedError("FeatureGroupSpec is not supported.")
-                assert isinstance(feature, FeatureSpec), "Invalid feature kind."
-                res.append(
-                    tf.convert_to_tensor(np.stack(df[feature.name].to_numpy()).astype(feature._dtype._numpy_type))
-                )
-            return res
-        return [tf.convert_to_tensor(np.stack(df[col].to_numpy())) for col in df]
+from snowflake.ml.model._signatures import (
+    base_handler,
+    builtins_handler as builtins_handler,
+    core,
+    numpy_handler,
+    pandas_handler,
+    pytorch_handler,
+    snowpark_handler,
+    tensorflow_handler,
+    utils,
+)
 
+DataType = core.DataType
+BaseFeatureSpec = core.BaseFeatureSpec
+FeatureSpec = core.FeatureSpec
+FeatureGroupSpec = core.FeatureGroupSpec
+ModelSignature = core.ModelSignature
 
-class _ListOfBuiltinHandler(_BaseDataHandler[model_types._SupportedBuiltinsList]):
-    @staticmethod
-    def can_handle(data: model_types.SupportedDataType) -> TypeGuard[model_types._SupportedBuiltinsList]:
-        return (
-            isinstance(data, list)
-            and len(data) > 0
-            and all(isinstance(data_col, (int, float, bool, str, bytes, list)) for data_col in data)
-        )
-
-    @staticmethod
-    def count(data: model_types._SupportedBuiltinsList) -> int:
-        return len(data)
-
-    @staticmethod
-    def truncate(data: model_types._SupportedBuiltinsList) -> model_types._SupportedBuiltinsList:
-        return data[: min(_ListOfBuiltinHandler.count(data), _ListOfBuiltinHandler.SIG_INFER_ROWS_COUNT_LIMIT)]
-
-    @staticmethod
-    def validate(data: model_types._SupportedBuiltinsList) -> None:
-        if not all(isinstance(data_row, type(data[0])) for data_row in data):
-            raise ValueError(f"Data Validation Error: Inconsistent type of object found in data {data}.")
-        df = pd.DataFrame(data)
-        if df.isnull().values.any():
-            raise ValueError(f"Data Validation Error: Ill-shaped list data {data} confronted.")
-
-    @staticmethod
-    def infer_signature(
-        data: model_types._SupportedBuiltinsList, role: Literal["input", "output"]
-    ) -> Sequence[BaseFeatureSpec]:
-        return _PandasDataFrameHandler.infer_signature(pd.DataFrame(data), role)
-
-    @staticmethod
-    def convert_to_df(
-        data: model_types._SupportedBuiltinsList,
-        ensure_serializable: bool = True,
-    ) -> pd.DataFrame:
-        return pd.DataFrame(data)
-
-
-class _SnowparkDataFrameHandler(_BaseDataHandler[snowflake.snowpark.DataFrame]):
-    @staticmethod
-    def can_handle(data: model_types.SupportedDataType) -> TypeGuard[snowflake.snowpark.DataFrame]:
-        return isinstance(data, snowflake.snowpark.DataFrame)
-
-    @staticmethod
-    def count(data: snowflake.snowpark.DataFrame) -> int:
-        return data.count()
-
-    @staticmethod
-    def truncate(data: snowflake.snowpark.DataFrame) -> snowflake.snowpark.DataFrame:
-        return cast(snowflake.snowpark.DataFrame, data.limit(_SnowparkDataFrameHandler.SIG_INFER_ROWS_COUNT_LIMIT))
-
-    @staticmethod
-    def validate(data: snowflake.snowpark.DataFrame) -> None:
-        schema = data.schema
-        for field in schema.fields:
-            data_type = field.datatype
-            if isinstance(data_type, spt.ArrayType):
-                actual_data_type = data_type.element_type
-            else:
-                actual_data_type = data_type
-            if not any(type.is_same_snowpark_type(actual_data_type) for type in DataType):
-                raise ValueError(
-                    f"Data Validation Error: Unsupported data type {field.datatype} in column {field.name}."
-                )
 
-    @staticmethod
-    def infer_signature(
-        data: snowflake.snowpark.DataFrame, role: Literal["input", "output"]
-    ) -> Sequence[BaseFeatureSpec]:
-        features: List[BaseFeatureSpec] = []
-        schema = data.schema
-        for field in schema.fields:
-            name = identifier.get_unescaped_names(field.name)
-            if isinstance(field.datatype, spt.ArrayType):
-                raise NotImplementedError("Cannot infer model signature from Snowpark DataFrame with Array Type.")
-            else:
-                features.append(FeatureSpec(name=name, dtype=DataType.from_snowpark_type(field.datatype)))
-        return features
-
-    @staticmethod
-    def convert_to_df(
-        data: snowflake.snowpark.DataFrame,
-        ensure_serializable: bool = True,
-        features: Optional[Sequence[BaseFeatureSpec]] = None,
-    ) -> pd.DataFrame:
-        # This method do things on top of to_pandas, to make sure the local dataframe got is in correct shape.
-        dtype_map = {}
-        if features:
-            for feature in features:
-                if isinstance(feature, FeatureGroupSpec):
-                    raise NotImplementedError("FeatureGroupSpec is not supported.")
-                assert isinstance(feature, FeatureSpec), "Invalid feature kind."
-                dtype_map[feature.name] = feature.as_dtype()
-        df_local = data.to_pandas()
-        # This is because Array will become string (Even though the correct schema is set)
-        # and object will become variant type and requires an additional loads
-        # to get correct data otherwise it would be string.
-        for field in data.schema.fields:
-            if isinstance(field.datatype, spt.ArrayType):
-                df_local[identifier.get_unescaped_names(field.name)] = df_local[
-                    identifier.get_unescaped_names(field.name)
-                ].map(json.loads)
-        # Only when the feature is not from inference, we are confident to do the type casting.
-        # Otherwise, dtype_map will be empty
-        df_local = df_local.astype(dtype=dtype_map)
-        return df_local
-
-    @staticmethod
-    def convert_from_df(
-        session: snowflake.snowpark.Session, df: pd.DataFrame, keep_order: bool = True
-    ) -> snowflake.snowpark.DataFrame:
-        # This method is necessary to create the Snowpark Dataframe in correct schema.
-        # Snowpark ignore the schema argument when providing a pandas DataFrame.
-        # However, in this case, if a cell of the original Dataframe is some array type,
-        # they will be inferred as VARIANT.
-        # To make sure Snowpark get the correct schema, we have to provide in a list of records.
-        # However, in this case, the order could not be preserved. Thus, a _ID column has to be added,
-        # if keep_order is True.
-        # Although in this case, the column with array type can get correct ARRAY type, however, the element
-        # type is not preserved, and will become string type. This affect the implementation of convert_from_df.
-        df = _PandasDataFrameHandler.convert_to_df(df)
-        df_cols = df.columns
-        if df_cols.dtype != np.object_:
-            raise ValueError("Cannot convert a Pandas DataFrame whose column index is not a string")
-        features = _PandasDataFrameHandler.infer_signature(df, role="input")
-        # Role will be no effect on the column index. That is to say, the feature name is the actual column name.
-        schema_list = []
-        for feature in features:
-            if isinstance(feature, FeatureGroupSpec):
-                raise NotImplementedError("FeatureGroupSpec is not supported.")
-            assert isinstance(feature, FeatureSpec), "Invalid feature kind."
-            schema_list.append(
-                spt.StructField(
-                    identifier.get_inferred_name(feature.name),
-                    feature.as_snowpark_type(),
-                    nullable=df[feature.name].isnull().any(),
-                )
-            )
-
-        data = df.rename(columns=identifier.get_inferred_name).to_dict("records")
-        if keep_order:
-            for idx, data_item in enumerate(data):
-                data_item[infer_template._KEEP_ORDER_COL_NAME] = idx
-            schema_list.append(spt.StructField(infer_template._KEEP_ORDER_COL_NAME, spt.LongType(), nullable=False))
-        sp_df = session.create_dataframe(
-            data,  # To make sure the schema can be used, otherwise, array will become variant.
-            spt.StructType(schema_list),
-        )
-        return sp_df
-
-
-_LOCAL_DATA_HANDLERS: List[Type[_BaseDataHandler[Any]]] = [
-    _PandasDataFrameHandler,
-    _NumpyArrayHandler,
-    _ListOfBuiltinHandler,
-    _SeqOfNumpyArrayHandler,
-    _SeqOfPyTorchTensorHandler,
-    _SeqOfTensorflowTensorHandler,
+_LOCAL_DATA_HANDLERS: List[Type[base_handler.BaseDataHandler[Any]]] = [
+    pandas_handler.PandasDataFrameHandler,
+    numpy_handler.NumpyArrayHandler,
+    builtins_handler.ListOfBuiltinHandler,
+    numpy_handler.SeqOfNumpyArrayHandler,
+    pytorch_handler.SeqOfPyTorchTensorHandler,
+    tensorflow_handler.SeqOfTensorflowTensorHandler,
 ]
-_ALL_DATA_HANDLERS = _LOCAL_DATA_HANDLERS + [_SnowparkDataFrameHandler]
+_ALL_DATA_HANDLERS = _LOCAL_DATA_HANDLERS + [snowpark_handler.SnowparkDataFrameHandler]
 
 
 def _truncate_data(data: model_types.SupportedDataType) -> model_types.SupportedDataType:
     for handler in _ALL_DATA_HANDLERS:
         if handler.can_handle(data):
             row_count = handler.count(data)
             if row_count <= handler.SIG_INFER_ROWS_COUNT_LIMIT:
@@ -1106,15 +59,15 @@
     raise NotImplementedError(
         f"Unable to infer model signature: Un-supported type provided {type(data)} for data truncate."
     )
 
 
 def _infer_signature(
     data: model_types.SupportedLocalDataType, role: Literal["input", "output"]
-) -> Sequence[BaseFeatureSpec]:
+) -> Sequence[core.BaseFeatureSpec]:
     """Infer the inputs/outputs signature given a data that could be dataframe, numpy array or list.
         Dispatching is used to separate logic for different types.
         (Not using Python's singledispatch for unsupported feature of union dispatching in 3.8)
 
     Args:
         data: The data that we want to infer signature from.
         role: a flag indicating that if this is to infer an input or output feature.
@@ -1130,95 +83,15 @@
             handler.validate(data)
             return handler.infer_signature(data, role)
     raise NotImplementedError(
         f"Unable to infer model signature: Un-supported type provided {type(data)} for X type inference."
     )
 
 
-def _convert_list_to_ndarray(data: List[Any]) -> npt.NDArray[Any]:
-    """Create a numpy array from list or nested list. Avoid ragged list and unaligned types.
-
-    Args:
-        data: List or nested list.
-
-    Raises:
-        ValueError: Raised when ragged nested list or list containing non-basic type confronted.
-        ValueError: Raised when ragged nested list or list containing non-basic type confronted.
-
-    Returns:
-        The converted numpy array.
-    """
-    warnings.filterwarnings("error", category=np.VisibleDeprecationWarning)
-    try:
-        arr = np.array(data)
-    except np.VisibleDeprecationWarning:
-        # In recent version of numpy, this warning should be raised when bad list provided.
-        raise ValueError(
-            f"Unable to construct signature: Ragged nested or Unsupported list-like data {data} confronted."
-        )
-    warnings.filterwarnings("default", category=np.VisibleDeprecationWarning)
-    if arr.dtype == object:
-        # If not raised, then a array of object would be created.
-        raise ValueError(
-            f"Unable to construct signature: Ragged nested or Unsupported list-like data {data} confronted."
-        )
-    return arr
-
-
-def _rename_features(
-    features: Sequence[BaseFeatureSpec], feature_names: Optional[List[str]] = None
-) -> Sequence[BaseFeatureSpec]:
-    """It renames the feature in features provided optional feature names.
-
-    Args:
-        features: A sequence of feature specifications and feature group specifications.
-        feature_names: A list of names to assign to features and feature groups. Defaults to None.
-
-    Raises:
-        ValueError: Raised when provided feature_names does not match the data shape.
-
-    Returns:
-        A sequence of feature specifications and feature group specifications being renamed if names provided.
-    """
-    if feature_names:
-        if len(feature_names) == len(features):
-            for ft, ft_name in zip(features, feature_names):
-                ft._name = ft_name
-        else:
-            raise ValueError(
-                f"{len(feature_names)} feature names are provided, while there are {len(features)} features."
-            )
-    return features
-
-
-def _rename_pandas_df(data: pd.DataFrame, features: Sequence[BaseFeatureSpec]) -> pd.DataFrame:
-    """It renames pandas dataframe that has non-object column index with provided features.
-
-    Args:
-        data: A pandas dataframe to be renamed.
-        features: A sequence of feature specifications and feature group specifications to rename the dataframe.
-
-    Raises:
-        ValueError: Raised when the data does not have the same number of features as signature.
-
-    Returns:
-        A pandas dataframe with columns renamed.
-    """
-    df_cols = data.columns
-    if df_cols.dtype in [np.int64, np.uint64, np.float64]:
-        if len(features) != len(data.columns):
-            raise ValueError(
-                "Data does not have the same number of features as signature. "
-                + f"Signature requires {len(features)} features, but have {len(data.columns)} in input data."
-            )
-        data.columns = pd.Index([feature.name for feature in features])
-    return data
-
-
-def _validate_pandas_df(data: pd.DataFrame, features: Sequence[BaseFeatureSpec]) -> None:
+def _validate_pandas_df(data: pd.DataFrame, features: Sequence[core.BaseFeatureSpec]) -> None:
     """It validates pandas dataframe with provided features.
 
     Args:
         data: A pandas dataframe to be validated.
         features: A sequence of feature specifications and feature group specifications, where the dataframe should fit.
 
     Raises:
@@ -1237,22 +110,22 @@
         ft_name = feature.name
         try:
             data_col = data[ft_name]
         except KeyError:
             raise ValueError(f"Data Validation Error: feature {ft_name} does not exist in data.")
 
         df_col_dtype = data_col.dtype
-        if isinstance(feature, FeatureGroupSpec):
+        if isinstance(feature, core.FeatureGroupSpec):
             raise NotImplementedError("FeatureGroupSpec is not supported.")
 
-        assert isinstance(feature, FeatureSpec), "Invalid feature kind."
+        assert isinstance(feature, core.FeatureSpec), "Invalid feature kind."
         ft_type = feature._dtype
         ft_shape = feature._shape
         if df_col_dtype != np.dtype("O"):
-            if ft_type != DataType.from_numpy_type(df_col_dtype):
+            if ft_type != core.DataType.from_numpy_type(df_col_dtype):
                 raise ValueError(
                     f"Data Validation Error in feature {ft_name}: "
                     + f"Feature type {ft_type} is not met by all elements in {data_col}."
                 )
             elif ft_shape is not None:
                 raise ValueError(
                     f"Data Validation Error in feature {ft_name}: "
@@ -1262,18 +135,19 @@
             if isinstance(data_col[0], list):
                 if not ft_shape:
                     raise ValueError(
                         f"Data Validation Error in feature {ft_name}: "
                         + "Feature is a scalar feature while list data is provided."
                     )
 
-                converted_data_list = [_convert_list_to_ndarray(data_row) for data_row in data_col]
+                converted_data_list = [utils.convert_list_to_ndarray(data_row) for data_row in data_col]
 
                 if not all(
-                    DataType.from_numpy_type(converted_data.dtype) == ft_type for converted_data in converted_data_list
+                    core.DataType.from_numpy_type(converted_data.dtype) == ft_type
+                    for converted_data in converted_data_list
                 ):
                     raise ValueError(
                         f"Data Validation Error in feature {ft_name}: "
                         + f"Feature type {ft_type} is not met by all elements in {data_col}."
                     )
 
                 if ft_shape and ft_shape != (-1,):
@@ -1285,15 +159,15 @@
             elif isinstance(data_col[0], np.ndarray):
                 if not ft_shape:
                     raise ValueError(
                         f"Data Validation Error in feature {ft_name}: "
                         + "Feature is a scalar feature while array data is provided."
                     )
 
-                if not all(DataType.from_numpy_type(data_row.dtype) == ft_type for data_row in data_col):
+                if not all(core.DataType.from_numpy_type(data_row.dtype) == ft_type for data_row in data_col):
                     raise ValueError(
                         f"Data Validation Error in feature {ft_name}: "
                         + f"Feature type {ft_type} is not met by all elements in {data_col}."
                     )
 
                 ft_shape = feature._shape
                 if ft_shape and ft_shape != (-1,):
@@ -1305,33 +179,33 @@
                         )
             elif isinstance(data_col[0], str):
                 if ft_shape is not None:
                     raise ValueError(
                         f"Data Validation Error in feature {ft_name}: "
                         + "Feature is a array type feature while scalar data is provided."
                     )
-                if ft_type != DataType.STRING:
+                if ft_type != core.DataType.STRING:
                     raise ValueError(
                         f"Data Validation Error in feature {ft_name}: "
                         + f"Feature type {ft_type} is not met by all elements in {data_col}."
                     )
             elif isinstance(data_col[0], bytes):
                 if ft_shape is not None:
                     raise ValueError(
                         f"Data Validation Error in feature {ft_name}: "
                         + "Feature is a array type feature while scalar data is provided."
                     )
-                if ft_type != DataType.BYTES:
+                if ft_type != core.DataType.BYTES:
                     raise ValueError(
                         f"Data Validation Error in feature {ft_name}: "
                         + f"Feature type {ft_type} is not met by all elements in {data_col}."
                     )
 
 
-def _validate_snowpark_data(data: snowflake.snowpark.DataFrame, features: Sequence[BaseFeatureSpec]) -> None:
+def _validate_snowpark_data(data: snowflake.snowpark.DataFrame, features: Sequence[core.BaseFeatureSpec]) -> None:
     """Validate Snowpark DataFrame as input
 
     Args:
         data: A snowpark dataframe to be validated.
         features: A sequence of feature specifications and feature group specifications, where the dataframe should fit.
 
     Raises:
@@ -1349,17 +223,17 @@
                 found = True
                 if field.nullable:
                     warnings.warn(
                         f"Warn in feature {ft_name}: Nullable column {field.name} provided,"
                         + " inference might fail if there is null value.",
                         category=RuntimeWarning,
                     )
-                if isinstance(feature, FeatureGroupSpec):
+                if isinstance(feature, core.FeatureGroupSpec):
                     raise NotImplementedError("FeatureGroupSpec is not supported.")
-                assert isinstance(feature, FeatureSpec), "Invalid feature kind."
+                assert isinstance(feature, core.FeatureSpec), "Invalid feature kind."
                 ft_type = feature._dtype
                 field_data_type = field.datatype
                 if isinstance(field_data_type, spt.ArrayType):
                     if feature._shape is None:
                         raise ValueError(
                             f"Data Validation Error in feature {ft_name}: "
                             + f"Feature is a array feature, while {field.name} is not."
@@ -1403,39 +277,39 @@
             break
     if df is None:
         raise ValueError(f"Data Validation Error: Un-supported type {type(data)} provided.")
     return df
 
 
 def _convert_and_validate_local_data(
-    data: model_types.SupportedLocalDataType, features: Sequence[BaseFeatureSpec]
+    data: model_types.SupportedLocalDataType, features: Sequence[core.BaseFeatureSpec]
 ) -> pd.DataFrame:
     """Validate the data with features in model signature and convert to DataFrame
 
     Args:
         features: A list of feature specs that the data should follow.
         data: The provided data.
 
     Returns:
         The converted dataframe with renamed column index.
     """
     df = _convert_local_data_to_df(data)
-    df = _rename_pandas_df(df, features)
+    df = utils.rename_pandas_df(df, features)
     _validate_pandas_df(df, features)
-    df = _PandasDataFrameHandler.convert_to_df(df, ensure_serializable=True)
+    df = pandas_handler.PandasDataFrameHandler.convert_to_df(df, ensure_serializable=True)
 
     return df
 
 
 def infer_signature(
     input_data: model_types.SupportedLocalDataType,
     output_data: model_types.SupportedLocalDataType,
     input_feature_names: Optional[List[str]] = None,
     output_feature_names: Optional[List[str]] = None,
-) -> ModelSignature:
+) -> core.ModelSignature:
     """Infer model signature from given input and output sample data.
 
     Currently, we support infer the model signature from example input/output data in the following cases:
         - Pandas data frame whose column could have types of supported data types,
             list (including list of supported data types, list of numpy array of supported data types, and nested list),
             and numpy array of supported data types.
             - Does not support DataFrame with CategoricalIndex column index.
@@ -1456,11 +330,11 @@
         input_feature_names: Name for input features. Defaults to None.
         output_feature_names: Name for output features. Defaults to None.
 
     Returns:
         A model signature.
     """
     inputs = _infer_signature(input_data, role="input")
-    inputs = _rename_features(inputs, input_feature_names)
+    inputs = utils.rename_features(inputs, input_feature_names)
     outputs = _infer_signature(output_data, role="output")
-    outputs = _rename_features(outputs, output_feature_names)
-    return ModelSignature(inputs, outputs)
+    outputs = utils.rename_features(outputs, output_feature_names)
+    return core.ModelSignature(inputs, outputs)
```

## snowflake/ml/model/type_hints.py

```diff
@@ -1,14 +1,15 @@
 # mypy: disable-error-code="import"
 from typing import TYPE_CHECKING, Sequence, TypedDict, TypeVar, Union
 
 import numpy.typing as npt
 from typing_extensions import NotRequired, TypeAlias
 
 if TYPE_CHECKING:
+    import mlflow
     import numpy as np
     import pandas as pd
     import sklearn.base
     import sklearn.pipeline
     import tensorflow
     import torch
     import xgboost
@@ -44,61 +45,60 @@
 
 SupportedDataType = Union[SupportedLocalDataType, "snowflake.snowpark.DataFrame"]
 
 _DataType = TypeVar("_DataType", bound=SupportedDataType)
 
 CustomModelType = TypeVar("CustomModelType", bound="snowflake.ml.model.custom_model.CustomModel")
 
-SupportedLocalModelType = Union[
+SupportedRequireSignatureModelType = Union[
     "snowflake.ml.model.custom_model.CustomModel",
     "sklearn.base.BaseEstimator",
     "sklearn.pipeline.Pipeline",
     "xgboost.XGBModel",
     "xgboost.Booster",
     "torch.nn.Module",
     "torch.jit.ScriptModule",  # type:ignore[name-defined]
+    "tensorflow.Module",
 ]
 
-SupportedSnowMLModelType: TypeAlias = "base.BaseEstimator"
+SupportedNoSignatureRequirementsModelType: TypeAlias = Union["base.BaseEstimator", "mlflow.pyfunc.PyFuncModel"]
 
 SupportedModelType = Union[
-    SupportedLocalModelType,
-    SupportedSnowMLModelType,
+    SupportedRequireSignatureModelType,
+    SupportedNoSignatureRequirementsModelType,
 ]
 """This is defined as the type that Snowflake native model packaging could accept.
 Here is all acceptable types of Snowflake native model packaging and its handler file in _handlers/ folder.
 
 | Type                            | Handler File | Handler             |
 |---------------------------------|--------------|---------------------|
 | snowflake.ml.model.custom_model.CustomModel | custom.py    | _CustomModelHandler |
 | sklearn.base.BaseEstimator      | sklearn.py   | _SKLModelHandler    |
 | sklearn.pipeline.Pipeline       | sklearn.py   | _SKLModelHandler    |
 | xgboost.XGBModel       | xgboost.py   | _XGBModelHandler    |
 | xgboost.Booster        | xgboost.py   | _XGBModelHandler    |
 | snowflake.ml.framework.base.BaseEstimator      | snowmlmodel.py   | _SnowMLModelHandler    |
 | torch.nn.Module      | pytroch.py   | _PyTorchHandler    |
-| torch.jit.ScriptModule      | torchscript.py   | _TorchScripthHandler    |
+| torch.jit.ScriptModule      | torchscript.py   | _TorchScriptHandler    |
+| tensorflow.Module     | tensorflow.py   | _TensorFlowHandler    |
 """
 
 
 _ModelType = TypeVar("_ModelType", bound=SupportedModelType)
 
 
 class DeployOptions(TypedDict):
     """Common Options for deploying to Snowflake.
 
-    disable_local_conda_resolver: Set to disable use local conda resolver to do pre-check on environment and rely on
-        the information schema only. Defaults to False.
     keep_order: Whether or not preserve the row order when predicting. Only available for dataframe has fewer than 2**64
         rows. Defaults to True.
     output_with_input_features: Whether or not preserve the input columns in the output when predicting.
         Defaults to False.
     """
 
-    disable_local_conda_resolver: NotRequired[bool]
     keep_order: NotRequired[bool]
     output_with_input_features: NotRequired[bool]
 
 
 class WarehouseDeployOptions(DeployOptions):
     """Options for deploying to the Snowflake Warehouse.
 
@@ -111,41 +111,94 @@
     """
 
     permanent_udf_stage_location: NotRequired[str]
     relax_version: NotRequired[bool]
     replace_udf: NotRequired[bool]
 
 
-class ModelSaveOption(TypedDict):
+class SnowparkContainerServiceDeployOptions(DeployOptions):
+    """Deployment options for deploying to SnowService.
+    When type hint is updated, please ensure the concrete class is updated accordingly at:
+    //snowflake/ml/model/_deploy_client/snowservice/_deploy_options
+
+    compute_pool[REQUIRED]: SnowService compute pool name. Please refer to official doc for how to create a
+        compute pool: https://docs.snowflake.com/LIMITEDACCESS/snowpark-containers/reference/compute-pool
+    image_repo: SnowService image repo path. e.g. "<image_registry>/<db>/<schema>/<repo>". Default to auto
+        inferred based on session information.
+    min_instances: Minimum number of service replicas. Default to 1.
+    max_instances: Maximum number of service replicas. Default to 1.
+    endpoint: The specific name of the endpoint that the service function will communicate with. This option is
+        useful when the service has multiple endpoints. Default to predict.
+    prebuilt_snowflake_image: When provided, the image-building step is skipped, and the pre-built image from
+        Snowflake is used as is. This option is for users who consistently use the same image for multiple use
+        cases, allowing faster deployment. The snowflake image used for deployment is logged to the console for
+        future use. Default to None.
+    use_gpu: When set to True, a CUDA-enabled Docker image will be used to provide a runtime CUDA environment.
+        Default to False.
+    """
+
+    compute_pool: str
+    image_repo: NotRequired[str]
+    min_instances: NotRequired[int]
+    max_instances: NotRequired[int]
+    endpoint: NotRequired[str]
+    prebuilt_snowflake_image: NotRequired[str]
+    use_gpu: NotRequired[bool]
+
+
+class BaseModelSaveOption(TypedDict):
     """Options for saving the model.
 
     embed_local_ml_library: Embedding local SnowML into the code directory of the folder.
     allow_overwritten_stage_file: Flag to indicate when saving the model as a stage file, whether overwriting existed
         file is allowed. Default to False.
     """
 
     embed_local_ml_library: NotRequired[bool]
     allow_overwritten_stage_file: NotRequired[bool]
 
 
-class CustomModelSaveOption(ModelSaveOption):
+class CustomModelSaveOption(BaseModelSaveOption):
     ...
 
 
-class SKLModelSaveOptions(ModelSaveOption):
+class SKLModelSaveOptions(BaseModelSaveOption):
     target_methods: NotRequired[Sequence[str]]
 
 
-class XGBModelSaveOptions(ModelSaveOption):
+class XGBModelSaveOptions(BaseModelSaveOption):
     target_methods: NotRequired[Sequence[str]]
 
 
-class SNOWModelSaveOptions(ModelSaveOption):
+class SNOWModelSaveOptions(BaseModelSaveOption):
     target_methods: NotRequired[Sequence[str]]
 
 
-class PyTorchSaveOptions(ModelSaveOption):
+class PyTorchSaveOptions(BaseModelSaveOption):
     target_methods: NotRequired[Sequence[str]]
 
 
-class TorchScriptSaveOptions(ModelSaveOption):
+class TorchScriptSaveOptions(BaseModelSaveOption):
     target_methods: NotRequired[Sequence[str]]
+
+
+class TensorflowSaveOptions(BaseModelSaveOption):
+    target_methods: NotRequired[Sequence[str]]
+
+
+class MLFlowSaveOptions(BaseModelSaveOption):
+    model_uri: NotRequired[str]
+    ignore_mlflow_metadata: NotRequired[bool]
+    ignore_mlflow_dependencies: NotRequired[bool]
+
+
+ModelSaveOption = Union[
+    BaseModelSaveOption,
+    CustomModelSaveOption,
+    SKLModelSaveOptions,
+    XGBModelSaveOptions,
+    SNOWModelSaveOptions,
+    PyTorchSaveOptions,
+    TorchScriptSaveOptions,
+    TensorflowSaveOptions,
+    MLFlowSaveOptions,
+]
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## snowflake/ml/modeling/calibration/calibrated_classifier_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.calibration".replace("sklearn.", "").split("_")])
 
@@ -350,33 +350,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -406,15 +402,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -422,17 +418,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -481,15 +479,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1107,16 +1105,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1124,18 +1122,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1162,30 +1156,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1219,15 +1215,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1241,15 +1237,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/affinity_propagation.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -327,33 +327,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -383,15 +379,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -399,17 +395,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -458,15 +456,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1080,16 +1078,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1097,18 +1095,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1135,30 +1129,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1192,15 +1188,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1214,15 +1210,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/agglomerative_clustering.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -360,33 +360,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -416,15 +412,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -432,17 +428,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -491,15 +489,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1111,16 +1109,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1128,18 +1126,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1166,30 +1160,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1223,15 +1219,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1245,15 +1241,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/birch.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -318,33 +318,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -374,15 +370,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -390,17 +386,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -449,15 +447,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1073,16 +1071,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1090,18 +1088,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1128,30 +1122,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1185,15 +1181,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1207,15 +1203,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/bisecting_k_means.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -367,33 +367,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -423,15 +419,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -439,17 +435,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -498,15 +496,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1122,16 +1120,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1139,18 +1137,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1177,30 +1171,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1234,15 +1230,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1256,15 +1252,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/dbscan.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -335,33 +335,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -391,15 +387,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -407,17 +403,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -466,15 +464,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1086,16 +1084,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1103,18 +1101,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1141,30 +1135,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1198,15 +1194,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1220,15 +1216,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/feature_agglomeration.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -367,33 +367,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -423,15 +419,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -439,17 +435,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -498,15 +496,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1120,16 +1118,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1137,18 +1135,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1175,30 +1169,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1232,15 +1228,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1254,15 +1250,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/k_means.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -362,33 +362,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -418,15 +414,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -434,17 +430,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -493,15 +491,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1117,16 +1115,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1134,18 +1132,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1172,30 +1166,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1229,15 +1225,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1251,15 +1247,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/mean_shift.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -338,33 +338,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -394,15 +390,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -410,17 +406,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -469,15 +467,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1091,16 +1089,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1108,18 +1106,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1146,30 +1140,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1203,15 +1199,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1225,15 +1221,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/mini_batch_k_means.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -388,33 +388,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -444,15 +440,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -460,17 +456,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -519,15 +517,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1143,16 +1141,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1160,18 +1158,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1198,30 +1192,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1255,15 +1251,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1277,15 +1273,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/optics.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -408,33 +408,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -464,15 +460,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -480,17 +476,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -539,15 +537,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1159,16 +1157,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1176,18 +1174,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1214,30 +1208,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1271,15 +1267,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1293,15 +1289,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/spectral_biclustering.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -346,33 +346,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -402,15 +398,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -418,17 +414,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -477,15 +475,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1097,16 +1095,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1114,18 +1112,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1152,30 +1146,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1209,15 +1205,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1231,15 +1227,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/spectral_clustering.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -404,33 +404,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -460,15 +456,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -476,17 +472,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -535,15 +533,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1155,16 +1153,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1172,18 +1170,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1210,30 +1204,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1267,15 +1263,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1289,15 +1285,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/cluster/spectral_coclustering.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
@@ -325,33 +325,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -381,15 +377,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -397,17 +393,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -456,15 +454,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1076,16 +1074,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1093,18 +1091,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1131,30 +1125,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1188,15 +1184,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1210,15 +1206,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/compose/column_transformer.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.compose".replace("sklearn.", "").split("_")])
 
@@ -355,33 +355,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -411,15 +407,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -427,17 +423,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -486,15 +484,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1108,16 +1106,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1125,18 +1123,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1163,30 +1157,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1220,15 +1216,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1242,15 +1238,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/compose/transformed_target_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.compose".replace("sklearn.", "").split("_")])
 
@@ -314,33 +314,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -370,15 +366,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -386,17 +382,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -445,15 +443,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1067,16 +1065,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1084,18 +1082,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1122,30 +1116,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1179,15 +1175,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1201,15 +1197,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/covariance/elliptic_envelope.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
@@ -311,33 +311,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -367,15 +363,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -383,17 +379,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -442,15 +440,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1066,16 +1064,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1083,18 +1081,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1121,30 +1115,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1178,15 +1174,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1200,15 +1196,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/covariance/empirical_covariance.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
@@ -287,33 +287,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -343,15 +339,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -359,17 +355,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -418,15 +416,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1038,16 +1036,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1055,18 +1053,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1093,30 +1087,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1150,15 +1146,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1172,15 +1168,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/covariance/graphical_lasso.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
@@ -321,33 +321,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -377,15 +373,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -393,17 +389,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -452,15 +450,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1072,16 +1070,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1089,18 +1087,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1127,30 +1121,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1184,15 +1180,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1206,15 +1202,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/covariance/graphical_lasso_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
@@ -354,33 +354,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -410,15 +406,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -426,17 +422,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -485,15 +483,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1105,16 +1103,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1122,18 +1120,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1160,30 +1154,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1217,15 +1213,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1239,15 +1235,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/covariance/ledoit_wolf.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
@@ -294,33 +294,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -350,15 +346,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -366,17 +362,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -425,15 +423,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1045,16 +1043,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1062,18 +1060,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1100,30 +1094,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1157,15 +1153,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1179,15 +1175,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/covariance/min_cov_det.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
@@ -306,33 +306,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -362,15 +358,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -378,17 +374,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -437,15 +435,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1057,16 +1055,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1074,18 +1072,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1112,30 +1106,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1169,15 +1165,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1191,15 +1187,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/covariance/oas.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
@@ -287,33 +287,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -343,15 +339,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -359,17 +355,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -418,15 +416,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1038,16 +1036,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1055,18 +1053,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1093,30 +1087,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1150,15 +1146,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1172,15 +1168,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/covariance/shrunk_covariance.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
@@ -293,33 +293,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -349,15 +345,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -365,17 +361,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -424,15 +422,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1044,16 +1042,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1061,18 +1059,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1099,30 +1093,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1156,15 +1152,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1178,15 +1174,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/decomposition/dictionary_learning.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
@@ -394,33 +394,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -450,15 +446,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -466,17 +462,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -525,15 +523,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1147,16 +1145,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1164,18 +1162,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1202,30 +1196,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1259,15 +1255,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1281,15 +1277,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/decomposition/factor_analysis.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
@@ -336,33 +336,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -392,15 +388,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -408,17 +404,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -467,15 +465,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1089,16 +1087,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1106,18 +1104,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1144,30 +1138,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1201,15 +1197,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1223,15 +1219,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/decomposition/fast_ica.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
@@ -354,33 +354,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -410,15 +406,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -426,17 +422,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -485,15 +483,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1107,16 +1105,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1124,18 +1122,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1162,30 +1156,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1219,15 +1215,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1241,15 +1237,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/decomposition/incremental_pca.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
@@ -306,33 +306,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -362,15 +358,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -378,17 +374,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -437,15 +435,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1059,16 +1057,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1076,18 +1074,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1114,30 +1108,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1171,15 +1167,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1193,15 +1189,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/decomposition/kernel_pca.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
@@ -402,33 +402,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -458,15 +454,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -474,17 +470,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -533,15 +531,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1155,16 +1153,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1172,18 +1170,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1210,30 +1204,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1267,15 +1263,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1289,15 +1285,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
@@ -424,33 +424,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -480,15 +476,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -496,17 +492,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -555,15 +553,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1177,16 +1175,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1194,18 +1192,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1232,30 +1226,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1289,15 +1285,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1311,15 +1307,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
@@ -369,33 +369,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -425,15 +421,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -441,17 +437,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -500,15 +498,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1122,16 +1120,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1139,18 +1137,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1177,30 +1171,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1234,15 +1230,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1256,15 +1252,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/decomposition/pca.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
@@ -371,33 +371,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -427,15 +423,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -443,17 +439,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -502,15 +500,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1124,16 +1122,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1141,18 +1139,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1179,30 +1173,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1236,15 +1232,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1258,15 +1254,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/decomposition/sparse_pca.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
@@ -344,33 +344,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -400,15 +396,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -416,17 +412,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -475,15 +473,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1097,16 +1095,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1114,18 +1112,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1152,30 +1146,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1209,15 +1205,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1231,15 +1227,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/decomposition/truncated_svd.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
@@ -325,33 +325,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -381,15 +377,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -397,17 +393,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -456,15 +454,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1078,16 +1076,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1095,18 +1093,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1133,30 +1127,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1190,15 +1186,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1212,15 +1208,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.discriminant_analysis".replace("sklearn.", "").split("_")])
 
@@ -340,33 +340,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -396,15 +392,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -412,17 +408,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -471,15 +469,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1101,16 +1099,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1118,18 +1116,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1156,30 +1150,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1213,15 +1209,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1235,15 +1231,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.discriminant_analysis".replace("sklearn.", "").split("_")])
 
@@ -302,33 +302,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -358,15 +354,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -374,17 +370,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -433,15 +431,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1061,16 +1059,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1078,18 +1076,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1116,30 +1110,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1173,15 +1169,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1195,15 +1191,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/ada_boost_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -327,33 +327,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -383,15 +379,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -399,17 +395,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -458,15 +456,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1086,16 +1084,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1103,18 +1101,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1141,30 +1135,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1198,15 +1194,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1220,15 +1216,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/ada_boost_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -324,33 +324,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -380,15 +376,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -396,17 +392,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -455,15 +453,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1077,16 +1075,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1094,18 +1092,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1132,30 +1126,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1189,15 +1185,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1211,15 +1207,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/bagging_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -359,33 +359,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -415,15 +411,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -431,17 +427,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -490,15 +488,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1118,16 +1116,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1135,18 +1133,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1173,30 +1167,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1230,15 +1226,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1252,15 +1248,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/bagging_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -359,33 +359,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -415,15 +411,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -431,17 +427,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -490,15 +488,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1112,16 +1110,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1129,18 +1127,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1167,30 +1161,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1224,15 +1220,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1246,15 +1242,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/extra_trees_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -461,33 +461,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -517,15 +513,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -533,17 +529,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -592,15 +590,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1218,16 +1216,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1235,18 +1233,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1273,30 +1267,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1330,15 +1326,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1352,15 +1348,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/extra_trees_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -440,33 +440,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -496,15 +492,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -512,17 +508,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -571,15 +569,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1193,16 +1191,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1210,18 +1208,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1248,30 +1242,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1305,15 +1301,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1327,15 +1323,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -475,33 +475,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -531,15 +527,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -547,17 +543,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -606,15 +604,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1234,16 +1232,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1251,18 +1249,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1289,30 +1283,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1346,15 +1342,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1368,15 +1364,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -484,33 +484,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -540,15 +536,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -556,17 +552,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -615,15 +613,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1237,16 +1235,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1254,18 +1252,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1292,30 +1286,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1349,15 +1345,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1371,15 +1367,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -453,33 +453,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -509,15 +505,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -525,17 +521,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -584,15 +582,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1212,16 +1210,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1229,18 +1227,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1267,30 +1261,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1324,15 +1320,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1346,15 +1342,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -443,33 +443,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -499,15 +495,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -515,17 +511,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -574,15 +572,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1196,16 +1194,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1213,18 +1211,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1251,30 +1245,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1308,15 +1304,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1330,15 +1326,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/isolation_forest.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -348,33 +348,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -404,15 +400,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -420,17 +416,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -479,15 +477,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1103,16 +1101,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1120,18 +1118,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1158,30 +1152,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1215,15 +1211,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1237,15 +1233,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/random_forest_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -457,33 +457,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -513,15 +509,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -529,17 +525,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -588,15 +586,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1214,16 +1212,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1231,18 +1229,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1269,30 +1263,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1326,15 +1322,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1348,15 +1344,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/random_forest_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -436,33 +436,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -492,15 +488,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -508,17 +504,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -567,15 +565,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1189,16 +1187,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1206,18 +1204,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1244,30 +1238,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1301,15 +1297,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1323,15 +1319,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/stacking_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -338,33 +338,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -394,15 +390,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -410,17 +406,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -469,15 +467,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1093,16 +1091,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1110,18 +1108,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1148,30 +1142,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1205,15 +1201,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1227,15 +1223,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/voting_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -320,33 +320,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -376,15 +372,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -392,17 +388,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -451,15 +449,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1079,16 +1077,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1096,18 +1094,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1134,30 +1128,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1191,15 +1187,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1213,15 +1209,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/ensemble/voting_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
@@ -302,33 +302,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -358,15 +354,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -374,17 +370,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -433,15 +431,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1057,16 +1055,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1074,18 +1072,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1112,30 +1106,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1169,15 +1165,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1191,15 +1187,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/feature_selection/generic_univariate_select.py

```diff
@@ -32,16 +32,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
@@ -292,33 +292,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -348,15 +344,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -364,17 +360,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -423,15 +421,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1045,16 +1043,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1062,18 +1060,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1100,30 +1094,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1157,15 +1153,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1179,15 +1175,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/feature_selection/select_fdr.py

```diff
@@ -32,16 +32,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
@@ -288,33 +288,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -344,15 +340,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -360,17 +356,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -419,15 +417,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1041,16 +1039,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1058,18 +1056,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1096,30 +1090,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1153,15 +1149,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1175,15 +1171,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/feature_selection/select_fpr.py

```diff
@@ -32,16 +32,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
@@ -288,33 +288,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -344,15 +340,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -360,17 +356,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -419,15 +417,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1041,16 +1039,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1058,18 +1056,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1096,30 +1090,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1153,15 +1149,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1175,15 +1171,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/feature_selection/select_fwe.py

```diff
@@ -32,16 +32,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
@@ -288,33 +288,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -344,15 +340,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -360,17 +356,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -419,15 +417,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1041,16 +1039,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1058,18 +1056,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1096,30 +1090,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1153,15 +1149,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1175,15 +1171,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/feature_selection/select_k_best.py

```diff
@@ -32,16 +32,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
@@ -289,33 +289,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -345,15 +341,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -361,17 +357,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -420,15 +418,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1042,16 +1040,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1059,18 +1057,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1097,30 +1091,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1154,15 +1150,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1176,15 +1172,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/feature_selection/select_percentile.py

```diff
@@ -32,16 +32,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
@@ -288,33 +288,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -344,15 +340,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -360,17 +356,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -419,15 +417,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1041,16 +1039,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1058,18 +1056,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1096,30 +1090,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1153,15 +1149,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1175,15 +1171,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/feature_selection/sequential_feature_selector.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
@@ -348,33 +348,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -404,15 +400,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -420,17 +416,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -479,15 +477,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1101,16 +1099,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1118,18 +1116,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1156,30 +1150,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1213,15 +1209,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1235,15 +1231,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/feature_selection/variance_threshold.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
@@ -281,33 +281,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -337,15 +333,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -353,17 +349,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -412,15 +410,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1034,16 +1032,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1051,18 +1049,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1089,30 +1083,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1146,15 +1142,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1168,15 +1164,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/framework/_utils.py

```diff
@@ -9,15 +9,22 @@
 from typing import Any, Callable, Dict, Iterable, Optional, Union
 
 import numpy as np
 import sklearn
 from packaging import version
 
 from snowflake import snowpark
-from snowflake.snowpark import exceptions, functions as F
+from snowflake.ml._internal.exceptions import (
+    error_codes,
+    error_messages,
+    exceptions,
+    exceptions as snowml_exceptions,
+    modeling_error_messages,
+)
+from snowflake.snowpark import exceptions as snowpark_exceptions, functions as F
 from snowflake.snowpark._internal import utils
 
 DATETIME_FORMAT = "%Y-%m-%d %H:%M:%S.%f"
 
 # numeric states to the corresponding Snowpark functions
 NUMERIC_STATE_TO_FUNC_DICT = {
     "count": F.count,
@@ -99,16 +106,16 @@
         sklearn_removed_keyword_to_version_dict: Removed keywords mapped to the sklearn versions in which they
             were removed.
 
     Returns:
         Sklearn keyword arguments.
 
     Raises:
-        TypeError: If the input args contains an invalid key.
-        ImportError: If the scikit-learn package version does not meet the requirements
+        SnowflakeMLException: If the input args contains an invalid key.
+        SnowflakeMLException: If the scikit-learn package version does not meet the requirements
             for the keyword arguments.
     """
     # get args to be passed to sklearn
     sklearn_args = {}
     for key, val in args.items():
         # initial sklearn keyword
         if sklearn_initial_keywords and (key in sklearn_initial_keywords):
@@ -138,66 +145,78 @@
         elif sklearn_unused_keywords and (key in sklearn_unused_keywords):
             continue
         # snowml only keyword
         elif snowml_only_keywords and (key in snowml_only_keywords):
             continue
         # unknown keyword
         else:
-            msg = f"Unexpected keyword: {key}."
-            raise TypeError(msg)
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ARGUMENT,
+                original_exception=ValueError(error_messages.UNEXPECTED_KEYWORD.format(key)),
+            )
 
     # validate sklearn version
     sklearn_version = sklearn.__version__
     for key, val in sklearn_args.items():
         # added sklearn keyword
         if (
             sklearn_added_keyword_to_version_dict
             and (key in sklearn_added_keyword_to_version_dict)
             and (version.parse(sklearn_version) < version.parse(sklearn_added_keyword_to_version_dict[key]))
         ):
             required_version = sklearn_added_keyword_to_version_dict[key]
-            msg = (
-                f"scikit-learn version mismatch: parameter '{key}' requires scikit-learn>={required_version}, "
-                f"but got an incompatible version: {sklearn_version}."
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.DEPENDENCY_VERSION_ERROR,
+                original_exception=ImportError(
+                    modeling_error_messages.INCOMPATIBLE_NEW_SKLEARN_PARAM.format(
+                        key, required_version, sklearn_version
+                    )
+                ),
             )
-            raise ImportError(msg)
 
         # added keyword argument value
         if (
             sklearn_added_kwarg_value_to_version_dict
             and (key in sklearn_added_kwarg_value_to_version_dict)
             and (isinstance(val, str) and val in sklearn_added_kwarg_value_to_version_dict[key])
             and (version.parse(sklearn_version) < version.parse(sklearn_added_kwarg_value_to_version_dict[key][val]))
         ):
             required_version = sklearn_added_kwarg_value_to_version_dict[key][val]
-            msg = (
-                f"scikit-learn version mismatch: parameter '{key}={val}' requires "
-                f"scikit-learn>={required_version}, but got an incompatible version: {sklearn_version}."
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.DEPENDENCY_VERSION_ERROR,
+                original_exception=ImportError(
+                    modeling_error_messages.INCOMPATIBLE_NEW_SKLEARN_PARAM.format(
+                        f"{key}={val}", required_version, sklearn_version
+                    )
+                ),
             )
-            raise ImportError(msg)
 
         # deprecated sklearn keyword
         if (
             sklearn_deprecated_keyword_to_version_dict
             and (key in sklearn_deprecated_keyword_to_version_dict)
             and (version.parse(sklearn_version) >= version.parse(sklearn_deprecated_keyword_to_version_dict[key]))
         ):
             deprecated_version = sklearn_deprecated_keyword_to_version_dict[key]
-            msg = f"Parameter '{key}' deprecated since scikit-learn version {deprecated_version}."
+            msg = f"Incompatible scikit-learn version: '{key}' deprecated since scikit-learn={deprecated_version}.."
             warnings.warn(msg, DeprecationWarning)
 
         # removed sklearn keyword
         if (
             sklearn_removed_keyword_to_version_dict
             and (key in sklearn_removed_keyword_to_version_dict)
             and (version.parse(sklearn_version) >= version.parse(sklearn_removed_keyword_to_version_dict[key]))
         ):
             removed_version = sklearn_removed_keyword_to_version_dict[key]
-            msg = f"Parameter '{key}' removed since scikit-learn version {removed_version}."
-            raise ImportError(msg)
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.DEPENDENCY_VERSION_ERROR,
+                original_exception=ImportError(
+                    modeling_error_messages.REMOVED_SKLEARN_PARAM.format(key, removed_version, sklearn_version)
+                ),
+            )
 
     return sklearn_args
 
 
 def str_to_bool(value: str) -> Union[bool, None]:
     if value is None:
         return None
@@ -220,9 +239,9 @@
     return obj.to_sklearn()
 
 
 def table_exists(session: snowpark.Session, table_name: str, statement_params: Dict[str, Any]) -> bool:
     try:
         session.table(table_name).limit(0).collect(statement_params=statement_params)
         return True
-    except exceptions.SnowparkSQLException:
+    except snowpark_exceptions.SnowparkSQLException:
         return False
```

## snowflake/ml/modeling/framework/base.py

```diff
@@ -10,14 +10,19 @@
 
 import numpy as np
 import numpy.typing as npt
 import pandas as pd
 
 from snowflake import snowpark
 from snowflake.ml._internal import telemetry
+from snowflake.ml._internal.exceptions import (
+    error_codes,
+    exceptions,
+    modeling_error_messages,
+)
 from snowflake.ml._internal.utils import parallelize
 from snowflake.ml.modeling.framework import _utils
 from snowflake.snowpark import functions as F
 
 PROJECT = "ModelDevelopment"
 SUBPROJECT = "Preprocessing"
 
@@ -25,20 +30,18 @@
 def _process_cols(cols: Optional[Union[str, Iterable[str]]]) -> List[str]:
     """Convert cols to a list."""
     col_list: List[str] = []
     if cols is None:
         return col_list
     elif type(cols) is list:
         col_list = cols
-    elif type(cols) in [range, set, tuple]:
-        col_list = list(cols)
     elif type(cols) is str:
         col_list = [cols]
     else:
-        raise TypeError(f"Could not convert {cols} to list")
+        col_list = list(cols)
 
     return col_list
 
 
 class Base:
     def __init__(self) -> None:
         """
@@ -128,32 +131,63 @@
         return self
 
     def _check_input_cols(self) -> None:
         """
         Check if `self.input_cols` is set.
 
         Raises:
-            RuntimeError: If `self.input_cols` is not set.
+            SnowflakeMLException: `self.input_cols` is not set.
         """
         if not self.input_cols:
-            raise RuntimeError("input_cols is not set.")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.NOT_FOUND,
+                original_exception=RuntimeError(modeling_error_messages.ATTRIBUTE_NOT_SET.format("input_cols")),
+            )
 
     def _check_output_cols(self) -> None:
         """
         Check if `self.output_cols` is set.
 
         Raises:
-            RuntimeError: If `self.output_cols` is not set or if the size of `self.output_cols`
-                does not match that of `self.input_cols`.
+            SnowflakeMLException: `self.output_cols` is not set.
+            SnowflakeMLException: `self.output_cols` and `self.input_cols` are of different lengths.
         """
         if not self.output_cols:
-            raise RuntimeError("output_cols is not set.")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.NOT_FOUND,
+                original_exception=RuntimeError(modeling_error_messages.ATTRIBUTE_NOT_SET.format("output_cols")),
+            )
         if len(self.output_cols) != len(self.input_cols):
-            raise RuntimeError(
-                f"Size mismatch: input_cols: {len(self.input_cols)}, output_cols: {len(self.output_cols)}."
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=RuntimeError(
+                    modeling_error_messages.SIZE_MISMATCH.format(
+                        "input_cols", len(self.input_cols), "output_cols", len(self.output_cols)
+                    )
+                ),
+            )
+
+    @staticmethod
+    def _check_dataset_type(dataset: Any) -> None:
+        """
+        Check if the type of input dataset is supported.
+
+        Args:
+            dataset: Input dataset passed to an API.
+
+        Raises:
+            SnowflakeMLException: `self.input_cols` is not set.
+        """
+        if not (isinstance(dataset, snowpark.DataFrame) or isinstance(dataset, pd.DataFrame)):
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ARGUMENT,
+                original_exception=TypeError(
+                    f"Unexpected dataset type: {type(dataset)}."
+                    f"Supported dataset types: {type(snowpark.DataFrame)}, {type(pd.DataFrame)}."
+                ),
             )
 
     @classmethod
     def _get_param_names(cls) -> List[str]:
         """Get parameter names for the transformer"""
         # fetch the constructor or the original constructor before
         # deprecation wrapping if any
@@ -165,20 +199,23 @@
         # introspect the constructor arguments to find the model parameters
         # to represent
         init_signature = inspect.signature(init)
         # consider the constructor parameters excluding 'self'
         parameters = [p for p in init_signature.parameters.values() if p.name != "self" and p.kind != p.VAR_KEYWORD]
         for p in parameters:
             if p.kind == p.VAR_POSITIONAL:
-                raise RuntimeError(
-                    "Transformers should always specify"
-                    " their parameters in the signature"
-                    " of their __init__ (no varargs)."
-                    " %s with constructor %s doesn't "
-                    " follow this convention." % (cls, init_signature)
+                raise exceptions.SnowflakeMLException(
+                    error_code=error_codes.INTERNAL_PYTHON_ERROR,
+                    original_exception=RuntimeError(
+                        "Models should always specify"
+                        " their parameters in the signature"
+                        " of their __init__ (no varargs)."
+                        f" {cls} with constructor {init_signature} doesn't "
+                        " follow this convention."
+                    ),
                 )
         # Extract and sort argument names excluding 'self'
         return sorted(p.name for p in parameters)
 
     def get_params(self, deep: bool = True) -> Dict[str, Any]:
         """
         Get parameters for this transformer.
@@ -208,28 +245,33 @@
         The latter have parameters of the form ``<component>__<parameter>``
         so that it's possible to update each component of a nested object.
 
         Args:
             **params: Transformer parameter names mapped to their values.
 
         Raises:
-            ValueError: For invalid parameter keys.
+            SnowflakeMLException: Invalid parameter keys.
         """
         if not params:
             # simple optimization to gain speed (inspect is slow)
             return
         valid_params = self.get_params(deep=True)
 
         nested_params: Dict[str, Any] = defaultdict(dict)  # grouped by prefix
         for key, value in params.items():
             key, delim, sub_key = key.partition("__")
             if key not in valid_params:
                 local_valid_params = self._get_param_names()
-                raise ValueError(
-                    f"Invalid parameter {key} for transformer {self}. Valid parameters are: {local_valid_params}."
+                raise exceptions.SnowflakeMLException(
+                    error_code=error_codes.INVALID_ARGUMENT,
+                    original_exception=ValueError(
+                        modeling_error_messages.INVALID_MODEL_PARAM.format(
+                            key, self.__class__.__name__, local_valid_params
+                        )
+                    ),
                 )
 
             if delim:
                 nested_params[key][sub_key] = value
             else:
                 setattr(self, key, value)
                 valid_params[key] = value
@@ -355,17 +397,20 @@
         raise NotImplementedError()
 
     def _use_input_cols_only(self, dataset: pd.DataFrame) -> pd.DataFrame:
         """Returns the pandas dataframe filtered on `input_cols`, or raises an error if malformed."""
         input_cols = set(self.input_cols)
         dataset_cols = set(dataset.columns.to_list())
         if not input_cols.issubset(dataset_cols):
-            raise KeyError(
-                "The `input_cols` contains columns that do not match any of the columns in "
-                f"the dataframe: {input_cols - dataset_cols}."
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.NOT_FOUND,
+                original_exception=KeyError(
+                    "input_cols contains columns that do not match any of the columns in "
+                    f"the pandas dataframe: {input_cols - dataset_cols}."
+                ),
             )
         return dataset[self.input_cols]
 
     @telemetry.send_api_usage_telemetry(
         project=PROJECT,
         subproject=SUBPROJECT,
     )
@@ -446,33 +491,48 @@
     def transform(self, dataset: pd.DataFrame) -> pd.DataFrame:
         ...
 
     @abstractmethod
     def transform(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> Union[snowpark.DataFrame, pd.DataFrame]:
         raise NotImplementedError()
 
-    def enforce_fit(self) -> None:
+    def _enforce_fit(self) -> None:
         if not self._is_fitted:
-            raise RuntimeError("Transformer not fitted before calling transform().")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.METHOD_NOT_ALLOWED,
+                original_exception=RuntimeError(
+                    f"Model {self.__class__.__name__} not fitted before calling predict/transform."
+                ),
+            )
 
     def set_drop_input_cols(self, drop_input_cols: Optional[bool] = False) -> None:
         self._drop_input_cols = drop_input_cols
 
     def to_sklearn(self) -> Any:
         if self._sklearn_object is None:
             self._sklearn_object = self._create_sklearn_object()
         return self._sklearn_object
 
     # to_xgboost would be only used in XGB estimators
     # to_lightgbm would be only used in LightGBM estimators, but they function the same
     def to_xgboost(self) -> Any:
-        raise AttributeError("Object doesn't support to_xgboost. Please use to_sklearn()")
+        raise exceptions.SnowflakeMLException(
+            error_code=error_codes.METHOD_NOT_ALLOWED,
+            original_exception=AttributeError(
+                modeling_error_messages.UNSUPPORTED_MODEL_CONVERSION.format("to_xgboost", "to_sklearn")
+            ),
+        )
 
     def to_lightgbm(self) -> Any:
-        raise AttributeError("Object doesn't support to_lightgbm. Please use to_sklearn()")
+        raise exceptions.SnowflakeMLException(
+            error_code=error_codes.METHOD_NOT_ALLOWED,
+            original_exception=AttributeError(
+                modeling_error_messages.UNSUPPORTED_MODEL_CONVERSION.format("to_lightgbm", "to_sklearn")
+            ),
+        )
 
     def _reset(self) -> None:
         self._sklearn_object = None
         self._is_fitted = False
 
     def _convert_attribute_dict_to_ndarray(
         self,
@@ -516,25 +576,29 @@
         Args:
             dataset: Input dataset to transform.
 
         Returns:
             Transformed dataset
 
         Raises:
-            TypeError: If the supplied output columns don't match that of the transformed array.
+            SnowflakeMLException: If the supplied output columns don't match that of the transformed array.
         """
-        self.enforce_fit()
+        self._enforce_fit()
         dataset = dataset.copy()
         sklearn_transform = self.to_sklearn()
         transformed_data = sklearn_transform.transform(dataset[self.input_cols])
         shape = transformed_data.shape
         if (len(shape) == 1 and len(self.output_cols) != 1) or (len(shape) > 1 and shape[1] != len(self.output_cols)):
-            raise TypeError(
-                f"output_cols must be same length as transformed array. Got output_cols len: {len(self.output_cols)},"
-                f" transformed array shape: {shape}"
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=RuntimeError(
+                    modeling_error_messages.SIZE_MISMATCH.format(
+                        "output_cols", len(self.output_cols), "transformed array shape", shape
+                    )
+                ),
             )
 
         if len(shape) == 1:
             transformed_data = np.reshape(transformed_data, (-1, 1))
         dataset[self.output_cols] = transformed_data
         return dataset
 
@@ -542,34 +606,37 @@
         """
         Validate that the supplied data does not contain null values in the supplied input columns.
 
         Args:
             dataset: DataFrame to validate.
 
         Raises:
-            ValueError: If the dataset contains nulls in the input_cols.
+            SnowflakeMLException: If the dataset contains nulls in the input_cols.
         """
         self._check_input_cols()
 
         null_count_columns = []
         for input_col in self.input_cols:
-            col = F.count(F.lit("*")) - F.count(dataset[input_col])  # type:ignore[arg-type, operator]
+            col = F.count(F.lit("*")) - F.count(dataset[input_col])
             null_count_columns.append(col)
 
         null_counts = dataset.agg(*null_count_columns).collect(
             statement_params=telemetry.get_statement_params(PROJECT, SUBPROJECT, self.__class__.__name__)
         )
         assert len(null_counts) == 1
 
         invalid_columns = {col: n for (col, n) in zip(self.input_cols, null_counts[0].as_dict().values()) if n > 0}
 
         if any(invalid_columns):
-            raise ValueError(
-                "Dataset may not contain nulls, but "
-                f"the following columns have a non-zero number of nulls: {invalid_columns}."
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_DATA,
+                original_exception=ValueError(
+                    "Dataset may not contain nulls."
+                    f"The following columns have non-zero numbers of nulls: {invalid_columns}."
+                ),
             )
 
     def _drop_input_columns(
         self, dataset: Union[snowpark.DataFrame, pd.DataFrame]
     ) -> Union[snowpark.DataFrame, pd.DataFrame]:
         """
         Drop input column for given dataset.
@@ -577,25 +644,23 @@
         Args:
             dataset: The input Dataset. Either a Snowflake DataFrame or Pandas DataFrame.
 
         Returns:
             Return a dataset with input columns dropped.
 
         Raises:
-            TypeError: If the dataset is neither DataFrame or Pandas DataFrame.
-            RuntimeError: drop_input_cols flag must be true before calling this function.
+            SnowflakeMLException: drop_input_cols flag must be true before calling this function.
         """
         if not self._drop_input_cols:
-            raise RuntimeError("drop_input_cols must set true before calling.")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INTERNAL_PYTHON_ERROR,
+                original_exception=RuntimeError("drop_input_cols must set true."),
+            )
 
         # In case of input_cols and output_cols are the same, compare with get_output_cols()
         input_subset = list(set(self.input_cols) - set(self.get_output_cols()))
 
+        super()._check_dataset_type(dataset)
         if isinstance(dataset, snowpark.DataFrame):
             return dataset.drop(input_subset)
-        elif isinstance(dataset, pd.DataFrame):
-            return dataset.drop(columns=input_subset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            return dataset.drop(columns=input_subset)
```

## snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.gaussian_process".replace("sklearn.", "").split("_")])
 
@@ -374,33 +374,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -430,15 +426,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -446,17 +442,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -505,15 +503,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1131,16 +1129,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1148,18 +1146,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1186,30 +1180,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1243,15 +1239,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1265,15 +1261,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.gaussian_process".replace("sklearn.", "").split("_")])
 
@@ -357,33 +357,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -413,15 +409,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -429,17 +425,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -488,15 +486,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1110,16 +1108,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1127,18 +1125,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1165,30 +1159,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1222,15 +1218,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1244,15 +1240,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/impute/iterative_imputer.py

```diff
@@ -32,16 +32,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.impute".replace("sklearn.", "").split("_")])
 
@@ -400,33 +400,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -456,15 +452,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -472,17 +468,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -531,15 +529,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1153,16 +1151,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1170,18 +1168,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1208,30 +1202,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1265,15 +1261,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1287,15 +1283,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/impute/knn_imputer.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.impute".replace("sklearn.", "").split("_")])
 
@@ -335,33 +335,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -391,15 +387,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -407,17 +403,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -466,15 +464,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1088,16 +1086,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1105,18 +1103,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1143,30 +1137,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1200,15 +1196,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1222,15 +1218,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/impute/missing_indicator.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.impute".replace("sklearn.", "").split("_")])
 
@@ -309,33 +309,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -365,15 +361,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -381,17 +377,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -440,15 +438,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1062,16 +1060,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1079,18 +1077,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1117,30 +1111,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1174,15 +1170,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1196,15 +1192,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/impute/simple_imputer.py

```diff
@@ -8,14 +8,15 @@
 import numpy as np
 import numpy._typing as _npt
 import pandas as pd
 from sklearn import impute
 
 from snowflake import snowpark
 from snowflake.ml._internal import telemetry
+from snowflake.ml._internal.exceptions import error_codes, exceptions
 from snowflake.ml.modeling.framework import _utils, base
 from snowflake.snowpark import functions as F, types as T
 from snowflake.snowpark._internal import utils as snowpark_utils
 
 _SUBPROJECT = "Impute"
 
 STRATEGY_TO_STATE_DICT = {
@@ -120,25 +121,34 @@
         feature_names_in_: ndarray of shape (n_features_in,)
             Names of features seen during `fit`.
 
         TODO(thoyt): Implement logic for `add_indicator` parameter and `indicator_` attribute. Requires
             `snowflake.ml.impute.MissingIndicator` to be implemented.
 
         Raises:
-            ValueError: If strategy is invalid, or if fill value is specified for strategy that isn't "constant".
+            SnowflakeMLException: If strategy is invalid, or if fill value is specified for strategy that isn't
+                "constant".
         """
         super().__init__(drop_input_cols=drop_input_cols)
         if strategy in STRATEGY_TO_STATE_DICT:
             self.strategy = strategy
         else:
-            raise ValueError(f"Invalid strategy {strategy}. Strategy must be one of {STRATEGY_TO_STATE_DICT.keys()}")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ARGUMENT,
+                original_exception=ValueError(
+                    f"Invalid strategy {strategy}. Strategy must be one of {STRATEGY_TO_STATE_DICT.keys()}"
+                ),
+            )
 
         # Check that the strategy is "constant" if `fill_value` is specified.
         if fill_value is not None and strategy != "constant":
-            raise ValueError("fill_value may only be specified if the strategy is 'constant'.")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ARGUMENT,
+                original_exception=ValueError("fill_value may only be specified if the strategy is 'constant'."),
+            )
 
         self.fill_value = fill_value
         self.missing_values = missing_values
         # TODO(hayu): [SNOW-752265] Support SimpleImputer keep_empty_features.
         #  Add back when `keep_empty_features` is supported.
         # self.keep_empty_features = keep_empty_features
 
@@ -167,36 +177,45 @@
         Args:
             dataset: The input dataframe.
 
         Returns:
             The datatype of the input columns.
 
         Raises:
-            TypeError: If the input columns are not all the same datatype category or if the datatype is not supported.
+            SnowflakeMLException: If the input columns are not all the same datatype category or if the datatype is not
+                supported.
         """
 
         def check_type_consistency(col_types: Dict[str, T.DataType]) -> None:
             is_numeric_type = None
             for col_name, col_type in col_types.items():
                 if is_numeric_type is None:
                     is_numeric_type = True if col_type in _NUMERIC_TYPES else False
                 if (col_type in _NUMERIC_TYPES) ^ is_numeric_type:
-                    raise TypeError(
-                        f"Inconsistent input column types. Column {col_name} type {col_type} does not match previous"
-                        " type category."
+                    raise exceptions.SnowflakeMLException(
+                        error_code=error_codes.INVALID_DATA_TYPE,
+                        original_exception=TypeError(
+                            f"Inconsistent input column types. Column {col_name} type {col_type} does not match"
+                            " previous type category."
+                        ),
                     )
 
         input_col_datatypes = {}
         for field in dataset.schema.fields:
             if field.name in self.input_cols:
                 if not any(
                     isinstance(field.datatype, potential_type)
                     for potential_type in SNOWFLAKE_DATATYPE_TO_NUMPY_DTYPE_MAP.keys()
                 ):
-                    raise TypeError(f"Input column type {field.datatype} is not supported by the simple imputer.")
+                    raise exceptions.SnowflakeMLException(
+                        error_code=error_codes.INVALID_DATA_TYPE,
+                        original_exception=TypeError(
+                            f"Input column type {field.datatype} is not supported by the SimpleImputer."
+                        ),
+                    )
                 input_col_datatypes[field.name] = field.datatype
         if self.strategy != "most_frequent":
             check_type_consistency(input_col_datatypes)
 
         return input_col_datatypes
 
     @telemetry.send_api_usage_telemetry(project=base.PROJECT, subproject=_SUBPROJECT)
@@ -245,15 +264,15 @@
                     self.statistics_[input_col] = self.fill_value
         else:
             state = STRATEGY_TO_STATE_DICT[self.strategy]
             assert state is not None, "state cannot be None"
             dataset_copy = copy.copy(dataset)
             if not pd.isna(self.missing_values):
                 # Replace `self.missing_values` with null to avoid including it when computing states.
-                dataset_copy = dataset_copy.na.replace(self.missing_values, None)  # type: ignore[arg-type]
+                dataset_copy = dataset_copy.na.replace(self.missing_values, None)
             _computed_states = self._compute(dataset_copy, self.input_cols, states=[state])
             for input_col in self.input_cols:
                 statistic = _computed_states[input_col][state]
                 self.statistics_[input_col] = np.nan if statistic is None else statistic
                 if self.strategy == "mean":
                     self.statistics_[input_col] = float(self.statistics_[input_col])
                 elif self.strategy == "most_frequent":
@@ -283,33 +302,24 @@
         Transform the input dataset by imputing the computed statistics in the input columns.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
-
-        Raises:
-            RuntimeError: If transformer is not fitted first.
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
-        if not self._is_fitted:
-            raise RuntimeError("Transformer not fitted before calling transform().")
+        self._enforce_fit()
         super()._check_input_cols()
         super()._check_output_cols()
+        super()._check_dataset_type(dataset)
 
         if isinstance(dataset, snowpark.DataFrame):
             output_df = self._transform_snowpark(dataset)
-        elif isinstance(dataset, pd.DataFrame):
-            output_df = self._transform_sklearn(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            output_df = self._transform_sklearn(dataset)
 
         return self._drop_input_columns(output_df) if self._drop_input_cols is True else output_df
 
     def _transform_snowpark(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         """
         Perform imputation in snowpark dataframe.
 
@@ -354,15 +364,15 @@
             if self.missing_values is not None and pd.isna(self.missing_values):
                 # Use `fillna` for replacing nans. Check if the column has a string data type, or coerce a float.
                 if not isinstance(input_col_datatypes[input_col], T.StringType):
                     statistic = float(statistic)
                 transformed_dataset = transformed_dataset.na.fill({output_col: statistic})
             else:
                 transformed_dataset = transformed_dataset.na.replace(
-                    self.missing_values,  # type: ignore[arg-type]
+                    self.missing_values,
                     fill_value,
                     subset=[output_col],
                 )
 
         # Rename the input_cols as needed.
         for input_col, output_col in zip(self.input_cols, self.output_cols):
             if input_col != output_col:
```

## snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.kernel_approximation".replace("sklearn.", "").split("_")])
 
@@ -284,33 +284,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -340,15 +336,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -356,17 +352,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -415,15 +413,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1037,16 +1035,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1054,18 +1052,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1092,30 +1086,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1149,15 +1145,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1171,15 +1167,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/kernel_approximation/nystroem.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.kernel_approximation".replace("sklearn.", "").split("_")])
 
@@ -332,33 +332,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -388,15 +384,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -404,17 +400,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -463,15 +461,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1085,16 +1083,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1102,18 +1100,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1140,30 +1134,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1197,15 +1193,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1219,15 +1215,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.kernel_approximation".replace("sklearn.", "").split("_")])
 
@@ -308,33 +308,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -364,15 +360,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -380,17 +376,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -439,15 +437,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1061,16 +1059,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1078,18 +1076,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1116,30 +1110,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1173,15 +1169,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1195,15 +1191,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/kernel_approximation/rbf_sampler.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.kernel_approximation".replace("sklearn.", "").split("_")])
 
@@ -295,33 +295,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -351,15 +347,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -367,17 +363,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -426,15 +424,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1048,16 +1046,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1065,18 +1063,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1103,30 +1097,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1160,15 +1156,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1182,15 +1178,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.kernel_approximation".replace("sklearn.", "").split("_")])
 
@@ -293,33 +293,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -349,15 +345,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -365,17 +361,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -424,15 +422,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1046,16 +1044,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1063,18 +1061,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1101,30 +1095,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1158,15 +1154,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1180,15 +1176,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/kernel_ridge/kernel_ridge.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.kernel_ridge".replace("sklearn.", "").split("_")])
 
@@ -327,33 +327,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -383,15 +379,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -399,17 +395,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -458,15 +456,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1080,16 +1078,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1097,18 +1095,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1135,30 +1129,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1192,15 +1188,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1214,15 +1210,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/lightgbm/lgbm_classifier.py

```diff
@@ -30,16 +30,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "lightgbm".replace("sklearn.", "").split("_")])
 
@@ -316,33 +316,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -372,15 +368,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -388,17 +384,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import lightgbm
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -447,15 +445,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1073,16 +1071,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1090,18 +1088,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1128,30 +1122,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import lightgbm
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1185,15 +1181,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1207,15 +1203,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/lightgbm/lgbm_regressor.py

```diff
@@ -30,16 +30,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "lightgbm".replace("sklearn.", "").split("_")])
 
@@ -316,33 +316,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -372,15 +368,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -388,17 +384,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import lightgbm
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -447,15 +445,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1069,16 +1067,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1086,18 +1084,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1124,30 +1118,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import lightgbm
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1181,15 +1177,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1203,15 +1199,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/ard_regression.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -336,33 +336,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -392,15 +388,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -408,17 +404,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -467,15 +465,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1089,16 +1087,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1106,18 +1104,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1144,30 +1138,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1201,15 +1197,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1223,15 +1219,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/bayesian_ridge.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -345,33 +345,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -401,15 +397,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -417,17 +413,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -476,15 +474,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1098,16 +1096,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1115,18 +1113,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1153,30 +1147,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1210,15 +1206,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1232,15 +1228,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/elastic_net.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -351,33 +351,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -407,15 +403,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -423,17 +419,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -482,15 +480,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1104,16 +1102,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1121,18 +1119,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1159,30 +1153,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1216,15 +1212,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1238,15 +1234,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/elastic_net_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -387,33 +387,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -443,15 +439,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -459,17 +455,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -518,15 +516,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1140,16 +1138,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1157,18 +1155,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1195,30 +1189,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1252,15 +1248,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1274,15 +1270,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/gamma_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -332,33 +332,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -388,15 +384,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -404,17 +400,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -463,15 +461,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1085,16 +1083,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1102,18 +1100,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1140,30 +1134,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1197,15 +1193,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1219,15 +1215,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/huber_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -315,33 +315,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -371,15 +367,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -387,17 +383,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -446,15 +444,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1068,16 +1066,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1085,18 +1083,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1123,30 +1117,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1180,15 +1176,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1202,15 +1198,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/lars.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -344,33 +344,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -400,15 +396,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -416,17 +412,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -475,15 +473,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1097,16 +1095,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1114,18 +1112,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1152,30 +1146,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1209,15 +1205,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1231,15 +1227,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/lars_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -352,33 +352,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -408,15 +404,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -424,17 +420,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -483,15 +481,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1105,16 +1103,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1122,18 +1120,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1160,30 +1154,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1217,15 +1213,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1239,15 +1235,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/lasso.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -345,33 +345,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -401,15 +397,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -417,17 +413,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -476,15 +474,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1098,16 +1096,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1115,18 +1113,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1153,30 +1147,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1210,15 +1206,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1232,15 +1228,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/lasso_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -373,33 +373,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -429,15 +425,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -445,17 +441,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -504,15 +502,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1126,16 +1124,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1143,18 +1141,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1181,30 +1175,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1238,15 +1234,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1260,15 +1256,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/lasso_lars.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -365,33 +365,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -421,15 +417,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -437,17 +433,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -496,15 +494,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1118,16 +1116,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1135,18 +1133,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1173,30 +1167,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1230,15 +1226,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1252,15 +1248,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/lasso_lars_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -366,33 +366,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -422,15 +418,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -438,17 +434,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -497,15 +495,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1119,16 +1117,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1136,18 +1134,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1174,30 +1168,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1231,15 +1227,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1253,15 +1249,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/lasso_lars_ic.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -349,33 +349,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -405,15 +401,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -421,17 +417,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -480,15 +478,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1102,16 +1100,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1119,18 +1117,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1157,30 +1151,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1214,15 +1210,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1236,15 +1232,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/linear_regression.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -302,33 +302,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -358,15 +354,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -374,17 +370,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -433,15 +431,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1055,16 +1053,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1072,18 +1070,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1110,30 +1104,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1167,15 +1163,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1189,15 +1185,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/logistic_regression.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -416,33 +416,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -472,15 +468,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -488,17 +484,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -547,15 +545,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1175,16 +1173,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1192,18 +1190,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1230,30 +1224,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1287,15 +1283,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1309,15 +1305,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/logistic_regression_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -437,33 +437,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -493,15 +489,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -509,17 +505,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -568,15 +566,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1196,16 +1194,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1213,18 +1211,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1251,30 +1245,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1308,15 +1304,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1330,15 +1326,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/multi_task_elastic_net.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -335,33 +335,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -391,15 +387,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -407,17 +403,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -466,15 +464,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1088,16 +1086,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1105,18 +1103,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1143,30 +1137,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1200,15 +1196,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1222,15 +1218,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -376,33 +376,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -432,15 +428,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -448,17 +444,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -507,15 +505,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1129,16 +1127,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1146,18 +1144,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1184,30 +1178,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1241,15 +1237,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1263,15 +1259,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/multi_task_lasso.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -327,33 +327,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -383,15 +379,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -399,17 +395,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -458,15 +456,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1080,16 +1078,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1097,18 +1095,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1135,30 +1129,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1192,15 +1188,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1214,15 +1210,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -362,33 +362,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -418,15 +414,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -434,17 +430,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -493,15 +491,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1115,16 +1113,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1132,18 +1130,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1170,30 +1164,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1227,15 +1223,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1249,15 +1245,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -310,33 +310,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -366,15 +362,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -382,17 +378,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -441,15 +439,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1063,16 +1061,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1080,18 +1078,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1118,30 +1112,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1175,15 +1171,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1197,15 +1193,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -384,33 +384,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -440,15 +436,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -456,17 +452,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -515,15 +513,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1139,16 +1137,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1156,18 +1154,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1194,30 +1188,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1251,15 +1247,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1273,15 +1269,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -371,33 +371,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -427,15 +423,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -443,17 +439,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -502,15 +500,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1124,16 +1122,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1141,18 +1139,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1179,30 +1173,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1236,15 +1232,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1258,15 +1254,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/perceptron.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -384,33 +384,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -440,15 +436,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -456,17 +452,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -515,15 +513,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1139,16 +1137,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1156,18 +1154,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1194,30 +1188,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1251,15 +1247,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1273,15 +1269,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/poisson_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -332,33 +332,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -388,15 +384,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -404,17 +400,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -463,15 +461,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1085,16 +1083,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1102,18 +1100,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1140,30 +1134,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1197,15 +1193,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1219,15 +1215,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/ransac_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -395,33 +395,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -451,15 +447,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -467,17 +463,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -526,15 +524,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1148,16 +1146,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1165,18 +1163,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1203,30 +1197,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1260,15 +1256,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1282,15 +1278,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/ridge.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -365,33 +365,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -421,15 +417,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -437,17 +433,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -496,15 +494,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1118,16 +1116,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1135,18 +1133,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1173,30 +1167,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1230,15 +1226,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1252,15 +1248,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/ridge_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -365,33 +365,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -421,15 +417,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -437,17 +433,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -496,15 +494,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1120,16 +1118,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1137,18 +1135,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1175,30 +1169,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1232,15 +1228,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1254,15 +1250,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/ridge_classifier_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -331,33 +331,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -387,15 +383,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -403,17 +399,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -462,15 +460,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1086,16 +1084,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1103,18 +1101,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1141,30 +1135,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1198,15 +1194,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1220,15 +1216,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/ridge_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -352,33 +352,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -408,15 +404,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -424,17 +420,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -483,15 +481,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1105,16 +1103,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1122,18 +1120,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1160,30 +1154,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1217,15 +1213,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1239,15 +1235,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/sgd_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -471,33 +471,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -527,15 +523,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -543,17 +539,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -602,15 +600,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1230,16 +1228,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1247,18 +1245,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1285,30 +1279,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1342,15 +1338,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1364,15 +1360,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/sgd_one_class_svm.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -371,33 +371,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -427,15 +423,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -443,17 +439,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -502,15 +500,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1126,16 +1124,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1143,18 +1141,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1181,30 +1175,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1238,15 +1234,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1260,15 +1256,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/sgd_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -437,33 +437,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -493,15 +489,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -509,17 +505,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -568,15 +566,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1190,16 +1188,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1207,18 +1205,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1245,30 +1239,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1302,15 +1298,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1324,15 +1320,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/theil_sen_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -339,33 +339,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -395,15 +391,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -411,17 +407,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -470,15 +468,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1092,16 +1090,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1109,18 +1107,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1147,30 +1141,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1204,15 +1200,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1226,15 +1222,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/linear_model/tweedie_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
@@ -365,33 +365,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -421,15 +417,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -437,17 +433,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -496,15 +494,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1118,16 +1116,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1135,18 +1133,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1173,30 +1167,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1230,15 +1226,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1252,15 +1248,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/manifold/isomap.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.manifold".replace("sklearn.", "").split("_")])
 
@@ -363,33 +363,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -419,15 +415,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -435,17 +431,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -494,15 +492,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1116,16 +1114,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1133,18 +1131,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1171,30 +1165,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1228,15 +1224,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1250,15 +1246,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/manifold/mds.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.manifold".replace("sklearn.", "").split("_")])
 
@@ -346,33 +346,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -402,15 +398,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -418,17 +414,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -477,15 +475,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1097,16 +1095,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1114,18 +1112,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1152,30 +1146,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1209,15 +1205,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1231,15 +1227,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/manifold/spectral_embedding.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.manifold".replace("sklearn.", "").split("_")])
 
@@ -348,33 +348,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -404,15 +400,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -420,17 +416,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -479,15 +477,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1099,16 +1097,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1116,18 +1114,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1154,30 +1148,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1211,15 +1207,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1233,15 +1229,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/manifold/tsne.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.manifold".replace("sklearn.", "").split("_")])
 
@@ -413,33 +413,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -469,15 +465,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -485,17 +481,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -544,15 +542,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1164,16 +1162,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1181,18 +1179,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1219,30 +1213,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1276,15 +1272,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1298,15 +1294,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/metrics/classification.py

```diff
@@ -55,19 +55,19 @@
 
     if isinstance(y_true_col_names, str) or (len(y_true_col_names) == 1):
         y_true, y_pred = (
             (y_true_col_names, y_pred_col_names)
             if isinstance(y_true_col_names, str)
             else (y_true_col_names[0], y_pred_col_names[0])
         )
-        score_column = F.iff(df[y_true] == df[y_pred], 1, 0)  # type: ignore[arg-type]
+        score_column = F.iff(df[y_true] == df[y_pred], 1, 0)
     # multilabel
     else:
         expr = " and ".join([f"({y_true_col_names[i]} = {y_pred_col_names[i]})" for i in range(len(y_true_col_names))])
-        score_column = F.iff(expr, 1, 0)  # type: ignore[arg-type]
+        score_column = F.iff(expr, 1, 0)
     return metrics_utils.weighted_sum(
         df=df,
         sample_score_column=score_column,
         sample_weight_column=df[sample_weight_col_name] if sample_weight_col_name else None,
         normalize=normalize,
         statement_params=telemetry.get_statement_params(_PROJECT, _SUBPROJECT),
     )
@@ -139,15 +139,15 @@
             return np.zeros((n_labels, n_labels), dtype=int)
         elif df[[y_true_col_name]].join(label_df, df[y_true_col_name] == label_df[metrics_utils.LABEL]).count() == 0:
             raise ValueError("At least one label specified must be in the y true column")
 
     rand = snowpark_utils.generate_random_alphanumeric()
     if sample_weight_col_name is None:
         sample_weight_col_name = f'"_SAMPLE_WEIGHT_{rand}"'
-        df = df.with_column(sample_weight_col_name, F.lit(1))  # type: ignore[arg-type]
+        df = df.with_column(sample_weight_col_name, F.lit(1))
 
     if normalize not in ["true", "pred", "all", None]:
         raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")
 
     # Get indices of true and pred data.
     label_col = f'"_LABEL_{rand}"'
     y_true_index_col = f'"_Y_TRUE_INDEX_{rand}"'
@@ -171,21 +171,19 @@
     confusion_matrix_computer = _register_confusion_matrix_computer(session=session, statement_params=statement_params)
     confusion_matrix_computer_udtf = F.table_function(confusion_matrix_computer)
     accumulator = metrics_utils.register_accumulator_udtf(session=session, statement_params=statement_params)
     accumulator_udtf = F.table_function(accumulator)
 
     # Compute the confusion matrix.
     temp_df1 = ind_df.select(
-        F.array_construct(sample_weight_col_name, y_true_index_col, y_pred_index_col).alias(  # type: ignore[arg-type]
-            "ARR_COL"
-        )
+        F.array_construct(sample_weight_col_name, y_true_index_col, y_pred_index_col).alias("ARR_COL")
+    )
+    temp_df2 = temp_df1.select(confusion_matrix_computer_udtf(F.col("ARR_COL"), F.lit(n_labels))).with_column_renamed(
+        "RESULT", "RES"
     )
-    temp_df2 = temp_df1.select(
-        confusion_matrix_computer_udtf(F.col("ARR_COL"), F.lit(n_labels))  # type: ignore[arg-type]
-    ).with_column_renamed("RESULT", "RES")
     res_df = temp_df2.select(accumulator_udtf(F.col("RES")).over(partition_by="PART"), F.col("PART"))
     results = res_df.collect(statement_params=statement_params)
 
     cm = np.zeros((n_labels, n_labels))
     for i in range(len(results)):
         row = int(results[i][1].strip("row_"))
         cm[row, :] = cloudpickle.loads(results[i][0])
@@ -510,29 +508,31 @@
     """
     session = df._session
     assert session is not None
     sproc_name = f"log_loss_{snowpark_utils.generate_random_alphanumeric()}"
     sklearn_release = version.parse(sklearn.__version__).release
     statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
     cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
-    query = df[cols].queries["queries"][-1]
+    queries = df[cols].queries["queries"]
 
     @F.sproc(  # type: ignore[misc]
         session=session,
         name=sproc_name,
         replace=True,
         packages=[
             "cloudpickle",
             f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
             "snowflake-snowpark-python",
         ],
         statement_params=statement_params,
     )
     def log_loss_sproc(session: snowpark.Session) -> float:
-        df = session.sql(query).to_pandas(statement_params=statement_params)
+        for query in queries[:-1]:
+            _ = session.sql(query).collect(statement_params=statement_params)
+        df = session.sql(queries[-1]).to_pandas(statement_params=statement_params)
         y_true = df[y_true_col_names]
         y_pred = df[y_pred_col_names]
         sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
         return metrics.log_loss(  # type: ignore[no-any-return]
             y_true,
             y_pred,
             eps=eps,
@@ -648,29 +648,31 @@
 
     session = df._session
     assert session is not None
     sproc_name = f"precision_recall_fscore_support_{snowpark_utils.generate_random_alphanumeric()}"
     sklearn_release = version.parse(sklearn.__version__).release
     statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
     cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
-    query = df[cols].queries["queries"][-1]
+    queries = df[cols].queries["queries"]
 
     @F.sproc(  # type: ignore[misc]
         session=session,
         name=sproc_name,
         replace=True,
         packages=[
             "cloudpickle",
             f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
             "snowflake-snowpark-python",
         ],
         statement_params=statement_params,
     )
     def precision_recall_fscore_support_sproc(session: snowpark.Session) -> bytes:
-        df = session.sql(query).to_pandas(statement_params=statement_params)
+        for query in queries[:-1]:
+            _ = session.sql(query).collect(statement_params=statement_params)
+        df = session.sql(queries[-1]).to_pandas(statement_params=statement_params)
         y_true = df[y_true_col_names]
         y_pred = df[y_pred_col_names]
         sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
 
         with warnings.catch_warnings(record=True) as w:
             p, r, f, s = metrics.precision_recall_fscore_support(
                 y_true,
```

## snowflake/ml/modeling/metrics/correlation.py

```diff
@@ -58,17 +58,17 @@
         session=session, statement_params=statement_params
     )
     sharded_dot_and_sum_computer_udtf = F.table_function(sharded_dot_and_sum_computer)
     accumulator = metrics_utils.register_accumulator_udtf(session=session, statement_params=statement_params)
     accumulator_udtf = F.table_function(accumulator)
 
     # Compute the confusion matrix.
-    temp_df1 = input_df.select(F.array_construct(*input_df.columns).alias("ARR_COL"))  # type: ignore[arg-type]
+    temp_df1 = input_df.select(F.array_construct(*input_df.columns).alias("ARR_COL"))
     temp_df2 = temp_df1.select(
-        sharded_dot_and_sum_computer_udtf(F.col("ARR_COL"), F.lit(count), F.lit(0))  # type: ignore[arg-type]
+        sharded_dot_and_sum_computer_udtf(F.col("ARR_COL"), F.lit(count), F.lit(0))
     ).with_column_renamed("RESULT", "RES")
     res_df = temp_df2.select(accumulator_udtf(F.col("RES")).over(partition_by="PART"), F.col("PART"))
     results = res_df.collect(statement_params=statement_params)
 
     # The below computation can be moved to a third udtf. But there is not much benefit in terms of client side
     # resource consumption as the below computation is very fast (< 1 sec for 1000 cols). Memory is in the same order
     # as the resultant correlation matrix.
```

## snowflake/ml/modeling/metrics/covariance.py

```diff
@@ -60,17 +60,17 @@
         session=session, statement_params=statement_params
     )
     sharded_dot_and_sum_computer_udtf = F.table_function(sharded_dot_and_sum_computer)
     accumulator = metrics_utils.register_accumulator_udtf(session=session, statement_params=statement_params)
     accumulator_udtf = F.table_function(accumulator)
 
     # Compute the confusion matrix.
-    temp_df1 = input_df.select(F.array_construct(*input_df.columns).alias("ARR_COL"))  # type: ignore[arg-type]
+    temp_df1 = input_df.select(F.array_construct(*input_df.columns).alias("ARR_COL"))
     temp_df2 = temp_df1.select(
-        sharded_dot_and_sum_computer_udtf(F.col("ARR_COL"), F.lit(count), F.lit(ddof))  # type: ignore[arg-type]
+        sharded_dot_and_sum_computer_udtf(F.col("ARR_COL"), F.lit(count), F.lit(ddof))
     ).with_column_renamed("RESULT", "RES")
     res_df = temp_df2.select(accumulator_udtf(F.col("RES")).over(partition_by="PART"), F.col("PART"))
     results = res_df.collect(statement_params=statement_params)
 
     # The below computation can be moved to a third udtf. But there is not much benefit in terms of client side
     # resource consumption as the below computation is very fast (< 1 sec for 1000 cols). Memory is in the same order
     # as the resultant covariance matrix.
```

## snowflake/ml/modeling/metrics/metrics_utils.py

```diff
@@ -263,23 +263,21 @@
 
     Returns:
         If ``normalize == True``, return the fraction of weighted sum (float),
         else returns the weighted sum (int).
     """
     if normalize:
         if sample_weight_column is not None:
-            res = F.sum(sample_score_column * sample_weight_column) / F.sum(  # type: ignore[arg-type, operator]
-                sample_weight_column  # type: ignore[arg-type]
-            )
+            res = F.sum(sample_score_column * sample_weight_column) / F.sum(sample_weight_column)
         else:
-            res = F.avg(sample_score_column)  # type: ignore[arg-type]
+            res = F.avg(sample_score_column)
     elif sample_weight_column is not None:
-        res = F.sum(sample_score_column * sample_weight_column)  # type: ignore[arg-type, operator]
+        res = F.sum(sample_score_column * sample_weight_column)
     else:
-        res = F.sum(sample_score_column)  # type: ignore[arg-type]
+        res = F.sum(sample_score_column)
 
     return float(df.select(res).collect(statement_params=statement_params)[0][0])
 
 
 def unique_labels(
     *,
     df: snowpark.DataFrame,
@@ -302,11 +300,9 @@
             # uniqueness guaranteed through `DataFrame.union`
             union_df = union_df.union(temp_df)
         else:
             union_df = temp_df
 
     # append an index column dense ranking labels
     assert union_df is not None
-    res: snowpark.DataFrame = union_df.with_column(
-        INDEX, F.dense_rank().over(snowpark.Window.order_by(LABEL)) - 1  # type: ignore[arg-type, operator]
-    )
+    res: snowpark.DataFrame = union_df.with_column(INDEX, F.dense_rank().over(snowpark.Window.order_by(LABEL)) - 1)
     return res
```

## snowflake/ml/modeling/metrics/ranking.py

```diff
@@ -74,29 +74,31 @@
     """
     session = df._session
     assert session is not None
     sproc_name = f"precision_recall_curve_{snowpark_utils.generate_random_alphanumeric()}"
     sklearn_release = version.parse(sklearn.__version__).release
     statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
     cols = metrics_utils.flatten_cols([y_true_col_name, probas_pred_col_name, sample_weight_col_name])
-    query = df[cols].queries["queries"][-1]
+    queries = df[cols].queries["queries"]
 
     @F.sproc(  # type: ignore[misc]
         session=session,
         name=sproc_name,
         replace=True,
         packages=[
             "cloudpickle",
             f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
             "snowflake-snowpark-python",
         ],
         statement_params=statement_params,
     )
     def precision_recall_curve_sproc(session: snowpark.Session) -> bytes:
-        df = session.sql(query).to_pandas(statement_params=statement_params)
+        for query in queries[:-1]:
+            _ = session.sql(query).collect(statement_params=statement_params)
+        df = session.sql(queries[-1]).to_pandas(statement_params=statement_params)
         y_true = df[y_true_col_name]
         probas_pred = df[probas_pred_col_name]
         sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
         precision, recall, thresholds = metrics.precision_recall_curve(
             y_true,
             probas_pred,
             pos_label=pos_label,
@@ -205,29 +207,31 @@
     """
     session = df._session
     assert session is not None
     sproc_name = f"roc_auc_score_{snowpark_utils.generate_random_alphanumeric()}"
     sklearn_release = version.parse(sklearn.__version__).release
     statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
     cols = metrics_utils.flatten_cols([y_true_col_names, y_score_col_names, sample_weight_col_name])
-    query = df[cols].queries["queries"][-1]
+    queries = df[cols].queries["queries"]
 
     @F.sproc(  # type: ignore[misc]
         session=session,
         name=sproc_name,
         replace=True,
         packages=[
             "cloudpickle",
             f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
             "snowflake-snowpark-python",
         ],
         statement_params=statement_params,
     )
     def roc_auc_score_sproc(session: snowpark.Session) -> bytes:
-        df = session.sql(query).to_pandas(statement_params=statement_params)
+        for query in queries[:-1]:
+            _ = session.sql(query).collect(statement_params=statement_params)
+        df = session.sql(queries[-1]).to_pandas(statement_params=statement_params)
         y_true = df[y_true_col_names]
         y_score = df[y_score_col_names]
         sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
         auc = metrics.roc_auc_score(
             y_true,
             y_score,
             average=average,
@@ -291,29 +295,31 @@
     """
     session = df._session
     assert session is not None
     sproc_name = f"roc_curve_{snowpark_utils.generate_random_alphanumeric()}"
     sklearn_release = version.parse(sklearn.__version__).release
     statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
     cols = metrics_utils.flatten_cols([y_true_col_name, y_score_col_name, sample_weight_col_name])
-    query = df[cols].queries["queries"][-1]
+    queries = df[cols].queries["queries"]
 
     @F.sproc(  # type: ignore[misc]
         session=session,
         name=sproc_name,
         replace=True,
         packages=[
             "cloudpickle",
             f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
             "snowflake-snowpark-python",
         ],
         statement_params=statement_params,
     )
     def roc_curve_sproc(session: snowpark.Session) -> bytes:
-        df = session.sql(query).to_pandas(statement_params=statement_params)
+        for query in queries[:-1]:
+            _ = session.sql(query).collect(statement_params=statement_params)
+        df = session.sql(queries[-1]).to_pandas(statement_params=statement_params)
         y_true = df[y_true_col_name]
         y_score = df[y_score_col_name]
         sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
         fpr, tpr, thresholds = metrics.roc_curve(
             y_true,
             y_score,
             pos_label=pos_label,
```

## snowflake/ml/modeling/metrics/regression.py

```diff
@@ -63,29 +63,31 @@
 
     session = df._session
     assert session is not None
     sproc_name = f"d2_absolute_error_score_{snowpark_utils.generate_random_alphanumeric()}"
     sklearn_release = version.parse(sklearn.__version__).release
     statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
     cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
-    query = df[cols].queries["queries"][-1]
+    queries = df[cols].queries["queries"]
 
     @F.sproc(  # type: ignore[misc]
         session=session,
         name=sproc_name,
         replace=True,
         packages=[
             "cloudpickle",
             f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
             "snowflake-snowpark-python",
         ],
         statement_params=statement_params,
     )
     def d2_absolute_error_score_sproc(session: snowpark.Session) -> bytes:
-        df = session.sql(query).to_pandas(statement_params=statement_params)
+        for query in queries[:-1]:
+            _ = session.sql(query).collect(statement_params=statement_params)
+        df = session.sql(queries[-1]).to_pandas(statement_params=statement_params)
         y_true = df[y_true_col_names]
         y_pred = df[y_pred_col_names]
         sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
 
         score = metrics.d2_absolute_error_score(
             y_true,
             y_pred,
@@ -145,29 +147,31 @@
 
     session = df._session
     assert session is not None
     sproc_name = f"d2_pinball_score_{snowpark_utils.generate_random_alphanumeric()}"
     sklearn_release = version.parse(sklearn.__version__).release
     statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
     cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
-    query = df[cols].queries["queries"][-1]
+    queries = df[cols].queries["queries"]
 
     @F.sproc(  # type: ignore[misc]
         session=session,
         name=sproc_name,
         replace=True,
         packages=[
             "cloudpickle",
             f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
             "snowflake-snowpark-python",
         ],
         statement_params=statement_params,
     )
     def d2_pinball_score_sproc(session: snowpark.Session) -> bytes:
-        df = session.sql(query).to_pandas(statement_params=statement_params)
+        for query in queries[:-1]:
+            _ = session.sql(query).collect(statement_params=statement_params)
+        df = session.sql(queries[-1]).to_pandas(statement_params=statement_params)
         y_true = df[y_true_col_names]
         y_pred = df[y_pred_col_names]
         sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
 
         score = metrics.d2_pinball_score(
             y_true,
             y_pred,
@@ -244,29 +248,31 @@
 
     session = df._session
     assert session is not None
     sproc_name = f"explained_variance_score_{snowpark_utils.generate_random_alphanumeric()}"
     sklearn_release = version.parse(sklearn.__version__).release
     statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
     cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
-    query = df[cols].queries["queries"][-1]
+    queries = df[cols].queries["queries"]
 
     @F.sproc(  # type: ignore[misc]
         session=session,
         name=sproc_name,
         replace=True,
         packages=[
             "cloudpickle",
             f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
             "snowflake-snowpark-python",
         ],
         statement_params=statement_params,
     )
     def explained_variance_score_sproc(session: snowpark.Session) -> bytes:
-        df = session.sql(query).to_pandas(statement_params=statement_params)
+        for query in queries[:-1]:
+            _ = session.sql(query).collect(statement_params=statement_params)
+        df = session.sql(queries[-1]).to_pandas(statement_params=statement_params)
         y_true = df[y_true_col_names]
         y_pred = df[y_pred_col_names]
         sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
 
         score = metrics.explained_variance_score(
             y_true,
             y_pred,
@@ -322,29 +328,31 @@
 
     session = df._session
     assert session is not None
     sproc_name = f"mean_absolute_error_{snowpark_utils.generate_random_alphanumeric()}"
     sklearn_release = version.parse(sklearn.__version__).release
     statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
     cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
-    query = df[cols].queries["queries"][-1]
+    queries = df[cols].queries["queries"]
 
     @F.sproc(  # type: ignore[misc]
         session=session,
         name=sproc_name,
         replace=True,
         packages=[
             "cloudpickle",
             f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
             "snowflake-snowpark-python",
         ],
         statement_params=statement_params,
     )
     def mean_absolute_error_sproc(session: snowpark.Session) -> bytes:
-        df = session.sql(query).to_pandas(statement_params=statement_params)
+        for query in queries[:-1]:
+            _ = session.sql(query).collect(statement_params=statement_params)
+        df = session.sql(queries[-1]).to_pandas(statement_params=statement_params)
         y_true = df[y_true_col_names]
         y_pred = df[y_pred_col_names]
         sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
 
         loss = metrics.mean_absolute_error(
             y_true,
             y_pred,
@@ -408,29 +416,31 @@
 
     session = df._session
     assert session is not None
     sproc_name = f"mean_absolute_percentage_error_{snowpark_utils.generate_random_alphanumeric()}"
     sklearn_release = version.parse(sklearn.__version__).release
     statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
     cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
-    query = df[cols].queries["queries"][-1]
+    queries = df[cols].queries["queries"]
 
     @F.sproc(  # type: ignore[misc]
         session=session,
         name=sproc_name,
         replace=True,
         packages=[
             "cloudpickle",
             f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
             "snowflake-snowpark-python",
         ],
         statement_params=statement_params,
     )
     def mean_absolute_percentage_error_sproc(session: snowpark.Session) -> bytes:
-        df = session.sql(query).to_pandas(statement_params=statement_params)
+        for query in queries[:-1]:
+            _ = session.sql(query).collect(statement_params=statement_params)
+        df = session.sql(queries[-1]).to_pandas(statement_params=statement_params)
         y_true = df[y_true_col_names]
         y_pred = df[y_pred_col_names]
         sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
 
         loss = metrics.mean_absolute_percentage_error(
             y_true,
             y_pred,
@@ -483,29 +493,31 @@
 
     session = df._session
     assert session is not None
     sproc_name = f"mean_squared_error_{snowpark_utils.generate_random_alphanumeric()}"
     sklearn_release = version.parse(sklearn.__version__).release
     statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
     cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
-    query = df[cols].queries["queries"][-1]
+    queries = df[cols].queries["queries"]
 
     @F.sproc(  # type: ignore[misc]
         session=session,
         name=sproc_name,
         replace=True,
         packages=[
             "cloudpickle",
             f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
             "snowflake-snowpark-python",
         ],
         statement_params=statement_params,
     )
     def mean_squared_error_sproc(session: snowpark.Session) -> bytes:
-        df = session.sql(query).to_pandas(statement_params=statement_params)
+        for query in queries[:-1]:
+            _ = session.sql(query).collect(statement_params=statement_params)
+        df = session.sql(queries[-1]).to_pandas(statement_params=statement_params)
         y_true = df[y_true_col_names]
         y_pred = df[y_pred_col_names]
         sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
 
         loss = metrics.mean_squared_error(
             y_true,
             y_pred,
@@ -539,19 +551,19 @@
         y_true_col_name: Column name representing actual values.
         y_pred_col_name: Column name representing predicted values.
 
     Returns:
         R squared metric.
     """
 
-    df_avg = df.select(F.avg(y_true_col_name).as_("avg_y_true"))  # type: ignore[arg-type]
+    df_avg = df.select(F.avg(y_true_col_name).as_("avg_y_true"))
     df_r_square = df.join(df_avg).select(
-        F.lit(1)  # type: ignore[arg-type]
-        - F.sum((df[y_true_col_name] - df[y_pred_col_name]) ** 2)  # type: ignore[operator]
-        / F.sum((df[y_true_col_name] - df_avg["avg_y_true"]) ** 2)  # type: ignore[operator]
+        F.lit(1)
+        - F.sum((df[y_true_col_name] - df[y_pred_col_name]) ** 2)
+        / F.sum((df[y_true_col_name] - df_avg["avg_y_true"]) ** 2)
     )
 
     statement_params = telemetry.get_function_usage_statement_params(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         function_name=telemetry.get_statement_params_full_func_name(inspect.currentframe(), None),
     )
```

## snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.mixture".replace("sklearn.", "").split("_")])
 
@@ -410,33 +410,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -466,15 +462,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -482,17 +478,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -541,15 +539,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1167,16 +1165,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1184,18 +1182,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1222,30 +1216,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1279,15 +1275,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1301,15 +1297,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/mixture/gaussian_mixture.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.mixture".replace("sklearn.", "").split("_")])
 
@@ -383,33 +383,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -439,15 +435,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -455,17 +451,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -514,15 +512,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1140,16 +1138,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1157,18 +1155,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1195,30 +1189,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1252,15 +1248,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1274,15 +1270,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/model_selection/grid_search_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.model_selection".replace("sklearn.", "").split("_")])
 
@@ -419,33 +419,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -475,15 +471,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -491,17 +487,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -550,15 +548,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1180,16 +1178,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1197,18 +1195,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1235,30 +1229,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1292,15 +1288,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1314,15 +1310,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/model_selection/randomized_search_cv.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.model_selection".replace("sklearn.", "").split("_")])
 
@@ -434,33 +434,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -490,15 +486,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -506,17 +502,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -565,15 +563,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1195,16 +1193,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1212,18 +1210,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1250,30 +1244,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1307,15 +1303,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1329,15 +1325,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/multiclass/one_vs_one_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.multiclass".replace("sklearn.", "").split("_")])
 
@@ -293,33 +293,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -349,15 +345,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -365,17 +361,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -424,15 +422,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1048,16 +1046,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1065,18 +1063,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1103,30 +1097,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1160,15 +1156,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1182,15 +1178,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.multiclass".replace("sklearn.", "").split("_")])
 
@@ -302,33 +302,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -358,15 +354,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -374,17 +370,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -433,15 +431,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1061,16 +1059,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1078,18 +1076,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1116,30 +1110,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1173,15 +1169,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1195,15 +1191,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/multiclass/output_code_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.multiclass".replace("sklearn.", "").split("_")])
 
@@ -305,33 +305,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -361,15 +357,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -377,17 +373,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -436,15 +434,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1058,16 +1056,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1075,18 +1073,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1113,30 +1107,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1170,15 +1166,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1192,15 +1188,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/naive_bayes/bernoulli_nb.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.naive_bayes".replace("sklearn.", "").split("_")])
 
@@ -305,33 +305,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -361,15 +357,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -377,17 +373,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -436,15 +434,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1062,16 +1060,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1079,18 +1077,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1117,30 +1111,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1174,15 +1170,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1196,15 +1192,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/naive_bayes/categorical_nb.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.naive_bayes".replace("sklearn.", "").split("_")])
 
@@ -311,33 +311,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -367,15 +363,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -383,17 +379,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -442,15 +440,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1068,16 +1066,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1085,18 +1083,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1123,30 +1117,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1180,15 +1176,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1202,15 +1198,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/naive_bayes/complement_nb.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.naive_bayes".replace("sklearn.", "").split("_")])
 
@@ -305,33 +305,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -361,15 +357,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -377,17 +373,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -436,15 +434,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1062,16 +1060,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1079,18 +1077,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1117,30 +1111,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1174,15 +1170,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1196,15 +1192,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/naive_bayes/gaussian_nb.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.naive_bayes".replace("sklearn.", "").split("_")])
 
@@ -286,33 +286,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -342,15 +338,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -358,17 +354,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -417,15 +415,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1043,16 +1041,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1060,18 +1058,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1098,30 +1092,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1155,15 +1151,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1177,15 +1173,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/naive_bayes/multinomial_nb.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.naive_bayes".replace("sklearn.", "").split("_")])
 
@@ -299,33 +299,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -355,15 +351,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -371,17 +367,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -430,15 +428,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1056,16 +1054,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1073,18 +1071,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1111,30 +1105,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1168,15 +1164,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1190,15 +1186,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/neighbors/k_neighbors_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
@@ -356,33 +356,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -412,15 +408,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -428,17 +424,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -487,15 +485,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1113,16 +1111,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1130,18 +1128,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1168,30 +1162,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1225,15 +1221,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1247,15 +1243,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/neighbors/k_neighbors_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
@@ -358,33 +358,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -414,15 +410,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -430,17 +426,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -489,15 +487,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1111,16 +1109,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1128,18 +1126,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1166,30 +1160,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1223,15 +1219,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1245,15 +1241,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/neighbors/kernel_density.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
@@ -337,33 +337,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -393,15 +389,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -409,17 +405,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -468,15 +466,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1088,16 +1086,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1105,18 +1103,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1143,30 +1137,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1200,15 +1196,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1222,15 +1218,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/neighbors/local_outlier_factor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
@@ -365,33 +365,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -421,15 +417,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -437,17 +433,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -496,15 +494,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1120,16 +1118,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1137,18 +1135,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1175,30 +1169,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1232,15 +1228,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1254,15 +1250,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/neighbors/nearest_centroid.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
@@ -296,33 +296,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -352,15 +348,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -368,17 +364,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -427,15 +425,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1049,16 +1047,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1066,18 +1064,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1104,30 +1098,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1161,15 +1157,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1183,15 +1179,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/neighbors/nearest_neighbors.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
@@ -348,33 +348,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -404,15 +400,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -420,17 +416,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -479,15 +477,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1099,16 +1097,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1116,18 +1114,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1154,30 +1148,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1211,15 +1207,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1233,15 +1229,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
@@ -367,33 +367,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -423,15 +419,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -439,17 +435,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -498,15 +496,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1120,16 +1118,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1137,18 +1135,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1175,30 +1169,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1232,15 +1228,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1254,15 +1250,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
@@ -368,33 +368,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -424,15 +420,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -440,17 +436,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -499,15 +497,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1125,16 +1123,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1142,18 +1140,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1180,30 +1174,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1237,15 +1233,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1259,15 +1255,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
@@ -358,33 +358,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -414,15 +410,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -430,17 +426,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -489,15 +487,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1111,16 +1109,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1128,18 +1126,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1166,30 +1160,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1223,15 +1219,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1245,15 +1241,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/neural_network/bernoulli_rbm.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neural_network".replace("sklearn.", "").split("_")])
 
@@ -317,33 +317,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -373,15 +369,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -389,17 +385,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -448,15 +446,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1070,16 +1068,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1087,18 +1085,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1125,30 +1119,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1182,15 +1178,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1204,15 +1200,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/neural_network/mlp_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neural_network".replace("sklearn.", "").split("_")])
 
@@ -470,33 +470,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -526,15 +522,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -542,17 +538,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -601,15 +599,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1227,16 +1225,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1244,18 +1242,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1282,30 +1276,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1339,15 +1335,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1361,15 +1357,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/neural_network/mlp_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neural_network".replace("sklearn.", "").split("_")])
 
@@ -466,33 +466,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -522,15 +518,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -538,17 +534,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -597,15 +595,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1219,16 +1217,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1236,18 +1234,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1274,30 +1268,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1331,15 +1327,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1353,15 +1349,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/pipeline/pipeline.py

```diff
@@ -10,14 +10,15 @@
 from sklearn import __version__ as skversion, pipeline
 from sklearn.compose import ColumnTransformer
 from sklearn.preprocessing import FunctionTransformer
 from sklearn.utils import metaestimators
 
 from snowflake import snowpark
 from snowflake.ml._internal import telemetry
+from snowflake.ml._internal.exceptions import error_codes, exceptions
 from snowflake.ml.model.model_signature import ModelSignature, _infer_signature
 from snowflake.ml.modeling.framework import _utils, base
 
 _PROJECT = "ModelDevelopment"
 _SUBPROJECT = "Framework"
 
 
@@ -54,27 +55,30 @@
         all_columns: List of all the columns in a dataframe.
         target_columns: List of target column names to be extracted.
 
     Returns:
         Return the list of indices of target column in the original column array.
 
     Raises:
-        ValueError: If the target column is not present in the original column array.
+        SnowflakeMLException: If the target column is not present in the original column array.
     """
     column_indices = []
     for col in target_columns:
         found = False
         for i, c in enumerate(all_columns):
             if c == col:
                 column_indices.append(i)
                 found = True
                 break
         if not found:
-            raise ValueError(
-                f"Selected column {col} is not found in the input dataframe. Columns in the input df : {all_columns}"
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.NOT_FOUND,
+                original_exception=ValueError(
+                    f"Selected column {col} is not found in the input dataframe. Columns in the input df: {all_columns}"
+                ),
             )
     return column_indices
 
 
 class Pipeline(base.BaseTransformer):
     def __init__(self, steps: List[Tuple[str, Any]]) -> None:
         """
@@ -126,18 +130,21 @@
 
     def _get_estimator(self) -> Optional[Tuple[str, Any]]:
         return self.steps[-1] if self._is_final_step_estimator else None
 
     def _validate_steps(self) -> None:
         for name, t in self._get_transformers():
             if not Pipeline._is_transformer(t):
-                raise TypeError(
-                    "All intermediate steps should be "
-                    "transformers and implement both fit() and transform() methods, but"
-                    f"{name} (type {type(t)}) doesn't."
+                raise exceptions.SnowflakeMLException(
+                    error_code=error_codes.INVALID_ATTRIBUTE,
+                    original_exception=TypeError(
+                        "All intermediate steps should be "
+                        "transformers and implement both fit() and transform() methods, but"
+                        f"{name} (type {type(t)}) doesn't."
+                    ),
                 )
 
     def _reset(self) -> None:
         super()._reset()
         self._feature_names_in = []
         self._n_features_in = []
         self._transformers_to_input_indices = {}
@@ -258,21 +265,16 @@
         Call `transform` of each transformer in the pipeline.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Transformed data. Output datatype will be same as input datatype.
-
-        Raises:
-            RuntimeError: If the pipeline is not fitted first.
         """
-
-        if not self._is_fitted:
-            raise RuntimeError("Pipeline is not fitted before calling transform().")
+        self._enforce_fit()
 
         transformed_dataset = self._transform_dataset(dataset=dataset)
         estimator = self._get_estimator()
         if estimator:
             return estimator[1].transform(transformed_dataset)
         return transformed_dataset
 
@@ -435,21 +437,24 @@
         Transform the dataset by applying all the transformers in order and apply specified estimator function.
 
         Args:
             func_name: Target function name.
             dataset: Input dataset.
 
         Raises:
-            RuntimeError: If the pipeline is not fitted first.
+            SnowflakeMLException: If the pipeline is not fitted first.
 
         Returns:
             Output dataset.
         """
         if not self._is_fitted:
-            raise RuntimeError(f"Pipeline is not fitted before calling {func_name}().")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.METHOD_NOT_ALLOWED,
+                original_exception=RuntimeError(f"Pipeline is not fitted before calling {func_name}()."),
+            )
 
         transformed_dataset = self._transform_dataset(dataset=dataset)
         estimator = self._get_estimator()
         assert estimator is not None, "estimator cannot be None"
         res: snowpark.DataFrame = getattr(estimator[1], func_name)(transformed_dataset)
         return res
 
@@ -521,17 +526,20 @@
         return ct
 
     def _create_sklearn_object(self) -> pipeline.Pipeline:
         if not self._is_fitted:
             return self._create_unfitted_sklearn_object()
 
         if not self._is_convertable_to_sklearn:
-            raise ValueError(
-                "The pipeline can't be converted to SKLearn equivalent because it processing label or sample_weight "
-                "columns as part of pipeline preprocessing steps which is not allowed in SKLearn."
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.METHOD_NOT_ALLOWED,
+                original_exception=ValueError(
+                    "The pipeline can't be converted to SKLearn equivalent because it processing label or "
+                    "sample_weight columns as part of pipeline preprocessing steps which is not allowed in SKLearn."
+                ),
             )
 
         # Create a fitted sklearn pipeline object by translating each non-estimator step in pipeline with with
         # a fitted column transformer.
         sksteps = []
         for i, (name, trans) in enumerate(self._get_transformers()):
             if isinstance(trans, base.BaseTransformer):
@@ -577,9 +585,12 @@
             estimator_signatures = estimator_step[1].model_signatures
             for method, signature in estimator_signatures.items():
                 self._model_signature_dict[method] = ModelSignature(inputs=inputs_signature, outputs=signature.outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
-            raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=RuntimeError("Estimator not fitted before accessing property model_signatures!"),
+            )
         return self._model_signature_dict
```

## snowflake/ml/modeling/preprocessing/binarizer.py

```diff
@@ -5,14 +5,15 @@
 from typing import Iterable, Optional, Union
 
 import pandas as pd
 from sklearn import preprocessing
 
 from snowflake import snowpark
 from snowflake.ml._internal import telemetry
+from snowflake.ml._internal.exceptions import error_codes, exceptions
 from snowflake.ml.modeling.framework import base
 from snowflake.snowpark import functions as F, types as T
 
 
 class Binarizer(base.BaseTransformer):
     r"""Binarizes data (sets feature values to 0 or 1) according to the given threshold.
 
@@ -79,18 +80,21 @@
         Args:
             dataset: Input dataset.
 
         Returns:
             self
 
         Raises:
-            TypeError: If the threshold is not a float.
+            SnowflakeMLException: If the threshold is not a float.
         """
         if not isinstance(self.threshold, float):
-            raise TypeError(f"Binarizer threshold must be a float, but got {type(self.threshold)}.")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=TypeError(f"Binarizer threshold must be a float, but got {type(self.threshold)}."),
+            )
 
         self._is_fitted = True
         return self
 
     @telemetry.send_api_usage_telemetry(
         project=base.PROJECT,
         subproject=base.SUBPROJECT,
@@ -104,39 +108,32 @@
         Binarize the data. Map to 1 if it is strictly greater than the threshold, otherwise 0.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
-
-        Raises:
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
         super()._check_input_cols()
         super()._check_output_cols()
+        super()._check_dataset_type(dataset)
 
         if isinstance(dataset, snowpark.DataFrame):
             output_df = self._transform_snowpark(dataset)
-        elif isinstance(dataset, pd.DataFrame):
-            output_df = self._transform_sklearn(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            output_df = self._transform_sklearn(dataset)
 
         return self._drop_input_columns(output_df) if self._drop_input_cols is True else output_df
 
     def _transform_snowpark(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         passthrough_columns = [c for c in dataset.columns if c not in self.output_cols]
         self._validate_data_has_no_nulls(dataset)
         output_columns = []
         for input_col in self.input_cols:
-            col = F.iff(dataset[input_col] > self.threshold, 1.0, 0.0).cast(T.FloatType())  # type: ignore[arg-type]
+            col = F.iff(dataset[input_col] > self.threshold, 1.0, 0.0).cast(T.FloatType())
             output_columns.append(col)
 
         transformed_dataset: snowpark.DataFrame = dataset.with_columns(self.output_cols, output_columns)
         # Reorder columns. Passthrough columns are added at the right to the output of the transformers.
         transformed_dataset = transformed_dataset[self.output_cols + passthrough_columns]
         return transformed_dataset
```

## snowflake/ml/modeling/preprocessing/k_bins_discretizer.py

```diff
@@ -11,14 +11,15 @@
 import numpy.typing as npt
 import pandas as pd
 from scipy import sparse
 from sklearn.preprocessing import KBinsDiscretizer as SK_KBinsDiscretizer
 
 from snowflake import snowpark
 from snowflake.ml._internal import telemetry
+from snowflake.ml._internal.exceptions import error_codes, exceptions
 from snowflake.ml.modeling.framework import base
 from snowflake.snowpark import functions as F, types as T
 from snowflake.snowpark._internal import utils as snowpark_utils
 
 # constants used to validate the compatibility of the kwargs passed to the sklearn
 # transformer with the sklearn version
 _SKLEARN_INITIAL_KEYWORDS = [
@@ -104,22 +105,36 @@
         self.strategy = strategy
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
 
     def _enforce_params(self) -> None:
         self.n_bins = self.n_bins if isinstance(self.n_bins, Iterable) else [self.n_bins] * len(self.input_cols)
         if len(self.n_bins) != len(self.input_cols):
-            raise ValueError(f"n_bins must have same size as input_cols, got: {self.n_bins} vs {self.input_cols}")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=ValueError(
+                    f"n_bins must have same size as input_cols, got: {self.n_bins} vs {self.input_cols}"
+                ),
+            )
         for idx, b in enumerate(self.n_bins):
             if b < 2:
-                raise ValueError(f"n_bins cannot be less than 2, got: {b} at index {idx}")
+                raise exceptions.SnowflakeMLException(
+                    error_code=error_codes.INVALID_ATTRIBUTE,
+                    original_exception=ValueError(f"n_bins cannot be less than 2, got: {b} at index {idx}"),
+                )
         if self.encode not in _VALID_ENCODING_SCHEME:
-            raise ValueError(f"encode must be one of f{_VALID_ENCODING_SCHEME}, got: {self.encode}")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=ValueError(f"encode must be one of f{_VALID_ENCODING_SCHEME}, got: {self.encode}"),
+            )
         if self.strategy not in _VALID_STRATEGY:
-            raise ValueError(f"strategy must be one of f{_VALID_STRATEGY}, got: {self.strategy}")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=ValueError(f"strategy must be one of f{_VALID_STRATEGY}, got: {self.strategy}"),
+            )
 
     def _reset(self) -> None:
         super()._reset()
         self.bin_edges_: Optional[npt.NDArray[np.float32]] = None
         self.n_bins_: Optional[npt.NDArray[np.int32]] = None
 
     @telemetry.send_api_usage_telemetry(
@@ -131,31 +146,24 @@
         Fit KBinsDiscretizer with dataset.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Fitted self instance.
-
-        Raises:
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
         self._reset()
         self._enforce_params()
         super()._check_input_cols()
+        super()._check_dataset_type(dataset)
 
         if isinstance(dataset, pd.DataFrame):
             self._fit_sklearn(dataset)
-        elif isinstance(dataset, snowpark.DataFrame):
-            self._fit_snowpark(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            self._fit_snowpark(dataset)
 
         self._is_fitted = True
         return self
 
     @telemetry.send_api_usage_telemetry(
         project=base.PROJECT,
         subproject=base.SUBPROJECT,
@@ -174,41 +182,37 @@
             dataset: Input dataset.
 
         Returns:
             Discretized output data based on input type.
             - If input is snowpark DataFrame, returns snowpark DataFrame
             - If input is a pd.DataFrame and 'self.encdoe=onehot', returns 'csr_matrix'
             - If input is a pd.DataFrame and 'self.encode in ['ordinal', 'onehot-dense']', returns 'pd.DataFrame'
-
-        Raises:
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
-        self.enforce_fit()
+        self._enforce_fit()
         super()._check_input_cols()
         super()._check_output_cols()
+        super()._check_dataset_type(dataset)
 
         if isinstance(dataset, snowpark.DataFrame):
             output_df = self._transform_snowpark(dataset)
-        elif isinstance(dataset, pd.DataFrame):
-            output_df = self._transform_sklearn(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            output_df = self._transform_sklearn(dataset)
 
         return self._drop_input_columns(output_df) if self._drop_input_cols is True else output_df
 
     def _fit_snowpark(self, dataset: snowpark.DataFrame) -> None:
         if self.strategy == "quantile":
             self._handle_quantile(dataset)
         elif self.strategy == "uniform":
             self._handle_uniform(dataset)
         elif self.strategy == "kmeans":
-            raise NotImplementedError("kmeans not supported yet")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=NotImplementedError("kmeans not supported yet"),
+            )
 
     def _handle_quantile(self, dataset: snowpark.DataFrame) -> None:
         """
         Compute bins with percentile values of the feature.
         All bins in each feature will have the same number of points.
 
         Args:
@@ -217,17 +221,15 @@
         # 1. Collect percentiles for each feature column
         # NB: if SQL compilation ever take too long on wide schema, consider applying optimization mentioned in
         # https://docs.google.com/document/d/1cilfCCtKYv6HvHqaqdZxfHAvQ0gg-t1AM8KYCQtJiLE/edit
         agg_queries = []
         for idx, col_name in enumerate(self.input_cols):
             percentiles = np.linspace(0, 1, cast(List[int], self.n_bins)[idx] + 1)
             for i, pct in enumerate(percentiles.tolist()):
-                agg_queries.append(
-                    F.percentile_cont(pct).within_group(col_name).alias(f"{col_name}_pct_{i}")  # type: ignore[arg-type]
-                )
+                agg_queries.append(F.percentile_cont(pct).within_group(col_name).alias(f"{col_name}_pct_{i}"))
         state_df = dataset.agg(agg_queries)
         state = (
             state_df.to_pandas(
                 statement_params=telemetry.get_statement_params(base.PROJECT, base.SUBPROJECT, self.__class__.__name__)
             )
             .to_numpy()
             .ravel()
@@ -249,18 +251,15 @@
         All bins in each feature will have identical widths.
 
         Args:
             dataset: Input dataset.
         """
         # 1. Collect min and max for each feature column
         agg_queries = list(
-            chain.from_iterable(
-                (F.min(x).alias(f"{x}_min"), F.max(x).alias(f"{x}_max"))  # type: ignore[arg-type]
-                for x in self.input_cols
-            )
+            chain.from_iterable((F.min(x).alias(f"{x}_min"), F.max(x).alias(f"{x}_max")) for x in self.input_cols)
         )
         state_df = dataset.select(*agg_queries)
         state = (
             state_df.to_pandas(
                 statement_params=telemetry.get_statement_params(base.PROJECT, base.SUBPROJECT, self.__class__.__name__)
             )
             .to_numpy()
@@ -306,15 +305,18 @@
         if self.encode == "ordinal":
             return self._handle_ordinal(dataset)
         elif self.encode == "onehot":
             return self._handle_onehot(dataset)
         elif self.encode == "onehot-dense":
             return self._handle_onehot_dense(dataset)
         else:
-            raise ValueError(f"{self.encode} is not a valid encoding scheme.")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=ValueError(f"{self.encode} is not a valid encoding scheme."),
+            )
 
     def _handle_ordinal(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         """
         Transform dataset with bucketization and output as ordinal encoding.
 
         Args:
             dataset: Input dataset.
@@ -341,20 +343,19 @@
             boarders = boarders[0]
             res = np.searchsorted(boarders[1:-1], x, side="right")
             return pd.Series(res)  # type: ignore[no-any-return]
 
         # 2. compute bucket per feature column
         for idx, input_col in enumerate(self.input_cols):
             output_col = self.output_cols[idx]
-            boarders = [F.lit(float(x)) for x in self.bin_edges_[idx]]  # type: ignore[arg-type, index]
+            assert self.bin_edges_ is not None
+            boarders = [F.lit(float(x)) for x in self.bin_edges_[idx]]
             dataset = dataset.select(
                 *dataset.columns,
-                F.call_udf(udf_name, F.col(input_col), F.array_construct(*boarders)).alias(  # type: ignore[arg-type]
-                    output_col
-                ),
+                F.call_udf(udf_name, F.col(input_col), F.array_construct(*boarders)).alias(output_col),
             )
         # Reorder columns. Passthrough columns are added at the right to the output of the transformers.
         dataset = dataset[self.output_cols + passthrough_columns]
         return dataset
 
     def _handle_onehot(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         """
@@ -386,19 +387,20 @@
             assert isinstance(buckets, np.ndarray), f"expecting buckets to be numpy ndarray, got {type(buckets)}"
             array_length = len(boarders) - 1
             for bucket in buckets:
                 res.append({str(bucket): 1, "array_length": array_length})
             return pd.Series(res)
 
         for idx, input_col in enumerate(self.input_cols):
+            assert self.bin_edges_ is not None
             output_col = self.output_cols[idx]
-            boarders = [F.lit(float(x)) for x in self.bin_edges_[idx]]  # type: ignore[arg-type, index]
+            boarders = [F.lit(float(x)) for x in self.bin_edges_[idx]]
             dataset = dataset.select(
                 *dataset.columns,
-                F.call_udf(udf_name, F.col(input_col), F.array_construct(*boarders)).alias(output_col),  # type: ignore
+                F.call_udf(udf_name, F.col(input_col), F.array_construct(*boarders)).alias(output_col),
             )
         # Reorder columns. Passthrough columns are added at the right to the output of the transformers.
         dataset = dataset[self.output_cols + passthrough_columns]
         return dataset
 
     def _handle_onehot_dense(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         """
@@ -433,19 +435,20 @@
             for bucket in buckets:
                 encoded = np.zeros(len(boarders), dtype=int)
                 encoded[bucket] = 1
                 res.append(encoded)
             return pd.Series(res)
 
         for idx, input_col in enumerate(self.input_cols):
+            assert self.bin_edges_ is not None
             output_col = self.output_cols[idx]
-            boarders = [F.lit(float(x)) for x in self.bin_edges_[idx]]  # type: ignore[arg-type, index]
+            boarders = [F.lit(float(x)) for x in self.bin_edges_[idx]]
             dataset = dataset.select(
                 *dataset.columns,
-                F.call_udf(udf_name, F.col(input_col), F.array_construct(*boarders)).alias(output_col),  # type: ignore
+                F.call_udf(udf_name, F.col(input_col), F.array_construct(*boarders)).alias(output_col),
             )
             dataset = dataset.with_columns(
                 [f"{output_col}_{i}" for i in range(len(boarders) - 1)],
                 [F.col(output_col)[i].cast(T.IntegerType()) for i in range(len(boarders) - 1)],
             ).drop(output_col)
             all_output_cols += [f"{output_col}_{i}" for i in range(len(boarders) - 1)]
 
@@ -460,15 +463,15 @@
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
         """
-        self.enforce_fit()
+        self._enforce_fit()
         encoder_sklearn = self.to_sklearn()
 
         transformed_dataset = encoder_sklearn.transform(dataset[self.input_cols])
 
         if self.encode == "ordinal":
             dataset = dataset.copy()
             dataset[self.output_cols] = transformed_dataset
```

## snowflake/ml/modeling/preprocessing/label_encoder.py

```diff
@@ -5,14 +5,15 @@
 from typing import Iterable, Optional, Union
 
 import pandas as pd
 from sklearn import preprocessing
 
 from snowflake import snowpark
 from snowflake.ml._internal import telemetry, type_utils
+from snowflake.ml._internal.exceptions import error_codes, exceptions
 from snowflake.ml.modeling.framework import base
 from snowflake.ml.modeling.preprocessing import ordinal_encoder
 
 
 class LabelEncoder(base.BaseTransformer):
     r"""Encodes target labels with values between 0 and n_classes-1.
 
@@ -77,22 +78,28 @@
         Args:
             dataset: Input dataset.
 
         Returns:
             self
 
         Raises:
-            ValueError: If length of input_cols is not 1 or length of output_cols is greater than 1.
+            SnowflakeMLException: If length of input_cols is not 1 or length of output_cols is greater than 1.
         """
         if len(self.input_cols) != 1:
-            raise ValueError("Label encoder must specify one input column.")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=ValueError("Label encoder must specify one input column."),
+            )
         input_col = self.input_cols[0]
 
         if len(self.output_cols) != 1:
-            raise ValueError("Label encoder must specify one output column.")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=ValueError("Label encoder must specify one output column."),
+            )
 
         self._reset()
 
         # Use `OrdinalEncoder` to handle fits and transforms.
         self._ordinal_encoder = ordinal_encoder.OrdinalEncoder(input_cols=self.input_cols, output_cols=self.output_cols)
 
         self._ordinal_encoder.fit(dataset)
@@ -117,36 +124,28 @@
         the transform result column added will be returned.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
-
-        Raises:
-            RuntimeError: If transformer is not fitted first.
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
-        if not self._is_fitted or self._ordinal_encoder is None or self.classes_ is None:
-            raise RuntimeError("Label encoder must be fitted before calling transform().")
+        self._enforce_fit()
+        super()._check_dataset_type(dataset)
 
         if isinstance(dataset, snowpark.DataFrame):
             # [SNOW-802691] Support for mypy type checking
+            assert self._ordinal_encoder is not None
             output_df = self._ordinal_encoder.transform(dataset).na.replace(
-                float("nan"),  # type: ignore[arg-type]
+                float("nan"),
                 len(self.classes_) - 1,  # type: ignore[arg-type]
                 subset=self.output_cols,
             )
-        elif isinstance(dataset, pd.DataFrame):
-            output_df = self._transform_sklearn(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            output_df = self._transform_sklearn(dataset)
 
         return self._drop_input_columns(output_df) if self._drop_input_cols is True else output_df
 
     def _create_unfitted_sklearn_object(self) -> preprocessing.LabelEncoder:
         return preprocessing.LabelEncoder()
 
     def _create_sklearn_object(self) -> preprocessing.LabelEncoder:
```

## snowflake/ml/modeling/preprocessing/max_abs_scaler.py

```diff
@@ -101,30 +101,23 @@
         Returns the transformer instance.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Return self as fitted scaler.
-
-        Raises:
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
         super()._check_input_cols()
+        super()._check_dataset_type(dataset)
         self._reset()
 
         if isinstance(dataset, pd.DataFrame):
             self._fit_sklearn(dataset)
-        elif isinstance(dataset, snowpark.DataFrame):
-            self._fit_snowpark(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            self._fit_snowpark(dataset)
 
         self._is_fitted = True
         return self
 
     def _fit_sklearn(self, dataset: pd.DataFrame) -> None:
         dataset = self._use_input_cols_only(dataset)
         sklearn_scaler = self._create_unfitted_sklearn_object()
@@ -157,33 +150,24 @@
         Scale the data.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
-
-        Raises:
-            RuntimeError: If transformer is not fitted first.
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
-        if not self._is_fitted:
-            raise RuntimeError("Transformer not fitted before calling transform().")
+        self._enforce_fit()
         super()._check_input_cols()
         super()._check_output_cols()
+        super()._check_dataset_type(dataset)
 
         if isinstance(dataset, snowpark.DataFrame):
             output_df = self._transform_snowpark(dataset)
-        elif isinstance(dataset, pd.DataFrame):
-            output_df = self._transform_sklearn(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            output_df = self._transform_sklearn(dataset)
 
         return self._drop_input_columns(output_df) if self._drop_input_cols is True else output_df
 
     def _transform_snowpark(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         """
         Scale the data on snowflake DataFrame.
```

## snowflake/ml/modeling/preprocessing/min_max_scaler.py

```diff
@@ -110,30 +110,23 @@
         Returns the transformer instance.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Fitted scaler.
-
-        Raises:
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
         super()._check_input_cols()
+        super()._check_dataset_type(dataset)
         self._reset()
 
         if isinstance(dataset, pd.DataFrame):
             self._fit_sklearn(dataset)
-        elif isinstance(dataset, snowpark.DataFrame):
-            self._fit_snowpark(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            self._fit_snowpark(dataset)
 
         self._is_fitted = True
         return self
 
     def _fit_sklearn(self, dataset: pd.DataFrame) -> None:
         dataset = self._use_input_cols_only(dataset)
         sklearn_scaler = self._create_unfitted_sklearn_object()
@@ -177,33 +170,24 @@
         Scale features according to feature_range.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
-
-        Raises:
-            RuntimeError: If transformer is not fitted first.
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
-        if not self._is_fitted:
-            raise RuntimeError("Transformer not fitted before calling transform().")
+        self._enforce_fit()
         super()._check_input_cols()
         super()._check_output_cols()
+        super()._check_dataset_type(dataset)
 
         if isinstance(dataset, snowpark.DataFrame):
             output_df = self._transform_snowpark(dataset)
-        elif isinstance(dataset, pd.DataFrame):
-            output_df = self._transform_sklearn(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            output_df = self._transform_sklearn(dataset)
 
         return self._drop_input_columns(output_df) if self._drop_input_cols is True else output_df
 
     def _transform_snowpark(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         """
         Scale features according to feature_range on
         Snowpark dataframe.
@@ -218,19 +202,19 @@
         output_columns = []
         for _, input_col in enumerate(self.input_cols):
             output_column = dataset[input_col] * self.scale_[input_col] + self.min_[input_col]
 
             if self.clip:
                 output_column = F.greatest(
                     output_column,
-                    F.lit(self.feature_range[0]),  # type: ignore[arg-type]
+                    F.lit(self.feature_range[0]),
                 )
                 output_column = F.least(
                     output_column,
-                    F.lit(self.feature_range[1]),  # type: ignore[arg-type]
+                    F.lit(self.feature_range[1]),
                 )
 
             output_columns.append(output_column)
 
         transformed_dataset: snowpark.DataFrame = dataset.with_columns(self.output_cols, output_columns)
         # Reorder columns. Passthrough columns are added at the right to the output of the transformers.
         transformed_dataset = transformed_dataset[self.output_cols + passthrough_columns]
```

## snowflake/ml/modeling/preprocessing/normalizer.py

```diff
@@ -5,14 +5,15 @@
 from typing import Iterable, Optional, Union
 
 import pandas as pd
 from sklearn import preprocessing
 
 from snowflake import snowpark
 from snowflake.ml._internal import telemetry
+from snowflake.ml._internal.exceptions import error_codes, exceptions
 from snowflake.ml.modeling.framework import base
 from snowflake.snowpark import functions as F, types as T
 
 _VALID_NORMS = ["l1", "l2", "max"]
 
 
 class Normalizer(base.BaseTransformer):
@@ -86,64 +87,68 @@
         Args:
             dataset: Input dataset.
 
         Returns:
             transformed_dataset: Output dataset.
 
         Raises:
-            ValueError: If the dataset contains nulls, or if the supplied norm is invalid.
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
+            SnowflakeMLException: If the dataset contains nulls, or if the supplied norm is invalid.
         """
         if self.norm not in _VALID_NORMS:
-            raise ValueError(f"'{self.norm}' is not a supported norm.")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=ValueError(f"'{self.norm}' is not a supported norm."),
+            )
 
         super()._check_input_cols()
         super()._check_output_cols()
+        super()._check_dataset_type(dataset)
 
         if isinstance(dataset, snowpark.DataFrame):
             output_df = self._transform_snowpark(dataset)
-        elif isinstance(dataset, pd.DataFrame):
-            output_df = self._transform_sklearn(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            output_df = self._transform_sklearn(dataset)
 
         return self._drop_input_columns(output_df) if self._drop_input_cols is True else output_df
 
     def _transform_snowpark(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         passthrough_columns = [c for c in dataset.columns if c not in self.output_cols]
         self._validate_data_has_no_nulls(dataset)
         if len(self.input_cols) == 0:
-            raise ValueError("Found array with 0 columns, but a minimum of 1 is required.")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=ValueError("Found array with 0 columns, but a minimum of 1 is required."),
+            )
 
         if self.norm == "l1":
-            norm = F.lit("0")  # type: ignore[arg-type]
+            norm = F.lit("0")
             for input_col in self.input_cols:
-                norm += F.abs(dataset[input_col])  # type: ignore[operator]
+                norm += F.abs(dataset[input_col])
 
         elif self.norm == "l2":
-            norm = F.lit("0")  # type: ignore[arg-type]
+            norm = F.lit("0")
             for input_col in self.input_cols:
                 norm += dataset[input_col] * dataset[input_col]
-            norm = F.sqrt(norm)  # type: ignore[arg-type]
+            norm = F.sqrt(norm)
 
         elif self.norm == "max":
-            norm = F.greatest(*[F.abs(dataset[input_col]) for input_col in self.input_cols])  # type: ignore[arg-type]
+            norm = F.greatest(*[F.abs(dataset[input_col]) for input_col in self.input_cols])
 
         else:
-            raise ValueError(f"'{self.norm}' is not a supported norm.")
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=ValueError(f"'{self.norm}' is not a supported norm."),
+            )
 
         output_columns = []
         for input_col in self.input_cols:
             # Set the entry to 0 if the norm is 0, because the norm is 0 only when all entries are 0.
             output_column = F.div0(
                 dataset[input_col].cast(T.FloatType()),
-                norm,  # type: ignore[arg-type]
+                norm,
             )
             output_columns.append(output_column)
 
         transformed_dataset: snowpark.DataFrame = dataset.with_columns(self.output_cols, output_columns)
         # Reorder columns. Passthrough columns are added at the right to the output of the transformers.
         transformed_dataset = transformed_dataset[self.output_cols + passthrough_columns]
         return transformed_dataset
```

## snowflake/ml/modeling/preprocessing/one_hot_encoder.py

```diff
@@ -1,12 +1,13 @@
 #!/usr/bin/env python3
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import numbers
+import uuid
 from typing import Any, Dict, Iterable, List, Optional, Union
 
 import numpy as np
 import numpy.typing as npt
 import pandas as pd
 import sklearn
 from packaging import version
@@ -203,17 +204,18 @@
 
         # Fit state
         self.categories_: Dict[str, type_utils.LiteralNDArrayType] = {}
         self._categories_list: List[type_utils.LiteralNDArrayType] = []
         self.drop_idx_: Optional[npt.NDArray[np.int_]] = None
         self._drop_idx_after_grouping: Optional[npt.NDArray[np.int_]] = None
         self._n_features_outs: List[int] = []
-        self._dense_output_cols_mappings: Dict[
-            str, List[str]
-        ] = {}  # transform state when output columns are unset before fitting
+
+        # Fit state if output columns are set before fitting
+        self._dense_output_cols_mappings: Dict[str, List[str]] = {}
+        self._inferred_output_cols: List[str] = []
 
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
 
     @property
     def infrequent_categories_(self) -> List[Optional[type_utils.LiteralNDArrayType]]:
         """Infrequent categories for each feature."""
@@ -275,18 +277,21 @@
         elif isinstance(dataset, snowpark.DataFrame):
             self._fit_snowpark(dataset)
         else:
             raise TypeError(
                 f"Unexpected dataset type: {type(dataset)}."
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
+        self._is_fitted = True
+
         if not self.sparse and self.output_cols:
-            self._get_dense_output_cols_mappings()
+            self._handle_dense_output_cols()
+        if self.output_cols:
+            self._handle_inferred_output_cols(dataset)
 
-        self._is_fitted = True
         return self
 
     def _fit_sklearn(self, dataset: pd.DataFrame) -> None:
         dataset = self._use_input_cols_only(dataset)
         sklearn_encoder = self._create_unfitted_sklearn_object()
         sklearn_encoder.fit(dataset)
 
@@ -389,33 +394,29 @@
             ValueError: If `self.categories` is provided, `self.handle_unknown="error"`,
                 and unknown categories exist in dataset.
         """
         # states of categories found in dataset
         found_state_df: Optional[snowpark.DataFrame] = None
         for input_col in self.input_cols:
             state_columns = [
-                F.lit(input_col).alias(_COLUMN_NAME),  # type: ignore[arg-type]
+                F.lit(input_col).alias(_COLUMN_NAME),
                 F.col(input_col).cast(T.StringType()).alias(_CATEGORY),
                 F.iff(
                     # null or nan values
-                    F.col(input_col).is_null()  # type: ignore[arg-type]
-                    | (F.col(input_col).cast(T.StringType()).equal_nan()),
+                    F.col(input_col).is_null() | (F.col(input_col).cast(T.StringType()).equal_nan()),
                     # count null and nan values
-                    F.sum(  # type: ignore[arg-type]
-                        F.iff(  # type: ignore[arg-type]
-                            F.col(input_col).is_null()  # type: ignore[arg-type]
-                            | (F.col(input_col).cast(T.StringType()).equal_nan()),
-                            1,  # type: ignore[arg-type]
-                            0,  # type: ignore[arg-type]
+                    F.sum(
+                        F.iff(
+                            F.col(input_col).is_null() | (F.col(input_col).cast(T.StringType()).equal_nan()),
+                            1,
+                            0,
                         )
-                    ).over(
-                        snowpark.Window.partition_by(input_col)  # type: ignore[arg-type]
-                    ),
+                    ).over(snowpark.Window.partition_by(input_col)),
                     # count values that are not null or nan
-                    F.count(input_col).over(snowpark.Window.partition_by(input_col)),  # type: ignore[arg-type]
+                    F.count(input_col).over(snowpark.Window.partition_by(input_col)),
                 ).alias(_COUNT),
             ]
             temp_df = dataset.select(state_columns).distinct()
             found_state_df = found_state_df.union_by_name(temp_df) if found_state_df is not None else temp_df
 
         assert found_state_df is not None, "found_state_df cannot be None"
         if self.categories != "auto":
@@ -656,25 +657,23 @@
         Returns:
             Output dataset. The output type depends on the input dataset type:
                 - If input is DataFrame, returns DataFrame
                 - If input is a pd.DataFrame and `self.sparse=True`, returns `csr_matrix`
                 - If input is a pd.DataFrame and `self.sparse=False`, returns `pd.DataFrame`
 
         Raises:
-            RuntimeError: If transformer is not fitted first.
             TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
-        if not self._is_fitted:
-            raise RuntimeError("Transformer not fitted before calling transform().")
+        self._enforce_fit()
         super()._check_input_cols()
         super()._check_output_cols()
 
         # output columns are unset before fitting
         if not self.sparse and not self._dense_output_cols_mappings:
-            self._get_dense_output_cols_mappings()
+            self._handle_dense_output_cols()
 
         if isinstance(dataset, snowpark.DataFrame):
             output_df = self._transform_snowpark(dataset)
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._transform_sklearn(dataset)
         else:
             raise TypeError(
@@ -730,34 +729,50 @@
         # TODO: [SNOW-730357] Support NUMBER as the key of Snowflake OBJECT for OneHotEncoder sparse output
         state_pandas[_ENCODED_VALUE] = state_pandas.apply(lambda x: map_encoded_value(x), axis=1)
 
         # columns: COLUMN_NAME, CATEGORY, COUNT, FITTED_CATEGORY, ENCODING, N_FEATURES_OUT, ENCODED_VALUE
         assert dataset._session is not None, "dataset._session cannot be None"
         state_df = dataset._session.create_dataframe(state_pandas)
 
+        suffix = "_" + uuid.uuid4().hex.upper()
         transformed_dataset = dataset
-        origional_dataset_columns = transformed_dataset.columns[:]
+        original_dataset_cols = transformed_dataset.columns[:]
         all_output_cols = []
+        suffixed_input_cols = []
+        joined_input_cols = []
         for idx, input_col in enumerate(self.input_cols):
             output_col = self.output_cols[idx]
             all_output_cols += [output_col]
             input_col_state_df = state_df.filter(F.col(_COLUMN_NAME) == input_col)[
                 [_CATEGORY, _ENCODED_VALUE]
             ].with_column_renamed(_ENCODED_VALUE, output_col)
 
             # index values through a left join over the dataset and its states
             transformed_dataset = transformed_dataset.join(
                 input_col_state_df,
                 on=transformed_dataset[input_col].equal_null(input_col_state_df[_CATEGORY]),
                 how="left",
-            )[transformed_dataset.columns + [output_col]]
+                lsuffix=suffix,
+            ).drop(_CATEGORY)
 
-        transformed_dataset = self._handle_unknown_in_transform(transformed_dataset)
+            # handle identical input & output cols
+            if input_col == output_col:
+                col = identifier.concat_names([input_col, suffix])
+                suffixed_input_cols.append(col)
+                joined_input_cols.append(col)
+            else:
+                joined_input_cols.append(input_col)
+
+        if not self._inferred_output_cols:
+            self._inferred_output_cols = transformed_dataset[all_output_cols].columns
+
+        transformed_dataset = self._handle_unknown_in_transform(transformed_dataset, joined_input_cols)
         # Reorder columns. Passthrough columns are added at the right to the output of the transformers.
-        transformed_dataset = transformed_dataset[all_output_cols + origional_dataset_columns]
+        passthrough_cols = list(set(original_dataset_cols) - set(all_output_cols))
+        transformed_dataset = transformed_dataset.drop(suffixed_input_cols)[all_output_cols + passthrough_cols]
         return transformed_dataset
 
     def _transform_snowpark_dense(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         """
         Transform Snowpark dataframe using one-hot encoding when
         `self.sparse=False`. Return the dense representation. For
         `self.input_cols[i]`, its output columns are named as
@@ -812,14 +827,17 @@
             # index values through a left join over the dataset and its states
             transformed_dataset = transformed_dataset.join(
                 input_col_state_df,
                 on=transformed_dataset[input_col].equal_null(input_col_state_df[_CATEGORY]),
                 how="left",
             )[transformed_dataset.columns + output_cols]
 
+        if not self._inferred_output_cols:
+            self._inferred_output_cols = transformed_dataset[all_output_cols].columns
+
         transformed_dataset = self._handle_unknown_in_transform(transformed_dataset)
         # Reorder columns. Passthrough columns are added at the right to the output of the transformers.
         transformed_dataset = transformed_dataset[all_output_cols + original_dataset_columns]
         return transformed_dataset
 
     def _transform_snowpark_sparse_udf(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         """
@@ -896,14 +914,17 @@
         """
         encoder_sklearn = self.to_sklearn()
         transformed_dataset = encoder_sklearn.transform(dataset[self.input_cols])
 
         if self.sparse:
             return transformed_dataset
 
+        if not self._inferred_output_cols:
+            self._inferred_output_cols = self._get_inferred_output_cols()
+
         dataset = dataset.copy()
         dataset[self.get_output_cols()] = transformed_dataset
         return dataset
 
     def _create_unfitted_sklearn_object(self) -> preprocessing.OneHotEncoder:
         sklearn_args = self.get_sklearn_args(
             default_sklearn_obj=preprocessing.OneHotEncoder(),
@@ -975,43 +996,52 @@
                 msg = "`min_frequency` must be an integer at least 1, a float in (0.0, 1.0), or None, " "got integer {}"
                 raise ValueError(msg.format(self.min_frequency))
         elif isinstance(self.min_frequency, numbers.Real):
             if not (0.0 < float(self.min_frequency) < 1.0):
                 msg = "`min_frequency` must be an integer at least 1, a float in (0.0, 1.0), or None, " "got float {}"
                 raise ValueError(msg.format(self.min_frequency))
 
-    def _handle_unknown_in_transform(self, transformed_dataset: snowpark.DataFrame) -> snowpark.DataFrame:
+    def _handle_unknown_in_transform(
+        self,
+        transformed_dataset: snowpark.DataFrame,
+        input_cols: Optional[List[str]] = None,
+    ) -> snowpark.DataFrame:
         """
         Handle unknown values in the transformed dataset.
 
         Args:
             transformed_dataset: Transformed dataset without unknown values handled.
+            input_cols: Input columns (may be suffixed).
 
         Returns:
             Transformed dataset with unknown values handled.
 
         Raises:
             ValueError: If `self.handle_unknown="error"` and unknown values exist in the
                 transformed dataset.
         """
         if self.handle_unknown == "error":
             # dataframe with unknown values
             # columns: COLUMN_NAME, UNKNOWN_VALUE
             unknown_df: Optional[snowpark.DataFrame] = None
-            for idx, input_col in enumerate(self.input_cols):
+            cols = input_cols or self.input_cols
+            for idx, input_col in enumerate(cols):
                 output_col = self.output_cols[idx]
                 check_col = output_col
                 if not self.sparse:
-                    output_cat_cols = [x for x in transformed_dataset.columns if f"{output_col}_" in x]
+                    output_cat_cols = [
+                        identifier.quote_name_without_upper_casing(col)
+                        for col in self._dense_output_cols_mappings[input_col]
+                    ]
                     if not output_cat_cols:
                         continue
                     check_col = output_cat_cols[0]
 
                 unknown_columns = [
-                    F.lit(input_col),  # type: ignore[arg-type]
+                    F.lit(self.input_cols[idx]),
                     F.col(input_col),
                 ]
                 temp_df = (
                     transformed_dataset[[input_col, check_col]]
                     .distinct()
                     .filter(F.col(check_col).is_null())
                     .select(unknown_columns)
@@ -1024,22 +1054,16 @@
                     statement_params=telemetry.get_statement_params(
                         base.PROJECT, base.SUBPROJECT, self.__class__.__name__
                     )
                 )
                 if not unknown_pandas.empty:
                     msg = f"Found unknown categories during transform:\n{unknown_pandas.to_string()}"
                     raise ValueError(msg)
-
         if self.handle_unknown == "ignore" and not self.sparse:
-            all_output_cat_cols = []
-            for idx, _ in enumerate(self.input_cols):
-                output_col = self.output_cols[idx]
-                output_cat_cols = [x for x in transformed_dataset.columns if f"{output_col}_" in x]
-                all_output_cat_cols.extend(output_cat_cols)
-            transformed_dataset = transformed_dataset.na.fill(0, all_output_cat_cols)  # type: ignore[arg-type]
+            transformed_dataset = transformed_dataset.na.fill(0, self._inferred_output_cols)
 
         # TODO(hayu): [SNOW-752263] Support OneHotEncoder handle_unknown="infrequent_if_exist".
         #  Add back when `handle_unknown="infrequent_if_exist"` is supported.
         # if self.handle_unknown == "infrequent_if_exist" and not self.sparse:
         #     all_output_freq_cols = []
         #     all_output_infreq_cols = []
         #     for idx, _ in enumerate(self.input_cols):
@@ -1325,33 +1349,33 @@
     def get_output_cols(self) -> List[str]:
         """
         Output columns getter.
 
         Returns:
             Output columns.
         """
-        if self.sparse:
-            return self.output_cols
+        return self._inferred_output_cols
 
-        output_cols = (
-            [
-                identifier.get_inferred_name(col)
-                for input_col in self.input_cols
-                for col in self._dense_output_cols_mappings[input_col]
-            ]
-            if self._dense_output_cols_mappings
-            else []
-        )
-        return output_cols
-
-    def _get_dense_output_cols_mappings(self) -> None:
+    def _get_inferred_output_cols(self) -> List[str]:
         """
-        Get input column to dense output columns mappings and assign them to
-        `self._dense_output_cols_mappings`.
+        Get output column names meeting Snowflake requirements.
+        Only useful when fitting a pandas dataframe.
+
+        Returns:
+            Inferred output columns.
         """
+        cols = (
+            self.output_cols
+            if self.sparse
+            else [col for input_col in self.input_cols for col in self._dense_output_cols_mappings[input_col]]
+        )
+        return [identifier.get_inferred_name(c) for c in cols]
+
+    def _handle_dense_output_cols(self) -> None:
+        """Assign input column to dense output columns mappings to `self._dense_output_cols_mappings`."""
         for idx, input_col in enumerate(self.input_cols):
             output_col = self.output_cols[idx]
             n_features_out = self._n_features_outs[idx]
             self._dense_output_cols_mappings[input_col] = []
 
             # whether there are infrequent categories in the input column
             has_infrequent_categories = self._infrequent_enabled and self.infrequent_categories_[idx] is not None
@@ -1389,14 +1413,30 @@
                         cat = self.categories_[input_col][cat_idx]
                 else:
                     cat = self.categories_[input_col][orig_encoding]
                 if cat and isinstance(cat, str):
                     cat = cat.replace('"', "'")
                 self._dense_output_cols_mappings[input_col].append(f"{output_col}_{cat}")
 
+    def _handle_inferred_output_cols(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> None:
+        """
+        Assign output column names used to transform pandas dataframes to `self._inferred_output_cols`.
+        This ensures consistent (double quoted) column names in Snowpark and pandas transformed dataframes.
+
+        Args:
+            dataset: Input dataset.
+        """
+        if isinstance(dataset, snowpark.DataFrame):
+            temp = self.handle_unknown
+            self.handle_unknown = "ignore"
+            self.transform(dataset[self.input_cols].limit(0))
+            self.handle_unknown = temp
+        else:
+            self._inferred_output_cols = self._get_inferred_output_cols()
+
     def get_sklearn_args(
         self,
         default_sklearn_obj: Optional[object] = None,
         sklearn_initial_keywords: Optional[Union[str, Iterable[str]]] = None,
         sklearn_unused_keywords: Optional[Union[str, Iterable[str]]] = None,
         snowml_only_keywords: Optional[Union[str, Iterable[str]]] = None,
         sklearn_added_keyword_to_version_dict: Optional[Dict[str, str]] = None,
```

## snowflake/ml/modeling/preprocessing/ordinal_encoder.py

```diff
@@ -263,35 +263,32 @@
         # states of categories found in dataset
         found_state_df: Optional[snowpark.DataFrame] = None
         for input_col in self.input_cols:
             distinct_dataset = dataset[[input_col]].distinct()
 
             # encode non-missing categories
             encoded_value_columns = [
-                F.lit(input_col).alias(_COLUMN_NAME),  # type: ignore[arg-type]
+                F.lit(input_col).alias(_COLUMN_NAME),
                 F.col(input_col).alias(_CATEGORY),
-                (
-                    F.dense_rank().over(snowpark.Window.order_by(input_col))  # type: ignore[arg-type]
-                    - 1  # type: ignore[operator]
-                )
+                (F.dense_rank().over(snowpark.Window.order_by(input_col)) - 1)
                 .cast(T.FloatType())
                 .alias(_INDEX),  # index categories
             ]
             encoded_value_df = (
                 distinct_dataset.filter(F.col(input_col).is_not_null())
                 .sort(F.col(input_col).asc())
                 .select(encoded_value_columns)
             )
 
             # encode missing categories
             encoded_missing_value_columns = [
-                F.lit(input_col).alias(_COLUMN_NAME),  # type: ignore[arg-type]
+                F.lit(input_col).alias(_COLUMN_NAME),
                 F.col(input_col).alias(_CATEGORY),
                 # index missing categories
-                F.lit(self.encoded_missing_value).alias(_INDEX),  # type: ignore[arg-type]
+                F.lit(self.encoded_missing_value).alias(_INDEX),
             ]
             encoded_missing_value_df = distinct_dataset.filter(F.col(input_col).is_null()).select(
                 encoded_missing_value_columns
             )
 
             all_encoded_value_df = encoded_value_df.union(encoded_missing_value_df)
             found_state_df = (
@@ -439,19 +436,17 @@
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
 
         Raises:
-            RuntimeError: If transformer is not fitted first.
             TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
-        if not self._is_fitted:
-            raise RuntimeError("Transformer not fitted before calling transform().")
+        self._enforce_fit()
         super()._check_input_cols()
         super()._check_output_cols()
 
         if isinstance(dataset, snowpark.DataFrame):
             output_df = self._transform_snowpark(dataset)
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._transform_sklearn(dataset)
@@ -483,15 +478,15 @@
                 telemetry.get_statement_params(base.PROJECT, base.SUBPROJECT, self.__class__.__name__),
             )
             else dataset._session.create_dataframe(self._state_pandas)
         )
 
         # replace NULL with nan
         null_category_state_df = state_df.filter(F.col(_CATEGORY).is_null()).with_column(
-            _INDEX, F.lit(self.encoded_missing_value)  # type: ignore[arg-type]
+            _INDEX, F.lit(self.encoded_missing_value)
         )
         state_df = state_df.filter(F.col(_CATEGORY).is_not_null()).union_by_name(null_category_state_df)
 
         suffix = "_" + uuid.uuid4().hex.upper()
         transformed_dataset = dataset
 
         for idx, input_col in enumerate(self.input_cols):
@@ -598,15 +593,15 @@
         if self.handle_unknown == "error":
             # dataframe with unknown values
             # columns: COLUMN_NAME, UNKNOWN_VALUE
             unknown_df: Optional[snowpark.DataFrame] = None
             for idx, input_col in enumerate(self.input_cols):
                 output_col = self.output_cols[idx]
                 unknown_columns = [
-                    F.lit(input_col),  # type: ignore[arg-type]
+                    F.lit(input_col),
                     F.col(input_col),
                 ]
                 temp_df = (
                     transformed_dataset[list({input_col, output_col})]
                     .distinct()
                     .filter(F.col(output_col).is_null())
                     .select(unknown_columns)
@@ -623,12 +618,10 @@
             if not unknown_pandas.empty:
                 msg = f"Found unknown categories during transform:\n{unknown_pandas.to_string()}"
                 raise ValueError(msg)
 
         if self.handle_unknown == "use_encoded_value":
             # left outer join has already filled unknown values with null
             if not (self.unknown_value is None or sklearn_utils.is_scalar_nan(self.unknown_value)):
-                transformed_dataset = transformed_dataset.na.fill(
-                    self.unknown_value, self.output_cols  # type: ignore[arg-type]
-                )
+                transformed_dataset = transformed_dataset.na.fill(self.unknown_value, self.output_cols)
 
         return transformed_dataset
```

## snowflake/ml/modeling/preprocessing/polynomial_features.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.preprocessing".replace("sklearn.", "").split("_")])
 
@@ -307,33 +307,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -363,15 +359,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -379,17 +375,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -438,15 +436,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1060,16 +1058,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1077,18 +1075,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1115,30 +1109,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1172,15 +1168,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1194,15 +1190,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/preprocessing/robust_scaler.py

```diff
@@ -8,14 +8,15 @@
 import pandas as pd
 from scipy import stats
 from sklearn import preprocessing
 from sklearn.preprocessing import _data as sklearn_preprocessing_data
 
 from snowflake import snowpark
 from snowflake.ml._internal import telemetry
+from snowflake.ml._internal.exceptions import error_codes, exceptions
 from snowflake.ml.modeling.framework import _utils, base
 
 
 class RobustScaler(base.BaseTransformer):
     r"""Scales features using statistics that are robust to outliers. Values must be of float type.
 
     For more details on what this transformer does, see [sklearn.preprocessing.RobustScaler]
@@ -125,30 +126,23 @@
         Compute center, scale and quantile values of the dataset.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Return self as fitted scaler.
-
-        Raises:
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
         super()._check_input_cols()
+        super()._check_dataset_type(dataset)
         self._reset()
 
         if isinstance(dataset, pd.DataFrame):
             self._fit_sklearn(dataset)
-        elif isinstance(dataset, snowpark.DataFrame):
-            self._fit_snowpark(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            self._fit_snowpark(dataset)
 
         self._is_fitted = True
         self._state_is_set = True
         return self
 
     def _fit_sklearn(self, dataset: pd.DataFrame) -> None:
         dataset = self._use_input_cols_only(dataset)
@@ -162,15 +156,18 @@
                 self._scale[input_col] = float(sklearn_scaler.scale_[i])
 
     def _fit_snowpark(self, dataset: snowpark.DataFrame) -> None:
         computed_states = self._compute(dataset, self.input_cols, self.custom_states)
 
         q_min, q_max = self.quantile_range
         if not 0 <= q_min <= q_max <= 100:
-            raise ValueError("Invalid quantile range: %s" % str(self.quantile_range))
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=ValueError("Invalid quantile range: %s" % str(self.quantile_range)),
+            )
 
         pcont_left = self.custom_states[1]
         pcont_right = self.custom_states[2]
 
         for input_col in self.input_cols:
             numeric_stats = computed_states[input_col]
             if self.with_centering:
@@ -202,33 +199,24 @@
         Center and scale the data.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
-
-        Raises:
-            RuntimeError: If transformer is not fitted first.
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
-        if not self._is_fitted:
-            raise RuntimeError("Transformer not fitted before calling transform().")
+        self._enforce_fit()
         super()._check_input_cols()
         super()._check_output_cols()
+        super()._check_dataset_type(dataset)
 
         if isinstance(dataset, snowpark.DataFrame):
             output_df = self._transform_snowpark(dataset)
-        elif isinstance(dataset, pd.DataFrame):
-            output_df = self._transform_sklearn(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            output_df = self._transform_sklearn(dataset)
 
         return self._drop_input_columns(output_df) if self._drop_input_cols is True else output_df
 
     def _transform_snowpark(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         """
         Center and scale the data on snowflake DataFrame.
 
@@ -264,19 +252,13 @@
 
     def _create_sklearn_object(self) -> preprocessing.RobustScaler:
         """
         Get an equivalent sklearn RobustScaler.
 
         Returns:
             Sklearn RobustScaler.
-
-        Raises:
-            RuntimeError: If transformer is not fitted first.
         """
-        if self.scale_ is None or self.center_ is None:
-            raise RuntimeError("Transformer not fitted before calling transform().")
-
         scaler = self._create_unfitted_sklearn_object()
         if self._is_fitted:
             scaler.scale_ = self._convert_attribute_dict_to_ndarray(self.scale_, np.float64)
             scaler.center_ = self._convert_attribute_dict_to_ndarray(self.center_, np.float64)
         return scaler
```

## snowflake/ml/modeling/preprocessing/standard_scaler.py

```diff
@@ -116,30 +116,23 @@
         Compute mean and std values of the dataset.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Fitted scaler.
-
-        Raises:
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
         super()._check_input_cols()
+        super()._check_dataset_type(dataset)
         self._reset()
 
         if isinstance(dataset, pd.DataFrame):
             self._fit_sklearn(dataset)
-        elif isinstance(dataset, snowpark.DataFrame):
-            self._fit_snowpark(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            self._fit_snowpark(dataset)
 
         self._is_fitted = True
         return self
 
     def _fit_sklearn(self, dataset: pd.DataFrame) -> None:
         dataset = self._use_input_cols_only(dataset)
         sklearn_scaler = self._create_unfitted_sklearn_object()
@@ -184,33 +177,24 @@
         Perform standardization by centering and scaling.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             transformed_dataset: Output dataset.
-
-        Raises:
-            RuntimeError: If transformer is not fitted first.
-            TypeError: If the input dataset is neither a pandas nor Snowpark DataFrame.
         """
-        if not self._is_fitted:
-            raise RuntimeError("Transformer not fitted before calling transform().")
+        self._enforce_fit()
         super()._check_input_cols()
         super()._check_output_cols()
+        super()._check_dataset_type(dataset)
 
         if isinstance(dataset, snowpark.DataFrame):
             output_df = self._transform_snowpark(dataset)
-        elif isinstance(dataset, pd.DataFrame):
-            output_df = self._transform_sklearn(dataset)
         else:
-            raise TypeError(
-                f"Unexpected dataset type: {type(dataset)}."
-                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
-            )
+            output_df = self._transform_sklearn(dataset)
 
         return self._drop_input_columns(output_df) if self._drop_input_cols is True else output_df
 
     def _transform_snowpark(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         """
         Perform standardization by centering and scaling on
         Snowpark dataframe.
@@ -244,14 +228,13 @@
     def _create_sklearn_object(self) -> preprocessing.StandardScaler:
         """
         Get an equivalent sklearn StandardScaler.
 
         Returns:
             The Sklearn StandardScaler.
         """
-
         scaler = self._create_unfitted_sklearn_object()
         if self._is_fitted:
             scaler.scale_ = self._convert_attribute_dict_to_ndarray(self.scale_, np.float64)
             scaler.mean_ = self._convert_attribute_dict_to_ndarray(self.mean_, np.float64)
             scaler.var_ = self._convert_attribute_dict_to_ndarray(self.var_, np.float64)
         return scaler
```

## snowflake/ml/modeling/semi_supervised/label_propagation.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.semi_supervised".replace("sklearn.", "").split("_")])
 
@@ -311,33 +311,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -367,15 +363,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -383,17 +379,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -442,15 +440,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1068,16 +1066,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1085,18 +1083,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1123,30 +1117,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1180,15 +1176,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1202,15 +1198,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/semi_supervised/label_spreading.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.semi_supervised".replace("sklearn.", "").split("_")])
 
@@ -320,33 +320,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -376,15 +372,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -392,17 +388,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -451,15 +449,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1077,16 +1075,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1094,18 +1092,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1132,30 +1126,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1189,15 +1185,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1211,15 +1207,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/svm/linear_svc.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.svm".replace("sklearn.", "").split("_")])
 
@@ -371,33 +371,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -427,15 +423,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -443,17 +439,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -502,15 +500,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1126,16 +1124,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1143,18 +1141,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1181,30 +1175,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1238,15 +1234,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1260,15 +1256,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/svm/linear_svr.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.svm".replace("sklearn.", "").split("_")])
 
@@ -344,33 +344,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -400,15 +396,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -416,17 +412,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -475,15 +473,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1097,16 +1095,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1114,18 +1112,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1152,30 +1146,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1209,15 +1205,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1231,15 +1227,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/svm/nu_svc.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.svm".replace("sklearn.", "").split("_")])
 
@@ -382,33 +382,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -438,15 +434,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -454,17 +450,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -513,15 +511,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1141,16 +1139,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1158,18 +1156,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1196,30 +1190,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1253,15 +1249,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1275,15 +1271,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/svm/nu_svr.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.svm".replace("sklearn.", "").split("_")])
 
@@ -343,33 +343,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -399,15 +395,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -415,17 +411,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -474,15 +472,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1096,16 +1094,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1113,18 +1111,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1151,30 +1145,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1208,15 +1204,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1230,15 +1226,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/svm/svc.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.svm".replace("sklearn.", "").split("_")])
 
@@ -385,33 +385,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -441,15 +437,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -457,17 +453,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -516,15 +514,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1144,16 +1142,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1161,18 +1159,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1199,30 +1193,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1256,15 +1252,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1278,15 +1274,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/svm/svr.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.svm".replace("sklearn.", "").split("_")])
 
@@ -346,33 +346,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -402,15 +398,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -418,17 +414,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -477,15 +475,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1099,16 +1097,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1116,18 +1114,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1154,30 +1148,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1211,15 +1207,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1233,15 +1229,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/tree/decision_tree_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.tree".replace("sklearn.", "").split("_")])
 
@@ -414,33 +414,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -470,15 +466,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -486,17 +482,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -545,15 +543,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1171,16 +1169,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1188,18 +1186,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1226,30 +1220,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1283,15 +1279,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1305,15 +1301,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/tree/decision_tree_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.tree".replace("sklearn.", "").split("_")])
 
@@ -396,33 +396,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -452,15 +448,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -468,17 +464,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -527,15 +525,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1149,16 +1147,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1166,18 +1164,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1204,30 +1198,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1261,15 +1257,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1283,15 +1279,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/tree/extra_tree_classifier.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.tree".replace("sklearn.", "").split("_")])
 
@@ -406,33 +406,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -462,15 +458,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -478,17 +474,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -537,15 +535,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1163,16 +1161,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1180,18 +1178,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1218,30 +1212,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1275,15 +1271,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1297,15 +1293,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/tree/extra_tree_regressor.py

```diff
@@ -31,16 +31,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.tree".replace("sklearn.", "").split("_")])
 
@@ -388,33 +388,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -444,15 +440,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -460,17 +456,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -519,15 +517,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1141,16 +1139,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1158,18 +1156,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1196,30 +1190,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import sklearn
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1253,15 +1249,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1275,15 +1271,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/xgboost/xgb_classifier.py

```diff
@@ -30,16 +30,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "xgboost".replace("sklearn.", "").split("_")])
 
@@ -488,33 +488,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -544,15 +540,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -560,17 +556,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import xgboost
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -619,15 +617,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1245,16 +1243,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1262,18 +1260,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1300,30 +1294,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import xgboost
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1357,15 +1353,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1379,15 +1375,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/xgboost/xgb_regressor.py

```diff
@@ -30,16 +30,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "xgboost".replace("sklearn.", "").split("_")])
 
@@ -487,33 +487,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -543,15 +539,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -559,17 +555,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import xgboost
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -618,15 +616,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1240,16 +1238,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1257,18 +1255,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1295,30 +1289,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import xgboost
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1352,15 +1348,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1374,15 +1370,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/xgboost/xgbrf_classifier.py

```diff
@@ -30,16 +30,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "xgboost".replace("sklearn.", "").split("_")])
 
@@ -492,33 +492,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -548,15 +544,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -564,17 +560,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import xgboost
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -623,15 +621,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1249,16 +1247,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1266,18 +1264,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1304,30 +1298,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import xgboost
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1361,15 +1357,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1383,15 +1379,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/modeling/xgboost/xgbrf_regressor.py

```diff
@@ -30,16 +30,16 @@
 from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
-    _rename_features,
 )
+from snowflake.ml.model._signatures import utils as model_signature_utils
 
 _PROJECT = "ModelDevelopment"
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "xgboost".replace("sklearn.", "").split("_")])
 
@@ -492,33 +492,29 @@
             self.label_cols +
             [self.sample_weight_col] if self.sample_weight_col is not None else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
         # Extract query that generated the datafrome. We will need to pass it to the fit procedure.
-        query = str(dataset.queries["queries"][0])
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
         transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
 
@@ -548,15 +544,15 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> str:
@@ -564,17 +560,19 @@
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import xgboost
 
-            # Execute snowpark query and obtain the results as pandas dataframe
+            # Execute snowpark queries and obtain the results as pandas dataframe
             # NB: this implies that the result data must fit into memory.
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_transform_file = tempfile.NamedTemporaryFile(delete=True)
             local_transform_file_name = local_transform_file.name
             local_transform_file.close()
 
             session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
 
@@ -623,15 +621,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         sproc_export_file_name = fit_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
@@ -1245,16 +1243,16 @@
             self.input_cols + self.label_cols + [self.sample_weight_col]
             if self.sample_weight_col is not None
             else []
         )
         if len(selected_cols) > 0:
             dataset = dataset.select(selected_cols)
 
-        # Extract query that generated the dataframe. We will need to pass it to score procedure.
-        query = str(dataset.queries["queries"][0])
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        queries = dataset.queries["queries"]
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
@@ -1262,18 +1260,14 @@
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
-        ).has_value_match(
-            row_idx=0,
-            col_idx=0,
-            expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
         # Use posixpath to construct stage paths
         stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
         score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
@@ -1300,30 +1294,32 @@
             replace=True,
             session=session,
             statement_params=statement_params,
             anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
-            sql_query: str,
+            sql_queries: List[str],
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
             sample_weight_col: Optional[str],
             statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
             import xgboost
 
-            df = session.sql(sql_query).to_pandas(statement_params=statement_params)
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            df = session.sql(sql_queries[-1]).to_pandas(statement_params=statement_params)
 
             local_score_file = tempfile.NamedTemporaryFile(delete=True)
             local_score_file_name = local_score_file.name
             local_score_file.close()
 
             session.file.get(stage_score_file_name, local_score_file_name, statement_params=statement_params)
 
@@ -1357,15 +1353,15 @@
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
         score = score_wrapper_sproc(
             session,
-            query,
+            queries,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
             statement_params,
         )
 
@@ -1379,15 +1375,15 @@
         PROB_FUNCTIONS = ["predict_log_proba", "predict_proba", "decision_function"]
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
-                outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
+                outputs = model_signature_utils.rename_features(outputs, self.output_cols)  # rename the output columns
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
                 self._model_signature_dict["predict"] = ModelSignature(inputs, 
                                                                        ([] if self._drop_input_cols else inputs) + outputs)
```

## snowflake/ml/registry/model_registry.py

```diff
@@ -2,15 +2,15 @@
 import json
 import os
 import posixpath
 import sys
 import tempfile
 import types
 import zipfile
-from typing import TYPE_CHECKING, Any, Dict, List, Optional, cast
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union, cast
 from uuid import uuid1
 
 from absl import logging
 
 from snowflake import connector, snowpark
 from snowflake.ml._internal import file_utils, telemetry
 from snowflake.ml._internal.utils import (
@@ -610,15 +610,17 @@
         *,
         id: str,
         name: str,
         platform: str,
         stage_path: str,
         signature: Dict[str, Any],
         target_method: str,
-        options: Optional[model_types.WarehouseDeployOptions] = None,
+        options: Optional[
+            Union[model_types.WarehouseDeployOptions, model_types.SnowparkContainerServiceDeployOptions]
+        ] = None,
     ) -> List[snowpark.Row]:
         """Insert a new row into the model deployment table.
 
         Each row in the deployment table is a deployment event.
 
         Args:
             id: Model id of the deployed model.
@@ -656,17 +658,18 @@
 
         Returns:
             Path to the stage that was created.
         """
         schema = self._fully_qualified_schema_name()
         fully_qualified_deployment_stage_name = f"{schema}.{self._permanent_deployment_stage}"
         statement_params = self._get_statement_params(inspect.currentframe())
-        self._session.sql(f"CREATE STAGE IF NOT EXISTS {fully_qualified_deployment_stage_name}").collect(
-            statement_params=statement_params
-        )
+        self._session.sql(
+            f"CREATE STAGE IF NOT EXISTS {fully_qualified_deployment_stage_name} "
+            f"ENCRYPTION = (TYPE= 'SNOWFLAKE_SSE')"
+        ).collect(statement_params=statement_params)
         return f"@{fully_qualified_deployment_stage_name}"
 
     def _prepare_model_stage(self, model_id: str) -> str:
         """Create a stage in the model registry for storing the model with the given id.
 
         Creating a permanent stage here since we do not have a way to swtich a stage from temporary to permanent.
         This can result in orphaned stages in case the process fails. It might be better to try to create a
@@ -687,31 +690,25 @@
         # Uppercasing the model_stage_name to avoid having to quote the the stage name.
         stage_name = model_id.upper()
 
         model_stage_name = f"SNOWML_MODEL_{stage_name}"
         fully_qualified_model_stage_name = f"{schema}.{model_stage_name}"
         statement_params = self._get_statement_params(inspect.currentframe())
 
-        create_stage_result = self._session.sql(f"CREATE OR REPLACE STAGE {fully_qualified_model_stage_name}").collect(
-            statement_params=statement_params
-        )
+        create_stage_result = self._session.sql(
+            f"CREATE OR REPLACE STAGE {fully_qualified_model_stage_name} ENCRYPTION = (TYPE= 'SNOWFLAKE_SSE')"
+        ).collect(statement_params=statement_params)
         if not create_stage_result:
             raise connector.DatabaseError("Unable to create stage for model. Operation returned not result.")
         if len(create_stage_result) != 1:
             raise connector.DatabaseError(
                 "Unable to create stage for model. Creating the model stage returned unexpected result: {}.".format(
                     str(create_stage_result)
                 )
             )
-        if create_stage_result[0]["status"] != f"Stage area {model_stage_name} successfully created.":
-            raise connector.DatabaseError(
-                "Unable to create stage for model. Return status of operation was: {}".format(
-                    create_stage_result[0]["status"]
-                )
-            )
 
         return fully_qualified_model_stage_name
 
     def _get_fully_qualified_stage_name_from_uri(self, model_uri: str) -> Optional[str]:
         """Get fully qualified stage path pointed by the URI.
 
         Args:
@@ -1467,15 +1464,15 @@
         description: Optional[str] = None,
         tags: Optional[Dict[str, str]] = None,
         conda_dependencies: Optional[List[str]] = None,
         pip_requirements: Optional[List[str]] = None,
         signatures: Optional[Dict[str, model_signature.ModelSignature]] = None,
         sample_input_data: Optional[Any] = None,
         code_paths: Optional[List[str]] = None,
-        options: Optional[model_types.ModelSaveOption] = None,
+        options: Optional[model_types.BaseModelSaveOption] = None,
     ) -> str:
         """Uploads and register a model to the Model Registry.
 
         Args:
             model_name: The given name for the model. The combination (name + version) must be unique for each model.
             model_version: Version string to be set for the model. The combination (name + version) must be unique for
                 each model.
@@ -1597,59 +1594,88 @@
         self,
         model_name: str,
         model_version: str,
         *,
         deployment_name: str,
         target_method: str,
         permanent: bool = False,
-        options: Optional[model_types.WarehouseDeployOptions] = None,
+        platform: _deployer.TargetPlatform = _deployer.TargetPlatform.WAREHOUSE,
+        options: Optional[
+            Union[model_types.WarehouseDeployOptions, model_types.SnowparkContainerServiceDeployOptions]
+        ] = None,
     ) -> None:
-        """Deploy the model with the the given deployment name.
+        """Deploy the model with the given deployment name.
 
         Args:
             model_name: Model Name string.
             model_version: Model Version string.
             deployment_name: name of the generated UDF.
             target_method: The method name to use in deployment.
             permanent: Whether the deployment is permanent or not. Permanent deployment will generate a permanent UDF.
+                (Only applicable for Warehouse deployment)
+            platform: Target platform to deploy the model to. Currently supported platforms are
+                ['warehouse', 'snowpark_container_service']
             options: Optional options for model deployment. Defaults to None.
-                The following keys are acceptable:
-                - "output_with_input_features": Whether or not preserve the input columns in the output when predicting.
-                    Defaults to False.
-                - "keep_order": Whether or not preserve the row order when predicting. Only available for dataframe has
-                    fewer than 2**64 rows. Defaults to True.
-                - "permanent_udf_stage_location": Customized Snowflake stage option where the UDF should be persisted.
-                - "relax_version": Whether or not relax the version constraints of the dependencies if unresolvable.
-                    Defaults to False.
+
+        Raises:
+            RuntimeError: Raised when parameters are not properly enabled when deploying to Warehouse with temporary UDF
         """
         if options is None:
             options = {}
 
         deployment_stage_path = ""
 
-        if permanent:
-            # Every deployment-generated UDF should reside in its own unique directory. As long as each deployment
-            # is allocated a distinct directory, multiple deployments can coexist within the same stage.
-            # Given that each permanent deployment possesses a unique deployment_name, sharing the same stage doesn't
-            # present any issues
-            deployment_stage_path = (
-                options.get("permanent_udf_stage_location") or f"{self._prepare_deployment_stage()}/{deployment_name}/"
-            )
-            options["permanent_udf_stage_location"] = deployment_stage_path
+        if platform == _deployer.TargetPlatform.SNOWPARK_CONTAINER_SERVICE:
+            permanent = True
+            options = cast(model_types.SnowparkContainerServiceDeployOptions, options)
+            deployment_stage_path = f"{self._prepare_deployment_stage()}/{deployment_name}/"
+        elif platform == _deployer.TargetPlatform.WAREHOUSE:
+            options = cast(model_types.WarehouseDeployOptions, options)
+            if permanent:
+                # Every deployment-generated UDF should reside in its own unique directory. As long as each deployment
+                # is allocated a distinct directory, multiple deployments can coexist within the same stage.
+                # Given that each permanent deployment possesses a unique deployment_name, sharing the same stage does
+                # not present any issues
+                deployment_stage_path = (
+                    options.get("permanent_udf_stage_location")
+                    or f"{self._prepare_deployment_stage()}/{deployment_name}/"
+                )
+                options["permanent_udf_stage_location"] = deployment_stage_path
 
         remote_model_path = "@" + self._get_model_path(model_name=model_name, model_version=model_version)
         model_id = self._get_model_id(model_name, model_version)
 
+        # https://snowflakecomputing.atlassian.net/browse/SNOW-858376
+        # During temporary deployment on the Warehouse, Snowpark creates an unencrypted temporary stage for UDF-related
+        # artifacts. However, UDF generation fails when importing from a mix of encrypted and unencrypted stages.
+        # The following workaround copies model between stages (PrPr as of July 7th, 2023) to transfer the SSE
+        # encrypted model zip from model stage to the temporary unencrypted stage.
+        if not permanent and platform == _deployer.TargetPlatform.WAREHOUSE:
+            schema = self._fully_qualified_schema_name()
+            unencrypted_stage = f"@{schema}.TEMP_UNENCRYPTED_{self._get_new_unique_identifier()}"
+            self._session.sql(f"CREATE TEMPORARY STAGE {unencrypted_stage[1:]}").collect()
+            try:
+                self._session.sql(f"COPY FILES INTO {unencrypted_stage} from {remote_model_path}").collect()
+            except Exception:
+                raise RuntimeError(
+                    "Please ensure parameters are enabled in your Snowflake account by running "
+                    "'ALTER ACCOUNT <ACCOUNT_NAME> SET ENABLE_COPY_FILES=TRUE, "
+                    "ENABLE_COPY_FILES_API_IN_STORAGE=TRUE'"
+                )
+            remote_model_path = f"{unencrypted_stage}/{os.path.basename(remote_model_path)}"
+
         # Step 1: Deploy to get the UDF
         deployment_info = _deployer.deploy(
             session=self._session,
             name=self._fully_qualified_deployment_name(deployment_name),
-            platform=_deployer.TargetPlatform.WAREHOUSE,
+            platform=platform,
             target_method=target_method,
             model_stage_file_path=remote_model_path,
+            deployment_stage_path=deployment_stage_path,
+            model_id=model_id,
             options=options,
         )
 
         # Step 2: Record the deployment
 
         # Assert to convince mypy.
         assert deployment_info
@@ -1835,16 +1861,16 @@
             self._session, f"DELETE FROM {self._fully_qualified_registry_table_name()} WHERE ID='{id}'"
         ).deletion_success(expected_num_rows=1).validate()
 
         # Step 2/3: Delete the artifact (if desired).
         if delete_artifact:
             if uri.is_snowflake_stage_uri(model_uri):
                 stage_path = self._get_fully_qualified_stage_name_from_uri(model_uri)
-                query_result_checker.SqlResultValidator(self._session, f"DROP STAGE {stage_path}").has_value_match(
-                    row_idx=0, col_idx=0, expected_value="successfully dropped."
+                query_result_checker.SqlResultValidator(self._session, f"DROP STAGE {stage_path}").has_dimensions(
+                    expected_rows=1, expected_cols=1
                 ).validate()
 
         # Step 3/3: Record the deletion event.
         self._set_metadata_attribute(
             id=id,
             attribute=_METADATA_ATTRIBUTE_DELETION,
             value={"delete_artifact": True, "URI": model_uri},
@@ -2025,15 +2051,21 @@
             # Mypy enforce to refer to the registry for calling the function
             deployment = self._registry.get_deployment(
                 self._model_name, self._model_version, deployment_name=deployment_name
             ).collect()[0]
             platform = _deployer.TargetPlatform(deployment["TARGET_PLATFORM"])
             signature = model_signature.ModelSignature.from_dict(json.loads(deployment["SIGNATURE"]))
             options_dict = cast(Dict[str, Any], json.loads(deployment["OPTIONS"]))
-            options = model_types.WarehouseDeployOptions(options_dict)  # type: ignore
+            platform_options = {
+                _deployer.TargetPlatform.WAREHOUSE: model_types.WarehouseDeployOptions,
+                _deployer.TargetPlatform.SNOWPARK_CONTAINER_SERVICE: model_types.SnowparkContainerServiceDeployOptions,
+            }
+            if platform not in platform_options:
+                raise ValueError(f"Unsupported target Platform: {platform}")
+            options = platform_options[platform](options_dict)
             di = _deployer.Deployment(
                 name=self._registry._fully_qualified_deployment_name(deployment_name),
                 platform=platform,
                 signature=signature,
                 options=options,
             )
             return _deployer.predict(session=self._registry._session, deployment=di, X=data)
```

## snowflake/ml/version.py

```diff
@@ -1 +1 @@
-VERSION="1.0.3"
+VERSION="1.0.4"
```

## Comparing `snowflake/ml/fileset/fileset_errors.py` & `snowflake/ml/_internal/exceptions/fileset_errors.py`

 * *Files identical despite different names*

## Comparing `snowflake_ml_python-1.0.3.dist-info/METADATA` & `snowflake_ml_python-1.0.4.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -36,71 +36,92 @@
 Requires-Dist: numpy>=1.23,<2
 Requires-Dist: packaging>=20.9,<24
 Requires-Dist: pandas>=1.0.0,<2
 Requires-Dist: pyyaml>=6.0,<7
 Requires-Dist: scikit-learn>=1.2.1,<1.3
 Requires-Dist: scipy>=1.9,<2
 Requires-Dist: snowflake-connector-python[pandas]>=3.0.3,<4
-Requires-Dist: snowflake-snowpark-python>=1.4.0,<2
+Requires-Dist: snowflake-snowpark-python>=1.5.1,<2
 Requires-Dist: sqlparse>=0.4,<1
 Requires-Dist: typing-extensions>=4.1.0,<5
 Requires-Dist: xgboost>=1.7.3,<2
 Provides-Extra: all
 Requires-Dist: lightgbm==3.3.5; extra == 'all'
+Requires-Dist: mlflow>=2.1.0,<3; extra == 'all'
 Requires-Dist: tensorflow>=2.9,<3; extra == 'all'
 Requires-Dist: torchdata>=0.4,<1; extra == 'all'
 Provides-Extra: lightgbm
 Requires-Dist: lightgbm==3.3.5; extra == 'lightgbm'
+Provides-Extra: mlflow
+Requires-Dist: mlflow>=2.1.0,<3; extra == 'mlflow'
 Provides-Extra: tensorflow
 Requires-Dist: tensorflow>=2.9,<3; extra == 'tensorflow'
 Provides-Extra: torch
 Requires-Dist: torchdata>=0.4,<1; extra == 'torch'
-Version: 1.0.3
+Version: 1.0.4
 
 # Snowpark ML
 
 Snowpark ML is a set of tools including SDKs and underlying infrastructure to build and deploy machine learning models. With Snowpark ML, you can pre-process data, train, manage and deploy ML models all within Snowflake, using a single SDK, and benefit from Snowflakes proven performance, scalability, stability and governance at every stage of the Machine Learning workflow.
 
 ## Key Components of Snowpark ML
+
 The Snowpark ML Python SDK provides a number of APIs to support each stage of an end-to-end Machine Learning development and deployment process, and includes two key components.
 
 ### Snowpark ML Development [Public Preview]
 
 A collection of python APIs to enable efficient model development directly in Snowflake:
 
 1. Modeling API (snowflake.ml.modeling) for data preprocessing, feature engineering and model training in Snowflake. This includes snowflake.ml.modeling.preprocessing for scalable data transformations on large data sets utilizing the compute resources of underlying Snowpark Optimized High Memory Warehouses, and a large collection of ML model development classes based on sklearn, xgboost, and lightgbm. See the private preview limited access docs (Preprocessing, Modeling for more details on these.
 
 1. Framework Connectors: Optimized, secure and performant data provisioning for Pytorch and Tensorflow frameworks in their native data loader formats.
 
 ### Snowpark ML Ops [Private Preview]
 
 Snowpark MLOps complements the Snowpark ML Development API, and provides model management capabilities along with integrated deployment into Snowflake. Currently, the API consists of
+
 1. FileSet API: FileSet provides a Python fsspec-compliant API for materializing data into a Snowflake internal stage from a query or Snowpark Dataframe along with a number of convenience APIs.
 
 1. Model Registry: A python API for managing models within Snowflake which also supports deployment of ML models into Snowflake Warehouses as vectorized UDFs.
 
 During PrPr, we are iterating on API without backward compatibility guarantees. It is better to recreate your registry everytime you update the package. This means, at this time, you cannot use the registry for production use.
 
 - [Documentation](https://docs.snowflake.com/developer-guide/snowpark-ml)
 
 ## Getting started
+
 ### Have your Snowflake account ready
+
 If you don't have a Snowflake account yet, you can [sign up for a 30-day free trial account](https://signup.snowflake.com/).
 
 ### Create a Python virtual environment
-Python 3.8 is required. You can use [miniconda](https://docs.conda.io/en/latest/miniconda.html), [anaconda](https://www.anaconda.com/), or [virtualenv](https://docs.python.org/3/tutorial/venv.html) to create a Python 3.8 virtual environment.
+
+Python version 3.8, 3.9 & 3.10 are supported. You can use [miniconda](https://docs.conda.io/en/latest/miniconda.html), [anaconda](https://www.anaconda.com/), or [virtualenv](https://docs.python.org/3/tutorial/venv.html) to create a virtual environment.
 
 To have the best experience when using this library, [creating a local conda environment with the Snowflake channel](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-packages.html#local-development-and-testing) is recommended.
 
 ### Install the library to the Python virtual environment
+
 ```
 pip install snowflake-ml-python
 ```
 # Release History
 
+## 1.0.4
+
+### New Features
+- Model Registry: Added support save/load/deploy Tensorflow models (`tensorflow.Module`).
+- Model Registry: Added support save/load/deploy MLFlow PyFunc models (`mlflow.pyfunc.PyFuncModel`).
+- Model Development: Input dataframes can now be joined against data loaded from staged files.
+- Model Development: Added support for non-English languages.
+
+### Bug Fixes
+
+- Model Registry: Fix an issue that model dependencies are incorrectly reported as unresolvable on certain platforms.
+
 ## 1.0.3 (2023-07-14)
 
 ### Behavior Changes
 - Model Registry: When predicting a model whose output is a list of NumPy ndarray, the output would not be flattened, instead, every ndarray will act as a feature(column) in the output.
 
 ### New Features
 - Model Registry: Added support save/load/deploy PyTorch models (`torch.nn.Module` and `torch.jit.ScriptModule`).
```

## Comparing `snowflake_ml_python-1.0.3.dist-info/RECORD` & `snowflake_ml_python-1.0.4.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,259 +1,275 @@
 snowflake/ml/_internal/env.py,sha256=kCrJTRnqQ97VGUVI1cWUPD8HuBWeL5vOOtwUR0NB9Mg,161
-snowflake/ml/_internal/env_utils.py,sha256=9h9-UMX6l8qjz5tENbrJKcezwicoFMF7z2zhe72kJZg,14175
-snowflake/ml/_internal/file_utils.py,sha256=AWBSgyQjMJJB33GMKRXOVYIgbeYM4zSYTzG_oBSX05I,7502
+snowflake/ml/_internal/env_utils.py,sha256=y0pqRhW2SGG5R1rRWE0yaNmQULWFf8Mbl38NurYkfyI,13555
+snowflake/ml/_internal/exceptions/error_codes.py,sha256=Jl9ZVi3LKscLiBcSBKhFpuJ0Z9yR1WmcjSXYhDo7pB8,3200
+snowflake/ml/_internal/exceptions/error_messages.py,sha256=vF9XOWJoBuKvFxBkGcDelhXK1dipzTt-AdK4NkCbwTo,47
+snowflake/ml/_internal/exceptions/exceptions.py,sha256=ZBRMuIYMELomZk68fysXYvs-JcQd1Tnt6Lkk8Lc1QPo,1423
+snowflake/ml/_internal/exceptions/fileset_error_messages.py,sha256=dqPpRu0cKyQA_0gahvbizgQBTwNhnwveN286JrJLvi8,419
+snowflake/ml/_internal/exceptions/fileset_errors.py,sha256=ZJfkpeDgRIw3qA876fk9FIzxIrm-yZ8I9RXUbzaeM84,1040
+snowflake/ml/_internal/exceptions/modeling_error_messages.py,sha256=cWDJHjHst8P-gPTPOY2EYapjhlB9tUm159VPBxNYefc,466
+snowflake/ml/_internal/file_utils.py,sha256=YwEA_z1gix88kaixr_2GjIcv2_gnSMw_0E1PcEgZbb4,7682
 snowflake/ml/_internal/init_utils.py,sha256=U-oPOtyVf22hCwDH_CH2uDr9yuN6Mr3kwQ_yRAs1mcM,2696
-snowflake/ml/_internal/telemetry.py,sha256=pM1irUwe5-caRFP-fjUiYPTbTyjh0U-RwKyoat7pCU4,20145
+snowflake/ml/_internal/telemetry.py,sha256=6-EZf7mxscZXzyQ9QsnKihQcobGbSaOu1pQUGlFPero,20711
 snowflake/ml/_internal/type_utils.py,sha256=0AjimiQoAPHGnpLV_zCR6vlMR5lJ8CkZkKFwiUHYDCo,2168
 snowflake/ml/_internal/utils/formatting.py,sha256=pz3dFq11BzeHVcZugrU5lQOmPeBKmfkggEsTnDm8ggw,3678
 snowflake/ml/_internal/utils/identifier.py,sha256=zA2Eoc_p8u4kphGuVUbaYt1Fl6xSTjIYu6Qu8BrDZ1c,7703
 snowflake/ml/_internal/utils/import_utils.py,sha256=eexwIe7auT17s4aVxAns7se0_K15rcq3O17MkIvDpPI,2068
-snowflake/ml/_internal/utils/parallelize.py,sha256=zYtkYBq2_N7R49AvSzJynmvixNhUw3YBBZQ3uxVtTEA,4550
+snowflake/ml/_internal/utils/parallelize.py,sha256=Q6_-P2t4DoYNO8DyC1kOl7H3qNL-bUK6EgtlQ_b5ThY,4534
 snowflake/ml/_internal/utils/pkg_version_utils.py,sha256=AMR97AZCOr26Je2Q4fIePJRMf7cASr910R5-wr7ANpM,3722
-snowflake/ml/_internal/utils/query_result_checker.py,sha256=IrzUJ4fJvxjJ5ma-6mejWHpxoEtwnMKo9XTJ-YsECnk,12205
+snowflake/ml/_internal/utils/query_result_checker.py,sha256=jdYl8uqcrdzjYJyoykjXJ93Wy2PxfE0XFFnVe7b27LY,10240
 snowflake/ml/_internal/utils/temp_file_utils.py,sha256=77k4ZAZJfyJBMw0IOfn4aItW2mUFGIl_3RgCNS_U4f4,1400
 snowflake/ml/_internal/utils/uri.py,sha256=sZIf7Ph9TXZZ7lU4IPpVfsc7oHflWEm6atzNPJ7qpsw,2117
-snowflake/ml/fileset/fileset.py,sha256=hwKtNENBiNpEeHKyNra2QM11TYklzjyB_PtIQ8x5r_g,26746
-snowflake/ml/fileset/fileset_errors.py,sha256=ZJfkpeDgRIw3qA876fk9FIzxIrm-yZ8I9RXUbzaeM84,1040
+snowflake/ml/fileset/fileset.py,sha256=CRnnPp2W5V1nHbBY5_HBrfL-ghDH9cbn_r_TfmumDy0,28300
 snowflake/ml/fileset/parquet_parser.py,sha256=yTJdYFTzaTPsgb1rGMj_jv_wDjmuwJZzbVRRmk--yA8,5915
 snowflake/ml/fileset/sfcfs.py,sha256=YWL2D8P-3KcSoGmz6_nvMjQgRNTKzXbwGRhIZYYVZQo,11536
-snowflake/ml/fileset/stage_fs.py,sha256=deFiXBXqab_v2WG6-A0BaepWvNxh4afpDsGbYh0jNWA,14859
+snowflake/ml/fileset/stage_fs.py,sha256=E7B4EpkXTcnceI05yCLj4wCOHzjJZGPHVbBheb20j9I,15902
 snowflake/ml/fileset/tf_dataset.py,sha256=MrFtGiFu1FX3MSjAjWnZcEa5Ow4fsAHlUXW-BLqFWus,3462
 snowflake/ml/fileset/torch_datapipe.py,sha256=kjfUmAqEQ55Gd1nMUFP-3crp1XG46oJ4E74Euk4HEW8,2386
-snowflake/ml/model/_core_requirements.py,sha256=6HGtzvyZVGSIMYkJQ-J4TSyWwPt69uXnPXj7A4Nm34Q,197
+snowflake/ml/model/_core_requirements.py,sha256=wCO1BecSpep1uELgWTLXEDuXhQ8K75yHAorHDoEzcp0,197
 snowflake/ml/model/_deploy_client/image_builds/base_image_builder.py,sha256=hslB0piUdrw15xUOK0rdvS4dOuwQFRnG6WSxgua_UWA,345
-snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py,sha256=Wmz7I-KMoCqd8lSAXgmbM74vghRBG-2rQ6b6QdDLAYI,11239
-snowflake/ml/model/_deploy_client/image_builds/docker_context.py,sha256=RIKXtfPFYd96JBktJSbYR0bvkj70G4Cc68ISveX1dFk,3998
+snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py,sha256=Lu7Gf1pGnUlrvG7yTbh2wsq3uzJySpIPdmZQIqHMvXk,10125
+snowflake/ml/model/_deploy_client/image_builds/docker_context.py,sha256=tBZULfpJZFybNJ6Fp1YYETSpVKBLqDH5jUf5h55cqqo,3525
 snowflake/ml/model/_deploy_client/image_builds/gunicorn_run.sh,sha256=iZWbzTZXpHmjIr9qRLzSCwgWIO6K3-8YgBu-1Q2zrEE,887
-snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py,sha256=MzYtOVUOxJPV239dy3WA2wBcWwMB0zpnBlrVRw4QZ6Q,4894
-snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template,sha256=YDOjxmU9-GqBYVn8_pW6wz0v5tEe62qXtv05uQdL3lM,1264
-snowflake/ml/model/_deploy_client/snowservice/deploy.py,sha256=dg5evnL-hx6LG5JFGjqx7HC06LjokVHwfL4BmduvobQ,8602
-snowflake/ml/model/_deploy_client/snowservice/deploy_options.py,sha256=gIzSUpMf1AhlD-s10L6HU1CKrUIq3kHEHlTKLyXy1LQ,3801
-snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template,sha256=pB-DK7y7BxOS2n-yUepo8Okwnx273nDqbkjq_U1iEAg,579
-snowflake/ml/model/_deploy_client/utils/constants.py,sha256=dzN8F8E_aQv7m1q1zx_0sKYPp9I_0tlE0vnREg1YjfY,1470
-snowflake/ml/model/_deploy_client/utils/snowservice_client.py,sha256=OWT2VQnuVlHblhv34camONpv0dzYLh-VT_R8ygp0KcQ,7494
-snowflake/ml/model/_deploy_client/warehouse/deploy.py,sha256=mr83w-CamAQn6jL1txaWqimoykYhCThSDzwmlhT_Los,9160
+snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py,sha256=G0UW1u8jJEilVF9YhyuweLdSFbG-r62DfsDBYnVVdeY,6166
+snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template,sha256=qpHl0cqZuUxqCIoTTJBYiFJVhLrv9BuHfzz3yGshKHg,1143
+snowflake/ml/model/_deploy_client/snowservice/deploy.py,sha256=1E8N7phOAsciehOxna1LsmiWz1AkHpCEsUjw4HfPN7Y,12367
+snowflake/ml/model/_deploy_client/snowservice/deploy_options.py,sha256=a4IUjiW5Gk59PY2JBqyTmkdWscVTwHErX5I46kNkvdE,3205
+snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template,sha256=8QqMXRmeH_1g8AYwnRefxGgdnbC-QviwH_qw8SgOxCE,591
+snowflake/ml/model/_deploy_client/utils/constants.py,sha256=mhni-3M0wsw9lH-2BA27l-T-MLOMB8HuhGz9Lw2jEgk,1672
+snowflake/ml/model/_deploy_client/utils/snowservice_client.py,sha256=NE1mbylBL6Djlk6Gqsc0M6NFkp5HvLWkLF83XNeTsfo,7858
+snowflake/ml/model/_deploy_client/warehouse/deploy.py,sha256=1KK_ZOLkJx7KSaNJtpyIOnYu4gkNRJWWm-VECmMrdko,8189
 snowflake/ml/model/_deploy_client/warehouse/infer_template.py,sha256=ofWbsLZhyWor8KMmV_r-EpZwzbFxo9IgVEnNynfJq_w,2489
-snowflake/ml/model/_deployer.py,sha256=xi8tss6FYcJIetfl3H6IT4d57gl_IoVtxpR1kGMf-qk,8855
-snowflake/ml/model/_env.py,sha256=VnGo8RnbO0snFcI-1iGhAkln_pY0vHCpmFPmetD3qrs,4560
-snowflake/ml/model/_handlers/_base.py,sha256=JUPnwTCGgMkKzqVns2zeVCF4-MtxnVKDieqNZR1X8sc,2299
+snowflake/ml/model/_deployer.py,sha256=0NVtFUcJGzYTUOoW9lWl9VT6kI8ukOqYso3mFsXivUc,12334
+snowflake/ml/model/_env.py,sha256=Ca1rE0WMimAN9lsGDIJSA4kZl8xBkTGWw2p5CDXib90,5059
+snowflake/ml/model/_handlers/_base.py,sha256=knwUmCbHNTbkQdedodxroAA6HfBViEa2cb8AMP08MZs,2303
 snowflake/ml/model/_handlers/custom.py,sha256=q6IritKreteNZ_mQ6hzortkeGgH6C0LgHSwcGFkvZCs,6423
-snowflake/ml/model/_handlers/pytorch.py,sha256=0ZNi94cpDW1ALU9KUnoTkVEZ4n5KxgyPDrLuI6ATdas,7166
-snowflake/ml/model/_handlers/sklearn.py,sha256=dEJNor34rvKfClFo0ukFB-G9intdoXiGL3ilwgBHCao,7843
-snowflake/ml/model/_handlers/snowmlmodel.py,sha256=3RG6cveksBgwlqsQrOJIrwG-dnS5CgM2EESle3xS1wQ,7993
-snowflake/ml/model/_handlers/torchscript.py,sha256=OqpjJzXASaQjt3tws7C7RKQN_G9VPi7O9g84huK0Mag,7287
-snowflake/ml/model/_handlers/xgboost.py,sha256=FUGjmVtl9V0xXMalxC6FVVFyVX3T8Gds7V9HQXL2o2I,7603
-snowflake/ml/model/_model.py,sha256=Gm8uFI91cjxHdlH7HU_weP3J93cgrkekHjBnkchLNbs,26469
+snowflake/ml/model/_handlers/mlflow.py,sha256=vWAM3WOJ8T8j4YEyVGRaJ0eGnu-AruS1vLXGL9ePN0M,11844
+snowflake/ml/model/_handlers/pytorch.py,sha256=0Z_c-GgVD_B21Ar2hNT6p7VoOV2xgl-Lj4KflXcI30A,7271
+snowflake/ml/model/_handlers/sklearn.py,sha256=a72wsoSDisgXURfCkSu_RBCemQXuQaMDOknagD4zG8A,7934
+snowflake/ml/model/_handlers/snowmlmodel.py,sha256=NzHaZOXSLQW8bPv7y9CqHfuPxaalgXNLbZWTjWlFlS0,8084
+snowflake/ml/model/_handlers/tensorflow.py,sha256=dlzcE5GJ8rrYIFxljOCf2kcqM-YoX_svSS4t04CC3Es,7698
+snowflake/ml/model/_handlers/torchscript.py,sha256=KdUdyn-tFtJ-mTGu_2RhfvC7R4t8mvLO9pEAAig1rvU,7392
+snowflake/ml/model/_handlers/xgboost.py,sha256=6c0zSftiqpaBJvxOaBJ-xNxoYk-EgLuNyiInVJPtxho,7694
+snowflake/ml/model/_model.py,sha256=QtsWRsIIBlaCQJpGglP8PMidEn-r2Bl6TPfErA19Hrg,26762
 snowflake/ml/model/_model_handler.py,sha256=a1upCULZlNuxUiFoJbK85nERGkA2VkEsn5-IIZn7pro,2101
-snowflake/ml/model/_model_meta.py,sha256=NWknKLXh1oMBO9ED62PnnX2ltevnMN3wu6H8QiGLph0,17945
+snowflake/ml/model/_model_meta.py,sha256=YoDx8_VskyxzyEBFTLK-PguOMqNaEhbbyEg70g_HCPA,19501
+snowflake/ml/model/_signatures/base_handler.py,sha256=WwBfe-83Y0m-HcDx1YSYCGwanIe0fb2MWhTeXc1IeJI,1304
+snowflake/ml/model/_signatures/builtins_handler.py,sha256=1AEhIZ_uucSI-7ahbDlYaDB9M4p9ptiYOUI0K5UZ_XA,1872
+snowflake/ml/model/_signatures/core.py,sha256=As4ZXV8tptS6G2QWoMGwJCIaO21gchWRhhPYygfOhF8,16266
+snowflake/ml/model/_signatures/numpy_handler.py,sha256=e6j4aoIxouP4UPPQIXYUmB2sNJoN79VOf7VnAqp_roE,5432
+snowflake/ml/model/_signatures/pandas_handler.py,sha256=LW49cN4CxCDdEulMA-QvOXT3yjj2ZunD7hF2_767VZY,6503
+snowflake/ml/model/_signatures/pytorch_handler.py,sha256=1Hu7ebv4NqGuxjrjMqykV4m6QI_zkUwR9IsrVsvEW9c,3948
+snowflake/ml/model/_signatures/snowpark_handler.py,sha256=85Sj92q6acs6svVsuuNQJXjrFXBVeGNqS1TKhqZGHUs,6298
+snowflake/ml/model/_signatures/tensorflow_handler.py,sha256=yaFBVv2ObyChyxsA8EZmgYQF-GOjBstpq2nQJxsifkY,5152
+snowflake/ml/model/_signatures/utils.py,sha256=LFBMNxsx5crnxdzrGCpqAXQcgE8LQTTdmwPZgsMJRHw,3386
 snowflake/ml/model/custom_model.py,sha256=8qEHi8myHcp02jcpFbG9Kqscn9YRv3QnzehCrTSI8ds,8016
-snowflake/ml/model/model_signature.py,sha256=SoWjugyzw8yut5n17O1-PxtNCxlR0ZJwtFXeF4Xu9VA,60533
-snowflake/ml/model/type_hints.py,sha256=SrRLRJzhrVms22rO80Ee8JWHrOHV6y_EGtAOxRF54eQ,5187
+snowflake/ml/model/model_signature.py,sha256=M6mnEljReOgybE1ILa_TmtKNGxuxYK83FX2ssw6-ozg,15071
+snowflake/ml/model/type_hints.py,sha256=GrE2fgTpX0VVrrMzrXwLAe1-E3ueZt527b_wY7tiOc4,7456
 snowflake/ml/modeling/calibration/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/calibration/calibrated_classifier_cv.py,sha256=tuVQT6v63PTOZhHP3E3APYdiyFEWA0w4sQdWvuhg40Y,55506
+snowflake/ml/modeling/calibration/calibrated_classifier_cv.py,sha256=cdAxNgnfOfNyYGFgmktRBAQv7Bbao_WUpDliS_QlYGw,55539
 snowflake/ml/modeling/cluster/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/cluster/affinity_propagation.py,sha256=pFn8z4prI5O-thwhNE39g4jDPTlZjGWz__AhYw2_MYk,53433
-snowflake/ml/modeling/cluster/agglomerative_clustering.py,sha256=5PbmUs1wxwyUPY-B-iWDfZ6qlMA_NjiWzPBxDHRx5xE,55446
-snowflake/ml/modeling/cluster/birch.py,sha256=bNVfRZesHwYsL1Maep_FIzmWrbHtO75ZK-XtqSEVqXE,53271
-snowflake/ml/modeling/cluster/bisecting_k_means.py,sha256=D8NmOxC_0UvgctdtECdfCS1AJoymJCRJvTczxXbwmMw,55653
-snowflake/ml/modeling/cluster/dbscan.py,sha256=hG8AUIFlUD55klTHvZd9JA0WF9LLHVNGJk0jHgi_L8Q,53612
-snowflake/ml/modeling/cluster/feature_agglomeration.py,sha256=IHDwX6hUef6NyGGyApPEdNX6FQWTbaOdr98j_Ypo6l0,55986
-snowflake/ml/modeling/cluster/k_means.py,sha256=qOx-mWWDrdxVwj9Uffh-zw-HtzrQxpCfZCyNfwFjDJM,55240
-snowflake/ml/modeling/cluster/mean_shift.py,sha256=egzx5sFedAGPEI-wE7SbMIYlFp4ASZRhyOlW0AfRg1U,53814
-snowflake/ml/modeling/cluster/mini_batch_k_means.py,sha256=7NNevIzp8EXjYfbfZqjS9TQmvY2LmP1Kr-xvweK6CyY,56515
-snowflake/ml/modeling/cluster/optics.py,sha256=ogkbb0M-dJw9yclFxB8EEi2kyLoxbhW6cDAgihlVadY,56946
-snowflake/ml/modeling/cluster/spectral_biclustering.py,sha256=nLDe1dF0GaCWJxM9wY-jd8rTiQFAZXuGeORzDvJeojs,54004
-snowflake/ml/modeling/cluster/spectral_clustering.py,sha256=IrIqiSPbjYzDo1iqhEkhzT3N_2SIfh3jJylSFFbtQYA,56942
-snowflake/ml/modeling/cluster/spectral_coclustering.py,sha256=ecJkDo5sKtYYOCndJcUsnyUe517lmpkGvZWN7ZeUONQ,53134
+snowflake/ml/modeling/cluster/affinity_propagation.py,sha256=qwmIgnKKxH5KrAggNaSrxot80Itt_6o0U_EmXM1ZFgU,53466
+snowflake/ml/modeling/cluster/agglomerative_clustering.py,sha256=U5-1fhpeev2C98BYgbTN1493zH86Z71m0Junus4qdsg,55479
+snowflake/ml/modeling/cluster/birch.py,sha256=6cdn6h6B3y4NUxu2Z4Q-ccaS7g1LfH05QkaxKbHNT4Y,53304
+snowflake/ml/modeling/cluster/bisecting_k_means.py,sha256=GdZuD070Yh1ttMV0qhPd4wucvkYPnpSl7g6bnrSMhHw,55686
+snowflake/ml/modeling/cluster/dbscan.py,sha256=QC7tlV90fudnDPV0NCp8FCNkH8LMuT0GK8atEAIZHnI,53645
+snowflake/ml/modeling/cluster/feature_agglomeration.py,sha256=6_U7F8w9KGvmqk9PV_jJW5LIT3Wf23uBdlCLjQhIdTg,56019
+snowflake/ml/modeling/cluster/k_means.py,sha256=AIOIQnVC8h7NfiamuacFCz15PSVpeHbYOEmDVFuiYZY,55273
+snowflake/ml/modeling/cluster/mean_shift.py,sha256=0h-H1cyB-IgOd9duNgHiYNDOqoUFUJ8_eJBiHJWf3fI,53847
+snowflake/ml/modeling/cluster/mini_batch_k_means.py,sha256=KSz1nEF-CLT6YDWWFNYpAlA2dIalOXDhuu4TqWl6GYI,56548
+snowflake/ml/modeling/cluster/optics.py,sha256=dgkSEBVZ-H5IDRVNtbNVNO87CRgzNmPqgkf7ZRfTUaE,56979
+snowflake/ml/modeling/cluster/spectral_biclustering.py,sha256=e5XESpyhruHgvTQ_OE13n-D1ktVuZw83g2zbYlgu9K4,54037
+snowflake/ml/modeling/cluster/spectral_clustering.py,sha256=95p1x4d_g0eqG05N1T4-QLYUO1zasHwqhmdV9QKi_D8,56975
+snowflake/ml/modeling/cluster/spectral_coclustering.py,sha256=aZC7_Kugijmnz8cXDkgceDRTaQT20ZKzlNR_mwpJO2k,53167
 snowflake/ml/modeling/compose/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/compose/column_transformer.py,sha256=bHQ9xxA6TlTJiTDyRvJA3MN_RkH8Vff8OCz2Eitka5w,55717
-snowflake/ml/modeling/compose/transformed_target_regressor.py,sha256=zXsli2w6yTH1x0YIFRa7mlonxS4Ui7jRZmbHqlwEsgM,53302
+snowflake/ml/modeling/compose/column_transformer.py,sha256=OLdB8M6t7Relwb3qro-JQ5wntIM4c-N6i1M3NUG5Qx8,55750
+snowflake/ml/modeling/compose/transformed_target_regressor.py,sha256=SR4fab7fjXVbgB8OnPu1lR4NB9TXU1S-VrXUVaC7D1I,53335
 snowflake/ml/modeling/covariance/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/covariance/elliptic_envelope.py,sha256=W3EOdTi2JcPeFVR3L03Bg6kfwg2N-jwGZ8nC7wUATtM,53274
-snowflake/ml/modeling/covariance/empirical_covariance.py,sha256=L5zVU9NPs3qQ5MCFfTyFcxdEKQ4aUbrJVCgquR0xLeg,51550
-snowflake/ml/modeling/covariance/graphical_lasso.py,sha256=IbnfUfNkifEQ9bXLPTBicX6y8M0ibWjpCb-71UBwtRw,52824
-snowflake/ml/modeling/covariance/graphical_lasso_cv.py,sha256=fqxHKQNuz_CE6GKfcmvzLylWCiDNuhLB-rVp_Tt9z94,54288
-snowflake/ml/modeling/covariance/ledoit_wolf.py,sha256=CoL4LoAE87rdyCZiSKfp7bHeMvEUU909KLtCBPgkAgY,51752
-snowflake/ml/modeling/covariance/min_cov_det.py,sha256=cPa3cjq-2APJvGvwOcCqSPAoDuMPaTif6cD1X56yAww,52515
-snowflake/ml/modeling/covariance/oas.py,sha256=500og54f9Ubt_smfasP2ITXJAen0Fb000D5H53cbl1E,51441
-snowflake/ml/modeling/covariance/shrunk_covariance.py,sha256=zHH2jFGQ24dLlpMkI2Pt6cZycYzAQxaTS9KhJNsmnxc,51727
+snowflake/ml/modeling/covariance/elliptic_envelope.py,sha256=lxAG_fu4hY5nGr2M4R_tZSJ9qgt3igayikKPcGjlX3c,53307
+snowflake/ml/modeling/covariance/empirical_covariance.py,sha256=Y9p8Yo5xCBXrgjhL8R_VD6R7QiJl7ULOaIeJdKu2K_Q,51583
+snowflake/ml/modeling/covariance/graphical_lasso.py,sha256=fCbzqwir0RykFPP44UGxvqx3D5pVQU6qnWIkG5LM0Dc,52857
+snowflake/ml/modeling/covariance/graphical_lasso_cv.py,sha256=4q_9cPf0_gbeTWX03WMk6C7gu5wVEulx-wWtHHetXgY,54321
+snowflake/ml/modeling/covariance/ledoit_wolf.py,sha256=7XZVnA_cx2m_vOUrMD54JWA_UWzAH3PuH4iVD0gEHLg,51785
+snowflake/ml/modeling/covariance/min_cov_det.py,sha256=HktP2pL3czMbXT5MnjM2N2BrWSttZTsnxqVBCI9xD0M,52548
+snowflake/ml/modeling/covariance/oas.py,sha256=-w45o8Y1V4voUHA1EfEKBv5zqI1lXhp083U-b0x5Di8,51474
+snowflake/ml/modeling/covariance/shrunk_covariance.py,sha256=A_aVs4zQ3zhlkuud1ZlKkkWxrEq63vM1HCJ--azQkkc,51760
 snowflake/ml/modeling/decomposition/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/decomposition/dictionary_learning.py,sha256=GKRNAMxdCcJw90-FxDqZ9gNOtEy5JZz71qZeHJSLaG8,56542
-snowflake/ml/modeling/decomposition/factor_analysis.py,sha256=R8H16Suj-JmAI1MafJj4EZp-JKHzP00Z_V2NZ--Hyvw,53914
-snowflake/ml/modeling/decomposition/fast_ica.py,sha256=UBgMFe78BHGpPnAG8foDB41zRmldpQH8QSLEMpkspw0,54376
-snowflake/ml/modeling/decomposition/incremental_pca.py,sha256=5MUCSQwpK1n-M8742wUB3ApjoQ_H44y_N6fLw8RKdWc,52711
-snowflake/ml/modeling/decomposition/kernel_pca.py,sha256=BFsn3p36-U_9qQmztHoCnXN9QLXCcgns55c9eMnuXDw,56742
-snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py,sha256=MUoRNIf2WuqbQ0NF-8rAq-jkxCjNy3u7xB-M4QrUYoc,57725
-snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py,sha256=J9_p5A1UMxG-6UC27n_Gq8xOsZ1Q6i0uINg7YOIwoic,55042
-snowflake/ml/modeling/decomposition/pca.py,sha256=3ntwAL2DU7mRSuXMQSuw_smDgJGU6z_0GbCGKfITB48,55586
-snowflake/ml/modeling/decomposition/sparse_pca.py,sha256=ixsbIaqssO64T8BeD3cA-WzebnlxfdSPzgGPUp-BI1w,53907
-snowflake/ml/modeling/decomposition/truncated_svd.py,sha256=Ds7wa1DIk4rPQWF3Y_6A6SK0NmM77LfSQjvNpXMtHPU,53479
+snowflake/ml/modeling/decomposition/dictionary_learning.py,sha256=T1PmD1i4W5wJQuXwNUk1A7Lk_vPwU_T3lOkpQubENX4,56575
+snowflake/ml/modeling/decomposition/factor_analysis.py,sha256=gFmYG_4PH9Farn1zg5GhBHbAtpQeKoNNGnQQEq6s1B0,53947
+snowflake/ml/modeling/decomposition/fast_ica.py,sha256=XcrZgJQni6Q-hTUPmVL-XfIWTuIfNzkTNbjxmIXdmiA,54409
+snowflake/ml/modeling/decomposition/incremental_pca.py,sha256=n5FYWTkt_NaegwtXdndvP2IDu-zH2ETOJwWz-PzM0is,52744
+snowflake/ml/modeling/decomposition/kernel_pca.py,sha256=YoFkV9Sb0b-S0-nx_feUER6QkWtLmTa4OgxDTskSbI8,56775
+snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py,sha256=GiDdutgrXf4VLwk9zTWP1QTmCQjf29Pmmq8d2UVXwfc,57758
+snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py,sha256=ndeBOpvgXM_BVRpsrAiUsHVX1SBzc9BdrSQTmuaLxns,55075
+snowflake/ml/modeling/decomposition/pca.py,sha256=c2HCssCQKK7f8iK0gmMLYzm_Ews187VpEZFK0Qncues,55619
+snowflake/ml/modeling/decomposition/sparse_pca.py,sha256=PGOhnBObHo8Ns_GMpDeuZ5tnpOVQhFa2tvwCGfcHFzA,53940
+snowflake/ml/modeling/decomposition/truncated_svd.py,sha256=3DrwPB25gnPJmhSIhV87Ovz_pNbDWGECFGG_JsVhHWo,53512
 snowflake/ml/modeling/discriminant_analysis/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py,sha256=Phy_u7CJ6xqhoUNWXI7BZdrCJsGemEsz8ZIG5NEZWDg,55727
-snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py,sha256=syBNrxFoIYtaLtijz9JW66yCT83o3MQkJHKylEylOu8,53792
+snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py,sha256=1j1qnTZExfOT3-uQSsoWxzJbzbhCpZ50DcayJRkCqFI,55760
+snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py,sha256=y_fntsWTmMoIOBoqdessiUl8ilsSlZ4VobhsytGqbuM,53825
 snowflake/ml/modeling/ensemble/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/ensemble/ada_boost_classifier.py,sha256=YJNdZYkT-K4tPUc9tBLWb9MfnsyCpeDjlY___-3Jxno,54745
-snowflake/ml/modeling/ensemble/ada_boost_regressor.py,sha256=oB4umnfZlORTqtjzM03s5w7cj-g3lG5h4fqT5qTvOgI,53644
-snowflake/ml/modeling/ensemble/bagging_classifier.py,sha256=5KYF_zT6WhTBbgzLqHuFH_zP_4e9WSa-uRuyYxAy2PM,55669
-snowflake/ml/modeling/ensemble/bagging_regressor.py,sha256=LRBh0I8U6lL_RrqFDhSQ3c3WbK09wYdxkRbN4HTf-bs,54913
-snowflake/ml/modeling/ensemble/extra_trees_classifier.py,sha256=FSlAb0RbE9yKulnjcGfu_cW1huKT3u-_F6GvxbYE6OY,60455
-snowflake/ml/modeling/ensemble/extra_trees_regressor.py,sha256=83jHLpSFuBr4lsApeQ3BvzF00Gbr9WfnJCY_awSb-j0,59066
-snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py,sha256=gtdgUDzv8Wf5Hnl291dnR7ziyqP7FENeKtzbExvX9Lc,62064
-snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py,sha256=J1NJVLPYRQDvNXwGhlbp1rdFBH0PgCT8Km0Va6cqdnY,61648
-snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py,sha256=7DpIW3IDiRBssTwU9Kxd4LDo5iuG3cu9qZNn74tLz_U,61714
-snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py,sha256=IAwdSzFzS7_f9bBPzjTXaYgyGkFgkq_mWIjCbA1zpbk,60036
-snowflake/ml/modeling/ensemble/isolation_forest.py,sha256=AXuvTAKDIDbC19c5g8HB6hLPwnteQTRaydcg_ZAwkqA,54690
-snowflake/ml/modeling/ensemble/random_forest_classifier.py,sha256=KOPrrJpaZplfeWrVc9SwLw9Augjwf_MGt_fQKg_BvWE,60410
-snowflake/ml/modeling/ensemble/random_forest_regressor.py,sha256=43TqrL5F2kNdO5xbQeVlzTkbVpNxBu9IECmxSRxZTFA,59009
-snowflake/ml/modeling/ensemble/stacking_regressor.py,sha256=dnycH8T_TMUvOXAHbxvJlHQCaEQ6cqbBlwZJBvdI7wE,54597
-snowflake/ml/modeling/ensemble/voting_classifier.py,sha256=z1kWVglksUkHFDMGcexqUwpQnz5tVfWUaquKIQ4m74s,54172
-snowflake/ml/modeling/ensemble/voting_regressor.py,sha256=MjPdM34PuhSnR4SxFAlsNPhC5hEbLQ6zi4no3RydzIY,52707
+snowflake/ml/modeling/ensemble/ada_boost_classifier.py,sha256=uE8aQi2gorfs-4pvi_ZoG4YXqAL5QmzCTKK85Yh6Q0Y,54778
+snowflake/ml/modeling/ensemble/ada_boost_regressor.py,sha256=U0ugvMlA1aVsJLmQd8rd6GbkE4BtFV4u9-E-lx5g4-0,53677
+snowflake/ml/modeling/ensemble/bagging_classifier.py,sha256=8BfoIr_Oo6srMaI3E6S318RMkL-k0gtp5KVFINuJA2E,55702
+snowflake/ml/modeling/ensemble/bagging_regressor.py,sha256=aDnEhYZaXby92dJLe8jByJMx6FY_V3cmVo5_4THWuTg,54946
+snowflake/ml/modeling/ensemble/extra_trees_classifier.py,sha256=9rTNA28zPWbpN5TrbwSrzq2vnEpoEPvdLaULHdzw2D8,60488
+snowflake/ml/modeling/ensemble/extra_trees_regressor.py,sha256=HhVq_pVjg5oXb8V9OCTgdNJRiRCXb9zYv-mg9N6Kiqo,59099
+snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py,sha256=05Q_lX6SMJu5_3lBLWDVISjRduHIaKr3bm-nNvuCbMA,62097
+snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py,sha256=4u1A75Bf2OBC9tAkzR39dO5pMwQ1s0-49h2QOAPJZPA,61681
+snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py,sha256=TQT78E9gY7A-K3batdbuG_Z9O5X6IpjkayFQR78LfZk,61747
+snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py,sha256=1fzx_ADsNdhTyQMycOCjd8S7J9u-pWxDD3QTDupmOCM,60069
+snowflake/ml/modeling/ensemble/isolation_forest.py,sha256=khDZOrkOchhdaQ_lSlvlXgzDLJr5o3TP1uWRbaaQHYA,54723
+snowflake/ml/modeling/ensemble/random_forest_classifier.py,sha256=pMBu6WXP6mBQbS-Pp2IgwJV63tcDvUWghY8Odju3h98,60443
+snowflake/ml/modeling/ensemble/random_forest_regressor.py,sha256=KsMtWdGfRkNxVujQIjj7-KIfqZAl0GUf3MJib9fXQTg,59042
+snowflake/ml/modeling/ensemble/stacking_regressor.py,sha256=7dvdcYltPFMhLEjO1Aj6Qiqa0B7fkhpgYGS_apGcRMc,54630
+snowflake/ml/modeling/ensemble/voting_classifier.py,sha256=hGp45lwd352kARB--kZ2eXzkKxWDdJJisuBd37C5UFc,54205
+snowflake/ml/modeling/ensemble/voting_regressor.py,sha256=BNq-1YjeqOX8Ym2XndTjXaluPHSjg-zqv6XHshTLYSs,52740
 snowflake/ml/modeling/feature_selection/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/feature_selection/generic_univariate_select.py,sha256=QOygJRRKHfxGBK1CFh2byjoVcnXsnz8VtIVebn2-ZDg,52220
-snowflake/ml/modeling/feature_selection/select_fdr.py,sha256=B_GPPVLwmIb9ScHCViCEwt-LGtvjlKK5rEtSkLoKTsQ,51918
-snowflake/ml/modeling/feature_selection/select_fpr.py,sha256=1wJJRFSC-qZWSz-hFcoZjYA7d8oVWfY1OlIgtCyhfNY,51912
-snowflake/ml/modeling/feature_selection/select_fwe.py,sha256=HL1MlwdFiHepBVDe_H-hjsQB9zb1rCaxLniyXs4Js2M,51920
-snowflake/ml/modeling/feature_selection/select_k_best.py,sha256=V7HoRkdtsVAFAE81U1FQQKkWUnBt9ZiAfunhxwod6lw,51997
-snowflake/ml/modeling/feature_selection/select_percentile.py,sha256=SalYYsSYikQ73TzkcuXe22mwkPKMvzEv-FfRNFT1oPk,52017
-snowflake/ml/modeling/feature_selection/sequential_feature_selector.py,sha256=qVOjicEqGVHujZuU14G-jXfViVkDyV0CCt8JhQtfCSk,54670
-snowflake/ml/modeling/feature_selection/variance_threshold.py,sha256=ucnTz_bLAyRApkcdt7J9pAFvijuJFYV-uearYMEinqc,51649
-snowflake/ml/modeling/framework/_utils.py,sha256=So72kQZXXP0U9D47rXx0U5mxbkkRujKwmCh-f2OVf3E,9110
-snowflake/ml/modeling/framework/base.py,sha256=hxRwBOKQtQFVZtHopgj_bgBUIU5TK9vJj4p4ZDynFWc,21900
+snowflake/ml/modeling/feature_selection/generic_univariate_select.py,sha256=leh0f4yJ8Z1CePuRod1LNt_cqZ0ABC-RXBW2xr0cSvs,52253
+snowflake/ml/modeling/feature_selection/select_fdr.py,sha256=mMXLi7J72KMoRbRTDbbdtxwCZ3Q38WsgB6PXv3_RnDQ,51951
+snowflake/ml/modeling/feature_selection/select_fpr.py,sha256=pWcJl7A7opt6nqIcq1m0n0C6K6IPLTCQhibYcW_RRdk,51945
+snowflake/ml/modeling/feature_selection/select_fwe.py,sha256=1gIhdQiz8VmvnOO4gydzCOQbl9xmXUeRv81RyY4xQyw,51953
+snowflake/ml/modeling/feature_selection/select_k_best.py,sha256=GBnJiHkXi-DsOmBdYzxVNF_YAhmCu_RVMJkviQxI2tY,52030
+snowflake/ml/modeling/feature_selection/select_percentile.py,sha256=eyaxmKYoPSAn58ekgIySIZZ7u89zN6gRK3Dt8uIKVQ0,52050
+snowflake/ml/modeling/feature_selection/sequential_feature_selector.py,sha256=suHOAsoIFGumkbcRnJRKeDIAorwTvA3g5IImPPSLOnQ,54703
+snowflake/ml/modeling/feature_selection/variance_threshold.py,sha256=xmw7i9qMzmhfxvoZRnSsbNGFKZVAzwtHzJClolAqMS0,51682
+snowflake/ml/modeling/framework/_utils.py,sha256=7YiRCLBJb9RdLKvoDSpAVMAIveaC4fXdO6G1D8r1Lqk,9903
+snowflake/ml/modeling/framework/base.py,sha256=O96AHTNmtWCp7AD3fbgbLltqmnaLw8fyhIArYBeuaRg,24514
 snowflake/ml/modeling/gaussian_process/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py,sha256=Po-HwoK-vELk0R36D86KH1iafgcL8Txs9EuJkRD9wXg,57215
-snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py,sha256=582hyNE0W-mZ58Vn2yCYROkydamXVsfF5UZxKHBDax4,55907
+snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py,sha256=rb_Xt8aBlpfNefpqL06PnenkL18oqitaED6cXV_7FFo,57248
+snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py,sha256=sq9AgC8n6BkvaZcnH-utE6IL-gGyGCXml8iEvlKvvz0,55940
 snowflake/ml/modeling/impute/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
-snowflake/ml/modeling/impute/iterative_imputer.py,sha256=kz4gc_dubKSu14DU77KV_XMeRfeRDZPBK-dbmToxJW8,57770
-snowflake/ml/modeling/impute/knn_imputer.py,sha256=ay7KVv2XuPuGcazIJPlO6lDrX5-0Kkl7kNkTxrhcDVk,53992
-snowflake/ml/modeling/impute/missing_indicator.py,sha256=Mvh9deXKmobw1d2Ow0fzBwZV7eMlzesLxQtVdu405Ko,52789
-snowflake/ml/modeling/impute/simple_imputer.py,sha256=AuqGFxRvVEuIdhTNhmk6T0Uz5K-k1RCKCTnQFCNQxWA,18118
+snowflake/ml/modeling/impute/iterative_imputer.py,sha256=_m8KKJBAM-RwxN_AvXC0brXGMnyZYeEFWXTrCxtwVEo,57803
+snowflake/ml/modeling/impute/knn_imputer.py,sha256=dsHrMuf_jkehOX9dci9EH8yACwXqkBUVc7g7XlJJAzg,54025
+snowflake/ml/modeling/impute/missing_indicator.py,sha256=lsWDqsIRBDhfDQHl9mMZkyz-LaW8VUVFQKNJ7Jb6Zr0,52822
+snowflake/ml/modeling/impute/simple_imputer.py,sha256=o3gppJ6gwPzRd9jqbrcp0PVUKk9rO7vSFah6XV7gowY,18458
 snowflake/ml/modeling/kernel_approximation/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py,sha256=Wi4eHHhqZqa2zWTOvNjZ654WfqJE45IiE5FURqg35fg,51733
-snowflake/ml/modeling/kernel_approximation/nystroem.py,sha256=1KbkTFSpLfgiHsttIKC0y6WybxEJmIJnWaucMSEPW2k,53606
-snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py,sha256=YmBuv1wvTmozyu1bWEUAEAjdR4YJAaUXyT1i2G62hXQ,52760
-snowflake/ml/modeling/kernel_approximation/rbf_sampler.py,sha256=CXkTlAbqUTXmIndPOaZJBzVJON7cptYpoX-Bl7VQTQY,52189
-snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py,sha256=H4VXdL5P_J4YLZXR4wHPfXg1gLQqE_f6xfZPbBvGnMw,52188
+snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py,sha256=dkTGg4jbJ0C-GaC_-q_Qt3bIQHpSs5auWeCMyezorJE,51766
+snowflake/ml/modeling/kernel_approximation/nystroem.py,sha256=gF_eilIxaDiHVxD-AvRco0oNPo7rJ0Yerph-rgQz3SY,53639
+snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py,sha256=GvrKTYkWOA47BYALcYzqlIsg2gZ5qTIoJRYQDhX1Sh8,52793
+snowflake/ml/modeling/kernel_approximation/rbf_sampler.py,sha256=wJo53yZ2tz5V2wL_hP4TCF6d3mu-bL_Zx9ejrY8x2pM,52222
+snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py,sha256=KMAStiUQGUvoorhTs8X4984H2JWzhCK-WXMzm5e65iw,52221
 snowflake/ml/modeling/kernel_ridge/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/kernel_ridge/kernel_ridge.py,sha256=QlEvYhnWYMEf5Vz-usvmLrP1nlYrNMZTEYF4FGdviAM,53706
+snowflake/ml/modeling/kernel_ridge/kernel_ridge.py,sha256=eWxESPJtQaQPUjVV4B0ja9PmQZjg6P9lazYhadYqxfQ,53739
 snowflake/ml/modeling/lightgbm/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/lightgbm/lgbm_classifier.py,sha256=UGz_qfrr3rehtQjRr-dozP_IgTZBHeak1OwkYXtf4WI,53228
-snowflake/ml/modeling/lightgbm/lgbm_regressor.py,sha256=pX_ewzyd1KhJLCZAlLbzNpYgk4cQfWoUnSrXVEu01n8,52739
+snowflake/ml/modeling/lightgbm/lgbm_classifier.py,sha256=RapUzlQl_ITK1JRcKumVq-dzOHKdj1SGepC4nK-c-9A,53261
+snowflake/ml/modeling/lightgbm/lgbm_regressor.py,sha256=PbI-IHt8y_AMfwXMbRIQIuw3yVhqZApH1vRstpRwWAI,52772
 snowflake/ml/modeling/linear_model/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/linear_model/ard_regression.py,sha256=qBLYHo6dWB-czD-VA3TRFviFtwEhWKrK7HNXpAVORb4,53454
-snowflake/ml/modeling/linear_model/bayesian_ridge.py,sha256=UYruGveqQzh2l4xQ4xprSmrj4WVzhj-7wVUxe0VfXAk,53767
-snowflake/ml/modeling/linear_model/elastic_net.py,sha256=qZuAJ19QTxCL1k69Wq8-tE99XLDp_0iaC-x--0oHkUg,54651
-snowflake/ml/modeling/linear_model/elastic_net_cv.py,sha256=gg1Mdg2C7zT7Fwu9slZUfk-fRQuuPw5S2N084GrQswE,55909
-snowflake/ml/modeling/linear_model/gamma_regressor.py,sha256=-PsCJ8RskWxaW3Ep4SGGCMOiH9QxIFosSLzNA5p_SGE,53707
-snowflake/ml/modeling/linear_model/huber_regressor.py,sha256=yA-5V0xJxQL4DNZk_UZY_I-YbU8_B-U00L-TnkdxxQA,52895
-snowflake/ml/modeling/linear_model/lars.py,sha256=XCqIF1K5fW4Z5e4lbFpyVdZ16DKP5ctI0iabtrJmqcQ,54192
-snowflake/ml/modeling/linear_model/lars_cv.py,sha256=mGY3qwPKXTrgsaOokKZTxLFH4JDHC9uqbXhose8tvWw,54399
-snowflake/ml/modeling/linear_model/lasso.py,sha256=DhbxwaaEjLHVB27WO4Do39sH8XJo8tjqo9JCITj2-rg,54291
-snowflake/ml/modeling/linear_model/lasso_cv.py,sha256=-9OTnRXuF7tR7pmGiBkbNYXtdpdBKQZdOHQoxGLzFsc,55066
-snowflake/ml/modeling/linear_model/lasso_lars.py,sha256=lUX3Waet6aBpZ2rM4mxFtbbHZLROOdhpJ7c4loWxIJE,55295
-snowflake/ml/modeling/linear_model/lasso_lars_cv.py,sha256=Yv4-ubZ0VFunMkFz5Ha1yD-6Q3qEWY6p7MkHskHW8x4,55241
-snowflake/ml/modeling/linear_model/lasso_lars_ic.py,sha256=0mPTAy4_AtbQ9CRZdhaHL9Ie6I0BCmUYb79kO6i4rSU,54586
-snowflake/ml/modeling/linear_model/linear_regression.py,sha256=e6O-N3DqbR64l2D4suZEp2YePe5bhtjqwg-7EY8IGdM,52421
-snowflake/ml/modeling/linear_model/logistic_regression.py,sha256=DTIK8OneFS4HJEMUzPvwuYTY_VUR7FQ_Z5VrURiwBKI,58672
-snowflake/ml/modeling/linear_model/logistic_regression_cv.py,sha256=9s8x5-7EBl2DgD_6ShPsSN3jAGn8Qg3BEWakZ3ekYuk,59692
-snowflake/ml/modeling/linear_model/multi_task_elastic_net.py,sha256=jgjH31l0Iq7d30Rig0KvaOt44vFlxZM_15eAv2vtkVw,53877
-snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py,sha256=h0L934Dc-ucQKCfI0Hg-7sLwYjPs1MzuKwqnZ6UtQ-s,55505
-snowflake/ml/modeling/linear_model/multi_task_lasso.py,sha256=Lq4eLjl0y75uTsYxu7zyse_uYqIoe2rJPlqJsgbaEbE,53459
-snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py,sha256=JrRbi_-a6UEJGPWTbcgWcttTdRZ2d2qddYm1fifL4is,54711
-snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py,sha256=E1e9IaO9GMPZbbcjv_jlyjlu8JyulPusX-2R-hgTIag,52986
-snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py,sha256=RCfEXkTltDsy55UWXWdmpqTNdTnw4vJ42Ti0dLQUS8g,56337
-snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py,sha256=3JXfzZbYSJlhAGN6vHQRUq7Uy1RlnHnYXVWSSe4xv3w,55412
-snowflake/ml/modeling/linear_model/perceptron.py,sha256=PFsyxUd6SVOA40rZQ5aVjmb8OD4Y1fYLzn5hRDj7dEc,55842
-snowflake/ml/modeling/linear_model/poisson_regressor.py,sha256=rpbURsXkCkh4WDLJXcg3YtV1PqtaBD5usOwS_-1NCSo,53738
-snowflake/ml/modeling/linear_model/ransac_regressor.py,sha256=JiWyvAg-LKL6qTUF0M9NChND49yQypt3lpCfaHn5etw,57212
-snowflake/ml/modeling/linear_model/ridge.py,sha256=EUTdq5QniCZDJVNqU7o9Ujxl4Mj_lz_9qmzxBkdN0t4,55272
-snowflake/ml/modeling/linear_model/ridge_classifier.py,sha256=4PwcLr-Rt1vW2zu9dwkJLHc_tZUjQMf7Hfnh6sKb6aw,55590
-snowflake/ml/modeling/linear_model/ridge_classifier_cv.py,sha256=a6Xh90A3ZOD47Ls6tQheaegrUaaq3wrbc1pSW4GHq8w,54129
-snowflake/ml/modeling/linear_model/ridge_cv.py,sha256=0nN19Drn4j62cpHikVxuFvrFYUhhhisUAiwdLFEDGWw,54905
-snowflake/ml/modeling/linear_model/sgd_classifier.py,sha256=aivpS5qfKnUX974mRaCHhckk_hWO0ROMypiBzEK3Muw,61258
-snowflake/ml/modeling/linear_model/sgd_one_class_svm.py,sha256=XSfwAJf7AlNFsyLn2iYHF--gnlYaGgrYQz2WDc1hxxs,55872
-snowflake/ml/modeling/linear_model/sgd_regressor.py,sha256=2_5EwFzgQvn8sri8k5s7TlmhQ2GzoGwvCWQRd5573GY,58727
-snowflake/ml/modeling/linear_model/theil_sen_regressor.py,sha256=TBDTE2nPajqqYnnX7HOnB84Fgnz3qjP1yUx0gAgAXKM,54160
-snowflake/ml/modeling/linear_model/tweedie_regressor.py,sha256=mX9qdhARD3RKBNF54lu3F8wPzmQkqH5xooUhUYELx94,55131
+snowflake/ml/modeling/linear_model/ard_regression.py,sha256=Bx8RKf1Bv8Ajloy4Swq9YP29hkN8VQzHFATJWKJqHio,53487
+snowflake/ml/modeling/linear_model/bayesian_ridge.py,sha256=6d4pYlH31nsjc_AQvt7Ltd1IQX6R69_Vc9nrmfRKiRo,53800
+snowflake/ml/modeling/linear_model/elastic_net.py,sha256=EaHaAm8_9zsADJIx_AtYyAfJ-1_vABL_gnr8_lgeBho,54684
+snowflake/ml/modeling/linear_model/elastic_net_cv.py,sha256=fMW0sGLnD7NyDy_TtVy_8LGqV1o9WrdjjeBmcNKGgKI,55942
+snowflake/ml/modeling/linear_model/gamma_regressor.py,sha256=QBqWN_0h6NYq_txEkRM38zUjPU-X0upBWGUMVA5T4eA,53740
+snowflake/ml/modeling/linear_model/huber_regressor.py,sha256=jgMsNU-qqrOaFhoFJk9ZFPiYd-Y7sO03Do32IP47mkM,52928
+snowflake/ml/modeling/linear_model/lars.py,sha256=NRez5i1S1gYYVSJrb9Dq_YIGLzmNOcasKEuaF7iR228,54225
+snowflake/ml/modeling/linear_model/lars_cv.py,sha256=XoSw3gOgwYDCA3NcOi-XHXdgk4MjnpSTpUxUPYZiRbU,54432
+snowflake/ml/modeling/linear_model/lasso.py,sha256=5Rcj8nEGQja-9cqpn_LI68FR3qpMMhI99gLpWl0gaW4,54324
+snowflake/ml/modeling/linear_model/lasso_cv.py,sha256=ap36b-psXRoR-P6vEP6gP8C3CihGrpap4dSsJH32iP0,55099
+snowflake/ml/modeling/linear_model/lasso_lars.py,sha256=ivMvs69ssjT4Kt1UPZ45guBHplYYt7wMpnT4-_d4-RU,55328
+snowflake/ml/modeling/linear_model/lasso_lars_cv.py,sha256=8XE_MJvnd6VWlzCm5gIjJgc9dNeeFXbnFMrz2neyg_s,55274
+snowflake/ml/modeling/linear_model/lasso_lars_ic.py,sha256=rbDzNUfXmgSYPKV-iFRbJ9HUXuYmAXEczm5u3oue6m8,54619
+snowflake/ml/modeling/linear_model/linear_regression.py,sha256=NWzycAOdfhG0CLwbK9t4CjgucYmU_Dt-YJUpP7cc8SI,52454
+snowflake/ml/modeling/linear_model/logistic_regression.py,sha256=-du4qHD2zGK6ZGa5PIXhv7vzVrqfybNGdMqgZnH1CEE,58705
+snowflake/ml/modeling/linear_model/logistic_regression_cv.py,sha256=w7wOGNl9DkZ39vjUNId0wgP3X7bKUpAKkn7gkMXCqnU,59725
+snowflake/ml/modeling/linear_model/multi_task_elastic_net.py,sha256=MvGmD5XCQVe_Yfys_gmfwzQlwBRACBd5dBuncw3vsaM,53910
+snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py,sha256=-boi-5Y1eJw4JIYaKs03rXvrmZv1y8sPWC5VEfHugxA,55538
+snowflake/ml/modeling/linear_model/multi_task_lasso.py,sha256=guzPMNwLTYTvAYExXaXNNi2kgq8P1gVood7NOepVKKg,53492
+snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py,sha256=TC81G8U2PCaM0xHqjjPGSphXGlcBZ3IEx9LhkIcR7BI,54744
+snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py,sha256=2S5urOKKr-fuwzoz6v7wEAwJpraRQKr0BEdVchRG5Og,53019
+snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py,sha256=QkRxcRA1oqay2fG70OFWA1X5SnglKxn1c2-4YOhUT24,56370
+snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py,sha256=iEDTqRK07fRhN4NgBnzn1pDhNuWpX0MDHtHCb5f5iPM,55445
+snowflake/ml/modeling/linear_model/perceptron.py,sha256=1iLqJUeQROMfq8MFpWS2V3M8e9veHGi_1E2MQKFdqeE,55875
+snowflake/ml/modeling/linear_model/poisson_regressor.py,sha256=L9AnxdRtIjU9HXraoNlFgl5fTuViqKsNwq06zuDtfsQ,53771
+snowflake/ml/modeling/linear_model/ransac_regressor.py,sha256=NckG9_8f1aHspcDSH39js9Z2cuqrxmkIK2uswrw3i7g,57245
+snowflake/ml/modeling/linear_model/ridge.py,sha256=NbW98VbBoycT14YgvU3wk0KDS9SkmlI5qXXhR0ircjg,55305
+snowflake/ml/modeling/linear_model/ridge_classifier.py,sha256=ZLtGyBRCUbtuTrZ4pL7OwRSzImjpva3BAVH28faFV5c,55623
+snowflake/ml/modeling/linear_model/ridge_classifier_cv.py,sha256=-CB_MCoLztxOL_8DvLqWF2GrxPKzaH097CAOCyOyP2M,54162
+snowflake/ml/modeling/linear_model/ridge_cv.py,sha256=atjWonIdSzp1gLRPfTy_maYyoNr5fG9xvdsgFtYCV7M,54938
+snowflake/ml/modeling/linear_model/sgd_classifier.py,sha256=Cps3X9q53NvI5tQ-XLeQnds2hTGJjh58Kc691SEQpcc,61291
+snowflake/ml/modeling/linear_model/sgd_one_class_svm.py,sha256=3nAJQCyM8jEL9ZV5kUjAtv_k5k03WfIPEmBg_HfGHwU,55905
+snowflake/ml/modeling/linear_model/sgd_regressor.py,sha256=4wsPlteg3T-Nwcg_VwhMZWAmnlQDwFhGWR748NFbh3c,58760
+snowflake/ml/modeling/linear_model/theil_sen_regressor.py,sha256=SxyDflRe852QA-HRJZUVjsKMfGuONMq5pDk1k-sBIaE,54193
+snowflake/ml/modeling/linear_model/tweedie_regressor.py,sha256=HJ0awNCFJ5WIM6RxWgc88m7JXYPqyFWJ4Es5J0AAUIE,55164
 snowflake/ml/modeling/manifold/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/manifold/isomap.py,sha256=9fiVKwZ8G09fvKJ2tgs_oU9wgzGRdHR1lGO6nDMlO68,54526
-snowflake/ml/modeling/manifold/mds.py,sha256=tyaIhilgwzt5JE9s5X-PdUjlouNvHhjrvkkWAtSQJ-w,53744
-snowflake/ml/modeling/manifold/spectral_embedding.py,sha256=PBed_JavNtd1a14jIxjTfbOa1QgF6s7UfAL8_PgoDrk,54515
-snowflake/ml/modeling/manifold/tsne.py,sha256=m_JKk_XYmDJJrlnypDyCWDjjTqf7hOcXjx__XAmjJ5s,57787
+snowflake/ml/modeling/manifold/isomap.py,sha256=t7lP31KFjfV_SkPRLNV9f0dGuSRJlIaqn7K2gAZ-eyg,54559
+snowflake/ml/modeling/manifold/mds.py,sha256=_fNrwp5CO6JnB_yD_HwshirbnZmcGkG1pPd4L-Y124s,53777
+snowflake/ml/modeling/manifold/spectral_embedding.py,sha256=E4-K0VPlAOV7sqs8lTiMzJrcA3GzpWVP-k5IPXNxf-s,54548
+snowflake/ml/modeling/manifold/tsne.py,sha256=ljEAijpTEx8_xLlmXtP2aVfLUrzE2pH0852tp4uHJXE,57820
 snowflake/ml/modeling/metrics/__init__.py,sha256=wp2LehkoLtyt4u_HBhglrKrV6E-dKt5vr-0N3MkJFaY,304
-snowflake/ml/modeling/metrics/classification.py,sha256=ZtTQ3ziMMglimNW1hG7oGDhAW5a6HBXOfQq8g3iptC8,40077
-snowflake/ml/modeling/metrics/correlation.py,sha256=4cjKDl07C3PGcx_VPwOqSFYjuBEA266btKuw9wd5D7w,4921
-snowflake/ml/modeling/metrics/covariance.py,sha256=hS_yILgo3OUjBVrPCL-NXR7cSyPjXOFftXlZJ1xaLus,4757
-snowflake/ml/modeling/metrics/metrics_utils.py,sha256=jvjOabIwGi02I1aEiSo_3NfgXLAIU7ggShQXDAAjCFs,12037
-snowflake/ml/modeling/metrics/ranking.py,sha256=KzRbI1bZf3G1U3wlSnvpX1GMTkddfGwy9y2gopxoW6E,15397
-snowflake/ml/modeling/metrics/regression.py,sha256=yqTiBnbFc1GtBR4LJfUiEGE8Pv3uNT2ZuFiaEyzxyhM,23144
+snowflake/ml/modeling/metrics/classification.py,sha256=SJqUIuClEZC8PBdONysjEdVJwVrkfcjw0HKp5pQw3NE,40159
+snowflake/ml/modeling/metrics/correlation.py,sha256=Cw0NCmrAb3apP_hh1OxQsErDM5ADdxRM5yFp_UhdiWc,4869
+snowflake/ml/modeling/metrics/covariance.py,sha256=2MdP7H1p1b_94JDEr2fS463XkbaGo1XQOeqls-zA-k4,4705
+snowflake/ml/modeling/metrics/metrics_utils.py,sha256=IQD9oq5uilDUBPHnm_5ZHi4T98K0ifK0DyG5COthVq8,11807
+snowflake/ml/modeling/metrics/ranking.py,sha256=SmEwCWQdDMgqF_sXCmu1EP8Cl0yCO1mnCuryXDyAnis,15748
+snowflake/ml/modeling/metrics/regression.py,sha256=fiqPbyawzM83DQk6v_VxNm0HTz0jvcoOLdzf69gBFeU,23742
 snowflake/ml/modeling/mixture/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py,sha256=-n1RTh-9K8irI5Z2ROti8EpyeMfYqHufr2XTcEaoJqk,58431
-snowflake/ml/modeling/mixture/gaussian_mixture.py,sha256=kMzOcefnjIYh0FX-HLpbfAuax6IroF50bAMhpk5h964,56433
+snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py,sha256=ghytPZc6PU1B7rm3S7ciVIabS2iGoJce-5EgA_OMD_A,58464
+snowflake/ml/modeling/mixture/gaussian_mixture.py,sha256=m8WKIdkcYfc3ALR1K6R6jsxoeKt4ekEdOrLNY_4Wl48,56466
 snowflake/ml/modeling/model_selection/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/model_selection/grid_search_cv.py,sha256=KohZGv0rroFdiEqg5WLFQJo5C-fprC9G1K49V0sYYEo,58982
-snowflake/ml/modeling/model_selection/randomized_search_cv.py,sha256=FZS-Fc7HWNKFZvgv4H3pNiezeAAk1Kgh8mbiMZ9u0K4,59826
+snowflake/ml/modeling/model_selection/grid_search_cv.py,sha256=btuSM3bazN1eE0LUeLu-fIwhXkcZ6PDOQVoXfPKIWPM,59015
+snowflake/ml/modeling/model_selection/randomized_search_cv.py,sha256=8uO2Fik8a-9zN6b4NTE3vgkU0_FrhdabN4SAUMPTJzU,59859
 snowflake/ml/modeling/multiclass/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/multiclass/one_vs_one_classifier.py,sha256=uTckORyZu2fXX-uBz2STZBFgoqKqH5MaDET6aDxy98U,52414
-snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py,sha256=zUe8KBBhZyFZHSd9dVVrmanErHSW045iZTaxcvMxeJ0,53342
-snowflake/ml/modeling/multiclass/output_code_classifier.py,sha256=G03cM3PD0fdQpjF2gZnKuMktxRg3ikopCPnXurE8ylA,52672
+snowflake/ml/modeling/multiclass/one_vs_one_classifier.py,sha256=CG4_PNZHb_0d4On73h1igAPoGyb-Par8Mzpty2T7DyM,52447
+snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py,sha256=SxK5gW7KR8dU-7Pf5_2FzGVWYOXNrhlf6QxqttckaI8,53375
+snowflake/ml/modeling/multiclass/output_code_classifier.py,sha256=IBn9wE4tKKs1ND30vsR4emlofZ9agswAOGWLK0oVxkg,52705
 snowflake/ml/modeling/naive_bayes/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/naive_bayes/bernoulli_nb.py,sha256=3vOEAj7kDxSNRRIfgs1Lq7OZHTXbmHPMZa8RNaDdYxg,52999
-snowflake/ml/modeling/naive_bayes/categorical_nb.py,sha256=WK3m0zx7_C8c1peHvC49F4YRr8EHfEMGJYzQEqBhOLY,53320
-snowflake/ml/modeling/naive_bayes/complement_nb.py,sha256=iFR2xR9klHkUJ0cdHHcGWtfrlw3R-2NuSf3piYa8ar4,53007
-snowflake/ml/modeling/naive_bayes/gaussian_nb.py,sha256=AFEdV5VNyqZm_YV71KAEP5F1uOPXHZ8ghRUPWCvKMrI,52147
-snowflake/ml/modeling/naive_bayes/multinomial_nb.py,sha256=xiZwZACAA5tHOK7DWhq4pCWWNKhp91zGKxDs5k7pa2I,52764
+snowflake/ml/modeling/naive_bayes/bernoulli_nb.py,sha256=HFUrYqHBy9nHhEjm73HaH0NA_JU_PEgJcOvgPlciwBo,53032
+snowflake/ml/modeling/naive_bayes/categorical_nb.py,sha256=7oAUxQ02B9GC00ohml20y4IaiWnjwuWBchwE0u7uFQ8,53353
+snowflake/ml/modeling/naive_bayes/complement_nb.py,sha256=A2INtlay0HFCOw8NClOsWpJMekfZjpIiyjKncV0W_Sw,53040
+snowflake/ml/modeling/naive_bayes/gaussian_nb.py,sha256=951GArcwq-zCOU_7F6aowutlPKX1b7Q04bGqA_VP-r4,52180
+snowflake/ml/modeling/naive_bayes/multinomial_nb.py,sha256=Iu25cv3H6FHu7RmykF_ohOhXJqCWSeFoiDdUu2bPNCQ,52797
 snowflake/ml/modeling/neighbors/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/neighbors/k_neighbors_classifier.py,sha256=N1Qb11dM2lAuEyP1RsXHZC9EZbVG5EaKUIy2gMTvQio,55551
-snowflake/ml/modeling/neighbors/k_neighbors_regressor.py,sha256=z-F3iOELgLHznQWpBpRb8NyQ_jflxJUL4DSLgAeyX1g,55033
-snowflake/ml/modeling/neighbors/kernel_density.py,sha256=kbJ3bfchkxqrtm_lodMFrrNBi2nUWPHWy7wJywonWh0,53510
-snowflake/ml/modeling/neighbors/local_outlier_factor.py,sha256=ej39Z2fyioeFjUsf5rfT093VUpmUbSS2kdSx3SSvMuY,55791
-snowflake/ml/modeling/neighbors/nearest_centroid.py,sha256=Soafun6CXIHSNW-v2w6XyEpxuPdGSEoQ2lwRXbzr3Ck,52314
-snowflake/ml/modeling/neighbors/nearest_neighbors.py,sha256=COicFuUJ2xgt6Cia4mrFLnTm4aRVmgQtbDl1Xh9RptI,54223
-snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py,sha256=wDzqewngg8lGhRz5WaFgnimpjRo2arDRghU9_ajJCbg,55699
-snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py,sha256=WhSbpXCjYY8IVh5FNbmBfBO5354gf5NaBYjctrTWG4g,56180
-snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py,sha256=q8H7AdBOjsfxj0YHtyNw9Jjn_-zsxCS0L-7F9HEF2fY,55066
+snowflake/ml/modeling/neighbors/k_neighbors_classifier.py,sha256=LWvA2X6zpadbS9xVdS7H4-AUxd1oBO6hdPFzBpQRDsc,55584
+snowflake/ml/modeling/neighbors/k_neighbors_regressor.py,sha256=-GFEF3k11rVn4OPJLtAPCHnzLw6yu3Z5ZaklmADRxVE,55066
+snowflake/ml/modeling/neighbors/kernel_density.py,sha256=HRErEPEZB9ayeIjYw0HRCIhJMSHetCnBghSdg1GHwqY,53543
+snowflake/ml/modeling/neighbors/local_outlier_factor.py,sha256=aXJXv9SprsmT7J9wKXHgdII2RwiEgannZwDfcRr1QLo,55824
+snowflake/ml/modeling/neighbors/nearest_centroid.py,sha256=TSd6pg-ggDE9fkeKFvqnlWTQP4QyTYJA6rjyXRjskDw,52347
+snowflake/ml/modeling/neighbors/nearest_neighbors.py,sha256=GKYvEflXySQD-T3d4MB9_Ql9cFONLstOprjC7N90Qnw,54256
+snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py,sha256=QxVMpA4jMuqWYcCalSyMZMqnh5vS8kfiWjltXGOjftE,55732
+snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py,sha256=6uDKlcZeJ4VZRjTQlkcHBUfStYfLL2PAvbqCRJ-9ytw,56213
+snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py,sha256=-Ok5xsuvGv0QTRn60y61j8zEzUFnD6oaCNqmYUKuW3k,55099
 snowflake/ml/modeling/neural_network/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/neural_network/bernoulli_rbm.py,sha256=cs0JZu0r9M1HKwCJR7h85K-BW99ow0wxC2cTv7aeW2o,52715
-snowflake/ml/modeling/neural_network/mlp_classifier.py,sha256=crxBNA4nGPBFjJqTjHco87I9yKt-ztDqm6HhBjwj2z4,60213
-snowflake/ml/modeling/neural_network/mlp_regressor.py,sha256=_tnYTJs1zrMaVmjYngVEjI2m2U2iw0kCqfyOZrgnrZA,59490
+snowflake/ml/modeling/neural_network/bernoulli_rbm.py,sha256=k7eA6JQ7PhYulurwxcjWKZ2GgFSTyeKj94gKZf5bqBU,52748
+snowflake/ml/modeling/neural_network/mlp_classifier.py,sha256=hOcAYZcA1XUJQgn5-1TrDB_e24BS35b4ffDYdGyhRD4,60246
+snowflake/ml/modeling/neural_network/mlp_regressor.py,sha256=nSB6Nu1Mrk-cT9qrbyFSJ8B5OMClsp48k0IGOnWR9sk,59523
 snowflake/ml/modeling/pipeline/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
-snowflake/ml/modeling/pipeline/pipeline.py,sha256=kIvKahAyF7zQoT8eYVm9dJPafYLybGZ8ELaxrBIkQ34,23381
+snowflake/ml/modeling/pipeline/pipeline.py,sha256=h1ZfqHF0li-FSF0Y5DwVG6Taf0RWqG2WMbUZEWKt8-o,24044
 snowflake/ml/modeling/preprocessing/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
-snowflake/ml/modeling/preprocessing/binarizer.py,sha256=IoGdiZwqsLYRSkifmxzfCqCeOy5ir5Gq_ls_gsPu54I,6092
-snowflake/ml/modeling/preprocessing/k_bins_discretizer.py,sha256=upW9qxntwE0vZ8foc2J3BlVdKy61M7JBspZkKqAyKW0,20422
-snowflake/ml/modeling/preprocessing/label_encoder.py,sha256=r3S_-G5OIqjeBttyIicSar_4FNO68MOvRSyAi_6gzeA,6285
-snowflake/ml/modeling/preprocessing/max_abs_scaler.py,sha256=O2dXkX6PPJZaVbS7jIpC4DOfqUt85YFaDA-rLXz6pEc,8491
-snowflake/ml/modeling/preprocessing/min_max_scaler.py,sha256=1LDaOp-OJU-79B36ZxBhAMQe5AXDEU5f71PNVXwtLXU,10716
-snowflake/ml/modeling/preprocessing/normalizer.py,sha256=0pbgiOGqwC4Pv9MKnYfo_0vIUmBdyLFoPSd_Sr7Og4U,5951
-snowflake/ml/modeling/preprocessing/one_hot_encoder.py,sha256=ubZCjUhPdkqn_w4nuIpgozawjcV3HvnkqiKMYqo3ljA,66998
-snowflake/ml/modeling/preprocessing/ordinal_encoder.py,sha256=HMJKJ6D-uGVWy3GWNGFBXOf98AuG_HzMgh0eRRNkuuw,27956
-snowflake/ml/modeling/preprocessing/polynomial_features.py,sha256=6qFvX4rjQh3C5iPwzY-fo0CYsBxBmHIHzFoPRqkcH0w,52849
-snowflake/ml/modeling/preprocessing/robust_scaler.py,sha256=JGgkPZfgezS4X8YECSjeWDQIoLbU98j43qbwqP2RzZE,11981
-snowflake/ml/modeling/preprocessing/standard_scaler.py,sha256=hu2VnATyizCz-QKv7aaGdATeU8Fyug8MeNxau3-CllQ,10672
+snowflake/ml/modeling/preprocessing/binarizer.py,sha256=M5K-oPgY4lplKjmLdiejdGNjU4UpPliC49vtMHKDoGA,5995
+snowflake/ml/modeling/preprocessing/k_bins_discretizer.py,sha256=KNfv7zedP_u9VrbtSl46olnh5vkxrl0XOKk5zfht7fc,20608
+snowflake/ml/modeling/preprocessing/label_encoder.py,sha256=CKWXe4h4dXQsGiATVlIcdkUHVpfSWSy7nhHesH6_huc,6166
+snowflake/ml/modeling/preprocessing/max_abs_scaler.py,sha256=oOgniB2qzapyHUPhcUT2S_8oSLsKnwanzALgeA5YT5M,7748
+snowflake/ml/modeling/preprocessing/min_max_scaler.py,sha256=nrVKvFOEmg_1ftx_jBMGab2bMjc18nJHIhWyWmLGbUo,9921
+snowflake/ml/modeling/preprocessing/normalizer.py,sha256=EPsQBbqMOCLjoHmT1GdHdD9Ms7wpyyIhAynM4Sz_zxk,6022
+snowflake/ml/modeling/preprocessing/one_hot_encoder.py,sha256=NuFiOXFprlNyZWuJzwVYTkFGfPRm1kPEb_ROlwYfZVk,68331
+snowflake/ml/modeling/preprocessing/ordinal_encoder.py,sha256=gETMNaLPvc2_W3gSTWkoEVJP194RjjHiKzcAynZ2g0k,27501
+snowflake/ml/modeling/preprocessing/polynomial_features.py,sha256=dKnSDl2VWl0PSsCL2DiFSN7HOahR_nDu23rdd0cV5X8,52882
+snowflake/ml/modeling/preprocessing/robust_scaler.py,sha256=JI-w35t2gC_nJ0nnb4G0BIY2_ql-OdwNcYO5OH_aMe4,11228
+snowflake/ml/modeling/preprocessing/standard_scaler.py,sha256=0aOp_vOMi4FmlG0uJ81AsH4QcIiUPGiq7EBO2Dyl6jc,9928
 snowflake/ml/modeling/semi_supervised/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/semi_supervised/label_propagation.py,sha256=mTboTfsYQEZJ0BCuhJLOwNRo2TH2jsI1TuZHacKnj84,53186
-snowflake/ml/modeling/semi_supervised/label_spreading.py,sha256=53caZidl9h0Vhay3jdJLscp1JAy4hNiqUvUdnrkMR5U,53550
+snowflake/ml/modeling/semi_supervised/label_propagation.py,sha256=kfNGr_uv1rpmn-yBBGf9k-T7pVxTrTl15ASAcSVgnWk,53219
+snowflake/ml/modeling/semi_supervised/label_spreading.py,sha256=MlNDOz9hL_3lraxeBKubW1PrS2gYbrtOimlNFgc1jTo,53583
 snowflake/ml/modeling/svm/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/svm/linear_svc.py,sha256=7jHsoPhMevfk_yff8PbfHlQIxPKL67Qnbr40Y_Qkzws,55728
-snowflake/ml/modeling/svm/linear_svr.py,sha256=24rFzxyckshDieb7YjkTvB1TfXkTDe28FUtPpOJ0j1Q,54143
-snowflake/ml/modeling/svm/nu_svc.py,sha256=7eQjwN5mrE86sqteDaI0VW0I28TfFjczxQ4itl85JB8,56442
-snowflake/ml/modeling/svm/nu_svr.py,sha256=3PNHsbgD6jPEzvrQKhwGvf2tDXPhXr9bcinC_l3Moig,53518
-snowflake/ml/modeling/svm/svc.py,sha256=JT99xjA5Kk9jcqfKRxMJOODxLmmgOlIoahLkJsdzUlU,56605
-snowflake/ml/modeling/svm/svr.py,sha256=6-iB8pZirEXlbYRe4UaQBWtRqL9Fp1tC9p9LKTNyrxg,53721
+snowflake/ml/modeling/svm/linear_svc.py,sha256=MatV8huoKeOC--3G9Eb3wx0cY2-DQzmzZH_Ak9uegyY,55761
+snowflake/ml/modeling/svm/linear_svr.py,sha256=L7MZSFl22oNuLQ_RaQ-Mq5l1RShVPpNTRdDKktfsLh0,54176
+snowflake/ml/modeling/svm/nu_svc.py,sha256=6s-mNJ3QcXDPFrQqs7Ec16Y-N6I1BLyA6AnqnquTBIw,56475
+snowflake/ml/modeling/svm/nu_svr.py,sha256=oi7eAds_jnwODf-QOdxJHJUikQ4avnLYa-BvXFHMUCI,53551
+snowflake/ml/modeling/svm/svc.py,sha256=OJ1F1YXLclxN2iXaSK_qkXrYirFwFNURt_9sfKPkX14,56638
+snowflake/ml/modeling/svm/svr.py,sha256=Gxx0v9kYE8Wy6QKO3uLXaGnFDRB2HsWHhUkaXjekPp4,53754
 snowflake/ml/modeling/tree/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/tree/decision_tree_classifier.py,sha256=kTzXtuyn6PBeYRwy_FzCcE_dBrFuk6NMFclXDcE5PmM,58804
-snowflake/ml/modeling/tree/decision_tree_regressor.py,sha256=ymKoxcTqnll58AmfpC6L0EOnLBjl0PWM4_QBzxHb-2w,57500
-snowflake/ml/modeling/tree/extra_tree_classifier.py,sha256=eXIPdPA-vyDxNPl_VzD8Jj0qE4xfhV-zAc5beMs-a8g,58167
-snowflake/ml/modeling/tree/extra_tree_regressor.py,sha256=HFNGqy5VOOSDOiR0bR7fkbWVTrD96DeQYIC7KaU0E-Y,56872
+snowflake/ml/modeling/tree/decision_tree_classifier.py,sha256=df0YiIIInkKBzibmUwubra7AoDhvAvMNjLcbjoI4LOs,58837
+snowflake/ml/modeling/tree/decision_tree_regressor.py,sha256=28_81RU9-4EO9LAqi0z7AYjAmLfEavVN_k74jzzPiKw,57533
+snowflake/ml/modeling/tree/extra_tree_classifier.py,sha256=DSW9WrKYUbartX5ty7qMadNhsoyf6SWwiOmrQfUUJg0,58200
+snowflake/ml/modeling/tree/extra_tree_regressor.py,sha256=NHjyJ8gKB67IYJyTUCEvHVWu2-IC_Ks-xiMtmbd86D0,56905
 snowflake/ml/modeling/xgboost/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/xgboost/xgb_classifier.py,sha256=LAhLngDv_uxgrzWCOozUBH0vVbPhASyayzsWGME7taU,62574
-snowflake/ml/modeling/xgboost/xgb_regressor.py,sha256=_pGLNmfTljuUCqahuGHI_U6x9xuWrgYSdwvwjFaLxPo,62080
-snowflake/ml/modeling/xgboost/xgbrf_classifier.py,sha256=ZQb5dHrKGvxg8cleZh9Jh64qPVCYbU6-ZKve9Zrytt8,62738
-snowflake/ml/modeling/xgboost/xgbrf_regressor.py,sha256=q1iAct-f1e-KFzukPlrQnu5fk-MCTCD-ava-Z2MrhNU,62271
+snowflake/ml/modeling/xgboost/xgb_classifier.py,sha256=8jEp81qvwl1HTP89PaWTt6p98V4bjx0Ji_C8lgRv_GI,62607
+snowflake/ml/modeling/xgboost/xgb_regressor.py,sha256=f7A3tBpV4lEArLV1ULx1E5njAIifV2SZhxkQQf-cRaE,62113
+snowflake/ml/modeling/xgboost/xgbrf_classifier.py,sha256=BxHHK53V6OnNxLWuqG1DS56rijMeP1CUBRv4N7vgvRU,62771
+snowflake/ml/modeling/xgboost/xgbrf_regressor.py,sha256=7UETbSmk98GScpfP-cWkTIbHd2xF-o36YE8p9neO1q0,62304
 snowflake/ml/registry/_schema.py,sha256=7NezDozAqdbOjB9dYHSQQpxapSTKuXqnGrl394bDohc,1381
-snowflake/ml/registry/model_registry.py,sha256=qoPhHaA_fbF-6xTT2jZiYbekaeCrd2xtDsAQl4fFR7U,85709
+snowflake/ml/registry/model_registry.py,sha256=Gd0ukxed_WbhXY9C_fhQKVGBuH3xcitFBnhkY-VldSY,87636
 snowflake/ml/utils/connection_params.py,sha256=W_MwEw1xUARgrDehP_Kz5dmqt1sBXct80xQ7N56qFCc,6138
 snowflake/ml/utils/sparse.py,sha256=1mI2lOm-nMQEwNfbDtHpkJ4SDkKKqsRFyGwSQJJZAiE,3893
-snowflake/ml/version.py,sha256=YNTJssg_NQnP_MEsPYW475Gzld8FPxD_C1H5dD4ItLQ,16
-snowflake_ml_python-1.0.3.dist-info/METADATA,sha256=7J7r_mqwjnPPoFgxubEvGmd2yKTsTvsJlYFIm42Rcp4,13340
-snowflake_ml_python-1.0.3.dist-info/RECORD,,
-snowflake_ml_python-1.0.3.dist-info/WHEEL,sha256=sobxWSyDDkdg_rinUth-jxhXHqoNqlmNMJY3aTZn2Us,91
+snowflake/ml/version.py,sha256=KE81O5TtpALKaBnIQ08QHOVrhdM68RYwu6M-DuQNyfM,16
+snowflake_ml_python-1.0.4.dist-info/METADATA,sha256=dshhkFeQ-wnvxhWK4PirkC4fnTKEdISCdgXCZMvelUo,13989
+snowflake_ml_python-1.0.4.dist-info/RECORD,,
+snowflake_ml_python-1.0.4.dist-info/WHEEL,sha256=sobxWSyDDkdg_rinUth-jxhXHqoNqlmNMJY3aTZn2Us,91
```

