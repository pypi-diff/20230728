# Comparing `tmp/esi_syncopy-2023.5.tar.gz` & `tmp/esi_syncopy-2023.7.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "esi_syncopy-2023.5.tar", max compression
+gzip compressed data, was "esi_syncopy-2023.7.tar", max compression
```

## Comparing `esi_syncopy-2023.5.tar` & `esi_syncopy-2023.7.tar`

### file list

```diff
@@ -1,128 +1,135 @@
--rw-r--r--   0        0        0     1597 2023-04-05 11:26:16.656767 esi_syncopy-2023.5/LICENSE
--rw-r--r--   0        0        0     4297 2023-05-09 09:07:32.200585 esi_syncopy-2023.5/README.rst
--rw-r--r--   0        0        0     1401 2023-05-09 09:07:32.204584 esi_syncopy-2023.5/pyproject.toml
--rw-r--r--   0        0        0     6875 2023-05-09 09:07:32.204584 esi_syncopy-2023.5/syncopy/__init__.py
--rw-r--r--   0        0        0    17907 2023-04-05 11:26:16.676768 esi_syncopy-2023.5/syncopy/connectivity/AV_compRoutines.py
--rw-r--r--   0        0        0    22230 2023-04-05 11:26:16.676768 esi_syncopy-2023.5/syncopy/connectivity/ST_compRoutines.py
--rw-r--r--   0        0        0      252 2023-04-05 11:26:16.676768 esi_syncopy-2023.5/syncopy/connectivity/__init__.py
--rw-r--r--   0        0        0    28157 2023-05-09 09:07:32.204584 esi_syncopy-2023.5/syncopy/connectivity/connectivity_analysis.py
--rw-r--r--   0        0        0     6277 2023-04-05 11:26:16.676768 esi_syncopy-2023.5/syncopy/connectivity/csd.py
--rw-r--r--   0        0        0     2537 2023-04-05 11:26:16.676768 esi_syncopy-2023.5/syncopy/connectivity/granger.py
--rw-r--r--   0        0        0     7250 2023-04-05 11:26:16.676768 esi_syncopy-2023.5/syncopy/connectivity/wilson_sf.py
--rw-r--r--   0        0        0      859 2023-04-05 11:26:16.676768 esi_syncopy-2023.5/syncopy/datatype/__init__.py
--rw-r--r--   0        0        0    59490 2023-05-09 09:07:32.204584 esi_syncopy-2023.5/syncopy/datatype/base_data.py
--rw-r--r--   0        0        0    29559 2023-05-09 09:07:32.204584 esi_syncopy-2023.5/syncopy/datatype/continuous_data.py
--rw-r--r--   0        0        0    26600 2023-05-09 09:07:32.204584 esi_syncopy-2023.5/syncopy/datatype/discrete_data.py
--rw-r--r--   0        0        0    21746 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/datatype/methods/arithmetic.py
--rw-r--r--   0        0        0     2331 2023-04-05 11:26:16.676768 esi_syncopy-2023.5/syncopy/datatype/methods/copy.py
--rw-r--r--   0        0        0    14320 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/datatype/methods/definetrial.py
--rw-r--r--   0        0        0     8708 2023-04-05 11:26:16.676768 esi_syncopy-2023.5/syncopy/datatype/methods/redefinetrial.py
--rw-r--r--   0        0        0    20133 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/datatype/methods/selectdata.py
--rw-r--r--   0        0        0     7450 2023-04-05 11:26:16.676768 esi_syncopy-2023.5/syncopy/datatype/methods/show.py
--rw-r--r--   0        0        0    42546 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/datatype/selector.py
--rw-r--r--   0        0        0     5015 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/datatype/util.py
--rw-r--r--   0        0        0      591 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/io/__init__.py
--rw-r--r--   0        0        0    18316 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/io/load_ft.py
--rw-r--r--   0        0        0    11592 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/io/load_nwb.py
--rw-r--r--   0        0        0    13581 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/io/load_spy_container.py
--rw-r--r--   0        0        0    39102 2023-04-05 23:03:37.371806 esi_syncopy-2023.5/syncopy/io/load_tdt.py
--rw-r--r--   0        0        0    12487 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/io/save_spy_container.py
--rw-r--r--   0        0        0     8532 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/io/utils.py
--rw-r--r--   0        0        0      258 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/plotting/__init__.py
--rw-r--r--   0        0        0     5598 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/plotting/_helpers.py
--rw-r--r--   0        0        0     5239 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/plotting/_plotting.py
--rw-r--r--   0        0        0     2039 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/plotting/config.py
--rw-r--r--   0        0        0     5346 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/plotting/helpers.py
--rw-r--r--   0        0        0     9335 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/plotting/mp_plotting.py
--rw-r--r--   0        0        0    11023 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/plotting/sp_plotting.py
--rw-r--r--   0        0        0     2244 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/plotting/spy_plotting.py
--rw-r--r--   0        0        0      248 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/preproc/__init__.py
--rw-r--r--   0        0        0    30638 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/preproc/compRoutines.py
--rw-r--r--   0        0        0     7086 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/preproc/firws.py
--rw-r--r--   0        0        0    14915 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/preproc/preprocessing.py
--rw-r--r--   0        0        0     8363 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/preproc/resampledata.py
--rw-r--r--   0        0        0     3944 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/preproc/resampling.py
--rw-r--r--   0        0        0      647 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/shared/__init__.py
--rw-r--r--   0        0        0    51088 2023-04-05 23:03:37.371806 esi_syncopy-2023.5/syncopy/shared/computational_routine.py
--rw-r--r--   0        0        0     1826 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/shared/const_def.py
--rw-r--r--   0        0        0     1674 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/shared/dask_helpers.py
--rw-r--r--   0        0        0    16260 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/shared/errors.py
--rw-r--r--   0        0        0      739 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/shared/filetypes.py
--rw-r--r--   0        0        0    14519 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/shared/input_processors.py
--rw-r--r--   0        0        0    38882 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/shared/kwarg_decorators.py
--rw-r--r--   0        0        0     5905 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/shared/latency.py
--rw-r--r--   0        0        0     9274 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/shared/log.py
--rw-r--r--   0        0        0    13913 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/shared/metadata.py
--rw-r--r--   0        0        0    30214 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/shared/parsers.py
--rw-r--r--   0        0        0     1742 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/shared/queries.py
--rw-r--r--   0        0        0    12045 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/shared/tools.py
--rw-r--r--   0        0        0      829 2023-04-05 13:30:10.091089 esi_syncopy-2023.5/syncopy/specest/README.md
--rw-r--r--   0        0        0      254 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/specest/__init__.py
--rw-r--r--   0        0        0     1078 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/specest/_norm_spec.py
--rw-r--r--   0        0        0    44187 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/specest/compRoutines.py
--rw-r--r--   0        0        0     9165 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/specest/fooofspy.py
--rw-r--r--   0        0        0    45570 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/specest/freqanalysis.py
--rw-r--r--   0        0        0     4694 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/specest/mtmconvol.py
--rw-r--r--   0        0        0     4446 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/specest/mtmfft.py
--rw-r--r--   0        0        0     5550 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/specest/stft.py
--rw-r--r--   0        0        0    13340 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/specest/superlet.py
--rw-r--r--   0        0        0     3655 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/specest/wavelet.py
--rw-r--r--   0        0        0      129 2023-04-05 11:26:16.680768 esi_syncopy-2023.5/syncopy/specest/wavelets/__init__.py
--rw-r--r--   0        0        0    19699 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/specest/wavelets/transform.py
--rw-r--r--   0        0        0    10511 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/specest/wavelets/wavelets.py
--rw-r--r--   0        0        0      443 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/statistics/__init__.py
--rw-r--r--   0        0        0    12921 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/statistics/compRoutines.py
--rw-r--r--   0        0        0     6788 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/statistics/jackknifing.py
--rw-r--r--   0        0        0     7166 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/statistics/psth.py
--rw-r--r--   0        0        0     8734 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/statistics/spike_psth.py
--rw-r--r--   0        0        0    17812 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/statistics/summary_stats.py
--rw-r--r--   0        0        0     9407 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/statistics/timelockanalysis.py
--rw-r--r--   0        0        0      147 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/synthdata/__init__.py
--rw-r--r--   0        0        0     9539 2023-05-10 08:48:46.210434 esi_syncopy-2023.5/syncopy/synthdata/analog.py
--rw-r--r--   0        0        0     3688 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/synthdata/spikes.py
--rw-r--r--   0        0        0     3844 2023-05-09 17:45:16.934978 esi_syncopy-2023.5/syncopy/synthdata/utils.py
--rw-r--r--   0        0        0      958 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/README.md
--rw-r--r--   0        0        0        0 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/__init__.py
--rw-r--r--   0        0        0        0 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/backend/__init__.py
--rwxr-xr-x   0        0        0     1796 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/backend/run_tests.sh
--rw-r--r--   0        0        0     9769 2023-05-09 17:45:16.934978 esi_syncopy-2023.5/syncopy/tests/backend/test_conn.py
--rw-r--r--   0        0        0     9943 2023-05-09 17:45:16.934978 esi_syncopy-2023.5/syncopy/tests/backend/test_fooofspy.py
--rw-r--r--   0        0        0     4377 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/backend/test_resampling.py
--rw-r--r--   0        0        0    14593 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/backend/test_timefreq.py
--rw-r--r--   0        0        0     2708 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/conftest.py
--rw-r--r--   0        0        0     3975 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/helpers.py
--rw-r--r--   0        0        0     1810 2023-05-09 17:45:16.934978 esi_syncopy-2023.5/syncopy/tests/local_spy.py
--rw-r--r--   0        0        0    11500 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/misc.py
--rwxr-xr-x   0        0        0      195 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/no_slurm.sh
--rw-r--r--   0        0        0      646 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/run_tests.cmd
--rwxr-xr-x   0        0        0     2530 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/run_tests.sh
--rw-r--r--   0        0        0    13196 2023-04-05 23:03:37.371806 esi_syncopy-2023.5/syncopy/tests/test_attach_dataset.py
--rw-r--r--   0        0        0    17727 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/test_basedata.py
--rw-r--r--   0        0        0     5625 2023-05-09 11:00:13.325129 esi_syncopy-2023.5/syncopy/tests/test_cfg.py
--rw-r--r--   0        0        0    23813 2023-05-09 09:07:32.208585 esi_syncopy-2023.5/syncopy/tests/test_computationalroutine.py
--rw-r--r--   0        0        0    35654 2023-05-09 17:45:16.934978 esi_syncopy-2023.5/syncopy/tests/test_connectivity.py
--rw-r--r--   0        0        0    35030 2023-05-09 09:07:32.212585 esi_syncopy-2023.5/syncopy/tests/test_continuousdata.py
--rw-r--r--   0        0        0      958 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/test_datatype_util.py
--rw-r--r--   0        0        0     9059 2023-05-09 09:07:32.212585 esi_syncopy-2023.5/syncopy/tests/test_decorators.py
--rw-r--r--   0        0        0    25596 2023-04-05 23:03:37.375806 esi_syncopy-2023.5/syncopy/tests/test_discretedata.py
--rw-r--r--   0        0        0     2959 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/test_info.py
--rw-r--r--   0        0        0     5131 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/test_logging.py
--rw-r--r--   0        0        0    21856 2023-05-09 17:45:16.934978 esi_syncopy-2023.5/syncopy/tests/test_metadata.py
--rw-r--r--   0        0        0     3611 2023-05-09 09:07:32.212585 esi_syncopy-2023.5/syncopy/tests/test_packagesetup.py
--rwxr-xr-x   0        0        0    13207 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/test_parsers.py
--rw-r--r--   0        0        0    14243 2023-05-09 17:45:16.934978 esi_syncopy-2023.5/syncopy/tests/test_plotting.py
--rw-r--r--   0        0        0    23494 2023-05-09 11:00:13.325129 esi_syncopy-2023.5/syncopy/tests/test_preproc.py
--rw-r--r--   0        0        0    10965 2023-05-09 09:07:32.212585 esi_syncopy-2023.5/syncopy/tests/test_redefinetrial.py
--rw-r--r--   0        0        0     8973 2023-05-09 11:00:13.325129 esi_syncopy-2023.5/syncopy/tests/test_resampledata.py
--rw-r--r--   0        0        0    22068 2023-05-09 09:07:32.212585 esi_syncopy-2023.5/syncopy/tests/test_selectdata.py
--rw-r--r--   0        0        0    67192 2023-04-05 11:26:16.684768 esi_syncopy-2023.5/syncopy/tests/test_specest.py
--rw-r--r--   0        0        0     9735 2023-04-05 23:03:37.375806 esi_syncopy-2023.5/syncopy/tests/test_specest_fooof.py
--rw-r--r--   0        0        0    13844 2023-05-09 09:07:32.212585 esi_syncopy-2023.5/syncopy/tests/test_spike_psth.py
--rw-r--r--   0        0        0    25769 2023-04-05 11:26:16.688768 esi_syncopy-2023.5/syncopy/tests/test_spyio.py
--rw-r--r--   0        0        0     3737 2023-04-05 11:26:16.688768 esi_syncopy-2023.5/syncopy/tests/test_spytools.py
--rw-r--r--   0        0        0    23406 2023-05-09 17:45:16.934978 esi_syncopy-2023.5/syncopy/tests/test_statistics.py
--rw-r--r--   0        0        0     5826 2023-05-09 17:45:16.934978 esi_syncopy-2023.5/syncopy/tests/test_synthdata.py
--rw-r--r--   0        0        0     9202 2023-05-09 11:00:13.325129 esi_syncopy-2023.5/syncopy/tests/test_timelockanalysis.py
--rw-r--r--   0        0        0     5112 2023-04-05 11:26:16.688768 esi_syncopy-2023.5/syncopy/tests/test_tools.py
--rw-r--r--   0        0        0    16810 2023-05-09 09:07:32.212585 esi_syncopy-2023.5/syncopy/tests/test_welch.py
--rw-r--r--   0        0        0     5536 1970-01-01 00:00:00.000000 esi_syncopy-2023.5/PKG-INFO
+-rw-r--r--   0        0        0     1597 2023-04-05 11:26:16.656767 esi_syncopy-2023.7/LICENSE
+-rw-r--r--   0        0        0     4427 2023-07-28 10:10:23.683595 esi_syncopy-2023.7/README.rst
+-rw-r--r--   0        0        0     1466 2023-07-27 14:45:16.898502 esi_syncopy-2023.7/pyproject.toml
+-rw-r--r--   0        0        0     7093 2023-07-24 15:00:17.927543 esi_syncopy-2023.7/syncopy/__init__.py
+-rw-r--r--   0        0        0    17750 2023-07-24 15:00:17.927543 esi_syncopy-2023.7/syncopy/connectivity/AV_compRoutines.py
+-rw-r--r--   0        0        0    21744 2023-07-24 15:00:17.927543 esi_syncopy-2023.7/syncopy/connectivity/ST_compRoutines.py
+-rw-r--r--   0        0        0      251 2023-07-24 15:00:17.927543 esi_syncopy-2023.7/syncopy/connectivity/__init__.py
+-rw-r--r--   0        0        0    27809 2023-07-26 18:30:10.313320 esi_syncopy-2023.7/syncopy/connectivity/connectivity_analysis.py
+-rw-r--r--   0        0        0     6243 2023-07-24 15:00:17.927543 esi_syncopy-2023.7/syncopy/connectivity/csd.py
+-rw-r--r--   0        0        0     2539 2023-07-24 15:00:17.927543 esi_syncopy-2023.7/syncopy/connectivity/granger.py
+-rw-r--r--   0        0        0     7231 2023-07-24 15:00:17.927543 esi_syncopy-2023.7/syncopy/connectivity/wilson_sf.py
+-rw-r--r--   0        0        0      928 2023-07-19 23:03:38.273071 esi_syncopy-2023.7/syncopy/datatype/__init__.py
+-rw-r--r--   0        0        0    59021 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/datatype/base_data.py
+-rw-r--r--   0        0        0    35054 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/datatype/continuous_data.py
+-rw-r--r--   0        0        0    30642 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/datatype/discrete_data.py
+-rw-r--r--   0        0        0    21333 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/datatype/methods/arithmetic.py
+-rw-r--r--   0        0        0     6448 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/datatype/methods/concat.py
+-rw-r--r--   0        0        0     2305 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/datatype/methods/copy.py
+-rw-r--r--   0        0        0    14810 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/datatype/methods/definetrial.py
+-rw-r--r--   0        0        0     8757 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/datatype/methods/redefinetrial.py
+-rw-r--r--   0        0        0    20082 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/datatype/methods/selectdata.py
+-rw-r--r--   0        0        0     7405 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/datatype/methods/show.py
+-rw-r--r--   0        0        0    41825 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/datatype/selector.py
+-rw-r--r--   0        0        0     5011 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/datatype/util.py
+-rw-r--r--   0        0        0      731 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/io/__init__.py
+-rw-r--r--   0        0        0    17633 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/io/load_ft.py
+-rw-r--r--   0        0        0    17026 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/io/load_nwb.py
+-rw-r--r--   0        0        0    13573 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/io/load_spy_container.py
+-rw-r--r--   0        0        0    39413 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/io/load_tdt.py
+-rw-r--r--   0        0        0     6912 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/io/mne_conv.py
+-rw-r--r--   0        0        0    16303 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/io/nwb.py
+-rw-r--r--   0        0        0    12624 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/io/save_spy_container.py
+-rw-r--r--   0        0        0     8644 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/io/utils.py
+-rw-r--r--   0        0        0      256 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/plotting/__init__.py
+-rw-r--r--   0        0        0     5597 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/plotting/_helpers.py
+-rw-r--r--   0        0        0     5270 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/plotting/_plotting.py
+-rw-r--r--   0        0        0     1913 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/plotting/config.py
+-rw-r--r--   0        0        0     5345 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/plotting/helpers.py
+-rw-r--r--   0        0        0     7158 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/plotting/mp_plotting.py
+-rw-r--r--   0        0        0    11108 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/plotting/sp_plotting.py
+-rw-r--r--   0        0        0     9935 2023-07-24 15:00:17.931543 esi_syncopy-2023.7/syncopy/plotting/spike_plotting.py
+-rw-r--r--   0        0        0     2244 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/plotting/spy_plotting.py
+-rw-r--r--   0        0        0      248 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/preproc/__init__.py
+-rw-r--r--   0        0        0    29977 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/preproc/compRoutines.py
+-rw-r--r--   0        0        0     7056 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/preproc/firws.py
+-rw-r--r--   0        0        0    14711 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/preproc/preprocessing.py
+-rw-r--r--   0        0        0     8089 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/preproc/resampledata.py
+-rw-r--r--   0        0        0     3868 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/preproc/resampling.py
+-rw-r--r--   0        0        0      630 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/__init__.py
+-rw-r--r--   0        0        0    51024 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/computational_routine.py
+-rw-r--r--   0        0        0     1717 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/const_def.py
+-rw-r--r--   0        0        0     1643 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/dask_helpers.py
+-rw-r--r--   0        0        0    15518 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/errors.py
+-rw-r--r--   0        0        0      668 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/filetypes.py
+-rw-r--r--   0        0        0    14621 2023-07-26 18:30:10.313320 esi_syncopy-2023.7/syncopy/shared/input_processors.py
+-rw-r--r--   0        0        0    38449 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/kwarg_decorators.py
+-rw-r--r--   0        0        0     5965 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/latency.py
+-rw-r--r--   0        0        0     9521 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/log.py
+-rw-r--r--   0        0        0    14452 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/metadata.py
+-rw-r--r--   0        0        0    30094 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/parsers.py
+-rw-r--r--   0        0        0     1723 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/queries.py
+-rw-r--r--   0        0        0    11969 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/shared/tools.py
+-rw-r--r--   0        0        0      829 2023-04-05 13:30:10.091089 esi_syncopy-2023.7/syncopy/specest/README.md
+-rw-r--r--   0        0        0      252 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/specest/__init__.py
+-rw-r--r--   0        0        0     1078 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/specest/_norm_spec.py
+-rw-r--r--   0        0        0    44427 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/specest/compRoutines.py
+-rw-r--r--   0        0        0     9481 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/specest/fooofspy.py
+-rw-r--r--   0        0        0    45611 2023-07-26 18:30:10.313320 esi_syncopy-2023.7/syncopy/specest/freqanalysis.py
+-rw-r--r--   0        0        0     4777 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/specest/mtmconvol.py
+-rw-r--r--   0        0        0     4632 2023-07-28 09:30:17.315286 esi_syncopy-2023.7/syncopy/specest/mtmfft.py
+-rw-r--r--   0        0        0     5466 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/specest/stft.py
+-rw-r--r--   0        0        0    12798 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/specest/superlet.py
+-rw-r--r--   0        0        0     3669 2023-07-24 15:00:17.935543 esi_syncopy-2023.7/syncopy/specest/wavelet.py
+-rw-r--r--   0        0        0      129 2023-04-05 11:26:16.680768 esi_syncopy-2023.7/syncopy/specest/wavelets/__init__.py
+-rw-r--r--   0        0        0    19303 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/specest/wavelets/transform.py
+-rw-r--r--   0        0        0    10471 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/specest/wavelets/wavelets.py
+-rw-r--r--   0        0        0      441 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/statistics/__init__.py
+-rw-r--r--   0        0        0    12858 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/statistics/compRoutines.py
+-rw-r--r--   0        0        0     6674 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/statistics/jackknifing.py
+-rw-r--r--   0        0        0     7069 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/statistics/psth.py
+-rw-r--r--   0        0        0     8544 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/statistics/spike_psth.py
+-rw-r--r--   0        0        0    17318 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/statistics/summary_stats.py
+-rw-r--r--   0        0        0     9152 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/statistics/timelockanalysis.py
+-rw-r--r--   0        0        0      147 2023-05-09 09:07:32.208585 esi_syncopy-2023.7/syncopy/synthdata/__init__.py
+-rw-r--r--   0        0        0     9697 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/synthdata/analog.py
+-rw-r--r--   0        0        0     3674 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/synthdata/spikes.py
+-rw-r--r--   0        0        0     3850 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/synthdata/utils.py
+-rw-r--r--   0        0        0      958 2023-04-05 11:26:16.684768 esi_syncopy-2023.7/syncopy/tests/README.md
+-rw-r--r--   0        0        0        0 2023-04-05 11:26:16.684768 esi_syncopy-2023.7/syncopy/tests/__init__.py
+-rw-r--r--   0        0        0        0 2023-04-05 11:26:16.684768 esi_syncopy-2023.7/syncopy/tests/backend/__init__.py
+-rwxr-xr-x   0        0        0     1796 2023-04-05 11:26:16.684768 esi_syncopy-2023.7/syncopy/tests/backend/run_tests.sh
+-rw-r--r--   0        0        0     9450 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/backend/test_conn.py
+-rw-r--r--   0        0        0    10143 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/backend/test_fooofspy.py
+-rw-r--r--   0        0        0     4323 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/backend/test_resampling.py
+-rw-r--r--   0        0        0    13666 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/backend/test_timefreq.py
+-rw-r--r--   0        0        0     2701 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/conftest.py
+-rw-r--r--   0        0        0     4586 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/helpers.py
+-rw-r--r--   0        0        0     1681 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/local_spy.py
+-rw-r--r--   0        0        0    11500 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/misc.py
+-rwxr-xr-x   0        0        0      195 2023-04-05 11:26:16.684768 esi_syncopy-2023.7/syncopy/tests/no_slurm.sh
+-rw-r--r--   0        0        0      646 2023-04-05 11:26:16.684768 esi_syncopy-2023.7/syncopy/tests/run_tests.cmd
+-rwxr-xr-x   0        0        0     2530 2023-04-05 11:26:16.684768 esi_syncopy-2023.7/syncopy/tests/run_tests.sh
+-rw-r--r--   0        0        0    13149 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_attach_dataset.py
+-rw-r--r--   0        0        0    18065 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_basedata.py
+-rw-r--r--   0        0        0     5304 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_cfg.py
+-rw-r--r--   0        0        0    24377 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_computationalroutine.py
+-rw-r--r--   0        0        0     3239 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_concat.py
+-rw-r--r--   0        0        0    33968 2023-07-26 18:30:10.313320 esi_syncopy-2023.7/syncopy/tests/test_connectivity.py
+-rw-r--r--   0        0        0    34782 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_continuousdata.py
+-rw-r--r--   0        0        0      952 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_datatype_util.py
+-rw-r--r--   0        0        0     9123 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_decorators.py
+-rw-r--r--   0        0        0    26346 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_discretedata.py
+-rw-r--r--   0        0        0     2981 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_info.py
+-rw-r--r--   0        0        0     5214 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_logging.py
+-rw-r--r--   0        0        0    22612 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_metadata.py
+-rw-r--r--   0        0        0     5760 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_mne_conv.py
+-rw-r--r--   0        0        0    19884 2023-07-24 15:00:17.939543 esi_syncopy-2023.7/syncopy/tests/test_nwb.py
+-rw-r--r--   0        0        0     3588 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_packagesetup.py
+-rwxr-xr-x   0        0        0    12988 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_parsers.py
+-rw-r--r--   0        0        0    16589 2023-07-26 18:30:10.313320 esi_syncopy-2023.7/syncopy/tests/test_plotting.py
+-rw-r--r--   0        0        0    22667 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_preproc.py
+-rw-r--r--   0        0        0    10993 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_redefinetrial.py
+-rw-r--r--   0        0        0     8503 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_resampledata.py
+-rw-r--r--   0        0        0    22808 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_selectdata.py
+-rw-r--r--   0        0        0    68074 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_specest.py
+-rw-r--r--   0        0        0     9850 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_specest_fooof.py
+-rw-r--r--   0        0        0    13510 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_spike_psth.py
+-rw-r--r--   0        0        0    24892 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_spyio.py
+-rw-r--r--   0        0        0     3740 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_spytools.py
+-rw-r--r--   0        0        0    22853 2023-07-26 18:30:10.313320 esi_syncopy-2023.7/syncopy/tests/test_statistics.py
+-rw-r--r--   0        0        0     6214 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_synthdata.py
+-rw-r--r--   0        0        0     8702 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_timelockanalysis.py
+-rw-r--r--   0        0        0     5133 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_tools.py
+-rw-r--r--   0        0        0    17573 2023-07-24 15:00:17.943543 esi_syncopy-2023.7/syncopy/tests/test_welch.py
+-rw-r--r--   0        0        0     5708 1970-01-01 00:00:00.000000 esi_syncopy-2023.7/PKG-INFO
```

### Comparing `esi_syncopy-2023.5/LICENSE` & `esi_syncopy-2023.7/LICENSE`

 * *Files identical despite different names*

### Comparing `esi_syncopy-2023.5/README.rst` & `esi_syncopy-2023.7/README.rst`

 * *Files 6% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 .. image:: https://raw.githubusercontent.com/esi-neuroscience/syncopy/master/doc/source/_static/syncopy_logo_small.png
 	   :alt: Syncopy-Logo
 
 Systems Neuroscience Computing in Python
 ========================================
 
 
-|Conda Version| |PyPi Version| |License|
+|Conda Version| |PyPi Version| |License| |DOI|
 
 .. |Conda Version| image:: https://img.shields.io/conda/vn/conda-forge/esi-syncopy.svg
    :target: https://anaconda.org/conda-forge/esi-syncopy
 .. |PyPI version| image:: https://badge.fury.io/py/esi-syncopy.svg
    :target: https://badge.fury.io/py/esi-syncopy
 .. |License| image:: https://img.shields.io/github/license/esi-neuroscience/syncopy
+.. |DOI| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.8191941.svg
+   :target: https://doi.org/10.5281/zenodo.8191941
 
 |Master Tests| |Master Coverage|
 
 .. |Master Tests| image:: https://github.com/esi-neuroscience/syncopy/actions/workflows/cov_test_workflow.yml/badge.svg?branch=master
    :target: https://github.com/esi-neuroscience/syncopy/actions/workflows/cov_test_workflow.yml
 .. |Master Coverage| image:: https://codecov.io/gh/esi-neuroscience/syncopy/branch/master/graph/badge.svg?token=JEI3QQGNBQ
    :target: https://codecov.io/gh/esi-neuroscience/syncopy
```

### Comparing `esi_syncopy-2023.5/pyproject.toml` & `esi_syncopy-2023.7/pyproject.toml`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 [tool.poetry]
 name = "esi-syncopy"
 packages = [
     {include = "syncopy"}
 ]
-version = "2023.05"
+version = "2023.07"
 license = "BSD-3-Clause"
 readme="README.rst"
 homepage="https://syncopy.org"
 repository="https://github.com/esi-neuroscience/syncopy"
 include = [
     "LICENSE",
 ]
@@ -17,35 +17,38 @@
 	    "Framework :: Jupyter",
 	    "Operating System :: OS Independent"
 ]
 description = "A toolkit for user-friendly large-scale electrophysiology data analysis. Syncopy is compatible with the Matlab toolbox FieldTrip."
 authors = ["Stefan Fürtinger <sfuerti@esi-frankfurt.de>", "Tim Schäfer <tim.schaefer@esi-frankfurt.de>", "Joscha Schmiedt <schmiedt@uni-bremen.de>", "Gregor Mönke <gregor.moenke@esi-frankfurt.de>"]
 
 [tool.poetry.dependencies]
-python = "^3.8"
+python = "^3.8,<3.12"
 h5py = ">=2.9"
 dask = {version=">=2022.6", extras=["distributed"]}
 dask-jobqueue = ">=0.8"
 numpy = ">=1.10"
-scipy = ">=1.5"
+scipy = ">=1.10.0"
 matplotlib = ">=3.5"
 tqdm = ">=4.31"
 natsort = "^8.1.0"
 psutil = ">=5.9"
 fooof = ">=1.0"
+bokeh = "^3.1.1"
 
 [tool.poetry.group.dev.dependencies]
 black = "^22.6.0"
 pytest = "^7.0"
 ipython = ">=8.10"
 pytest-cov = "^3.0.0"
 sphinx-book-theme = ">=1.0.1"
 sphinx-automodapi = "^0.14.1"
 pydata-sphinx-theme = ">=0.13.3"
 numpydoc = "^1.4.0"
 ipdb = "^0.13.9"
 memory-profiler = "^0.60.0"
 flake8 = "^3.9"
+asv = "^0.5.1"
+virtualenv = "^20.23.0"
 
 [build-system]
 requires = ["poetry-core>=1.0.0"]
 build-backend = "poetry.core.masonry.api"
```

### Comparing `esi_syncopy-2023.5/syncopy/__init__.py` & `esi_syncopy-2023.7/syncopy/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -14,35 +14,45 @@
 from importlib.metadata import version, PackageNotFoundError
 import dask.distributed as dd
 
 # Get package version: either via meta-information from egg or via latest git commit
 try:
     __version__ = version("esi-syncopy")
 except PackageNotFoundError:
-    proc = subprocess.Popen("git describe --tags",
-                            stdout=subprocess.PIPE, stderr=subprocess.PIPE,
-                            text=True, shell=True)
+    proc = subprocess.Popen(
+        "git describe --tags",
+        stdout=subprocess.PIPE,
+        stderr=subprocess.PIPE,
+        text=True,
+        shell=True,
+    )
     out, err = proc.communicate()
     if proc.returncode != 0:
-        proc = subprocess.Popen("git rev-parse HEAD:syncopy/__init__.py",
-                                stdout=subprocess.PIPE, stderr=subprocess.PIPE,
-                                text=True, shell=True)
+        proc = subprocess.Popen(
+            "git rev-parse HEAD:syncopy/__init__.py",
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+            text=True,
+            shell=True,
+        )
         out, err = proc.communicate()
         if proc.returncode != 0:
-            msg = "\nSyncopy <core> WARNING: Package is not installed in site-packages nor cloned via git. " +\
-                "Please consider obtaining SyNCoPy sources from supported channels. "
+            msg = (
+                "\nSyncopy <core> WARNING: Package is not installed in site-packages nor cloned via git. "
+                + "Please consider obtaining SyNCoPy sources from supported channels. "
+            )
             print(msg)
             out = "-999"
     __version__ = out.rstrip("\n")
 
 # --- Greeting ---
 
+
 def startup_print_once(message, force=False):
-    """Print message once: do not spam message n times during all n worker imports.
-    """
+    """Print message once: do not spam message n times during all n worker imports."""
     try:
         dd.get_client()
     except ValueError:
         silence_file = os.path.join(os.path.expanduser("~"), ".spy", "silentstartup")
         if force or (os.getenv("SPYSILENTSTARTUP") is None and not os.path.isfile(silence_file)):
             print(message)
 
@@ -59,129 +69,155 @@
 np.set_printoptions(suppress=True, precision=4, linewidth=80)
 
 # Check concurrent computing  setup (if acme is installed, dask is present too)
 # Import `esi_cluster_setup` and `cluster_cleanup` from acme to make the routines
 # available in the `spy` package namespace
 try:
     from acme import esi_cluster_setup, cluster_cleanup
+
     __acme__ = True
 except ImportError:
     __acme__ = False
     # ACME is critical on ESI infrastructure
-    if socket.gethostname().startswith('esi-sv'):
-        msg = "\nSyncopy <core> WARNING: Could not import Syncopy's parallel processing engine ACME. \n" +\
-            "Please consider installing it via conda: \n" +\
-            "\tconda install -c conda-forge esi-acme\n" +\
-            "or using pip:\n" +\
-            "\tpip install esi-acme"
+    if socket.gethostname().startswith("esi-sv"):
+        msg = (
+            "\nSyncopy <core> WARNING: Could not import Syncopy's parallel processing engine ACME. \n"
+            + "Please consider installing it via conda: \n"
+            + "\tconda install -c conda-forge esi-acme\n"
+            + "or using pip:\n"
+            + "\tpip install esi-acme"
+        )
         # do not spam via worker imports
         try:
             dd.get_client()
         except ValueError:
             print(msg)
 
 # (Try to) set up visualization environment
 try:
     import matplotlib.pyplot as plt
     import matplotlib.style as mplstyle
     import matplotlib as mpl
+
     __plt__ = True
 except ImportError:
     __plt__ = False
 
-# See if NWB is available
 try:
     import pynwb
-    __nwb__ = True
+
+    __pynwb__ = True
 except ImportError:
-    __nwb__ = False
+    __pynwb__ = False
 
 # Set package-wide temp directory
 csHome = "/cs/home/{}".format(getpass.getuser())
 if os.environ.get("SPYDIR"):
     __spydir__ = os.path.abspath(os.path.expanduser(os.environ["SPYDIR"]))
     if not os.path.exists(__spydir__):
-        raise ValueError(f"Environment variable SPYDIR set to non-existent or unreadable directory '{__spydir__}'. Please unset SPYDIR or create the directory.")
+        raise ValueError(
+            f"Environment variable SPYDIR set to non-existent or unreadable directory '{__spydir__}'. Please unset SPYDIR or create the directory."
+        )
 else:
-    if os.path.exists(csHome): # ESI cluster.
+    if os.path.exists(csHome):  # ESI cluster.
         __spydir__ = os.path.join(csHome, ".spy")
     else:
         __spydir__ = os.path.abspath(os.path.join(os.path.expanduser("~"), ".spy"))
 
 if os.environ.get("SPYTMPDIR"):
     __storage__ = os.path.abspath(os.path.expanduser(os.environ["SPYTMPDIR"]))
 else:
     __storage__ = os.path.join(__spydir__, "tmp_storage")
 
 if not os.path.exists(__spydir__):
-        os.makedirs(__spydir__, exist_ok=True)
+    os.makedirs(__spydir__, exist_ok=True)
 
 # Set upper bound for temp directory size (in GB)
 __storagelimit__ = 10
 
 # Establish ID and log-file for current session
 __sessionid__ = blake2b(digest_size=2, salt=os.urandom(blake2b.SALT_SIZE)).hexdigest()
 
 # Set max. no. of lines for traceback info shown in prompt
 __tbcount__ = 5
 
 # Set checksum algorithm to be used
 __checksum_algorithm__ = sha1
 
 # Fill namespace
-from . import (
-    shared,
-    io,
-    datatype)
+from . import shared, io, datatype
 
 from .shared import *
 from .io import *
 from .datatype import *
 from .specest import *
 from .connectivity import *
 from .statistics import *
 from .plotting import *
 from .preproc import *
 from .synthdata import *
 
 from .datatype.util import setup_storage, get_dir_size
-storage_tmpdir_size_gb, storage_tmpdir_numfiles = setup_storage()  # Creates the storage dir if needed and computes size and number of files in there if any.
+
+(
+    storage_tmpdir_size_gb,
+    storage_tmpdir_numfiles,
+) = (
+    setup_storage()
+)  # Creates the storage dir if needed and computes size and number of files in there if any.
 spydir_size_gb, spydir_numfiles = get_dir_size(__spydir__, out="GB")
 
 from .shared.log import setup_logging
+
 __logdir__ = None  # Gets set in setup_logging() call below.
 setup_logging(spydir=__spydir__, session=__sessionid__)  # Sets __logdir__.
-startup_print_once(f"Logging to log directory '{__logdir__}'.\nTemporary storage directory set to '{__storage__}'.\n")
+startup_print_once(
+    f"Logging to log directory '{__logdir__}'.\nTemporary storage directory set to '{__storage__}'.\n"
+)
 
 storage_msg = (
-        "\nSyncopy <core> WARNING: {folder_desc}:s '{tmpdir:s}' "
-        + "contains {nfs:d} files taking up a total of {sze:4.2f} GB on disk. \n"
-        + "Please run `spy.cleanup()` and/or manually free up disk space."
-    )
+    "\nSyncopy <core> WARNING: {folder_desc}:s '{tmpdir:s}' "
+    + "contains {nfs:d} files taking up a total of {sze:4.2f} GB on disk. \n"
+    + "Please run `spy.cleanup()` and/or manually free up disk space."
+)
 if storage_tmpdir_size_gb > __storagelimit__:
-    msg_formatted = storage_msg.format(folder_desc="Temporary storage folder", tmpdir=__storage__, nfs=storage_tmpdir_numfiles, sze=storage_tmpdir_size_gb)
+    msg_formatted = storage_msg.format(
+        folder_desc="Temporary storage folder",
+        tmpdir=__storage__,
+        nfs=storage_tmpdir_numfiles,
+        sze=storage_tmpdir_size_gb,
+    )
     startup_print_once(msg_formatted, force=True)
 else:
     # We also check the size of the whole Syncopy cfg folder, as older Syncopy versions placed files directly into it.
     if spydir_size_gb > __storagelimit__:
-        msg_formatted = storage_msg.format(folder_desc="User config folder", tmpdir=__spydir__, nfs=spydir_numfiles, sze=spydir_size_gb)
+        msg_formatted = storage_msg.format(
+            folder_desc="User config folder",
+            tmpdir=__spydir__,
+            nfs=spydir_numfiles,
+            sze=spydir_size_gb,
+        )
         startup_print_once(msg_formatted, force=True)
 
 # Override default traceback (differentiate b/w Jupyter/iPython and regular Python)
 from .shared.errors import SPYExceptionHandler
+
 try:
     ipy = get_ipython()
     import IPython
+
     IPython.core.interactiveshell.InteractiveShell.showtraceback = SPYExceptionHandler
     IPython.core.interactiveshell.InteractiveShell.showsyntaxerror = SPYExceptionHandler
     sys.excepthook = SPYExceptionHandler
 except:
     sys.excepthook = SPYExceptionHandler
 
+# bring logging into global namespace
 from .shared.errors import log
+from .shared.log import set_loglevel
 
 # Manage user-exposed namespace imports
 __all__ = []
 __all__.extend(datatype.__all__)
 __all__.extend(io.__all__)
 __all__.extend(shared.__all__)
 __all__.extend(specest.__all__)
```

### Comparing `esi_syncopy-2023.5/syncopy/connectivity/AV_compRoutines.py` & `esi_syncopy-2023.7/syncopy/connectivity/AV_compRoutines.py`

 * *Files 7% similar despite different names*

```diff
@@ -17,29 +17,29 @@
 # backend method imports
 from .csd import normalize_csd
 from .wilson_sf import wilson_sf, regularize_csd
 from .granger import granger
 
 # syncopy imports
 from syncopy.shared.const_def import spectralDTypes
-from syncopy.shared.computational_routine import ComputationalRoutine, propagate_properties
+from syncopy.shared.computational_routine import (
+    ComputationalRoutine,
+    propagate_properties,
+)
 from syncopy.shared.metadata import metadata_from_hdf5_file, cast_0array
 from syncopy.shared.kwarg_decorators import process_io
 from syncopy.shared.errors import (
     SPYValueError,
 )
 
 
 @process_io
-def normalize_csd_cF(csd_av_dat,
-                     output='abs',
-                     chunkShape=None,
-                     noCompute=False):
+def normalize_csd_cF(csd_av_dat, output="abs", chunkShape=None, noCompute=False):
 
-    """
+    r"""
     Given the trial averaged cross spectral densities,
     calculates the normalizations to arrive at the
     channel x channel coherencies. If ``S_ij(f)`` is the
     averaged cross-spectrum between channel `i` and `j`, the
     coherency [1]_ is defined as:
 
     .. math::
@@ -96,18 +96,18 @@
     """
 
     # it's the same as the input shape!
     outShape = csd_av_dat.shape
 
     # For initialization of computational routine,
     # just return output shape and dtype
-    if output in ['complex', 'fourier']:
-        fmt = spectralDTypes['fourier']
+    if output in ["complex", "fourier"]:
+        fmt = spectralDTypes["fourier"]
     else:
-        fmt = spectralDTypes['abs']
+        fmt = spectralDTypes["abs"]
     if noCompute:
         return outShape, fmt
 
     CS_ij = normalize_csd(csd_av_dat, output)
 
     return CS_ij
 
@@ -125,51 +125,49 @@
 
     See also
     --------
     syncopy.connectivityanalysis : parent metafunction
     """
 
     # the hard wired dimord of the cF
-    dimord = ['time', 'freq', 'channel_i', 'channel_j']
+    dimord = ["time", "freq", "channel_i", "channel_j"]
 
     computeFunction = staticmethod(normalize_csd_cF)
 
     method = ""  # there is no backend
     # 1st argument,the data, gets omitted
     valid_kws = list(signature(normalize_csd_cF).parameters.keys())[1:]
 
     def pre_check(self):
-        '''
+        """
         Make sure we have a trial average,
         so the input data only consists of `1 trial`.
         Can only be performed after initialization!
-        '''
+        """
 
         if self.numTrials is None:
-            lgl = 'Initialize the computational Routine first!'
-            act = 'ComputationalRoutine not initialized!'
+            lgl = "Initialize the computational Routine first!"
+            act = "ComputationalRoutine not initialized!"
             raise SPYValueError(legal=lgl, varname=self.__class__.__name__, actual=act)
 
         if self.numTrials != 1:
             lgl = "1 trial: normalizations can only be done on averaged quantities!"
             act = f"DataSet contains {self.numTrials} trials"
             raise SPYValueError(legal=lgl, varname="data", actual=act)
 
     def process_metadata(self, data, out):
 
-        time_axis = np.any(np.diff(data.trialdefinition)[:,0] != 1)
+        time_axis = np.any(np.diff(data.trialdefinition)[:, 0] != 1)
 
         propagate_properties(data, out, self.keeptrials, time_axis)
         out.freq = data.freq
 
 
 @process_io
-def normalize_ccov_cF(trl_av_dat,
-                      chunkShape=None,
-                      noCompute=False):
+def normalize_ccov_cF(trl_av_dat, chunkShape=None, noCompute=False):
 
     """
     Given the trial averaged cross-covariances,
     we normalize with the 0-lag auto-covariances
     (~averaged single trial variances)
     to arrive at the cross-correlations.
 
@@ -209,15 +207,15 @@
     # it's the same as the input shape!
     outShape = trl_av_dat.shape
 
     # For initialization of computational routine,
     # just return output shape and dtype
     # cross spectra are complex!
     if noCompute:
-        return outShape, spectralDTypes['abs']
+        return outShape, spectralDTypes["abs"]
 
     # re-shape to (nLag x nChannels x nChannels)
     CCov_ij = trl_av_dat[:, 0, ...]
 
     # main diagonal has shape (nChannels x nChannels):
     # the auto-covariances at 0-lag (~stds)
     diag = trl_av_dat[0, 0, ...].diagonal()
@@ -243,32 +241,32 @@
 
     See also
     --------
     syncopy.connectivityanalysis : parent metafunction
     """
 
     # the hard wired dimord of the cF
-    dimord = ['time', 'freq', 'channel_i', 'channel_j']
+    dimord = ["time", "freq", "channel_i", "channel_j"]
 
     computeFunction = staticmethod(normalize_ccov_cF)
 
-    method = ""   # there is no backend
+    method = ""  # there is no backend
     # 1st argument,the data, gets omitted
     valid_kws = list(signature(normalize_ccov_cF).parameters.keys())[1:]
 
     def pre_check(self):
-        '''
+        """
         Make sure we have a trial average,
         so the input data only consists of `1 trial`.
         Can only be performed after initialization!
-        '''
+        """
 
         if self.numTrials is None:
-            lgl = 'Initialize the computational Routine first!'
-            act = 'ComputationalRoutine not initialized!'
+            lgl = "Initialize the computational Routine first!"
+            act = "ComputationalRoutine not initialized!"
             raise SPYValueError(legal=lgl, varname=self.__class__.__name__, actual=act)
 
         if self.numTrials != 1:
             lgl = "1 trial: normalizations can only be done on averaged quantities!"
             act = f"DataSet contains {self.numTrials} trials"
             raise SPYValueError(legal=lgl, varname="data", actual=act)
 
@@ -288,20 +286,15 @@
         # Attach remaining meta-data
         out.samplerate = data.samplerate
         out.channel_i = np.array(data.channel_i[chanSec_i])
         out.channel_j = np.array(data.channel_j[chanSec_j])
 
 
 @process_io
-def granger_cF(csd_av_dat,
-               rtol=5e-6,
-               nIter=100,
-               cond_max=1e4,
-               chunkShape=None,
-               noCompute=False):
+def granger_cF(csd_av_dat, rtol=5e-6, nIter=100, cond_max=1e4, chunkShape=None, noCompute=False):
 
     """
     Given the trial averaged cross spectral densities,
     calculates the pairwise Granger-Geweke causalities
     for all (non-symmetric!) channel combinations
     following the algorithm proposed in [1]_.
 
@@ -385,15 +378,15 @@
     # it's the same as the input shape!
     outShape = csd_av_dat.shape
 
     # For initialization of computational routine,
     # just return output shape and dtype
     # Granger causalities are real
     if noCompute:
-        return outShape, spectralDTypes['abs']
+        return outShape, spectralDTypes["abs"]
 
     # strip off singleton time dimension
     # for the backend calls
     CSD = csd_av_dat[0]
 
     # auto-regularize to `cond_max` condition number
     # maximal regularization factor is 1e-1
@@ -404,19 +397,20 @@
     # call Wilson
     H, Sigma, conv, err = wilson_sf(CSDreg, nIter=nIter, rtol=rtol)
 
     # calculate G-causality
     Granger = granger(CSDreg, H, Sigma)
 
     # format is 'label--cast'
-    metadata = {'converged--bool': np.array(conv),
-                'max rel. err--float': np.array(err),
-                'reg. factor--float': np.array(factor),
-                'initial cond. num--float': np.array(ini_cn)
-                }
+    metadata = {
+        "converged--bool": np.array(conv),
+        "max rel. err--float": np.array(err),
+        "reg. factor--float": np.array(factor),
+        "initial cond. num--float": np.array(ini_cn),
+    }
 
     # reattach dummy time axis
     return Granger[None, ...], metadata
 
 
 class GrangerCausality(ComputationalRoutine):
 
@@ -438,35 +432,40 @@
     #: 'converged' : bool, ``True`` if the algoritm converged successfully
     #:
     #: 'max rel, err' : float, maximum relative error between the input CSD and the spectral factorization
     #:
     #: 'reg. factor' : float, brute force regularization factor in case the CSD is nearly singular
     #:
     #: 'initial cond. num' : float, condition number of the CSD, regularization kicks in if that is too high
-    metadata_keys = ("converged", "max rel. err", "reg. factor", "initial cond. num",)
+    metadata_keys = (
+        "converged",
+        "max rel. err",
+        "reg. factor",
+        "initial cond. num",
+    )
 
     # the hard wired dimord of the cF
-    dimord = ['time', 'freq', 'channel_i', 'channel_j']
+    dimord = ["time", "freq", "channel_i", "channel_j"]
 
     computeFunction = staticmethod(granger_cF)
 
-    method = ""   # there is no backend
+    method = ""  # there is no backend
     # 1st argument,the data, gets omitted
     valid_kws = list(signature(granger_cF).parameters.keys())[1:]
 
     def pre_check(self):
-        '''
+        """
         Make sure we have a trial average,
         so the input data only consists of `1 trial`.
         Can only be performed after initialization!
-        '''
+        """
 
         if self.numTrials is None:
-            lgl = 'Initialize the computational Routine first!'
-            act = 'ComputationalRoutine not initialized!'
+            lgl = "Initialize the computational Routine first!"
+            act = "ComputationalRoutine not initialized!"
             raise SPYValueError(legal=lgl, varname=self.__class__.__name__, actual=act)
 
         if self.numTrials != 1:
             lgl = "1 trial: Granger causality can only be computed on trial averages!"
             act = f"DataSet contains {self.numTrials} trials"
             raise SPYValueError(legal=lgl, varname="data", actual=act)
 
@@ -476,11 +475,11 @@
         out.freq = data.freq
 
         # digest metadata and attach to .info property
         mdata = metadata_from_hdf5_file(out.filename)
 
         for key, value in mdata.items():
             # we always have a (single) trial average here
-            label_cast = key.split('__')[0]
+            label_cast = key.split("__")[0]
             # learn how to serialize
-            label, cast = label_cast.split('--')
+            label, cast = label_cast.split("--")
             out.info[label] = cast_0array(cast, value)
```

### Comparing `esi_syncopy-2023.5/syncopy/connectivity/ST_compRoutines.py` & `esi_syncopy-2023.7/syncopy/connectivity/ST_compRoutines.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,23 +14,24 @@
 
 # backend method imports
 from .csd import csd
 
 # syncopy imports
 from syncopy.shared.const_def import spectralDTypes
 from syncopy.shared.tools import best_match
-from syncopy.shared.computational_routine import ComputationalRoutine, propagate_properties
+from syncopy.shared.computational_routine import (
+    ComputationalRoutine,
+    propagate_properties,
+)
 from syncopy.shared.metadata import metadata_from_hdf5_file, check_freq_hashes
 from syncopy.shared.kwarg_decorators import process_io
 
 
 @process_io
-def spectral_dyadic_product_cF(specs,
-                               chunkShape=None,
-                               noCompute=False):
+def spectral_dyadic_product_cF(specs, chunkShape=None, noCompute=False):
     """
     Single trial cross spectra directly from complex power spectra,
     hence no Fourier transforms are needed and all what is
     left to do is to take the outer product along the channel axis.
 
     In case the spectral input has a taper axis, those get averaged
     out after the cross products are calculated.
@@ -106,36 +107,32 @@
 
     See also
     --------
     syncopy.connectivityanalysis : parent metafunction
     """
 
     # The hard wired dimord of the cF
-    dimord = ['time', 'freq', 'channel_i', 'channel_j']
+    dimord = ["time", "freq", "channel_i", "channel_j"]
 
     computeFunction = staticmethod(spectral_dyadic_product_cF)
 
     # 1st argument, the data, gets omitted.
     valid_kws = list(signature(spectral_dyadic_product_cF).parameters.keys())[1:]
-    valid_kws += ['output']
+    valid_kws += ["output"]
 
     def process_metadata(self, data, out):
 
-        time_axis = np.any(np.diff(data.trialdefinition)[:,0] != 1)
+        time_axis = np.any(np.diff(data.trialdefinition)[:, 0] != 1)
         propagate_properties(data, out, self.keeptrials, time_axis)
         out.freq = data.freq
 
 
 @process_io
-def ppc_column_cF(cross_spectrum,
-                  trl2_idx=None,
-                  hdf5_path=None,
-                  chunkShape=None,
-                  noCompute=False):
-    """
+def ppc_column_cF(cross_spectrum, trl2_idx=None, hdf5_path=None, chunkShape=None, noCompute=False):
+    r"""
     The PPC involves computations on all nTrials(nTrials-1) pairs. This compute function
     in combination with the PPC CR can be used to compute one column, consisting
     of nTrials-1 pairs (and single diagonal entry), of the implicit
     nTrials x nTrials matrix of trial pairs.
 
     We express the dot product of equation 14 of the source publication
     (Vinck 2010) between the two unit vectors associated to the relative phases
@@ -190,15 +187,15 @@
 
     # ppc spectra are real
     if noCompute:
         return outShape, spectralDTypes["abs"]
 
     # get the 2nd trial as array
     with h5py.File(hdf5_path, "r") as h5file:
-        cross_spectrum2 = h5file['data'][trl2_idx]
+        cross_spectrum2 = h5file["data"][trl2_idx]
 
     # first compute the conjugate product for the complete cross spectra
     # (all channels, all freqs and even time if applicable)
     ppc_ij = cross_spectrum * cross_spectrum2.conj()
 
     # now all what remains is to extract the angular distance
     # and take the cosine
@@ -235,27 +232,28 @@
         propagate_properties(data, out, self.keeptrials, time_axis)
         out.freq = data.freq
 
         # same data type, so trivial propagation
         propagate_properties(data, out, self.keeptrials, time_axis)
 
 
-
 @process_io
-def cross_spectra_cF(trl_dat,
-                     samplerate=1,
-                     nSamples=None,
-                     foi=None,
-                     taper="hann",
-                     taper_opt=None,
-                     demean_taper=False,
-                     polyremoval=False,
-                     timeAxis=0,
-                     chunkShape=None,
-                     noCompute=False):
+def cross_spectra_cF(
+    trl_dat,
+    samplerate=1,
+    nSamples=None,
+    foi=None,
+    taper="hann",
+    taper_opt=None,
+    demean_taper=False,
+    polyremoval=False,
+    timeAxis=0,
+    chunkShape=None,
+    noCompute=False,
+):
 
     """
     Single trial Fourier cross spectral estimates between all channels
     of the input data. First all the individual Fourier transforms
     are calculated via a (multi-)tapered FFT, then the pairwise
     cross-spectra are computed.
 
@@ -343,15 +341,15 @@
     mtmfft : :func:`~syncopy.specest.mtmfft.mtmfft`
              (Multi-)tapered Fourier analysis
 
     """
 
     # Re-arrange array if necessary and get dimensional information
     if timeAxis != 0:
-        dat = trl_dat.T       # does not copy but creates view of `trl_dat`
+        dat = trl_dat.T  # does not copy but creates view of `trl_dat`
     else:
         dat = trl_dat
 
     if nSamples is None:
         nSamples = dat.shape[0]
 
     nChannels = dat.shape[1]
@@ -373,28 +371,30 @@
     # cross spectra are complex!
     if noCompute:
         return outShape, spectralDTypes["fourier"]
 
     # detrend
     if polyremoval == 0:
         # SciPy's overwrite_data not working for type='constant' :/
-        dat = detrend(dat, type='constant', axis=0, overwrite_data=True)
+        dat = detrend(dat, type="constant", axis=0, overwrite_data=True)
     elif polyremoval == 1:
-        dat = detrend(dat, type='linear', axis=0, overwrite_data=True)
+        dat = detrend(dat, type="linear", axis=0, overwrite_data=True)
 
-    CS_ij, freqs = csd(dat,
-                       samplerate,
-                       nSamples,
-                       taper=taper,
-                       taper_opt=taper_opt,
-                       demean_taper=demean_taper)
+    CS_ij, freqs = csd(
+        dat,
+        samplerate,
+        nSamples,
+        taper=taper,
+        taper_opt=taper_opt,
+        demean_taper=demean_taper,
+    )
 
     # Hash the freqs and add to second return value.
-    freqs_hash = blake2b(freqs).hexdigest().encode('utf-8')
-    metadata = {'freqs_hash': np.array(freqs_hash)}  # Will have dtype='|S128'
+    freqs_hash = blake2b(freqs).hexdigest().encode("utf-8")
+    metadata = {"freqs_hash": np.array(freqs_hash)}  # Will have dtype='|S128'
 
     return CS_ij[np.newaxis, freq_idx, ...], metadata
 
 
 class CrossSpectra(ComputationalRoutine):
 
     """
@@ -409,43 +409,45 @@
 
     See also
     --------
     syncopy.connectivityanalysis : parent metafunction
     """
 
     # the hard wired dimord of the cF
-    dimord = ['time', 'freq', 'channel_i', 'channel_j']
+    dimord = ["time", "freq", "channel_i", "channel_j"]
 
     computeFunction = staticmethod(cross_spectra_cF)
 
     # 1st argument,the data, gets omitted
     valid_kws = list(signature(cross_spectra_cF).parameters.keys())[1:]
     # hardcode some parameter names which got digested from the frontend
-    valid_kws += ['tapsmofrq', 'nTaper', 'pad', 'output']
+    valid_kws += ["tapsmofrq", "nTaper", "pad", "output"]
 
     def process_metadata(self, data, out):
 
         propagate_properties(data, out, self.keeptrials)
 
         # General-purpose loading of metadata.
         metadata = metadata_from_hdf5_file(out.filename)
         check_freq_hashes(metadata, out)
 
-        out.freq = self.cfg['foi']
+        out.freq = self.cfg["foi"]
 
 
 @process_io
-def cross_covariance_cF(trl_dat,
-                        samplerate=1,
-                        polyremoval=0,
-                        timeAxis=0,
-                        norm=False,
-                        fullOutput=False,
-                        chunkShape=None,
-                        noCompute=False):
+def cross_covariance_cF(
+    trl_dat,
+    samplerate=1,
+    polyremoval=0,
+    timeAxis=0,
+    norm=False,
+    fullOutput=False,
+    chunkShape=None,
+    noCompute=False,
+):
 
     """
     Single trial covariance estimates between all channels
     of the input data. Output consists of all ``(nChannels x nChannels+1)/2``
     different estimates arranged in a symmetric fashion
     (``COV_ij == COV_ji``). The elements on the
     main diagonal (`CS_ii`) are the channel variances.
@@ -495,15 +497,15 @@
     Consequently, this function does **not** perform any error checking and operates
     under the assumption that all inputs have been externally validated and cross-checked.
 
     """
 
     # Re-arrange array if necessary and get dimensional information
     if timeAxis != 0:
-        dat = trl_dat.T       # does not copy but creates view of `trl_dat`
+        dat = trl_dat.T  # does not copy but creates view of `trl_dat`
     else:
         dat = trl_dat
 
     nSamples = dat.shape[0]
     nChannels = dat.shape[1]
 
     # positive lags in time units
@@ -520,30 +522,30 @@
     # cross covariances are real!
     if noCompute:
         return outShape, spectralDTypes["abs"]
 
     # detrend, has to be done after noCompute!
     if polyremoval == 0:
         # SciPy's overwrite_data not working for type='constant' :/
-        dat = detrend(dat, type='constant', axis=0, overwrite_data=True)
+        dat = detrend(dat, type="constant", axis=0, overwrite_data=True)
     elif polyremoval == 1:
-        detrend(dat, type='linear', axis=0, overwrite_data=True)
+        detrend(dat, type="linear", axis=0, overwrite_data=True)
 
     # re-normalize output for different effective overlaps
-    norm_overlap = np.arange(nSamples, nSamples // 2, step = -1)
+    norm_overlap = np.arange(nSamples, nSamples // 2, step=-1)
 
     CC = np.empty(outShape)
     for i in range(nChannels):
         for j in range(i + 1):
-            cc12 = fftconvolve(dat[:, i], dat[::-1, j], mode='same')
-            CC[:, 0, i, j] = cc12[nSamples // 2:] / norm_overlap
+            cc12 = fftconvolve(dat[:, i], dat[::-1, j], mode="same")
+            CC[:, 0, i, j] = cc12[nSamples // 2 :] / norm_overlap
             if i != j:
                 # cross-correlation is symmetric with C(tau) = C(-tau)^T
                 cc21 = cc12[::-1]
-                CC[:, 0, j, i] = cc21[nSamples // 2:] / norm_overlap
+                CC[:, 0, j, i] = cc21[nSamples // 2 :] / norm_overlap
 
     # normalize with products of std
     if norm:
         STDs = np.std(dat, axis=0)
         N = STDs[:, None] * STDs[None, :]
         CC = CC / N
 
@@ -565,15 +567,15 @@
 
     See also
     --------
     syncopy.connectivityanalysis : parent metafunction
     """
 
     # the hard wired dimord of the cF
-    dimord = ['time', 'freq', 'channel_i', 'channel_j']
+    dimord = ["time", "freq", "channel_i", "channel_j"]
 
     computeFunction = staticmethod(cross_covariance_cF)
 
     # 1st argument,the data, gets omitted
     valid_kws = list(signature(cross_covariance_cF).parameters.keys())[1:]
 
     def process_metadata(self, data, out):
```

### Comparing `esi_syncopy-2023.5/syncopy/connectivity/connectivity_analysis.py` & `esi_syncopy-2023.7/syncopy/connectivity/connectivity_analysis.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,30 +4,38 @@
 #
 
 # Builtin/3rd party package imports
 import numpy as np
 
 # Syncopy imports
 import syncopy as spy
-from syncopy.connectivity.AV_compRoutines import NormalizeCrossSpectra, NormalizeCrossCov, GrangerCausality
-from syncopy.connectivity.ST_compRoutines import CrossSpectra, CrossCovariance, SpectralDyadicProduct, PPC_column
+from syncopy.connectivity.AV_compRoutines import (
+    NormalizeCrossSpectra,
+    NormalizeCrossCov,
+    GrangerCausality,
+)
+from syncopy.connectivity.ST_compRoutines import (
+    CrossSpectra,
+    CrossCovariance,
+    SpectralDyadicProduct,
+    PPC_column,
+)
 from syncopy.shared.input_processors import (
     process_taper,
     process_foi,
     process_padding,
     check_effective_parameters,
-    check_passed_kwargs
+    check_passed_kwargs,
+)
+from syncopy.shared.kwarg_decorators import (
+    unwrap_cfg,
+    unwrap_select,
+    detect_parallel_client,
 )
-from syncopy.shared.kwarg_decorators import (unwrap_cfg, unwrap_select,
-                                             detect_parallel_client)
-from syncopy.shared.errors import (
-    SPYValueError,
-    SPYWarning,
-    SPYInfo,
-    SPYTypeError)
+from syncopy.shared.errors import SPYValueError, SPYWarning, SPYInfo, SPYTypeError
 
 from syncopy.datatype import CrossSpectralData, AnalogData, SpectralData
 from syncopy.shared.tools import get_defaults, best_match, get_frontend_cfg
 from syncopy.shared.parsers import data_parser, scalar_parser
 from syncopy.shared.computational_routine import propagate_properties
 from syncopy.statistics import jackknifing as jk
 from syncopy.statistics import summary_stats as st
@@ -35,18 +43,30 @@
 availableMethods = ("coh", "corr", "granger", "csd", "ppc")
 connectivity_outputs = {"abs", "pow", "complex", "fourier", "angle", "real", "imag"}
 
 
 @unwrap_cfg
 @unwrap_select
 @detect_parallel_client
-def connectivityanalysis(data, method="coh", keeptrials=False, output="abs",
-                         foi=None, foilim=None, pad='maxperlen',
-                         polyremoval=0, tapsmofrq=None, nTaper=None,
-                         taper="hann", taper_opt=None, jackknife=False, **kwargs):
+def connectivityanalysis(
+    data,
+    method="coh",
+    keeptrials=False,
+    output="abs",
+    foi=None,
+    foilim=None,
+    pad="maxperlen",
+    polyremoval=0,
+    tapsmofrq=None,
+    nTaper=None,
+    taper="hann",
+    taper_opt=None,
+    jackknife=False,
+    **kwargs,
+):
     """
     Perform connectivity analysis of Syncopy :class:`~syncopy.SpectralData` OR directly
     :class:`~syncopy.AnalogData` objects
 
     In case the input is an :class:`~syncopy.AnalogData` object, a (multi-)tapered Fourier
     analysis is performed implicitly to arrive at the cross spectral densities needed for
     coherence, ppc and Granger causality estimates.
@@ -234,15 +254,15 @@
         data_parser(data, varname="data", writable=None, empty=False)
     except Exception as exc:
         raise exc
 
     if not isinstance(data, (AnalogData, SpectralData)):
         lgl = "either AnalogData or SpectralData as input"
         act = f"{data.__class__.__name__}"
-        raise SPYValueError(lgl, 'data', act)
+        raise SPYValueError(lgl, "data", act)
     timeAxis = data.dimord.index("time")
     # Get everything of interest in local namespace
     defaults = get_defaults(connectivityanalysis)
     lcls = locals()
     # check for ineffective additional kwargs
     check_passed_kwargs(lcls, defaults, frontend_name="connectivity")
 
@@ -250,214 +270,239 @@
 
     # Ensure a valid computational method was selected
     if method not in availableMethods:
         lgl = "'" + "or '".join(opt + "' " for opt in availableMethods)
         raise SPYValueError(legal=lgl, varname="method", actual=method)
 
     if not isinstance(jackknife, bool):
-        raise SPYTypeError(jackknife, 'jackknife', 'boolean')
+        raise SPYTypeError(jackknife, "jackknife", "boolean")
 
-    if jackknife and method not in ['coh', 'granger']:
+    if jackknife and method not in ["coh", "granger"]:
         # makes only sense for these methods
-        spy.log(f"Jackknife is not available for method {method}", level='WARNING',
-                caller='connectivityanalysis')
+        spy.log(
+            f"Jackknife is not available for method {method}",
+            level="WARNING",
+            caller="connectivityanalysis",
+        )
         jackknife = False
 
     # output settings are only relevant for coherence
-    if method != 'coh' and output != defaults['output']:
+    if method != "coh" and output != defaults["output"]:
         msg = f"Setting `output` for method {method} has not effect!"
         SPYWarning(msg)
 
     # if a subset selection is present
     # get sampleinfo and check for equidistancy
     if data.selection is not None:
         sinfo = data.selection.trialdefinition[:, :2]
         # user picked discrete set of time points
     else:
         sinfo = data.sampleinfo
     lenTrials = np.diff(sinfo).squeeze()
 
     # check padding
 
-    if method == "corr" and pad != 'maxperlen':
+    if method == "corr" and pad != "maxperlen":
         lgl = "'maxperlen', no padding needed/allowed for cross-correlations"
         actual = f"{pad}"
         raise SPYValueError(legal=lgl, varname="pad", actual=actual)
 
     # check polyremoval
     if polyremoval is not None:
         scalar_parser(polyremoval, varname="polyremoval", ntype="int_like", lims=[0, 1])
 
     # Prepare keyword dict for logging (use `lcls` to get actually provided
     # keyword values, not defaults set above)
-    log_dict = {"method": method,
-                "keeptrials": keeptrials,
-                "polyremoval": polyremoval,
-                "pad": pad}
+    log_dict = {
+        "method": method,
+        "keeptrials": keeptrials,
+        "polyremoval": polyremoval,
+        "pad": pad,
+    }
 
     # --- method specific processing ---
 
-    if method == 'corr':
+    if method == "corr":
         if not isinstance(data, AnalogData):
             lgl = f"AnalogData instance as input for method {method}"
             actual = f"{data.__class__.__name__}"
-            raise SPYValueError(lgl, 'data', actual)
+            raise SPYValueError(lgl, "data", actual)
 
-        if lcls['foi'] is not None:
-            msg = 'Parameter `foi` has no effect for method `corr`'
+        if lcls["foi"] is not None:
+            msg = "Parameter `foi` has no effect for method `corr`"
             SPYWarning(msg)
 
-        check_effective_parameters(CrossCovariance, defaults, lcls, besides=('jackknife',))
+        check_effective_parameters(CrossCovariance, defaults, lcls, besides=("jackknife",))
 
         # single trial cross-correlations
         if keeptrials:
-            av_compRoutine = None   # no trial average
-            norm = True   # normalize individual trials within the ST CR
+            av_compRoutine = None  # no trial average
+            norm = True  # normalize individual trials within the ST CR
         else:
             av_compRoutine = NormalizeCrossCov()
             norm = False
 
         # parallel computation over trials
-        st_compRoutine = CrossCovariance(samplerate=data.samplerate,
-                                         polyremoval=polyremoval,
-                                         timeAxis=timeAxis,
-                                         norm=norm)
+        st_compRoutine = CrossCovariance(
+            samplerate=data.samplerate,
+            polyremoval=polyremoval,
+            timeAxis=timeAxis,
+            norm=norm,
+        )
         # hard coded as class attribute
         st_dimord = CrossCovariance.dimord
 
     # all these methods need the single trial cross spectra
     # we just have to sort out if we need an mtmfft first
-    elif method in ['csd', 'coh', 'ppc', 'granger']:
+    elif method in ["csd", "coh", "ppc", "granger"]:
         nTrials = len(data.trials)
         if nTrials == 1:
-            lgl = "multi-trial input data, spectral connectivity measures critically depend on trial averaging!"
+            lgl = (
+                "multi-trial input data, spectral connectivity measures critically depend on trial averaging!"
+            )
             act = "only one trial"
-            raise SPYValueError(lgl, 'data', act)
+            raise SPYValueError(lgl, "data", act)
 
-        if keeptrials is not False and method in ('coh', 'ppc', 'granger'):
+        if keeptrials is not False and method in ("coh", "ppc", "granger"):
             lgl = f"False, trial averaging needed for method {method}!"
             act = keeptrials
             raise SPYValueError(lgl, varname="keeptrials", actual=act)
 
         # AnalogData - we have to setup implicit spectral analysis (mtmfft)
         if isinstance(data, AnalogData):
             # the actual number of samples in case of later padding
             nSamples = process_padding(pad, lenTrials, data.samplerate)
 
-            check_effective_parameters(CrossSpectra, defaults, lcls, besides=('jackknife',))
+            check_effective_parameters(CrossSpectra, defaults, lcls, besides=("jackknife",))
 
-            st_compRoutine, st_dimord = cross_spectra(data, method, nSamples,
-                                                      foi, foilim, tapsmofrq,
-                                                      nTaper, taper, taper_opt,
-                                                      polyremoval, log_dict, timeAxis)
+            st_compRoutine, st_dimord = cross_spectra(
+                data,
+                method,
+                nSamples,
+                foi,
+                foilim,
+                tapsmofrq,
+                nTaper,
+                taper,
+                taper_opt,
+                polyremoval,
+                log_dict,
+                timeAxis,
+            )
         # SpectralData input
         elif isinstance(data, SpectralData):
             # cross-spectra need complex input spectra
             if not np.issubdtype(data.data.dtype, np.complexfloating):
                 lgl = "complex valued spectra, set `output='fourier'` in spy.freqanalysis!"
                 act = "real valued spectral data"
-                raise SPYValueError(lgl, 'data', act)
+                raise SPYValueError(lgl, "data", act)
 
-            if method == 'granger':
+            if method == "granger":
                 # check that for SpectralData input, we have empty time axes
                 # no time-resolved Granger supported atm
                 if isinstance(data, SpectralData):
-                    if data.data.shape[data.dimord.index('time')] != len(data.trials):
-                        raise NotImplementedError("Time resolved Granger causality from tf-spectra not available atm")
+                    if data.data.shape[data.dimord.index("time")] != len(data.trials):
+                        raise NotImplementedError(
+                            "Time resolved Granger causality from tf-spectra not available atm"
+                        )
 
             # by constraining to output='fourier', detrimental taper averaging
             # gets already catched by freqanalysis!
 
-            check_effective_parameters(SpectralDyadicProduct, defaults, lcls, besides=('jackknife'),)
+            check_effective_parameters(
+                SpectralDyadicProduct,
+                defaults,
+                lcls,
+                besides=("jackknife"),
+            )
             # there are no free parameters here,
             # everything had to be setup during freqanalysis!
             st_compRoutine = SpectralDyadicProduct()
             st_dimord = SpectralDyadicProduct.dimord
 
     # --- Set up of computation of single trial cross quantities is complete ---
 
-    if method == 'coh':
+    if method == "coh":
         if output not in connectivity_outputs:
             lgl = f"one of {connectivity_outputs}"
             raise SPYValueError(lgl, varname="output", actual=output)
-        log_dict['output'] = output
+        log_dict["output"] = output
 
         # final normalization after trial averaging
         av_compRoutine = NormalizeCrossSpectra(output=output)
 
-    elif method == 'ppc':
+    elif method == "ppc":
         # besides = ['jackknife']
         # spectral analysis only possible with AnalogData
         if isinstance(data, AnalogData):
-            besides = ['taper', 'tapsmofrq', 'nTaper']
+            besides = ["taper", "tapsmofrq", "nTaper"]
         else:
             besides = None
         check_effective_parameters(PPC_column, defaults, lcls, besides=besides)
 
         # this needs to be treated differently, as we need repeated
         # inits of the PPC CR to compute all trial pairs
         av_compRoutine = "ppc"
 
-    elif method == 'granger':
-        besides = ['jackknife']
+    elif method == "granger":
+        besides = ["jackknife"]
         # spectral analysis only possible with AnalogData
         if isinstance(data, AnalogData):
-            besides += ['taper', 'tapsmofrq', 'nTaper']
+            besides += ["taper", "tapsmofrq", "nTaper"]
 
         check_effective_parameters(GrangerCausality, defaults, lcls, besides=besides)
 
         # after trial averaging
         # hardcoded numerical parameters
-        av_compRoutine = GrangerCausality(rtol=5e-6,
-                                          nIter=100,
-                                          cond_max=1e4
-                                          )
+        av_compRoutine = GrangerCausality(rtol=5e-6, nIter=100, cond_max=1e4)
     # here the single trial spectra are the final result
-    elif method == 'csd':
+    elif method == "csd":
         av_compRoutine = None
 
     # -------------------------------------------------
     # Call the chosen single trial ComputationalRoutine
     # -------------------------------------------------
 
     # the single trial results need a new DataSet
     st_out = CrossSpectralData(dimord=st_dimord)
 
     # we need single trials for the jackknife and the PPC
-    keeptrials = True if (keeptrials or (jackknife or method == 'ppc')) else False
+    keeptrials = True if (keeptrials or (jackknife or method == "ppc")) else False
 
     # Perform the trial-parallelized computation of the matrix quantity
-    st_compRoutine.initialize(data,
-                              st_out._stackingDim,
-                              chan_per_worker=None,   # no parallelisation over channels possible
-                              keeptrials=keeptrials)  # True for jackknifing
+    st_compRoutine.initialize(
+        data,
+        st_out._stackingDim,
+        chan_per_worker=None,  # no parallelisation over channels possible
+        keeptrials=keeptrials,
+    )  # True for jackknifing
     st_compRoutine.compute(data, st_out, parallel=kwargs.get("parallel"), log_dict=log_dict)
 
     if jackknife:
         jack_in = st_out  # single trials for the replicates
         # the trial average for the direct estimate by the av_compRoutine
-        st_out = spy.mean(st_out, dim='trials')
+        st_out = spy.mean(st_out, dim="trials")
         # compute all the leave-one-out (loo) trial average replicates
         replicates_avg = jk.trial_avg_replicates(jack_in)
 
     # for single trial cross-corr/cross spectra results
     # keeptrials can be True and hence we are done here
     if av_compRoutine is None:
         st_out.cfg.update(data.cfg)
-        st_out.cfg.update({'connectivityanalysis': new_cfg})
+        st_out.cfg.update({"connectivityanalysis": new_cfg})
         return st_out
 
     # ---------------
     # PPC computation
     # ---------------
 
     # set up nTrials(nTrials-1) pair computations
     # which need an outer loop over nTrials as a single CR
     # can only compute one column of all the nTrials x nTrials combinations
-    elif av_compRoutine == 'ppc':
+    elif av_compRoutine == "ppc":
         # we need to average all the CR results, shapes match
         accumulator = np.zeros(st_out.trials[0].shape, dtype=np.float32)
         nTrials = len(st_out.trials)
         # to create the trial selections
         trl_arr = np.arange(nTrials)
         # upper triangle weights for grand average
         weights = np.arange(1, nTrials) / (nTrials - 1)
@@ -472,25 +517,22 @@
             hdf5_path = st_out._filename
 
             # create selection for upper triangle
             trl_bi = trl_arr < trl_idx
             st_out.selectdata(trials=trl_arr[trl_bi], inplace=True)
 
             # set up CR
-            ppc_CR = PPC_column(trl2_idx=trl2_idx,
-                                hdf5_path=hdf5_path)
+            ppc_CR = PPC_column(trl2_idx=trl2_idx, hdf5_path=hdf5_path)
             # inner result
             trl_pairs = CrossSpectralData(dimord=st_dimord)
-            ppc_CR.initialize(st_out, trl_pairs._stackingDim,
-                              chan_per_worker=None,
-                              keeptrials=True)
+            ppc_CR.initialize(st_out, trl_pairs._stackingDim, chan_per_worker=None, keeptrials=True)
             ppc_CR.compute(st_out, trl_pairs, parallel=kwargs.get("parallel"), log_dict=log_dict)
 
             # now average the nTrials-1 remaining pairs
-            trl_pairs_avg = st.mean(trl_pairs, dim='trials')
+            trl_pairs_avg = st.mean(trl_pairs, dim="trials")
             accumulator += trl_pairs_avg.trials[0] * weights[trl_idx - 1]
 
             # reset selection
             st_out.selection = None
 
         # normalize and create single trial PPC output object
         accumulator *= 2 / nTrials
@@ -505,68 +547,81 @@
     # ComputationalRoutine for the averaged ST output
     # -----------------------------------------------
 
     else:
         out = CrossSpectralData(dimord=st_dimord)
         # now take the trial average from the single trial CR as input
         av_compRoutine.initialize(st_out, out._stackingDim, chan_per_worker=None)
-        av_compRoutine.pre_check()   # make sure we got a trial_average
-        av_compRoutine.compute(st_out, out, parallel=kwargs.get("parallel"),
-                               log_dict=log_dict)
+        av_compRoutine.pre_check()  # make sure we got a trial_average
+        av_compRoutine.compute(st_out, out, parallel=kwargs.get("parallel"), log_dict=log_dict)
 
         # `out` is the direct estimate
         if jackknife:
             jack_rep = CrossSpectralData(dimord=st_dimord)
             av_compRoutine.initialize(replicates_avg, jack_rep._stackingDim)
             # without `pre_check` we can compute the replicates for all loo averages (in parallel!)
-            av_compRoutine.compute(replicates_avg, jack_rep, parallel=kwargs.get("parallel"),
-                                   log_dict=log_dict)
+            av_compRoutine.compute(
+                replicates_avg,
+                jack_rep,
+                parallel=kwargs.get("parallel"),
+                log_dict=log_dict,
+            )
             # now compute bias and variance
             bias, variance = jk.bias_var(out, jack_rep)
 
             bias._persistent_hdf5 = True
             variance._persistent_hdf5 = True
 
             # and attach to output object
-            out._register_dataset('jack_var', inData=variance.data)
-            out._register_dataset('jack_bias', inData=bias.data)
+            out._register_dataset("jack_var", inData=variance.data)
+            out._register_dataset("jack_bias", inData=bias.data)
 
             # for now, as we don't have dynamic properties
             out.jack_var = out._jack_var
             out.jack_bias = out._jack_bias
 
     # attach potential older cfg's from the input
     # to support chained frontend calls..
     out.cfg.update(data.cfg)
     # attach frontend parameters for replay
-    new_cfg.update({'output': output})
-    out.cfg.update({'connectivityanalysis': new_cfg})
+    new_cfg.update({"output": output})
+    out.cfg.update({"connectivityanalysis": new_cfg})
 
     return out
 
 
-def cross_spectra(data, method, nSamples,
-                  foi, foilim, tapsmofrq,
-                  nTaper, taper, taper_opt,
-                  polyremoval, log_dict, timeAxis):
-    '''
+def cross_spectra(
+    data,
+    method,
+    nSamples,
+    foi,
+    foilim,
+    tapsmofrq,
+    nTaper,
+    taper,
+    taper_opt,
+    polyremoval,
+    log_dict,
+    timeAxis,
+):
+    """
     Sets up the CR to compute the single trial cross-spectra from AnalogData
-    '''
+    """
 
     # --- Basic foi sanitization ---
 
     foi, foilim = process_foi(foi, foilim, data.samplerate)
 
     # --- Setting up specific Methods ---
-    if method == 'granger':
+    if method == "granger":
 
         if foi is not None or foilim is not None:
             lgl = "no foi specification for Granger analysis"
             actual = "foi or foilim specification"
-            raise SPYValueError(lgl, 'foi/foilim', actual)
+            raise SPYValueError(lgl, "foi/foilim", actual)
 
         nChannels = len(data.channel)
         nTrials = len(data.trials)
         # warn user if this ratio is not small
         if nChannels / nTrials > 0.1:
             msg = "Multi-channel Granger analysis can be numerically unstable, it is recommended to have at least 10 times the number of trials compared to the number of channels. Try calculating in sub-groups of fewer channels!"
             SPYWarning(msg)
@@ -581,44 +636,54 @@
     # these are the frequencies attached to the CrossSpectralData by the CR!
     if foi is not None:
         foi, _ = best_match(freqs, foi, squash_duplicates=True)
     elif foilim is not None:
         foi, _ = best_match(freqs, foilim, span=True, squash_duplicates=True)
     elif foi is None and foilim is None:
         # Construct array of maximally attainable frequencies
-        msg = (f"Setting frequencies of interest to {freqs[0]:.1f}-"
-               f"{freqs[-1]:.1f}Hz")
+        msg = f"Setting frequencies of interest to {freqs[0]:.1f}-" f"{freqs[-1]:.1f}Hz"
         SPYInfo(msg)
         foi = freqs
 
     # sanitize taper selection and retrieve dpss settings
-    taper, taper_opt = process_taper(taper,
-                                     taper_opt,
-                                     tapsmofrq,
-                                     nTaper,
-                                     keeptapers=False,   # ST_CSD's always average tapers
-                                     foimax=foi.max(),
-                                     samplerate=data.samplerate,
-                                     nSamples=nSamples,
-                                     output="pow")   # ST_CSD's always have this unit/norm
+
+    if data.selection is None:
+        sinfo = data.sampleinfo
+    else:
+        sinfo = data.selection.trialdefinition[:, :2]
+    lenTrials = np.diff(sinfo).squeeze()
+
+    taper, taper_opt = process_taper(
+        taper,
+        taper_opt,
+        tapsmofrq,
+        nTaper,
+        keeptapers=False,  # ST_CSD's always average tapers
+        foimax=foi.max(),
+        samplerate=data.samplerate,
+        nSamples=lenTrials.mean(),
+        output="pow",
+    )  # ST_CSD's always have this unit/norm
 
     log_dict["foi"] = foi
     log_dict["taper"] = taper
-    if taper_opt and taper == 'dpss':
+    if taper_opt and taper == "dpss":
         log_dict["nTaper"] = taper_opt["Kmax"]
         log_dict["tapsmofrq"] = tapsmofrq
     elif taper_opt:
         log_dict["taper_opt"] = taper_opt
 
     # parallel computation over trials
-    st_compRoutine = CrossSpectra(samplerate=data.samplerate,
-                                  nSamples=nSamples,
-                                  taper=taper,
-                                  taper_opt=taper_opt,
-                                  demean_taper=method == 'granger',
-                                  polyremoval=polyremoval,
-                                  timeAxis=timeAxis,
-                                  foi=foi)
+    st_compRoutine = CrossSpectra(
+        samplerate=data.samplerate,
+        nSamples=nSamples,
+        taper=taper,
+        taper_opt=taper_opt,
+        demean_taper=method == "granger",
+        polyremoval=polyremoval,
+        timeAxis=timeAxis,
+        foi=foi,
+    )
     # hard coded as class attribute
     st_dimord = CrossSpectra.dimord
 
     return st_compRoutine, st_dimord
```

### Comparing `esi_syncopy-2023.5/syncopy/connectivity/csd.py` & `esi_syncopy-2023.7/syncopy/connectivity/csd.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,21 +9,23 @@
 
 # syncopy imports
 from syncopy.specest.mtmfft import mtmfft
 from syncopy.shared.errors import SPYValueError
 from syncopy.shared.const_def import spectralConversions
 
 
-def csd(trl_dat,
-        samplerate=1,
-        nSamples=None,
-        taper="hann",
-        taper_opt=None,
-        demean_taper=False,
-        norm=False):
+def csd(
+    trl_dat,
+    samplerate=1,
+    nSamples=None,
+    taper="hann",
+    taper_opt=None,
+    demean_taper=False,
+    norm=False,
+):
 
     """
     Single trial Fourier cross spectral estimates between all channels
     of the input data. First all the individual Fourier transforms
     are calculated via a (multi-)tapered FFT, then the pairwise
     cross-spectra are computed.
 
@@ -97,30 +99,29 @@
 
     # average tapers and transpose:
     # now has shape (nChannels x nChannels x nFreq)
     CS_ij = CS_ij.mean(axis=0).T
 
     if norm:
         # only meaningful for multi-tapering
-        if taper != 'dpss':
+        if taper != "dpss":
             msg = "Normalization of single trial csd only possible with taper='dpss'"
             raise SPYValueError(legal=msg, varname="taper", actual=taper)
         # main diagonal has shape (nChannels x nFreq): the auto spectra
         diag = CS_ij.diagonal()
         # get the needed product pairs of the autospectra
         Ciijj = np.sqrt(diag[:, :, None] * diag[:, None, :]).T
         CS_ij = CS_ij / Ciijj
 
     return CS_ij.transpose(2, 0, 1), freqs
 
 
-def normalize_csd(csd_av_dat,
-                  output='abs'):
+def normalize_csd(csd_av_dat, output="abs"):
 
-    """
+    r"""
     Given the trial averaged cross spectral densities,
     calculates the normalizations to arrive at the
     channel x channel coherencies. If ``S_ij(f)`` is the
     averaged cross-spectrum between channel `i` and `j`, the
     coherency [1]_ is defined as:
 
     .. math::
```

### Comparing `esi_syncopy-2023.5/syncopy/connectivity/granger.py` & `esi_syncopy-2023.7/syncopy/connectivity/granger.py`

 * *Files 0% similar despite different names*

```diff
@@ -56,15 +56,15 @@
     # we need the stacked auto-spectra of the form (nChannel=3):
     #           S_11 S_22 S_33
     # Smat(f) = S_11 S_22 S_33
     #           S_11 S_22 S_33
     Smat = auto_spectra[:, None, :] * np.ones(nChannels)[:, None]
 
     # Granger i->j needs H_ji entry
-    Hmat = np.abs(Hfunc.transpose(0, 2, 1))**2
+    Hmat = np.abs(Hfunc.transpose(0, 2, 1)) ** 2
     # Granger i->j needs Sigma_ji entry
     SigmaJI = np.abs(Sigma.T)
 
     # imag part should be 0
     auto_cov = np.abs(Sigma.diagonal())
     # same stacking as for the auto spectra (without freq axis)
     SigmaII = auto_cov[None, :] * np.ones(nChannels)[:, None]
```

### Comparing `esi_syncopy-2023.5/syncopy/connectivity/wilson_sf.py` & `esi_syncopy-2023.7/syncopy/connectivity/wilson_sf.py`

 * *Files 8% similar despite different names*

```diff
@@ -56,23 +56,23 @@
     """
 
     nFreq = CSD.shape[0]
 
     Ident = np.eye(*CSD.shape[1:])
 
     # attach negative frequencies
-    CSD = np.r_[CSD, CSD[nFreq - 2:0:-1].conj()]
+    CSD = np.r_[CSD, CSD[nFreq - 2 : 0 : -1].conj()]
 
     # nChannel x nChannel
     psi0 = _psi0_initial(CSD)
 
     # initial choice of psi, constant for all z(~f)
     psi = np.tile(psi0, (nFreq, 1, 1))
     # attach negative frequencies
-    psi = np.r_[psi, psi[nFreq - 2:0:-1].conj()]
+    psi = np.r_[psi, psi[nFreq - 2 : 0 : -1].conj()]
 
     g = np.zeros(CSD.shape, dtype=np.complex64)
     converged = False
     # use cholesky for performance
     U = np.linalg.cholesky(CSD)
     for _ in range(nIter):
 
@@ -80,27 +80,26 @@
             psi_inv = np.linalg.inv(psi)
 
             # the bracket of equation 3.1
             # g = psi_inv @ CSD @ psi_inv.conj().transpose(0, 2, 1)
 
             # equivalent using cholesky decomposition
             g = psi_inv @ U
-            g = (g @ g.conj().transpose(0, 2, 1))
+            g = g @ g.conj().transpose(0, 2, 1)
 
         else:
             for i in range(g.shape[0]):
                 C = np.linalg.lstsq(psi[i], CSD[i], rcond=None)[0]
-                g[i] = np.linalg.lstsq(
-                    psi[i], C.conj().T, rcond=None)[0].conj().T
+                g[i] = np.linalg.lstsq(psi[i], C.conj().T, rcond=None)[0].conj().T
 
         gplus, gplus_0 = _plusOperator(g + Ident)
 
         # the 'any' matrix
         S = np.triu(gplus_0)
-        S = S - S.conj().T   # S + S* = 0
+        S = S - S.conj().T  # S + S* = 0
 
         # the next step psi_{tau+1}
         psi = psi @ (gplus + S)
         psi0 = psi0 @ (gplus_0 + S)
 
         # max relative error
         CSDfac = psi @ psi.conj().transpose(0, 2, 1)
@@ -174,15 +173,15 @@
     g0 = beta[0, ...].copy()
 
     # take half of Nyquist bin
     # Dhamala "NewEdits" 28.01.22
     beta[nLag, ...] = 0.5 * beta[nLag, ...]
 
     # Zero out negative lags
-    beta[nLag + 1:, ...] = 0
+    beta[nLag + 1 :, ...] = 0
 
     gp = np.fft.fft(beta, axis=0)
 
     return gp, g0
 
 
 # --- End of Wilson's Algorithm ---
@@ -235,15 +234,15 @@
     """
 
     epsilons = np.logspace(-10, np.log10(eps_max), nSteps)
     I = np.eye(CSD.shape[1])
 
     CondNum = np.linalg.cond(CSD).max()
     iniCondNum = CondNum
-    
+
     # nothing to be done
     if CondNum < cond_max:
         return CSD, 0, iniCondNum
 
     for eps in epsilons:
         CSDreg = CSD + eps * I
         CondNum = np.linalg.cond(CSDreg).max()
```

### Comparing `esi_syncopy-2023.5/syncopy/datatype/__init__.py` & `esi_syncopy-2023.7/syncopy/datatype/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -9,20 +9,22 @@
 from .continuous_data import *
 from .discrete_data import *
 from .methods.definetrial import *
 from .methods.selectdata import *
 from .methods.show import *
 from .methods.copy import *
 from .methods.redefinetrial import *
+from .methods.concat import *
 from .util import *
 
 # Populate local __all__ namespace
 __all__ = []
 __all__.extend(base_data.__all__)
 __all__.extend(continuous_data.__all__)
 __all__.extend(discrete_data.__all__)
 __all__.extend(util.__all__)
 __all__.extend(methods.definetrial.__all__)
 __all__.extend(methods.selectdata.__all__)
 __all__.extend(methods.show.__all__)
 __all__.extend(methods.copy.__all__)
 __all__.extend(methods.redefinetrial.__all__)
+__all__.extend(methods.concat.__all__)
```

### Comparing `esi_syncopy-2023.5/syncopy/datatype/base_data.py` & `esi_syncopy-2023.7/syncopy/datatype/base_data.py`

 * *Files 1% similar despite different names*

```diff
@@ -28,15 +28,21 @@
 from syncopy.shared.tools import SerializableDict
 from syncopy.shared.parsers import (
     array_parser,
     io_parser,
     filename_parser,
     data_parser,
 )
-from syncopy.shared.errors import SPYInfo, SPYTypeError, SPYValueError, SPYError, SPYWarning
+from syncopy.shared.errors import (
+    SPYInfo,
+    SPYTypeError,
+    SPYValueError,
+    SPYError,
+    SPYWarning,
+)
 from syncopy.datatype.methods.definetrial import definetrial as _definetrial
 from syncopy import __version__, __storage__, __acme__, __sessionid__
 
 if __acme__:
     import acme
     import dask
 
@@ -65,15 +71,15 @@
     _infoFileProperties = ("dimord", "_version", "_log", "cfg", "info")
     _hdfFileAttributeProperties = (
         "dimord",
         "_version",
         "_log",
     )
     # all data types have a `trials` property
-    _selectionKeyWords = ('trials',)
+    _selectionKeyWords = ("trials",)
     #: properties that are mapped onto HDF5 datasets
     _hdfFileDatasetProperties = ()
 
     # Checksum algorithm
     _checksum_algorithm = spy.__checksum_algorithm__.__name__
 
     # Dummy allocations of class attributes that are actually initialized in subclasses
@@ -131,16 +137,16 @@
     @property
     def cfg(self):
         """Dictionary of previous operations on data"""
         return self._cfg
 
     @cfg.setter
     def cfg(self, dct):
-        """ For loading only, for processing the frontends
-        extend the existing (empty) cfg dictionary """
+        """For loading only, for processing the frontends
+        extend the existing (empty) cfg dictionary"""
 
         if not isinstance(dct, dict):
             raise SPYTypeError(dct, varname="cfg", expected="dictionary-like object")
         self._cfg = dct
 
     @property
     def info(self):
@@ -306,17 +312,15 @@
                 `propertyName`.
             propertyName : str
                 Name of the property to be filled with the dataset
             ndim : int
                 Number of expected array dimensions.
         """
 
-        fpath, fname = io_parser(
-            filename, varname="filename", isfile=True, exists=True
-        )
+        fpath, fname = io_parser(filename, varname="filename", isfile=True, exists=True)
         filename = os.path.join(fpath, fname)  # ensure `filename` is absolute path
 
         md = self.mode
         if md == "w":
             md = "r+"
 
         isHdf = False
@@ -328,17 +332,15 @@
         if not isHdf:
             raise SPYValueError("accessible HDF5 file", actual=err, varname="data")
 
         h5keys = list(h5f.keys())
         if propertyName not in h5keys and len(h5keys) != 1:
             lgl = "HDF5 file with only one 'data' dataset or single dataset of arbitrary name"
             act = "HDF5 file holding {} data-objects"
-            raise SPYValueError(
-                legal=lgl, actual=act.format(str(len(h5keys))), varname=propertyName
-            )
+            raise SPYValueError(legal=lgl, actual=act.format(str(len(h5keys))), varname=propertyName)
         if len(h5keys) == 1:
             setattr(self, propertyName, h5f[h5keys[0]])
         else:
             setattr(self, propertyName, h5f[propertyName])
 
         self.filename = filename
 
@@ -388,26 +390,30 @@
 
         # or create backing file on disk
         else:
             if self.filename is None:
                 self.filename = self._gen_filename()
 
             if propertyName not in self._hdfFileDatasetProperties:
-                if getattr(self, "_" + propertyName) is not None and not isinstance(getattr(self, "_" + propertyName), h5py.Dataset):
-                    raise SPYValueError(legal="propertyName that does not clash with existing attributes",
-                                        varname='propertyName', actual=propertyName)
+                if getattr(self, "_" + propertyName) is not None and not isinstance(
+                    getattr(self, "_" + propertyName), h5py.Dataset
+                ):
+                    raise SPYValueError(
+                        legal="propertyName that does not clash with existing attributes",
+                        varname="propertyName",
+                        actual=propertyName,
+                    )
 
             h5f = self._get_backing_hdf5_file_handle()
             if h5f is None:
                 with h5py.File(self.filename, "w") as h5f:
                     h5f.create_dataset(propertyName, data=inData)
             else:
                 h5f.create_dataset(propertyName, data=inData)
 
-
         md = self.mode
         if md == "w":
             md = "r+"
         setattr(self, "_" + propertyName, h5py.File(self.filename, md)[propertyName])
 
     def _set_dataset_property_with_dataset(self, inData, propertyName, ndim):
         """Set a dataset property with an already loaded HDF5 dataset
@@ -457,16 +463,17 @@
                 Can only be ``data`` for syncopy objects to be concatenated.
             ndim : int
                 Number of expected array dimensions.
         """
 
         # first catch empty lists
         if len(inData) == 0:
-            msg = ("Trying to set syncopy data with empty list, "
-                   f"setting `{propertyName}` dataset to `None`!")
+            msg = (
+                "Trying to set syncopy data with empty list, " f"setting `{propertyName}` dataset to `None`!"
+            )
             SPYWarning(msg)
             self._set_dataset_property_with_none(None, propertyName, ndim)
             return
 
         # check if we have consistent list entries
         check = np.sum([isinstance(val, np.ndarray) for val in inData])
         # check has to be either 0 (no arrays) or len(inData) (all arrays)
@@ -474,24 +481,22 @@
             lgl = "consistent data types"
             act = "mix of NumPy arrays and other data types"
             raise SPYValueError(lgl, "data", act)
 
         # as we catched empty lists above, and checked against inconsistent
         # types we can do a hard instance check on the 1st entry only
         if isinstance(inData[0], np.ndarray):
-            self._set_dataset_property_with_array_list(inData,
-                                                       propertyName,
-                                                       ndim)
+            self._set_dataset_property_with_array_list(inData, propertyName, ndim)
         # alternatively must be all syncopy data objects
         else:
             for val in inData:
                 data_parser(val)
 
             # this should not happen, as all derived classes hardcoded this in their setters
-            if propertyName != 'data':
+            if propertyName != "data":
                 raise SPYError(f"Cannot concatenate syncopy objects for dataset {propertyName}")
 
             # if we landed here all is clear
             self._set_dataset_property_with_spy_list(inData, ndim)
 
     def _set_dataset_property_with_array_list(self, inData, propertyName, ndim):
         """Set a dataset property with a list of NumPy arrays.
@@ -535,17 +540,15 @@
                 nCol = 3
             else:  # EventData
                 nCol = inData[0].shape[1]
             if any(val.shape[1] != nCol for val in inData):
                 lgl = "NumPy 2d-arrays with {} columns".format(nCol)
                 act = "NumPy arrays of different shape"
                 raise SPYValueError(legal=lgl, varname="data", actual=act)
-            trialLens = [
-                np.nanmax(val[:, self.dimord.index("sample")]) for val in inData
-            ]
+            trialLens = [np.nanmax(val[:, self.dimord.index("sample")]) for val in inData]
 
         nTrials = len(trialLens)
 
         # Use constructed quantities to set up trial layout matrix
         accumSamples = np.cumsum(trialLens)
         trialdefinition = np.zeros((nTrials, 3))
         trialdefinition[1:, 0] = accumSamples[:-1]
@@ -575,32 +578,31 @@
         ndim : int
             Number of expected array dimensions.
         """
 
         # --  dataset shape and object attribute inquiries --
 
         # take the 1st non-empty object as reference
-        i_ref = 0   # to avoid "probably undefined loop variable" linter warning
+        i_ref = 0  # to avoid "probably undefined loop variable" linter warning
         for i_ref, spy_obj in enumerate(inData):
             if spy_obj.data is None:
                 SPYWarning(f"Skipping empty dataset {spy_obj.filename} for concatenation")
                 continue
             else:
                 spy_obj_ref = spy_obj
                 shape_ref = np.array(spy_obj.data.shape)
 
                 if len(shape_ref) != ndim:
                     lgl = f"dataset with dimension of {ndim}"
                     act = f"got dataset with dimension {len(shape_ref)}"
-                    raise SPYValueError(lgl, 'data', act)
+                    raise SPYValueError(lgl, "data", act)
 
                 stacking_dim_ref = spy_obj._stackingDim
                 # collect remaining attribute names like channel, freq, etc.
-                attr_ref = [attr for attr in spy_obj._hdfFileAttributeProperties
-                            if not attr.startswith('_')]
+                attr_ref = [attr for attr in spy_obj._hdfFileAttributeProperties if not attr.startswith("_")]
                 # boolean array to index non-stacking dimensions
                 # for strict shape comparison
                 bvec = np.ones(shape_ref.size, dtype=bool)
                 bvec[stacking_dim_ref] = False
                 break
 
         # now loop again and check against all others
@@ -613,42 +615,42 @@
 
             if spy_obj.data is None:
                 SPYWarning(f"Skipping empty dataset {spy_obj.filename} for concatenation")
                 continue
 
             if spy_obj._stackingDim != stacking_dim_ref:
                 act = f"different stacking dimensions, {stacking_dim_ref} and {spy_obj._stackingDim}"
-                raise SPYValueError(lgl, 'data', act)
+                raise SPYValueError(lgl, "data", act)
 
             # catch mismatching dimensions (2d vs. 3d)
             if len(shape_ref) != len(spy_obj.data.shape):
                 act = f"mismatching shapes, {tuple(shape_ref)} and {spy_obj.data.shape}"
-                raise SPYValueError(lgl, 'data', act)
+                raise SPYValueError(lgl, "data", act)
 
             # shape tuple gets casted by numpy for array subtraction
             if not np.all((shape_ref - spy_obj.data.shape)[bvec] == 0):
                 act = f"mismatching shapes, {tuple(shape_ref)} and {spy_obj.data.shape}"
-                raise SPYValueError(lgl, 'data', act)
+                raise SPYValueError(lgl, "data", act)
 
             # check attributes like channel, freq, etc.
             # this also catches incompatible syncopy data types with same ndim,
             # e.g. SpectralData and CrossSpectralData
             for attr in spy_obj._hdfFileAttributeProperties:
-                if attr.startswith('_'):
+                if attr.startswith("_"):
                     continue
 
                 attr_val = getattr(spy_obj, attr, None)
                 if attr_val is None or attr not in attr_ref:
                     act = f"missing attribute `{attr}` in {spy_obj.filename}"
-                    raise SPYValueError(lgl, 'data', act)
+                    raise SPYValueError(lgl, "data", act)
                 # now hard check values, should be all arrays/sequences
                 # we want identical channel label, freq axis and so on..
                 if not np.all(getattr(spy_obj_ref, attr) == attr_val):
                     act = f"different attribute values for `{attr}`"
-                    raise SPYValueError(lgl, 'data', act)
+                    raise SPYValueError(lgl, "data", act)
 
             # finally increment stack count
             stack_count += spy_obj.data.shape[stacking_dim_ref]
 
         # now we have all we need to compute
         # the shape of the concatenated object
         res_shape = shape_ref
@@ -658,18 +660,17 @@
         trl_gen = chain(*[spy_obj.trials for spy_obj in inData])
 
         # this setter is only valid for empty (new) syncopy objects
         # hence it should be fine to potentially re-define the dimord here
         self._stackingDimLabel = spy_obj_ref._stackingDimLabel
 
         # and route through the generator setter
-        self._set_dataset_property_with_generator(trl_gen,
-                                                  propertyName='data',
-                                                  ndim=len(res_shape),
-                                                  shape=res_shape)
+        self._set_dataset_property_with_generator(
+            trl_gen, propertyName="data", ndim=len(res_shape), shape=res_shape
+        )
 
         # -- set attribute properties --
 
         # attach dummy selection to reference object
         # for easy propagation of properties
         spy.selectdata(spy_obj_ref, inplace=True)
 
@@ -680,18 +681,15 @@
                 if np.issubdtype(type(selection), np.number):
                     selection = [selection]
                 setattr(self, prop, getattr(spy_obj_ref, prop)[selection])
 
         self.samplerate = spy_obj_ref.samplerate
         spy_obj_ref.selection = None
 
-    def _set_dataset_property_with_generator(self, gen,
-                                             propertyName,
-                                             ndim,
-                                             shape=None):
+    def _set_dataset_property_with_generator(self, gen, propertyName, ndim, shape=None):
         """
         Create a dataset from a generator yielding (single trial) numpy arrays.
         If `shape` is not given fall back to HDF5 resizable datasets along
         the stacking dimension.
 
         Expects empty property - will not try to overwrite datasets with generators!
 
@@ -707,38 +705,41 @@
         shape : tuple
             The final shape of the hdf5 dataset. If left at `None`,
             the dataset will be resized along the stacking dimension
             for every trial drawn from the generator
         """
 
         if propertyName not in self._hdfFileDatasetProperties:
-            raise SPYValueError(legal=f"one of {self._hdfFileDatasetProperties}",
-                                varname='propertyName', actual=propertyName)
+            raise SPYValueError(
+                legal=f"one of {self._hdfFileDatasetProperties}",
+                varname="propertyName",
+                actual=propertyName,
+            )
 
         # If there is existing data, get out
         if isinstance(getattr(self, "_" + propertyName), h5py.Dataset):
             lgl = "empty syncopy object"
             act = "non-empty syncopy object"
-            raise SPYValueError(lgl, 'data', act)
+            raise SPYValueError(lgl, "data", act)
 
         # look at 1st trial to determine fixed dimensions
         try:
             trial1 = next(gen)
         except StopIteration:
             lgl = "non-exhausted generator"
             act = "exhausted generator"
-            raise SPYValueError(lgl, 'data', act)
+            raise SPYValueError(lgl, "data", act)
 
         shape1 = list(trial1.shape)  # initial shape
 
         # further generated arrays will be checked against shape1
         if len(shape1) != ndim:
             lgl = f"arrays of dimension {ndim}"
             act = f"got array with dimension {len(shape1)}"
-            raise SPYValueError(lgl, 'data', act)
+            raise SPYValueError(lgl, "data", act)
 
         # boolean array to index non-stacking dimensions
         # for strict shape comparison
         bvec = np.ones(len(shape1), dtype=bool)
         bvec[self._stackingDim] = False
 
         # prepare to resize hdf5
@@ -754,18 +755,15 @@
         # construct slicing index
         stack_idx = [np.s_[:] for _ in range(len(shape))]
 
         # -- write data --
         stack_count = 0
         trlSamples = []  # for constructing the trialdefinition
         with h5py.File(self.filename, "w") as h5f:
-            dset = h5f.create_dataset(propertyName,
-                                      shape=shape,
-                                      maxshape=maxshape,
-                                      dtype=trial1.dtype)
+            dset = h5f.create_dataset(propertyName, shape=shape, maxshape=maxshape, dtype=trial1.dtype)
 
             # we have to plug in the 1st trial already generated
             stack_step = trial1.shape[self._stackingDim]
             stack_idx[self._stackingDim] = np.s_[0:stack_step]
             dset[tuple(stack_idx)] = trial1
             stack_count += stack_step
             trlSamples.append(stack_step)
@@ -773,32 +771,32 @@
             # now stream through the arrays from the generator
             for trial in gen:
 
                 # check shape except stacking dim
                 if not np.all((shape1 - np.array(trial.shape))[bvec] == 0):
                     lgl = "compatible trial shapes"
                     act = f"mismatching shapes, {tuple(shape1)} and {trial.shape}"
-                    raise SPYValueError(lgl, 'data', act)
+                    raise SPYValueError(lgl, "data", act)
 
                 stack_step = trial.shape[self._stackingDim]
                 # we have to resize for every trial if no total shape was given
                 if resize:
                     dset.resize(stack_count + stack_step, axis=self._stackingDim)
-                stack_idx[self._stackingDim] = np.s_[stack_count:stack_count + stack_step]
+                stack_idx[self._stackingDim] = np.s_[stack_count : stack_count + stack_step]
                 dset[tuple(stack_idx)] = trial
                 stack_count += stack_step
                 trlSamples.append(stack_step)
 
-            setattr(self, '_' + propertyName, dset)
+            setattr(self, "_" + propertyName, dset)
 
         self._reopen()
 
         # -- construct trialdefinition --
 
-        if propertyName == 'data':
+        if propertyName == "data":
             si = np.r_[0, np.cumsum(trlSamples)]
             sampleinfo = np.column_stack([si[:-1], si[1:]])
             trialdefinition = np.column_stack([sampleinfo, np.zeros(len(sampleinfo))])
             if self.samplerate is not None:
                 # set standard offset to -1s
                 trialdefinition[:, 2] = -self.samplerate
 
@@ -819,17 +817,15 @@
             if len(self._defaultDimord) not in inData.shape:
                 lgl = "array with {} columns corresponding to dimord {}"
                 lgl = lgl.format(len(self._defaultDimord), self._defaultDimord)
                 act = "array with shape {}".format(str(inData.shape))
                 raise SPYValueError(legal=lgl, varname="data", actual=act)
 
     def _is_empty(self):
-        return all(
-            [getattr(self, "_" + attr, None) is None for attr in self._hdfFileDatasetProperties]
-        )
+        return all([getattr(self, "_" + attr, None) is None for attr in self._hdfFileDatasetProperties])
 
     @property
     def dimord(self):
         """list(str): ordered list of data dimension labels"""
         return self._dimord
 
     @dimord.setter
@@ -851,17 +847,15 @@
         if dims is None:
             self._dimord = None
             return
 
         # this enforces the _defaultDimord
         if set(dims) != set(self._defaultDimord):
             base = "dimensional labels {}"
-            lgl = base.format(
-                "'" + "' x '".join(str(dim) for dim in self._defaultDimord) + "'"
-            )
+            lgl = base.format("'" + "' x '".join(str(dim) for dim in self._defaultDimord) + "'")
             act = base.format("'" + "' x '".join(str(dim) for dim in dims) + "'")
             raise SPYValueError(legal=lgl, varname="dimord", actual=act)
 
         # this enforces that custom dimords are set for every axis
         if len(dims) != len(self._defaultDimord):
             lgl = f"Custom dimord has length {len(self._defaultDimord)}"
             act = f"Custom dimord has length {len(dims)}"
@@ -891,15 +885,15 @@
     @property
     def log(self):
         """str: log of previous operations on data"""
         print(self._log_header + self._log)
 
     @log.setter
     def log(self, msg):
-        """ This appends the assigned msg to the existing log """
+        """This appends the assigned msg to the existing log"""
         if not isinstance(msg, str):
             raise SPYTypeError(msg, varname="log", expected="str")
         prefix = "\n\n|=== {user:s}@{host:s}: {time:s} ===|\n\n\t{caller:s}"
         clr = sys._getframe().f_back.f_code.co_name
         if clr.startswith("_") and not clr.startswith("__"):
             clr = clr[1:]
         self._log += (
@@ -1001,40 +995,38 @@
         if self._trialdefinition is not None:
             return self._trialdefinition[:, :2]
         else:
             return None
 
     @sampleinfo.setter
     def sampleinfo(self, sinfo):
-        raise SPYError(
-            "Cannot set sampleinfo. Use `BaseData.trialdefinition` instead."
-        )
+        raise SPYError("Cannot set sampleinfo. Use `BaseData.trialdefinition` instead.")
 
     @property
     def trial_ids(self):
         """Index list of trials"""
         if self._trialdefinition is not None:
             return self._trial_ids
 
     @property
     def trialintervals(self):
-        """nTrials x 2 :class:`numpy.ndarray` of [start, end] times in seconds """
+        """nTrials x 2 :class:`numpy.ndarray` of [start, end] times in seconds"""
         if self._trialdefinition is not None and self._samplerate is not None:
             # trial lengths in samples
             start_end = self.sampleinfo - self.sampleinfo[:, 0][:, None]
             start_end[:, 1] -= 1  # account for last time point
-           # add offset and convert to seconds
+            # add offset and convert to seconds
             start_end = (start_end + self._t0[:, None]) / self._samplerate
             return start_end
         else:
             return None
 
     @property
     def _t0(self):
-        """ These are the (trigger) offsets """
+        """These are the (trigger) offsets"""
         if self._trialdefinition is not None:
             return self._trialdefinition[:, 2]
         else:
             return None
 
     @property
     def trials(self):
@@ -1113,19 +1105,23 @@
             dsetProp = getattr(self, "_" + propertyName)
             if isinstance(dsetProp, h5py.Dataset):
                 if dsetProp.id.valid != 0:
                     return dsetProp.file
         return None
 
     def _reopen(self):
-        """ Reattach datasets from backing hdf5 file. Respects current `self.mode`."""
+        """Reattach datasets from backing hdf5 file. Respects current `self.mode`."""
         for propertyName in self._hdfFileDatasetProperties:
             dsetProp = getattr(self, "_" + propertyName)
             if isinstance(dsetProp, h5py.Dataset):
-                setattr(self, "_" + propertyName, h5py.File(self.filename, mode=self.mode)[propertyName])
+                setattr(
+                    self,
+                    "_" + propertyName,
+                    h5py.File(self.filename, mode=self.mode)[propertyName],
+                )
 
     def copy(self):
         """
         Create a copy of the entire object on disk.
 
         Returns
         -------
@@ -1209,24 +1205,20 @@
             if self.container is None:
                 raise SPYError(
                     "Object is not associated to an existing spy container - "
                     + "please save object first using an explicit path. "
                 )
             container = filename_parser(self.filename)["folder"]
 
-        spy.save(
-            self, filename=filename, container=container, tag=tag, overwrite=overwrite
-        )
+        spy.save(self, filename=filename, container=container, tag=tag, overwrite=overwrite)
 
     # Helper function generating pseudo-random temp file-names
     def _gen_filename(self):
 
-        fname_hsh = blake2b(
-            digest_size=4, salt=os.urandom(blake2b.SALT_SIZE)
-        ).hexdigest()
+        fname_hsh = blake2b(digest_size=4, salt=os.urandom(blake2b.SALT_SIZE)).hexdigest()
         fname = os.path.join(
             __storage__,
             "spy_{sess:s}_{hash:s}{ext:s}".format(
                 sess=__sessionid__, hash=fname_hsh, ext=self._classname_to_extension()
             ),
         )
         return fname
@@ -1324,17 +1316,15 @@
         if self.selection is not None or other.selection is not None:
             err = "Cannot perform object comparison with existing in-place selection"
             raise SPYError(err)
 
         # Use `_infoFileProperties` to fetch dimensional object props: remove `dimord`
         # (has already been checked by `data_parser` above) and remove `cfg` (two
         # objects might be identical even if their history deviates)
-        dimProps = [
-            prop for prop in self._infoFileProperties if not prop.startswith("_")
-        ]
+        dimProps = [prop for prop in self._infoFileProperties if not prop.startswith("_")]
         dimProps = list(set(dimProps).difference(["dimord", "cfg"]))
         for prop in dimProps:
             val_this = getattr(self, prop)
             val_other = getattr(other, prop)
             if isinstance(val_this, np.ndarray) and isinstance(val_other, np.ndarray):
                 isEqual = val_this.tolist() == val_other.tolist()
             # catch None
@@ -1363,33 +1353,37 @@
                 if hasattr(self, "_" + dsetName) and hasattr(other, "_" + dsetName):
                     val_this = getattr(self, "_" + dsetName)
                     val_other = getattr(other, "_" + dsetName)
                     if isinstance(val_this, h5py.Dataset):
                         isEqual = val_this == val_other
 
                     if not isEqual:
-                        SPYInfo(f"HDF dataset '{dsetName}' mismatch for types '{type(val_this)}' and '{type(val_other)}'")
+                        SPYInfo(
+                            f"HDF dataset '{dsetName}' mismatch for types '{type(val_this)}' and '{type(val_other)}'"
+                        )
                         return False
                 else:
                     SPYInfo(f"HDF dataset mismatch: extra dataset '{dsetName}' in one instance")
                     return False
         else:
             for dsetName in both_hdfFileDatasetProperties:
                 if dsetName != "data":
                     if hasattr(self, "_" + dsetName) and hasattr(other, "_" + dsetName):
                         val_this = getattr(self, "_" + dsetName)
                         val_other = getattr(other, "_" + dsetName)
                         if isinstance(val_this, h5py.Dataset):
-                            #isEqual = True  # This case gets checked by trial below.
+                            # isEqual = True  # This case gets checked by trial below.
                             isEqual = val_this == val_other
                         elif val_this is None and val_other is None:
                             isEqual = True
 
                         if not isEqual:
-                            SPYInfo(f"HDF dataset '{dsetName}' mismatch for types '{type(val_this)}' and '{type(val_other)}'")
+                            SPYInfo(
+                                f"HDF dataset '{dsetName}' mismatch for types '{type(val_this)}' and '{type(val_other)}'"
+                            )
                             return False
                     else:
                         SPYInfo(f"HDF dataset mismatch: extra dataset '{dsetName}' in one instance")
                         return False
 
             # The other object really is a standalone Syncopy class instance and
             # everything but the data itself aligns; now the most expensive part:
@@ -1430,18 +1424,15 @@
         # Set mode
         self.mode = mode
 
         # If any dataset property contains data and no dimord is set, use the
         # default dimord
         if (
             any(
-                [
-                    key in self._hdfFileDatasetProperties and value is not None
-                    for key, value in kwargs.items()
-                ]
+                [key in self._hdfFileDatasetProperties and value is not None for key, value in kwargs.items()]
             )
             and dimord is None
         ):
             self.dimord = self._defaultDimord
         else:
             self.dimord = dimord
 
@@ -1521,11 +1512,8 @@
 
     @property
     def T(self):
         """
         Return a new `FauxTrial` instance with reversed dimensions
         (parroting the NumPy original :func:`numpy.transpose`)
         """
-        return FauxTrial(
-            self.shape[::-1], self.idx[::-1], self.dtype, self.dimord[::-1]
-        )
-
+        return FauxTrial(self.shape[::-1], self.idx[::-1], self.dtype, self.dimord[::-1])
```

### Comparing `esi_syncopy-2023.5/syncopy/datatype/continuous_data.py` & `esi_syncopy-2023.7/syncopy/datatype/continuous_data.py`

 * *Files 17% similar despite different names*

```diff
@@ -11,119 +11,153 @@
 # Builtin/3rd party package imports
 import inspect
 import numpy as np
 from abc import ABC
 from collections.abc import Iterator
 
 # Local imports
-from .base_data import BaseData, FauxTrial, _definetrial
+from .base_data import BaseData, FauxTrial
 from .methods.definetrial import definetrial
 from .base_data import BaseData
 from syncopy.shared.parsers import scalar_parser, array_parser
-from syncopy.shared.errors import SPYValueError, log
+from syncopy.shared.errors import SPYValueError, SPYError
 from syncopy.shared.tools import best_match
 from syncopy.plotting import sp_plotting, mp_plotting
+from syncopy.io.nwb import _analog_timelocked_to_nwbfile
 from .util import TimeIndexer
 
 
+from syncopy import __pynwb__
+
+if __pynwb__:  # pragma: no cover
+    from pynwb import NWBHDF5IO
+
 
 __all__ = ["AnalogData", "SpectralData", "CrossSpectralData", "TimeLockData"]
 
 
 class ContinuousData(BaseData, ABC):
     """Abstract class for uniformly sampled data
 
     Notes
     -----
     This class cannot be instantiated. Use one of the children instead.
 
     """
 
-    _infoFileProperties = BaseData._infoFileProperties + ("samplerate", "channel",)
+    _infoFileProperties = BaseData._infoFileProperties + (
+        "samplerate",
+        "channel",
+    )
     _hdfFileDatasetProperties = BaseData._hdfFileDatasetProperties + ("data",)
     # all continuous data types have a time axis
-    _selectionKeyWords = BaseData._selectionKeyWords + ('latency',)
+    _selectionKeyWords = BaseData._selectionKeyWords + ("latency",)
 
     @property
     def data(self):
         """
         HDF5 dataset property representing contiguous
         data without trialdefinition.
 
         Trials are concatenated along the time axis.
         """
 
         if getattr(self._data, "id", None) is not None:
             if self._data.id.valid == 0:
                 lgl = "open HDF5 file"
                 act = "backing HDF5 file {} has been closed"
-                raise SPYValueError(legal=lgl, actual=act.format(self.filename),
-                                    varname="data")
+                raise SPYValueError(legal=lgl, actual=act.format(self.filename), varname="data")
         return self._data
 
     @data.setter
     def data(self, inData):
 
         self._set_dataset_property(inData, "data")
 
         if inData is None:
             return
 
+    @property
+    def is_time_locked(self):
+
+        # check for equal offsets
+        if not np.unique(self.trialdefinition[:, 2]).size == 1:
+            return False
+
+        # check for equal sample sizes of the trials
+        if not np.unique(np.diff(self.sampleinfo, axis=1)).size == 1:
+            return False
+
+        return True
+
     def __str__(self):
         # Get list of print-worthy attributes
-        ppattrs = [attr for attr in self.__dir__()
-                   if not (attr.startswith("_") or attr in ["log", "trialdefinition"])]
-        ppattrs = [attr for attr in ppattrs
-                   if not (inspect.ismethod(getattr(self, attr))
-                           or isinstance(getattr(self, attr), Iterator))]
+        ppattrs = [
+            attr
+            for attr in self.__dir__()
+            if not (attr.startswith("_") or attr in ["log", "trialdefinition"])
+        ]
+        ppattrs = [
+            attr
+            for attr in ppattrs
+            if not (inspect.ismethod(getattr(self, attr)) or isinstance(getattr(self, attr), Iterator))
+        ]
         if self.__class__.__name__ == "CrossSpectralData":
             ppattrs.remove("channel")
         ppattrs.sort()
 
         # Construct string for pretty-printing class attributes
         dsep = " by "
         hdstr = "Syncopy {clname:s} object with fields\n\n"
         ppstr = hdstr.format(clname=self.__class__.__name__)
         maxKeyLength = max([len(k) for k in ppattrs])
         printString = "{0:>" + str(maxKeyLength + 5) + "} : {1:}\n"
         for attr in ppattrs:
             value = getattr(self, attr)
-            if hasattr(value, 'shape') and attr == "data" and self.sampleinfo is not None:
+            if hasattr(value, "shape") and attr == "data" and self.sampleinfo is not None:
                 tlen = np.unique(np.diff(self.sampleinfo))
                 if tlen.size == 1:
                     trlstr = "of length {} ".format(str(tlen[0]))
                 else:
                     trlstr = ""
-                dsize = np.prod(self.data.shape)*self.data.dtype.itemsize/1024**2
+                dsize = np.prod(self.data.shape) * self.data.dtype.itemsize / 1024**2
                 dunit = "MB"
                 if dsize > 1000:
                     dsize /= 1024
                     dunit = "GB"
                 valueString = "{} trials {}defined on ".format(str(len(self.trials)), trlstr)
-                valueString += "[" + " x ".join([str(numel) for numel in value.shape]) \
-                              + "] {dt:s} {tp:s} " +\
-                              "of size {sz:3.2f} {szu:s}"
-                valueString = valueString.format(dt=self.data.dtype.name,
-                                                 tp=self.data.__class__.__name__,
-                                                 sz=dsize,
-                                                 szu=dunit)
-            elif hasattr(value, 'shape'):
-                valueString = "[" + " x ".join([str(numel) for numel in value.shape]) \
-                              + "] element " + str(type(value))
+                valueString += (
+                    "["
+                    + " x ".join([str(numel) for numel in value.shape])
+                    + "] {dt:s} {tp:s} "
+                    + "of size {sz:3.2f} {szu:s}"
+                )
+                valueString = valueString.format(
+                    dt=self.data.dtype.name,
+                    tp=self.data.__class__.__name__,
+                    sz=dsize,
+                    szu=dunit,
+                )
+            elif hasattr(value, "shape"):
+                valueString = (
+                    "[" + " x ".join([str(numel) for numel in value.shape]) + "] element " + str(type(value))
+                )
             elif isinstance(value, list):
                 if attr == "dimord" and value is not None:
                     valueString = dsep.join(dim for dim in self.dimord)
                 else:
                     valueString = "{0} element list".format(len(value))
             elif isinstance(value, dict):
                 msg = "dictionary with {nk:s}keys{ks:s}"
                 keylist = value.keys()
                 showkeys = len(keylist) < 7
-                valueString = msg.format(nk=str(len(keylist)) + " " if not showkeys else "",
-                                         ks=" '" + "', '".join(key for key in keylist) + "'" if showkeys else "")
+                valueString = msg.format(
+                    nk=str(len(keylist)) + " " if not showkeys else "",
+                    ks=" '" + "', '".join(key for key in keylist) + "'" if showkeys else "",
+                )
             else:
                 valueString = str(value)
             ppstr += printString.format(attr, valueString)
         ppstr += "\nUse `.log` to see object history"
         return ppstr
 
     @property
@@ -132,75 +166,88 @@
             shp = [list(self.data.shape) for k in range(self.sampleinfo.shape[0])]
             for k, sg in enumerate(self.sampleinfo):
                 shp[k][self._stackingDim] = sg[1] - sg[0]
             return [tuple(sp) for sp in shp]
 
     @property
     def channel(self):
-        """ :class:`numpy.ndarray` : list of recording channel names """
+        """:class:`numpy.ndarray` : list of recording channel names"""
         # if data exists but no user-defined channel labels, create them on the fly
         if self._channel is None and self._data is not None:
             nChannel = self.data.shape[self.dimord.index("channel")]
-            return np.array(["channel" + str(i + 1).zfill(len(str(nChannel)))
-                           for i in range(nChannel)])
+            # default labels
+            return np.array(["channel" + str(i + 1).zfill(len(str(nChannel))) for i in range(nChannel)])
         return self._channel
 
     @channel.setter
     def channel(self, channel):
 
         if channel is None:
             self._channel = None
             return
 
         if self.data is None:
-            raise SPYValueError("Syncopy: Cannot assign `channels` without data. " +
-                  "Please assign data first")
-
-        array_parser(channel, varname="channel", ntype="str",
-                     dims=(self.data.shape[self.dimord.index("channel")],))
+            raise SPYValueError(
+                "Syncopy: Cannot assign `channels` without data. " + "Please assign data first"
+            )
+
+        array_parser(
+            channel,
+            varname="channel",
+            ntype="str",
+            dims=(self.data.shape[self.dimord.index("channel")],),
+        )
 
         self._channel = np.array(channel)
 
     @property
     def samplerate(self):
         """float: sampling rate of uniformly sampled data in Hz"""
         return self._samplerate
 
     @samplerate.setter
     def samplerate(self, sr):
         if sr is None:
             self._samplerate = None
             return
 
-        scalar_parser(sr, varname="samplerate", lims=[np.finfo('float').eps, np.inf])
+        scalar_parser(sr, varname="samplerate", lims=[np.finfo("float").eps, np.inf])
         self._samplerate = float(sr)
         # we need a new TimeIndexer
         if self.trialdefinition is not None:
-            self._time = TimeIndexer(self.trialdefinition,
-                                     self.samplerate,
-                                     list(self._trial_ids))
+            self._time = TimeIndexer(self.trialdefinition, self.samplerate, list(self._trial_ids))
 
     @BaseData.trialdefinition.setter
     def trialdefinition(self, trldef):
 
         # all-to-all trialdefinition
         if trldef is None:
             self._trialdefinition = np.array([[0, self.data.shape[self.dimord.index("time")], 0]])
             self._trial_ids = [0]
         else:
             scount = self.data.shape[self.dimord.index("time")]
             array_parser(trldef, varname="trialdefinition", dims=2)
-            array_parser(trldef[:, :2], varname="sampleinfo", hasnan=False,
-                         hasinf=False, ntype="int_like", lims=[0, scount])
+            if trldef.shape[-1] < 3:
+                lgl = "trialdefinition with at least 3 columns: [start, stop, offset]"
+                act = f"got only {trldef.shape[-1]} columns"
+                raise SPYValueError(lgl, "trialdefinition", act)
+
+            array_parser(
+                trldef[:, :2],
+                varname="sampleinfo",
+                hasnan=False,
+                hasinf=False,
+                ntype="int_like",
+                lims=[0, scount],
+            )
 
             self._trialdefinition = trldef.copy()
             self._trial_ids = np.arange(self.sampleinfo.shape[0])
-            self._time = TimeIndexer(self.trialdefinition,
-                                     self.samplerate,
-                                     list(self._trial_ids))
+
+        self._time = TimeIndexer(self.trialdefinition, self.samplerate, list(self._trial_ids))
 
     @property
     def time(self):
         """indexable iterable of the time arrays"""
         if self.samplerate is not None and self.sampleinfo is not None:
             return self._time
 
@@ -307,15 +354,15 @@
     def __init__(self, data=None, channel=None, samplerate=None, **kwargs):
 
         self._channel = None
         self._samplerate = None
         self._data = None
         self._time = None
 
-        self.samplerate = samplerate     # use setter for error-checking
+        self.samplerate = samplerate  # use setter for error-checking
 
         # Call initializer
         super().__init__(data=data, **kwargs)
 
         # catches channel propagation
         # from concatenation of syncopy data objects
         if self._channel is None:
@@ -353,24 +400,26 @@
 
     Data is only read from disk on demand, similar to HDF5 files.
     """
 
     _infoFileProperties = ContinuousData._infoFileProperties
     _defaultDimord = ["time", "channel"]
     _stackingDimLabel = "time"
-    _selectionKeyWords = ContinuousData._selectionKeyWords + ('channel',)
+    _selectionKeyWords = ContinuousData._selectionKeyWords + ("channel",)
 
     # "Constructor"
-    def __init__(self,
-                 data=None,
-                 filename=None,
-                 trialdefinition=None,
-                 samplerate=None,
-                 channel=None,
-                 dimord=None):
+    def __init__(
+        self,
+        data=None,
+        filename=None,
+        trialdefinition=None,
+        samplerate=None,
+        channel=None,
+        dimord=None,
+    ):
         """Initialize an :class:`AnalogData` object.
 
         Parameters
         ----------
             data : 2D :class:numpy.ndarray or HDF5 dataset
                 multi-channel time series data with uniform sampling
             filename : str
@@ -394,101 +443,178 @@
 
         # FIXME: I think escalating `dimord` to `BaseData` should be sufficient so that
         # the `if any(key...) loop in `BaseData.__init__()` takes care of assigning a default dimord
         if dimord is None:
             dimord = self._defaultDimord
 
         # Call parent initializer
-        super().__init__(data=data,
-                         filename=filename,
-                         trialdefinition=trialdefinition,
-                         samplerate=samplerate,
-                         channel=channel,
-                         dimord=dimord)
+        super().__init__(
+            data=data,
+            filename=filename,
+            trialdefinition=trialdefinition,
+            samplerate=samplerate,
+            channel=channel,
+            dimord=dimord,
+        )
 
         # set as instance attribute to allow modification
-        self._hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties + ("samplerate", "channel",)
+        self._hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties + (
+            "samplerate",
+            "channel",
+        )
 
     # implement plotting
     def singlepanelplot(self, shifted=True, **show_kwargs):
 
         figax = sp_plotting.plot_AnalogData(self, shifted, **show_kwargs)
         return figax
 
     def multipanelplot(self, **show_kwargs):
 
         figax = mp_plotting.plot_AnalogData(self, **show_kwargs)
         return figax
 
+    def save_nwb(self, outpath, nwbfile=None, with_trialdefinition=True, is_raw=True):
+        """Save AnalogData in Neurodata Without Borders (NWB) file format.
+        An NWBFile represents a single session of an experiment.
+
+        Parameters
+        ----------
+        outpath : str, path-like. Where to save the NWB file, including file name and `.nwb` extension.
+            All directories in the path must exist. Example: `'mydata.nwb'`.
+
+        nwbfile : :class:`~pynwb.file.NWBFile` instance
+            Set to an existing instance to add an LFP signal with `is_raw=False`
+
+        with_trialdefinition : Boolean, whether to save the trial definition in the NWB file.
+
+        is_raw : Boolean, whether this is raw data (that should never change), as opposed to LFP data that
+            typically originates from some preprocessing, e.g., down-sampling and detrending. Determines where
+            data is stored in the NWB container, to make it easier for other software to interprete what
+            the data represents. If `is_raw` is `True`, the ``ElectricalSeries`` is stored directly in an
+            acquisition of the :class:`pynwb.NWBFile`. If False, it is stored inside an `LFP` instance in
+            a processing group called `ecephys`.
+
+        Returns
+        -------
+        nwbfile : :class:`~pynwb.file.NWBFile` instance
+           Can be used to further add meta-information or even data via the pynwb API.
+           To save use the :class:`pynwb.NWBHDF5IO` interface.
+
+        Notes
+        -----
+        Due to the very general architecture of the NWB format, many fields need to be interpreted
+        by software reading the format. Thus,
+        providing a generic function to save Syncopy data in NWB format is possible only if you know who will read it.
+        Depending on your target software, you may need to manually format the data using pynwb before writing
+        it to disk, or manually open it using pynwb before using it with the target software.
+
+        In place selections are ignored, the full dataset is exported. Create a new Syncopy data object from a selection
+        before calling this function if you want to export a subset only.
+
+        The Syncopy NWB reader only supports the NWB raw data format.
+
+        This function requires the optional 'pynwb' dependency to be installed.
+        """
+        if not __pynwb__:
+            raise SPYError("NWB support is not available. Please install the 'pynwb' package.")
+
+        nwbfile = _analog_timelocked_to_nwbfile(
+            self,
+            nwbfile=nwbfile,
+            with_trialdefinition=with_trialdefinition,
+            is_raw=is_raw,
+        )
+        # Write the file to disk.
+        with NWBHDF5IO(outpath, "w") as io:
+            io.write(nwbfile)
+        return nwbfile
+
 
 class SpectralData(ContinuousData):
     """
     Multi-channel, real or complex spectral data
 
     This class can be used for representing any data with a frequency, channel,
     and optionally a time axis. The datatype can be complex or float.
     """
 
-    _infoFileProperties = ContinuousData._infoFileProperties + ("taper", "freq",)
-    _hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties +\
-        ("samplerate", "channel", "freq",)
+    _infoFileProperties = ContinuousData._infoFileProperties + (
+        "taper",
+        "freq",
+    )
+    _hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties + (
+        "samplerate",
+        "channel",
+        "freq",
+    )
     _defaultDimord = ["time", "taper", "freq", "channel"]
     _stackingDimLabel = "time"
-    _selectionKeyWords = ContinuousData._selectionKeyWords + ('channel', 'frequency', 'taper',)
+    _selectionKeyWords = ContinuousData._selectionKeyWords + (
+        "channel",
+        "frequency",
+        "taper",
+    )
 
     @property
     def taper(self):
-        """ :class:`numpy.ndarray` : list of window functions used """
+        """:class:`numpy.ndarray` : list of window functions used"""
         if self._taper is None and self._data is not None:
             nTaper = self.data.shape[self.dimord.index("taper")]
-            return np.array(["taper" + str(i + 1).zfill(len(str(nTaper)))
-                            for i in range(nTaper)])
+            return np.array(["taper" + str(i + 1).zfill(len(str(nTaper))) for i in range(nTaper)])
         return self._taper
 
     @taper.setter
     def taper(self, tpr):
 
         if tpr is None:
             self._taper = None
             return
 
         if self.data is None:
-            print("Syncopy core - taper: Cannot assign `taper` without data. "+\
-                  "Please assing data first")
+            print("Syncopy core - taper: Cannot assign `taper` without data. " + "Please assing data first")
 
         try:
-            array_parser(tpr, dims=(self.data.shape[self.dimord.index("taper")],),
-                         varname="taper", ntype="str", )
+            array_parser(
+                tpr,
+                dims=(self.data.shape[self.dimord.index("taper")],),
+                varname="taper",
+                ntype="str",
+            )
         except Exception as exc:
             raise exc
 
         self._taper = np.array(tpr)
 
     @property
     def freq(self):
-        """:class:`numpy.ndarray`: frequency axis in Hz """
+        """:class:`numpy.ndarray`: frequency axis in Hz"""
         # if data exists but no user-defined frequency axis, create one on the fly
         if self._freq is None and self._data is not None:
             return np.arange(self.data.shape[self.dimord.index("freq")])
         return self._freq
 
     @freq.setter
     def freq(self, freq):
 
         if freq is None:
             self._freq = None
             return
 
         if self.data is None:
-            print("Syncopy core - freq: Cannot assign `freq` without data. "+\
-                  "Please assing data first")
+            print("Syncopy core - freq: Cannot assign `freq` without data. " + "Please assing data first")
             return
 
-        array_parser(freq, varname="freq", hasnan=False, hasinf=False,
-                     dims=(self.data.shape[self.dimord.index("freq")],))
+        array_parser(
+            freq,
+            varname="freq",
+            hasnan=False,
+            hasinf=False,
+            dims=(self.data.shape[self.dimord.index("freq")],),
+        )
         self._freq = np.array(freq)
 
     # Helper function that extracts frequency-related indices
     def _get_freq(self, foi=None, foilim=None):
         """
         `foi` is legacy, we use `foilim` for frequency selection
         Error checking is performed by `Selector` class
@@ -509,38 +635,42 @@
 
         else:
             selFreq = slice(None)
 
         return selFreq
 
     # "Constructor"
-    def __init__(self,
-                 data=None,
-                 filename=None,
-                 trialdefinition=None,
-                 samplerate=None,
-                 channel=None,
-                 taper=None,
-                 freq=None,
-                 dimord=None):
+    def __init__(
+        self,
+        data=None,
+        filename=None,
+        trialdefinition=None,
+        samplerate=None,
+        channel=None,
+        taper=None,
+        freq=None,
+        dimord=None,
+    ):
 
         self._taper = None
         self._freq = None
 
         # FIXME: See similar comment above in `AnalogData.__init__()`
         if dimord is None:
             dimord = self._defaultDimord
 
         # Call parent initializer
-        super().__init__(data=data,
-                         filename=filename,
-                         trialdefinition=trialdefinition,
-                         samplerate=samplerate,
-                         channel=channel,
-                         dimord=dimord)
+        super().__init__(
+            data=data,
+            filename=filename,
+            trialdefinition=trialdefinition,
+            samplerate=samplerate,
+            channel=channel,
+            dimord=dimord,
+        )
 
         # If __init__ attached data, be careful
         if self.data is not None:
             # In case of manual data allocation (reading routine would leave a
             # mark in `cfg`), fill in missing info
             if len(self.cfg) == 0:
                 # concat operations will set this!
@@ -572,21 +702,33 @@
     Multi-channel real or complex spectral connectivity data
 
     This class can be used for representing channel-channel interactions involving
     frequency and optionally time or lag. The datatype can be complex or float.
     """
 
     # Adapt `infoFileProperties` and `hdfFileAttributeProperties` from `ContinuousData`
-    _infoFileProperties = BaseData._infoFileProperties +\
-        ("samplerate", "channel_i", "channel_j", "freq", )
-    _hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties +\
-        ("samplerate", "channel_i", "channel_j", "freq", )
+    _infoFileProperties = BaseData._infoFileProperties + (
+        "samplerate",
+        "channel_i",
+        "channel_j",
+        "freq",
+    )
+    _hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties + (
+        "samplerate",
+        "channel_i",
+        "channel_j",
+        "freq",
+    )
     _defaultDimord = ["time", "freq", "channel_i", "channel_j"]
     _stackingDimLabel = "time"
-    _selectionKeyWords = ContinuousData._selectionKeyWords + ('channel_i', 'channel_j', 'frequency',)
+    _selectionKeyWords = ContinuousData._selectionKeyWords + (
+        "channel_i",
+        "channel_j",
+        "frequency",
+    )
     _channel_i = None
     _channel_j = None
     _samplerate = None
     _data = None
 
     # Steal frequency-related stuff from `SpectralData`
     _get_freq = SpectralData._get_freq
@@ -603,120 +745,128 @@
             pass
         else:
             msg = f"CrossSpectralData has no 'channel' to set but dimord: {self._dimord}"
             raise NotImplementedError(msg)
 
     @property
     def channel_i(self):
-        """ :class:`numpy.ndarray` : list of recording channel names """
+        """:class:`numpy.ndarray` : list of recording channel names"""
         # if data exists but no user-defined channel labels, create them on the fly
         if self._channel_i is None and self._data is not None:
             nChannel = self.data.shape[self.dimord.index("channel_i")]
-            return np.array(["channel" + str(i + 1).zfill(len(str(nChannel)))
-                             for i in range(nChannel)])
+            return np.array(["channel" + str(i + 1).zfill(len(str(nChannel))) for i in range(nChannel)])
 
         return self._channel_i
 
     @channel_i.setter
     def channel_i(self, channel_i):
-        """ :class:`numpy.ndarray` : list of channel labels """
+        """:class:`numpy.ndarray` : list of channel labels"""
         if channel_i is None:
             self._channel_i = None
             return
 
         if self.data is None:
-            raise SPYValueError("Syncopy: Cannot assign `channels` without data. " +
-                  "Please assign data first")
+            raise SPYValueError(
+                "Syncopy: Cannot assign `channels` without data. " + "Please assign data first"
+            )
 
         try:
-            array_parser(channel_i, varname="channel_i", ntype="str",
-                         dims=(self.data.shape[self.dimord.index("channel_i")],))
+            array_parser(
+                channel_i,
+                varname="channel_i",
+                ntype="str",
+                dims=(self.data.shape[self.dimord.index("channel_i")],),
+            )
         except Exception as exc:
             raise exc
 
         self._channel_i = np.array(channel_i)
 
     @property
     def channel_j(self):
-        """ :class:`numpy.ndarray` : list of recording channel names """
+        """:class:`numpy.ndarray` : list of recording channel names"""
         # if data exists but no user-defined channel labels, create them on the fly
         if self._channel_j is None and self._data is not None:
             nChannel = self.data.shape[self.dimord.index("channel_j")]
-            return np.array(["channel" + str(i + 1).zfill(len(str(nChannel)))
-                             for i in range(nChannel)])
+            return np.array(["channel" + str(i + 1).zfill(len(str(nChannel))) for i in range(nChannel)])
 
         return self._channel_j
 
     @channel_j.setter
     def channel_j(self, channel_j):
-        """ :class:`numpy.ndarray` : list of channel labels """
+        """:class:`numpy.ndarray` : list of channel labels"""
         if channel_j is None:
             self._channel_j = None
             return
 
         if self.data is None:
-            raise SPYValueError("Syncopy: Cannot assign `channels` without data. " +
-                  "Please assign data first")
+            raise SPYValueError(
+                "Syncopy: Cannot assign `channels` without data. " + "Please assign data first"
+            )
 
         try:
-            array_parser(channel_j, varname="channel_j", ntype="str",
-                         dims=(self.data.shape[self.dimord.index("channel_j")],))
+            array_parser(
+                channel_j,
+                varname="channel_j",
+                ntype="str",
+                dims=(self.data.shape[self.dimord.index("channel_j")],),
+            )
         except Exception as exc:
             raise exc
 
         self._channel_j = np.array(channel_j)
 
-    def __init__(self,
-                 data=None,
-                 filename=None,
-                 channel_i=None,
-                 channel_j=None,
-                 samplerate=None,
-                 freq=None,
-                 dimord=None):
+    def __init__(
+        self,
+        data=None,
+        filename=None,
+        channel_i=None,
+        channel_j=None,
+        samplerate=None,
+        freq=None,
+        dimord=None,
+    ):
 
         self._freq = None
         # Set dimensional labels
         self.dimord = dimord
 
         # Call parent initializer
-        super().__init__(data=data,
-                         filename=filename,
-                         samplerate=samplerate,
-                         dimord=dimord)
+        super().__init__(data=data, filename=filename, samplerate=samplerate, dimord=dimord)
 
         if freq is not None:
             # set frequencies
             self.freq = freq
 
-
     def singlepanelplot(self, **show_kwargs):
 
-        sp_plotting.plot_CrossSpectralData(self, **show_kwargs)
+        return sp_plotting.plot_CrossSpectralData(self, **show_kwargs)
 
 
 class TimeLockData(ContinuousData):
 
     """
     Multi-channel, uniformly-sampled, time-locked data.
     """
 
     _infoFileProperties = ContinuousData._infoFileProperties
     _defaultDimord = ["time", "channel"]
-    _selectionKeyWords = ContinuousData._selectionKeyWords + ('channel',)
+    _selectionKeyWords = ContinuousData._selectionKeyWords + ("channel",)
     _stackingDimLabel = "time"
 
     # "Constructor"
-    def __init__(self,
-                 data=None,
-                 filename=None,
-                 trialdefinition=None,
-                 samplerate=None,
-                 channel=None,
-                 dimord=None):
+    def __init__(
+        self,
+        data=None,
+        filename=None,
+        trialdefinition=None,
+        samplerate=None,
+        channel=None,
+        dimord=None,
+    ):
 
         """
         Initialize an :class:`TimeLockData` object.
 
         Parameters
         ----------
         data : 2D :class:numpy.ndarray or HDF5 dataset
@@ -735,32 +885,38 @@
         """
 
         if dimord is None:
             dimord = self._defaultDimord
 
         # Call parent initializer
         # trialdefinition has to come from a CR!
-        super().__init__(data=data,
-                         filename=filename,
-                         trialdefinition=trialdefinition,
-                         samplerate=samplerate,
-                         channel=channel,
-                         dimord=dimord)
+        super().__init__(
+            data=data,
+            filename=filename,
+            trialdefinition=trialdefinition,
+            samplerate=samplerate,
+            channel=channel,
+            dimord=dimord,
+        )
 
         # A `h5py.Dataset` holding the average of `data`, or `None` if not computed yet.
         self._avg = None
 
         # A `h5py.Dataset` holding variance of `data`, or `None` if not computed yet.
         self._var = None
 
         # A `h5py.Dataset` holding covariance of `data`, or `None` if not computed yet.
         self._cov = None
 
         # set as instance attribute to allow modification
-        self._hdfFileDatasetProperties = ContinuousData._hdfFileDatasetProperties + ("avg", "var", "cov",)
+        self._hdfFileDatasetProperties = ContinuousData._hdfFileDatasetProperties + (
+            "avg",
+            "var",
+            "cov",
+        )
 
     @property
     def avg(self):
         return self._avg
 
     @property
     def var(self):
@@ -785,31 +941,63 @@
         """
 
         # we need parent setter for basic validation
         ContinuousData.trialdefinition.fset(self, trldef)
 
         # now check for additional conditions
 
-        # FIXME: not clear, is timelocked data to be expected
-        # to have same offsets?!
-        # if not np.unique(trldef[:, 2]).size == 1:
-        #     lgl = "equal offsets for timelocked data"
-        #     act = "different offsets"
-        #     raise SPYValueError(lgl, varname="trialdefinition", actual=act)
-
-        # diff-diff should give 0 -> same number of samples for each trial
-        if not np.all(np.diff(trldef, axis=0, n=2) == 0):
-            lgl = "all trials of same length for timelocked data"
-            act = "unequal sized trials defined"
-            raise SPYValueError(lgl, varname="trialdefinition", actual=act)
+        if not self.is_time_locked:
+            lgl = "trialdefinition with equally sized trials and common offsets"
+            act = "not timelock compatible trialdefinition"
+            raise SPYValueError(lgl, "trialdefinition", act)
 
     # TODO - overload `time` property, as there is only one by definition!
     # implement plotting
     def singlepanelplot(self, shifted=True, **show_kwargs):
 
         figax = sp_plotting.plot_AnalogData(self, shifted, **show_kwargs)
         return figax
 
     def multipanelplot(self, **show_kwargs):
 
         figax = mp_plotting.plot_AnalogData(self, **show_kwargs)
         return figax
+
+    def save_nwb(self, outpath, with_trialdefinition=True, is_raw=True):
+        """Save TimeLockData in Neurodata Without Borders (NWB) file format.
+        An NWBFile represents a single session of an experiment.
+
+        Parameters
+        ----------
+        outpath : str, path-like. Where to save the NWB file, including file name and `.nwb` extension. All directories in the path must exist. Example: `'mydata.nwb'`.
+
+        with_trialdefinition : Boolean, whether to save the trial definition in the NWB file.
+
+        is_raw : Boolean, whether this is raw data (that should never change), as opposed to LFP data that originates from some processing, e.g., down-sampling and
+         detrending. Determines where data is stored in the NWB container, to make it easier for other software to interprete what the data represents. If `is_raw` is `True`,
+         the `ElectricalSeries` is stored directly in an acquisition of the :class:`pynwb.NWBFile`. If False, it is stored inside an `LFP` instance in a processing group called `ecephys`.
+         Note that for the Syncopy NWB reader, the data should be stored as raw, so this is currently the default.
+
+        Returns
+        -------
+        None, called for side effect of writing the NWB file to disk.
+
+        Notes
+        -----
+        Due to the very general architecture of the NWB format, many fields need to be interpreted by software reading the format. Thus,
+        providing a generic function to save Syncopy data in NWB format is possible only if you know who will read it.
+        Depending on your target software, you may need to manually format the data using pynwb before writing it to disk, or manually
+        open it using pynwb before using it with the target software.
+
+        Selections are ignored, the full data is exported. Create a new Syncopy data object before calling this function if you want to export a subset only.
+
+        This function requires the optional 'pynwb' dependency to be installed.
+        """
+        if not __pynwb__:
+            raise SPYError("NWB support is not available. Please install the 'pynwb' package.")
+
+        nwbfile = _analog_timelocked_to_nwbfile(
+            self, nwbfile=None, with_trialdefinition=with_trialdefinition, is_raw=is_raw
+        )
+        # Write the file to disk.
+        with NWBHDF5IO(outpath, "w") as io:
+            io.write(nwbfile)
```

### Comparing `esi_syncopy-2023.5/syncopy/datatype/discrete_data.py` & `esi_syncopy-2023.7/syncopy/datatype/discrete_data.py`

 * *Files 5% similar despite different names*

```diff
@@ -11,109 +11,130 @@
 
 
 # Local imports
 from .base_data import BaseData, FauxTrial
 from .methods.definetrial import definetrial
 from syncopy.shared.parsers import scalar_parser, array_parser
 from syncopy.shared.errors import SPYValueError, SPYError, SPYTypeError
-from syncopy.shared.tools import best_match
+from syncopy.plotting import spike_plotting
+
+from syncopy.io.nwb import _spikedata_to_nwbfile
+
+from syncopy import __pynwb__
+
+if __pynwb__:  # pragma: no cover
+    from pynwb import NWBHDF5IO
+
 
 __all__ = ["SpikeData", "EventData"]
 
 
 class DiscreteData(BaseData, ABC):
     """
     Abstract class for non-uniformly sampled data where only time-stamps are recorded.
 
     Notes
     -----
     This class cannot be instantiated. Use one of the children instead.
     """
 
-    _infoFileProperties = BaseData._infoFileProperties + ("samplerate", )
+    _infoFileProperties = BaseData._infoFileProperties + ("samplerate",)
     _hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties + ("samplerate",)
-    _selectionKeyWords = BaseData._selectionKeyWords + ('latency',)
+    _selectionKeyWords = BaseData._selectionKeyWords + ("latency",)
 
     @property
     def data(self):
         """
         Array-like object representing data without trials.
 
         Trials are concatenated along the time axis.
         """
 
         if getattr(self._data, "id", None) is not None:
             if self._data.id.valid == 0:
                 lgl = "open HDF5 file"
                 act = "backing HDF5 file {} has been closed"
-                raise SPYValueError(legal=lgl, actual=act.format(self.filename),
-                                    varname="data")
+                raise SPYValueError(legal=lgl, actual=act.format(self.filename), varname="data")
         return self._data
 
     @data.setter
     def data(self, inData):
-        """ Also checks for integer type of data """
+        """Also checks for integer type of data"""
         # this comes from BaseData
         self._set_dataset_property(inData, "data")
 
         if inData is not None:
             if not np.issubdtype(self.data.dtype, np.integer):
-                raise SPYTypeError(self.data.dtype, 'data', "integer like")
+                raise SPYTypeError(self.data.dtype, "data", "integer like")
 
     def __str__(self):
         # Get list of print-worthy attributes
-        ppattrs = [attr for attr in self.__dir__()
-                   if not (attr.startswith("_") or attr in ["log", "trialdefinition"])]
-        ppattrs = [attr for attr in ppattrs
-                   if not (inspect.ismethod(getattr(self, attr))
-                           or isinstance(getattr(self, attr), Iterator))]
+        ppattrs = [
+            attr
+            for attr in self.__dir__()
+            if not (attr.startswith("_") or attr in ["log", "trialdefinition"])
+        ]
+        ppattrs = [
+            attr
+            for attr in ppattrs
+            if hasattr(self, attr)
+            and not (inspect.ismethod(getattr(self, attr)) or isinstance(getattr(self, attr), Iterator))
+        ]
 
         ppattrs.sort()
 
         # Construct string for pretty-printing class attributes
         dsep = " by "
         hdstr = "Syncopy {clname:s} object with fields\n\n"
         ppstr = hdstr.format(clname=self.__class__.__name__)
         maxKeyLength = max([len(k) for k in ppattrs])
         printString = "{0:>" + str(maxKeyLength + 5) + "} : {1:}\n"
         for attr in ppattrs:
             value = getattr(self, attr)
-            if hasattr(value, 'shape') and attr == "data" and self.sampleinfo is not None:
+            if hasattr(value, "shape") and attr == "data" and self.sampleinfo is not None:
                 tlen = np.unique([sinfo[1] - sinfo[0] for sinfo in self.sampleinfo])
                 if tlen.size == 1:
                     trlstr = "of length {} ".format(str(tlen[0]))
                 else:
                     trlstr = ""
-                dsize = np.prod(self.data.shape)*self.data.dtype.itemsize/1024**2
+                dsize = np.prod(self.data.shape) * self.data.dtype.itemsize / 1024**2
                 dunit = "MB"
                 if dsize > 1000:
                     dsize /= 1024
                     dunit = "GB"
                 valueString = "{} trials {}defined on ".format(str(len(self.trials)), trlstr)
-                valueString += "[" + " x ".join([str(numel) for numel in value.shape]) \
-                              + "] {dt:s} {tp:s} " +\
-                              "of size {sz:3.2f} {szu:s}"
-                valueString = valueString.format(dt=self.data.dtype.name,
-                                                 tp=self.data.__class__.__name__,
-                                                 sz=dsize,
-                                                 szu=dunit)
-            elif hasattr(value, 'shape'):
-                valueString = "[" + " x ".join([str(numel) for numel in value.shape]) \
-                              + "] element " + str(type(value))
+                valueString += (
+                    "["
+                    + " x ".join([str(numel) for numel in value.shape])
+                    + "] {dt:s} {tp:s} "
+                    + "of size {sz:3.2f} {szu:s}"
+                )
+                valueString = valueString.format(
+                    dt=self.data.dtype.name,
+                    tp=self.data.__class__.__name__,
+                    sz=dsize,
+                    szu=dunit,
+                )
+            elif hasattr(value, "shape"):
+                valueString = (
+                    "[" + " x ".join([str(numel) for numel in value.shape]) + "] element " + str(type(value))
+                )
             elif isinstance(value, list):
                 if attr == "dimord" and value is not None:
                     valueString = dsep.join(dim for dim in self.dimord)
                 else:
                     valueString = "{0} element list".format(len(value))
             elif isinstance(value, dict):
                 msg = "dictionary with {nk:s}keys{ks:s}"
                 keylist = value.keys()
                 showkeys = len(keylist) < 7
-                valueString = msg.format(nk=str(len(keylist)) + " " if not showkeys else "",
-                                         ks=" '" + "', '".join(key for key in keylist) + "'" if showkeys else "")
+                valueString = msg.format(
+                    nk=str(len(keylist)) + " " if not showkeys else "",
+                    ks=" '" + "', '".join(key for key in keylist) + "'" if showkeys else "",
+                )
             else:
                 valueString = str(value)
             ppstr += printString.format(attr, valueString)
         ppstr += "\nUse `.log` to see object history"
         return ppstr
 
     @property
@@ -141,62 +162,87 @@
         self._samplerate = sr
 
     @BaseData.trialdefinition.setter
     def trialdefinition(self, trldef):
 
         if trldef is None:
             sidx = self.dimord.index("sample")
-            self._trialdefinition = np.array([[np.nanmin(self.data[:, sidx]),
-                                               np.nanmax(self.data[:, sidx]), 0]])
+            self._trialdefinition = np.array(
+                [[np.nanmin(self.data[:, sidx]), np.nanmax(self.data[:, sidx]), 0]]
+            )
+            self._trial_ids = [0]
         else:
             array_parser(trldef, varname="trialdefinition", dims=2)
-            array_parser(trldef[:, :2], varname="sampleinfo", hasnan=False,
-                         hasinf=False, ntype="int_like", lims=[0, np.inf])
+            array_parser(
+                trldef[:, :2],
+                varname="sampleinfo",
+                hasnan=False,
+                hasinf=False,
+                ntype="int_like",
+                lims=[0, np.inf],
+            )
 
             self._trialdefinition = trldef.copy()
-
+            self._triald_ids = np.arange(self.sampleinfo.shape[0])
             # Compute trial-IDs by matching data samples with provided trial-bounds
             samples = self.data[:, self.dimord.index("sample")]
             idx = np.searchsorted(samples, self.sampleinfo.ravel())
             idx = idx.reshape(self.sampleinfo.shape)
 
-            self._trialslice = [slice(st,end) for st,end in idx]
+            self._trialslice = [slice(st, end) for st, end in idx]
             self.trialid = np.full((samples.shape), -1, dtype=int)
             for itrl, itrl_slice in enumerate(self._trialslice):
                 self.trialid[itrl_slice] = itrl
 
+            self._trial_ids = np.arange(self.sampleinfo.shape[0])
+
     @property
     def time(self):
-        """list(float): trigger-relative time of each event """
+        """list(float): trigger-relative time of each event"""
         if self.samplerate is not None and self.sampleinfo is not None:
-            return [(trl[:,self.dimord.index("sample")] - self.sampleinfo[tk,0] + self.trialdefinition[tk, 2]) / self.samplerate \
-                    for tk, trl in enumerate(self.trials)]
+            return [
+                (trl[:, self.dimord.index("sample")] - self.sampleinfo[tk, 0] + self.trialdefinition[tk, 2])
+                / self.samplerate
+                for tk, trl in enumerate(self.trials)
+            ]
 
     @property
     def trialid(self):
         """:class:`numpy.ndarray` of trial id associated with the sample"""
         return self._trialid
 
     @trialid.setter
     def trialid(self, trlid):
+        """
+        1d-array of the size of the total number of samples,
+        encoding which sample belongs to which trial.
+        """
         if trlid is None:
             self._trialid = None
             return
 
         if self.data is None:
-            SPYError("SyNCoPy core - trialid: Cannot assign `trialid` without data. " +
-                     "Please assign data first")
+            SPYError(
+                "SyNCoPy core - trialid: Cannot assign `trialid` without data. " + "Please assign data first"
+            )
             return
         if (self.data.shape[0] == 0) and (trlid.shape[0] == 0):
             self._trialid = np.array(trlid, dtype=int)
             return
         scount = np.nanmax(self.data[:, self.dimord.index("sample")])
         try:
-            array_parser(trlid, varname="trialid", dims=(self.data.shape[0],),
-                         hasnan=False, hasinf=False, ntype="int_like", lims=[-1, scount])
+            array_parser(
+                trlid,
+                varname="trialid",
+                dims=(self.data.shape[0],),
+                hasnan=False,
+                hasinf=False,
+                ntype="int_like",
+                lims=[-1, scount],
+            )
         except Exception as exc:
             raise exc
         self._trialid = np.array(trlid, dtype=int)
 
     @property
     def trialtime(self):
         """list(:class:`numpy.ndarray`): trigger-relative sample times in s"""
@@ -235,18 +281,18 @@
 
         See also
         --------
         syncopy.datatype.base_data.FauxTrial : class definition and further details
         syncopy.shared.computational_routine.ComputationalRoutine : Syncopy compute engine
         """
         trlSlice = self._trialslice[trialno]
-        trialIdx = np.arange(trlSlice.start, trlSlice.stop) #np.where(self.trialid == trialno)[0]
+        trialIdx = np.arange(trlSlice.start, trlSlice.stop)  # np.where(self.trialid == trialno)[0]
         nCol = len(self.dimord)
         idx = [[], slice(0, nCol)]
-        if self.selection is not None: # selections are harmonized, just take `.time`
+        if self.selection is not None:  # selections are harmonized, just take `.time`
             idx[0] = trialIdx[self.selection.time[self.selection.trial_ids.index(trialno)]].tolist()
         else:
             idx[0] = trialIdx.tolist()
 
         shp = [len(idx[0]), nCol]
 
         return FauxTrial(shp, tuple(idx), self.data.dtype, self.dimord)
@@ -267,37 +313,67 @@
         # Call initializer
         super().__init__(data=data, **kwargs)
 
         if self.data is not None:
 
             if self.data.size == 0:
                 # initialization with empty data not allowed
-                raise SPYValueError("non empty data set", 'data')
+                raise SPYValueError("non empty data set", "data")
 
             # In case of manual data allocation (reading routine would leave a
             # mark in `cfg`), fill in missing info
             if self.sampleinfo is None:
                 # Fill in dimensional info
                 definetrial(self, kwargs.get("trialdefinition"))
 
+    def save_nwb(self, **kwargs):
+        raise NotImplementedError("Saving of this datatype to NWB files is not supported.")
+
+    # plotting, only virtual in the abc
+    def singlepanelplot(self):
+        raise NotImplementedError
+
+    def multipanelplot(self):
+        raise NotImplementedError
+
 
 class SpikeData(DiscreteData):
     """Spike times of multi- and/or single units
 
     This class can be used for representing spike trains. The data is always
     stored as a two-dimensional [nSpikes x 3] array on disk with the columns
     being ``["sample", "channel", "unit"]``.
 
+    The "unit" is the neuron a spike originated from. Note that in the raw data,
+    a signal from one neuron may show up in several (nearby) channels, with different strengths. The waveform
+    of the action potential is used as a signature to identify the signal of a neuron
+    across channels. Once this has been done, it is known which neuron spiked when, and the
+    channel information is typically no longer of interest. I.e., with spike data that
+    is ready for the scientific analysis, there typically is only one channel.
+
+    Note that this means that "channel x unit z" is the same neuron as "channel y unit z", since
+    the unit should identify the same neuron, regardless of the channel.
+
+    Often, the raw data around individual spikes is save along with the spikes, so that
+    one can later infer the type of neuron (e.g., inhibitory/excitatory) from it. We support
+    this with the 'waveform' attribute of spy.SpikeData.
+
     Data is only read from disk on demand, similar to HDF5 files.
     """
 
-    _infoFileProperties = DiscreteData._infoFileProperties + ("channel", "unit",)
+    _infoFileProperties = DiscreteData._infoFileProperties + (
+        "channel",
+        "unit",
+    )
     _defaultDimord = ["sample", "channel", "unit"]
     _stackingDimLabel = "sample"
-    _selectionKeyWords = DiscreteData._selectionKeyWords + ('channel', 'unit',)
+    _selectionKeyWords = DiscreteData._selectionKeyWords + (
+        "channel",
+        "unit",
+    )
 
     def _compute_unique_idx(self):
         """
         Use `np.unique` on whole(!) dataset to compute globally
         available channel and unit indices only once
 
         This function gets triggered by the constructor
@@ -312,24 +388,26 @@
 
         # this is costly and loads the entire hdf5 dataset into memory!
         self.channel_idx = np.unique(self.data[:, self.dimord.index("channel")])
         self.unit_idx = np.unique(self.data[:, self.dimord.index("unit")])
 
     @property
     def channel(self):
-        """ :class:`numpy.ndarray` : list of original channel names for each unit"""
+        """:class:`numpy.ndarray` : list of original channel names for each unit"""
 
         return self._channel
 
     @channel.setter
     def channel(self, chan):
         if self.data is None:
             if chan is not None:
-                raise SPYValueError(f"non-empty SpikeData", "cannot assign `channel` without data. " +
-                                    "Please assign data first")
+                raise SPYValueError(
+                    f"non-empty SpikeData",
+                    "cannot assign `channel` without data. " + "Please assign data first",
+                )
             # No labels for no data is fine
             self._channel = chan
             return
 
         # there is data
         elif chan is None:
             raise SPYValueError("channel labels, cannot set `channel` to `None` with existing data.")
@@ -343,41 +421,44 @@
             self._compute_unique_idx()
 
         # we need as many labels as there are distinct channels
         nChan = self.channel_idx.size
 
         if nChan != len(chan):
             raise SPYValueError(f"exactly {nChan} channel label(s)")
-        array_parser(chan, varname="channel", ntype="str", dims=(nChan, ))
+        array_parser(chan, varname="channel", ntype="str", dims=(nChan,))
         self._channel = np.array(chan)
 
     def _default_channel_labels(self):
 
         """
         Creates the default channel labels
         """
 
         # channel entries in self.data are 0-based
         chan_max = self.channel_idx.max()
-        channel_labels = np.array(["channel" + str(int(i + 1)).zfill(len(str(chan_max)) + 1)
-                                   for i in self.channel_idx])
+        channel_labels = np.array(
+            ["channel" + str(int(i + 1)).zfill(len(str(chan_max))) for i in self.channel_idx]
+        )
         return channel_labels
 
     @property
     def unit(self):
-        """ :class:`numpy.ndarray(str)` : unit names"""
+        """:class:`numpy.ndarray(str)` : unit names"""
 
         return self._unit
 
     @unit.setter
     def unit(self, unit):
         if self.data is None:
             if unit is not None:
-                raise SPYValueError(f"non-empty SpikeData", "cannot assign `unit` without data. " +
-                                    "Please assign data first")
+                raise SPYValueError(
+                    f"non-empty SpikeData",
+                    "cannot assign `unit` without data. " + "Please assign data first",
+                )
             # empy labels for empty data is fine
             self._unit = unit
             return
 
         # there is data
         elif unit is None:
             raise SPYValueError("unit labels, cannot set `unit` to `None` with existing data.")
@@ -387,16 +468,17 @@
         # we have to compute this here
         if self.unit_idx is None:
             self._compute_unique_idx()
 
         if unit is None and self.data is not None:
             raise SPYValueError("Cannot set `unit` to `None` with existing data.")
         elif self.data is None and unit is not None:
-            raise SPYValueError("Syncopy - SpikeData - unit: Cannot assign `unit` without data. " +
-                  "Please assign data first")
+            raise SPYValueError(
+                "Syncopy - SpikeData - unit: Cannot assign `unit` without data. " + "Please assign data first"
+            )
         elif unit is None:
             self._unit = None
             return
 
         nunit = self.unit_idx.size
         if nunit != len(unit):
             raise SPYValueError(f"exactly {nunit} unit label(s)")
@@ -407,16 +489,15 @@
     def _default_unit_labels(self):
 
         """
         Creates the default unit labels
         """
 
         unit_max = self.unit_idx.max()
-        return np.array(["unit" + str(int(i + 1)).zfill(len(str(unit_max)) + 1)
-                         for i in self.unit_idx])
+        return np.array(["unit" + str(int(i + 1)).zfill(len(str(unit_max))) for i in self.unit_idx])
 
     # Helper function that extracts by-trial unit-indices
     def _get_unit(self, trials, units=None):
         """
         Get relative by-trial indices of unit selections
 
         Parameters
@@ -475,38 +556,51 @@
         return self._waveform
 
     @waveform.setter
     def waveform(self, waveform):
         """Set a waveform dataset from a numpy array, `None`, or an `h5py.Dataset` instance."""
         if self.data is None:
             if waveform is not None:
-                raise SPYValueError(legal="non-empty SpikeData", varname="waveform",
-                actual="empty SpikeData main dataset (None). Cannot assign `waveform` without data. Please assign data first.")
+                raise SPYValueError(
+                    legal="non-empty SpikeData",
+                    varname="waveform",
+                    actual="empty SpikeData main dataset (None). Cannot assign `waveform` without data. Please assign data first.",
+                )
         if waveform is None:
-            self._set_dataset_property(waveform, 'waveform') # None
+            self._set_dataset_property(waveform, "waveform")  # None
             self._unregister_dataset("waveform", del_attr=False)
             return
 
         if waveform.ndim < 2:
-            raise SPYValueError(legal="waveform data with at least 2 dimensions", varname="waveform", actual=f"data with {waveform.ndim} dimensions")
+            raise SPYValueError(
+                legal="waveform data with at least 2 dimensions",
+                varname="waveform",
+                actual=f"data with {waveform.ndim} dimensions",
+            )
 
         if waveform.shape[0] != self.data.shape[0]:
-            raise SPYValueError(f"waveform shape[0]={waveform.shape[0]} must equal nSpikes={self.data.shape[0]}. " +
-                                "Please create one waveform per spike in data.", varname="waveform", actual=f"wrong size waveform with shape {waveform.shape}")
-        self._set_dataset_property(waveform, 'waveform')
+            raise SPYValueError(
+                f"waveform shape[0]={waveform.shape[0]} must equal nSpikes={self.data.shape[0]}. "
+                + "Please create one waveform per spike in data.",
+                varname="waveform",
+                actual=f"wrong size waveform with shape {waveform.shape}",
+            )
+        self._set_dataset_property(waveform, "waveform")
 
     # "Constructor"
-    def __init__(self,
-                 data=None,
-                 filename=None,
-                 trialdefinition=None,
-                 samplerate=None,
-                 channel=None,
-                 unit=None,
-                 dimord=None):
+    def __init__(
+        self,
+        data=None,
+        filename=None,
+        trialdefinition=None,
+        samplerate=None,
+        channel=None,
+        unit=None,
+        dimord=None,
+    ):
         """Initialize a :class:`SpikeData` object.
 
         Parameters
         ----------
             data : [nSpikes x 3] :class:`numpy.ndarray`
 
             filename : str
@@ -530,29 +624,34 @@
         See also
         --------
         :func:`syncopy.definetrial`
 
         """
 
         # instance attribute to allow modification
-        self._hdfFileAttributeProperties = DiscreteData._hdfFileAttributeProperties + ("channel", "unit")
+        self._hdfFileAttributeProperties = DiscreteData._hdfFileAttributeProperties + (
+            "channel",
+            "unit",
+        )
 
         self._unit = None
         self.unit_idx = None
         self._channel = None
         self.channel_idx = None
 
         # Call parent initializer
-        super().__init__(data=data,
-                         filename=filename,
-                         trialdefinition=trialdefinition,
-                         samplerate=samplerate,
-                         dimord=dimord)
+        super().__init__(
+            data=data,
+            filename=filename,
+            trialdefinition=trialdefinition,
+            samplerate=samplerate,
+            dimord=dimord,
+        )
 
-        self._hdfFileDatasetProperties +=  ("waveform",)
+        self._hdfFileDatasetProperties += ("waveform",)
 
         self._waveform = None
 
         # for fast lookup and labels
         self._compute_unique_idx()
 
         # constructor gets `data=None` for
@@ -568,28 +667,71 @@
         # same for unit
         if unit is not None:
             # setter raises exception if data=None
             self.unit = unit
         elif data is not None:
             self.unit = self._default_unit_labels()
 
+    def save_nwb(self, outpath, with_trialdefinition=True):
+        """Save SpikeData in Neurodata Without Borders (NWB) file format.
+        An NWBFile represents a single session of an experiment.
+
+        Parameters
+        ----------
+        outpath : str, path-like. Where to save the NWB file, including file name and `.nwb` extension. All directories in the path must exist. Example: `'mydata.nwb'`.
+
+        with_trialdefinition : Boolean, whether to save the trial definition in the NWB file.
+
+        Returns
+        -------
+        None, called for side effect of writing the NWB file to disk.
+
+        Notes
+        -----
+        Due to the very general architecture of the NWB format, many fields need to be interpreted by software reading the format. Thus,
+        providing a generic function to save Syncopy data in NWB format is possible only if you know who will read it.
+        Depending on your target software, you may need to manually format the data using pynwb before writing it to disk, or manually
+        open it using pynwb before using it with the target software.
+
+        Selections are ignored, the full data is exported. Create a new Syncopy data object before calling this function if you want to export a subset only.
+        """
+        if not __pynwb__:
+            raise SPYError("NWB support is not available. Please install the 'pynwb' package.")
+
+        nwbfile = _spikedata_to_nwbfile(self, nwbfile=None, with_trialdefinition=with_trialdefinition)
+        # Write the file to disk.
+        with NWBHDF5IO(outpath, "w") as io:
+            io.write(nwbfile)
+
+    # implement plotting
+    def singlepanelplot(self, **show_kwargs):
+
+        figax = spike_plotting.plot_single_figure_SpikeData(self, **show_kwargs)
+        return figax
+
+    # implement plotting
+    def multipanelplot(self, **show_kwargs):
+
+        figax = spike_plotting.plot_multi_figure_SpikeData(self, **show_kwargs)
+        return figax
+
 
 class EventData(DiscreteData):
     """Timestamps and integer codes of experimental events
 
     This class can be used for representing events during an experiment, e.g.
     stimulus was turned on, etc. These usually occur at non-regular time points
     and have associated event codes.
 
     Data is only read from disk on demand, similar to HDF5 files.
     """
 
     _defaultDimord = ["sample", "eventid"]
     _stackingDimLabel = "sample"
-    _selectionKeyWords = DiscreteData._selectionKeyWords + ('eventid',)
+    _selectionKeyWords = DiscreteData._selectionKeyWords + ("eventid",)
 
     @property
     def eventid(self):
         """numpy.ndarray(int): integer event code assocated with each event"""
         if self.data is None:
             return None
         return np.unique(self.data[:, self.dimord.index("eventid")])
@@ -639,20 +781,22 @@
                 indices.append(trialEvents)
         else:
             indices = [slice(None)] * len(trials)
 
         return indices
 
     # "Constructor"
-    def __init__(self,
-                 data=None,
-                 filename=None,
-                 trialdefinition=None,
-                 samplerate=None,
-                 dimord=None):
+    def __init__(
+        self,
+        data=None,
+        filename=None,
+        trialdefinition=None,
+        samplerate=None,
+        dimord=None,
+    ):
         """Initialize a :class:`EventData` object.
 
         Parameters
         ----------
         data : [nEvents x 2] :class:`numpy.ndarray`
 
         filename : str
@@ -681,14 +825,16 @@
                     if col not in dimord:
                         base = "dimensional label {}"
                         lgl = base.format("'" + col + "'")
                         raise SPYValueError(legal=lgl, varname="dimord")
                 self._defaultDimord = dimord
 
         # Call parent initializer
-        super().__init__(data=data,
-                         filename=filename,
-                         trialdefinition=trialdefinition,
-                         samplerate=samplerate,
-                         dimord=dimord)
+        super().__init__(
+            data=data,
+            filename=filename,
+            trialdefinition=trialdefinition,
+            samplerate=samplerate,
+            dimord=dimord,
+        )
 
         self._hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties + ("samplerate",)
```

### Comparing `esi_syncopy-2023.5/syncopy/datatype/methods/arithmetic.py` & `esi_syncopy-2023.7/syncopy/datatype/methods/arithmetic.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,25 +2,22 @@
 #
 # Syncopy object arithmetics
 #
 
 # Builtin/3rd party package imports
 import numpy as np
 import h5py
+import dask.distributed as dd
 
 # Local imports
-from syncopy import __acme__
 from syncopy.shared.parsers import data_parser
-from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYWarning, SPYInfo
+from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYWarning
 from syncopy.shared.computational_routine import ComputationalRoutine
 from syncopy.shared.kwarg_decorators import process_io, detect_parallel_client
 
-if __acme__:
-    import dask.distributed as dd
-
 __all__ = []
 
 
 # Main entry point for overloaded operators
 def _process_operator(obj1, obj2, operator):
     """
     Perform binary arithmetic operation on Syncopy data object
@@ -173,53 +170,57 @@
 
         # Ensure complex and real values are not mashed up
         _check_complex_operand(baseTrials, operand, "array", operator)
 
         # Determine exact numeric type of the operation's result
         opres_type = np.result_type(*(trl.dtype for trl in baseTrials), operand.dtype)
 
-        # Ensure shapes match up
-        if not all(trl.shape == operand.shape for trl in baseTrials):
-            lgl = "array of compatible shape"
-            act = "array with shape {}"
-            raise SPYValueError(lgl, varname="operand", actual=act.format(operand.shape))
+        # Ensure shapes of all trials match up, according to NumPy broadcasting rules
+        for trl in baseTrials:
+            try:
+                np.broadcast_shapes(trl.shape, operand.shape)
+            except ValueError:
+                lgl = f"array compatible to trial shape {trl.shape}"
+                act = "array with shape {}"
+                raise SPYValueError(lgl, varname="operand", actual=act.format(operand.shape))
 
         # No more info needed, the array is the only quantity we need
         operand_dat = operand
         operand_idxs = None
 
-        # All good, nevertheless warn of potential havoc this operation may cause...
-        msg = "Performing arithmetic with NumPy arrays may cause inconsistency " +\
-            "in Syncopy objects (channels, samplerate, trialdefintions etc.)"
-        SPYWarning(msg, caller=operator)
-
     # Operand is another Syncopy object
     elif "BaseData" in str(operand.__class__.__mro__):
 
         # Ensure operand object class, and `dimord` match up (and it's non-empty)
         try:
-            data_parser(operand, varname="operand", dimord=baseObj.dimord,
-                        dataclass=baseObj.__class__.__name__, empty=False)
+            data_parser(
+                operand,
+                varname="operand",
+                dimord=baseObj.dimord,
+                dataclass=baseObj.__class__.__name__,
+                empty=False,
+            )
         except Exception as exc:
             raise exc
 
         # Make sure samplerates are identical (if present)
         baseSr = getattr(baseObj, "samplerate")
         opndSr = getattr(operand, "samplerate")
-        if baseSr  != opndSr:
+        if baseSr != opndSr:
             lgl = "Syncopy objects with identical samplerate"
             act = "Syncopy object with samplerates {} and {}, respectively"
-            raise SPYValueError(lgl, varname="operand",
-                                actual=act.format(baseSr, opndSr))
+            raise SPYValueError(lgl, varname="operand", actual=act.format(baseSr, opndSr))
 
         # If only a subset of `operand` is selected, adjust for this (and warn
         # that arbitrarily ugly things might happen with mis-matched selections)
         if operand.selection is not None:
-            wrng = "Found existing in-place selection in operand. " +\
-                "Shapes and trial counts of base and operand objects have to match up!"
+            wrng = (
+                "Found existing in-place selection in operand. "
+                + "Shapes and trial counts of base and operand objects have to match up!"
+            )
             SPYWarning(wrng, caller=operator)
             opndTrialList = operand.selection.trial_ids
         else:
             opndTrialList = list(range(len(operand.trials)))
 
         # Ensure the same number of trials is about to be processed
         opndTrials = [operand._preview_trial(trlno) for trlno in opndTrialList]
@@ -232,80 +233,80 @@
         baseIsComplex = ["complex" in trl.dtype.name for trl in baseTrials]
         opndIsComplex = ["complex" in trl.dtype.name for trl in opndTrials]
         if baseIsComplex != opndIsComplex:
             lgl = "Syncopy data object of same numerical type (real/complex)"
             raise SPYTypeError(operand, varname="operand", expected=lgl)
 
         # Determine the numeric type of the operation's result
-        opres_type = np.result_type(*(trl.dtype for trl in baseTrials),
-                                    *(trl.dtype for trl in opndTrials))
+        opres_type = np.result_type(*(trl.dtype for trl in baseTrials), *(trl.dtype for trl in opndTrials))
 
         # Ensure shapes align
         if not all(baseTrials[k].shape == opndTrials[k].shape for k in range(len(baseTrials))):
             lgl = "Syncopy object (selection) of compatible shapes {}"
             act = "Syncopy object (selection) with shapes {}"
             baseShapes = [trl.shape for trl in baseTrials]
             opndShapes = [trl.shape for trl in opndTrials]
-            raise SPYValueError(lgl.format(baseShapes), varname="operand",
-                                actual=act.format(opndShapes))
+            raise SPYValueError(lgl.format(baseShapes), varname="operand", actual=act.format(opndShapes))
 
         # Avoid things becoming too nasty: if `operand`` contains wild selections
         # (unordered lists or index repetitions) or selections requiring advanced
         # (aka fancy) indexing (multiple slices mixed with lists), abort
         for trl in opndTrials:
-            if any(np.diff(sel).min() <= 0 if isinstance(sel, list) and len(sel) > 1 \
-                else False for sel in trl.idx):
+            if any(
+                np.diff(sel).min() <= 0 if isinstance(sel, list) and len(sel) > 1 else False
+                for sel in trl.idx
+            ):
                 lgl = "Syncopy object with ordered unreverberated subset selection"
                 act = "Syncopy object with selection {}"
                 raise SPYValueError(lgl, varname="operand", actual=act.format(operand.selection))
-            if sum(isinstance(sel, slice) for sel in trl.idx) > 1 and \
-                sum(isinstance(sel, list) for sel in trl.idx) > 1:
+            if (
+                sum(isinstance(sel, slice) for sel in trl.idx) > 1
+                and sum(isinstance(sel, list) for sel in trl.idx) > 1
+            ):
                 lgl = "Syncopy object without selections requiring advanced indexing"
                 act = "Syncopy object with selection {}"
                 raise SPYValueError(lgl, varname="operand", actual=act.format(operand.selection))
 
         # Propagate indices for fetching data from operand
         operand_idxs = [trl.idx for trl in opndTrials]
 
         # Assemble dict with relevant info for performing operation
-        operand_dat = {"filename" : operand.filename,
-                       "dsetname" : operand._hdfFileDatasetProperties[0]}
+        operand_dat = {
+            "filename": operand.filename,
+            "dsetname": operand._hdfFileDatasetProperties[0],
+        }
 
     # If `operand` is anything else it's invalid for performing arithmetic on
     else:
         lgl = "Syncopy object, scalar or array-like"
         raise SPYTypeError(operand, varname="operand", expected=lgl)
 
     return baseObj, operand, operand_dat, opres_type, operand_idxs
 
+
 # Check for complexity in `operand` vs. `baseObj`
 def _check_complex_operand(baseTrials, operand, opDimType, operator):
     """
     Local helper to determine if provided scalar/array and `baseObj` are both real/complex
     """
 
     # Ensure complex and real values are not mashed up
     if np.iscomplexobj(operand):
-        sameType = lambda dt : "complex" in dt.name
+        sameType = lambda dt: "complex" in dt.name
     else:
-        sameType = lambda dt : "complex" not in dt.name
+        sameType = lambda dt: "complex" not in dt.name
     if not all(sameType(trl.dtype) for trl in baseTrials):
         wrng = "Operand is {} of different mathematical type (real/complex)"
         SPYWarning(wrng.format(opDimType), caller=operator)
 
     return
 
 
 # Invoke `ComputationalRoutine` to compute arithmetic operation
-def _perform_computation(baseObj,
-                         operand,
-                         operand_dat,
-                         operand_idxs,
-                         opres_type,
-                         operator):
+def _perform_computation(baseObj, operand, operand_dat, operand_idxs, opres_type, operator):
     """
     Leverage `ComputationalRoutine` to process arithmetic operation
 
     Parameters
     ----------
     baseObj : Syncopy data object
         See :func:`_parse_input` for details.
@@ -342,61 +343,58 @@
 
     # Prepare logging info in dictionary: we know that `baseObj` is definitely
     # a Syncopy data object, operand may or may not be; account for this
     if "BaseData" in str(operand.__class__.__mro__):
         opSel = operand.selection
     else:
         opSel = None
-    log_dct = {"operator": operator,
-               "base": baseObj.__class__.__name__,
-               "base selection": baseObj.selection,
-               "operand": operand.__class__.__name__,
-               "operand selection": opSel}
+    log_dct = {
+        "operator": operator,
+        "base": baseObj.__class__.__name__,
+        "base selection": baseObj.selection,
+        "operand": operand.__class__.__name__,
+        "operand selection": opSel,
+    }
 
     # Create output object
     out = baseObj.__class__(dimord=baseObj.dimord)
 
     # Now create actual functional operations: wrap operator in lambda
     if operator == "+":
-        operation = lambda x, y : x + y
+        operation = lambda x, y: x + y
     elif operator == "-":
-        operation = lambda x, y : x - y
+        operation = lambda x, y: x - y
     elif operator == "*":
-        operation = lambda x, y : x * y
+        operation = lambda x, y: x * y
     elif operator == "/":
-        operation = lambda x, y : x / y
+        operation = lambda x, y: x / y
     elif operator == "**":
-        operation = lambda x, y : x ** y
+        operation = lambda x, y: x**y
     else:
         raise SPYValueError("supported arithmetic operator", actual=operator)
 
-    # If ACME is available, try to attach (already running) parallel computing client
+    # try to attach (already running) parallel computing client
     parallel = False
-    if __acme__:
-        try:
-            dd.get_client()
-            parallel = True
-        except ValueError:
-            parallel = False
+    try:
+        dd.get_client()
+        parallel = True
+    except ValueError:
+        parallel = False
 
     # Perform actual computation: instantiate `ComputationalRoutine` w/extracted info
-    opMethod = SpyArithmetic(operand_dat, operand_idxs, operation=operation,
-                             opres_type=opres_type)
-    opMethod.initialize(baseObj,
-                        out._stackingDim,
-                        chan_per_worker=None,
-                        keeptrials=True)
+    opMethod = SpyArithmetic(operand_dat, operand_idxs, operation=operation, opres_type=opres_type)
+    opMethod.initialize(baseObj, out._stackingDim, chan_per_worker=None, keeptrials=True)
 
     # In case of parallel execution, be careful: use a distributed lock to prevent
     # ACME from performing chained operations (`x + y + 3``) simultaneously (thereby
     # wrecking the underlying HDF5 datasets). Similarly, if `operand` is a Syncopy
     # object, close its corresponding dataset(s) before starting to concurrently read
     # from them (triggering locking errors)
     if parallel:
-        lock = dd.lock.Lock(name='arithmetic_ops')
+        lock = dd.lock.Lock(name="arithmetic_ops")
         lock.acquire()
         if "BaseData" in str(operand.__class__.__mro__):
             for dsetName in operand._hdfFileDatasetProperties:
                 dset = getattr(operand, dsetName)
                 dset.file.close()
 
     opMethod.compute(baseObj, out, parallel=parallel, log_dict=log_dct)
@@ -412,16 +410,23 @@
     if hasattr(baseObj.selection, "_cleanup"):
         baseObj.selection = None
 
     return out
 
 
 @process_io
-def arithmetic_cF(base_dat, operand_dat, operand_idx, operation=None, opres_type=None,
-                  noCompute=False, chunkShape=None):
+def arithmetic_cF(
+    base_dat,
+    operand_dat,
+    operand_idx,
+    operation=None,
+    opres_type=None,
+    noCompute=False,
+    chunkShape=None,
+):
     """
     Perform arithmetic operation
 
     Parameters
     ----------
     base_dat : :class:`numpy.ndarray`
         Trial data
@@ -474,14 +479,15 @@
             # selections that squeezed the array
             operand.shape = chunkShape
     else:
         operand = operand_dat
 
     return operation(base_dat, operand)
 
+
 class SpyArithmetic(ComputationalRoutine):
     """
     Compute class for performing arithmetic operations with Syncopy objects
 
     Sub-class of :class:`~syncopy.shared.computational_routine.ComputationalRoutine`,
     see :doc:`/developer/compute_kernels` for technical details on Syncopy's compute
     classes and metafunctions.
```

### Comparing `esi_syncopy-2023.5/syncopy/datatype/methods/copy.py` & `esi_syncopy-2023.7/syncopy/datatype/methods/copy.py`

 * *Files 1% similar despite different names*

```diff
@@ -45,19 +45,16 @@
     :func:`syncopy.save` : save to specific file path
     :func:`syncopy.selectdata` : creates copy of a selection with `inplace=False`
     """
 
     # Make sure `data` is a valid Syncopy data object
     data_parser(spydata, varname="data", writable=None, empty=False)
 
-    dsize = np.prod(spydata.data.shape) * spydata.data.dtype.itemsize / 1024 ** 2
-    msg = (
-        f"Copying {dsize:.2f} MB of data "
-        f"to create new {spydata.__class__.__name__} object on disk"
-    )
+    dsize = np.prod(spydata.data.shape) * spydata.data.dtype.itemsize / 1024**2
+    msg = f"Copying {dsize:.2f} MB of data " f"to create new {spydata.__class__.__name__} object on disk"
     SPYInfo(msg)
 
     # Shallow copy, captures also non-default/temporary attributes.
     copy_spydata = py_copy(spydata)
     copy_filename = spydata._gen_filename()
     copy_spydata.filename = copy_filename
     spydata.clear()
```

### Comparing `esi_syncopy-2023.5/syncopy/datatype/methods/definetrial.py` & `esi_syncopy-2023.7/syncopy/datatype/methods/definetrial.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,16 +11,24 @@
 from syncopy.shared.parsers import data_parser, array_parser, scalar_parser
 from syncopy.shared.errors import SPYTypeError, SPYValueError
 from ...datatype.util import TimeIndexer
 
 __all__ = ["definetrial"]
 
 
-def definetrial(obj, trialdefinition=None, pre=None, post=None, start=None,
-                trigger=None, stop=None, clip_edges=False):
+def definetrial(
+    obj,
+    trialdefinition=None,
+    pre=None,
+    post=None,
+    start=None,
+    trigger=None,
+    stop=None,
+    clip_edges=False,
+):
     """(Re-)define trials of a Syncopy data object
 
     Data can be structured into trials based on timestamps of a start, trigger
     and end events::
 
                     start    trigger    stop
         |---- pre ----|--------|---------|--- post----|
@@ -80,70 +88,74 @@
         lgl = "non-empty Syncopy data object"
         act = "empty Syncopy data object"
         raise SPYValueError(legal=lgl, varname="obj", actual=act)
 
     # Check array/object holding trial specifications
     if trialdefinition is not None:
         if trialdefinition.__class__.__name__ == "EventData":
-            data_parser(trialdefinition, varname="trialdefinition",
-                        writable=None, empty=False)
+            data_parser(trialdefinition, varname="trialdefinition", writable=None, empty=False)
             evt = True
         else:
             array_parser(trialdefinition, varname="trialdefinition", dims=2)
 
             if any(["ContinuousData" in str(base) for base in obj.__class__.__mro__]):
                 scount = obj.data.shape[obj.dimord.index("time")]
             else:
                 scount = np.inf
-            array_parser(trialdefinition[:, :2], varname="sampleinfo", dims=(None, 2), hasnan=False,
-                         hasinf=False, ntype="int_like", lims=[0, scount])
+            array_parser(
+                trialdefinition[:, :2],
+                varname="sampleinfo",
+                dims=(None, 2),
+                hasnan=False,
+                hasinf=False,
+                ntype="int_like",
+                lims=[0, scount],
+            )
 
             trl = np.array(trialdefinition, dtype="float")
             ref = obj
             tgt = obj
             evt = False
     else:
         # Construct object-class-specific `trl` arrays treating data-set as single trial
         if any(["ContinuousData" in str(base) for base in obj.__class__.__mro__]):
             trl = np.array([[0, obj.data.shape[obj.dimord.index("time")], 0]])
         else:
             sidx = obj.dimord.index("sample")
-            trl = np.array([[np.nanmin(obj.data[:,sidx]),
-                             np.nanmax(obj.data[:,sidx]), 0]])
+            trl = np.array([[np.nanmin(obj.data[:, sidx]), np.nanmax(obj.data[:, sidx]), 0]])
         ref = obj
         tgt = obj
         evt = False
 
     # AnalogData + EventData w/sampleinfo
     if obj.__class__.__name__ == "AnalogData" and evt and trialdefinition.sampleinfo is not None:
         if obj.samplerate is None or trialdefinition.samplerate is None:
             lgl = "non-`None` value - make sure `samplerate` is set before defining trials"
             act = "None"
             raise SPYValueError(legal=lgl, varname="samplerate", actual=act)
         ref = trialdefinition
         tgt = obj
         trl = np.array(ref.trialinfo)
-        t0 = np.array(ref._t0).reshape((ref._t0.size,1))
+        t0 = np.array(ref._t0).reshape((ref._t0.size, 1))
         trl = np.hstack([ref.sampleinfo, t0, trl])
-        trl = np.round((trl/ref.samplerate) * tgt.samplerate).astype(int)
+        trl = np.round((trl / ref.samplerate) * tgt.samplerate).astype(int)
 
     # AnalogData + EventData w/keywords or just EventData w/keywords
     if any([kw is not None for kw in [pre, post, start, trigger, stop]]):
 
         # Make sure we actually have valid data objects to work with
         if obj.__class__.__name__ == "EventData" and evt is False:
             ref = obj
             tgt = obj
         elif obj.__class__.__name__ == "AnalogData" and evt is True:
             ref = trialdefinition
             tgt = obj
         else:
             lgl = "AnalogData with associated EventData object"
-            act = "{} and {}".format(obj.__class__.__name__,
-                                     trialdefinition.__class__.__name__)
+            act = "{} and {}".format(obj.__class__.__name__, trialdefinition.__class__.__name__)
             raise SPYValueError(legal=lgl, actual=act, varname="input")
 
         # The only case we might actually need it: ensure `clip_edges` is valid
         if not isinstance(clip_edges, bool):
             raise SPYTypeError(clip_edges, varname="clip_edges", expected="Boolean")
 
         # Ensure that objects have their sampling-rates set, otherwise break
@@ -155,15 +167,15 @@
         # Get input dimensions
         szin = []
         for var in [pre, post, start, trigger, stop]:
             if isinstance(var, (np.ndarray, list)):
                 szin.append(len(var))
         if np.unique(szin).size > 1:
             lgl = "all trial-related arrays to have the same length"
-            act = "arrays with sizes {}".format(str(np.unique(szin)).replace("[","").replace("]",""))
+            act = "arrays with sizes {}".format(str(np.unique(szin)).replace("[", "").replace("]", ""))
             raise SPYValueError(legal=lgl, varname="trial-keywords", actual=act)
         if len(szin):
             ntrials = szin[0]
             ninc = 1
         else:
             ntrials = 1
             ninc = 0
@@ -176,33 +188,59 @@
         if (trigger is None) and (pre is not None or post is not None):
             lgl = "non-None `trigger` with `pre`/`post` timing information"
             act = "`trigger` = `None`"
             raise SPYValueError(legal=lgl, actual=act)
 
         # If provided, ensure keywords make sense, otherwise allocate defaults
         kwrds = {}
-        vdict = {"pre": {"var": pre, "hasnan": False, "ntype": None, "fillvalue": 0},
-                 "post": {"var": post, "hasnan": False, "ntype": None, "fillvalue": 0},
-                 "start": {"var": start, "hasnan": None, "ntype": "int_like", "fillvalue": np.nan},
-                 "trigger": {"var": trigger, "hasnan": None, "ntype": "int_like", "fillvalue": np.nan},
-                 "stop": {"var": stop, "hasnan": None, "ntype": "int_like", "fillvalue": np.nan}}
+        vdict = {
+            "pre": {"var": pre, "hasnan": False, "ntype": None, "fillvalue": 0},
+            "post": {"var": post, "hasnan": False, "ntype": None, "fillvalue": 0},
+            "start": {
+                "var": start,
+                "hasnan": None,
+                "ntype": "int_like",
+                "fillvalue": np.nan,
+            },
+            "trigger": {
+                "var": trigger,
+                "hasnan": None,
+                "ntype": "int_like",
+                "fillvalue": np.nan,
+            },
+            "stop": {
+                "var": stop,
+                "hasnan": None,
+                "ntype": "int_like",
+                "fillvalue": np.nan,
+            },
+        }
         for vname, opts in vdict.items():
             if opts["var"] is not None:
                 if np.issubdtype(type(opts["var"]), np.number):
                     try:
-                        scalar_parser(opts["var"], varname=vname, ntype=opts["ntype"],
-                                      lims=[-np.inf, np.inf])
+                        scalar_parser(
+                            opts["var"],
+                            varname=vname,
+                            ntype=opts["ntype"],
+                            lims=[-np.inf, np.inf],
+                        )
                     except Exception as exc:
                         raise exc
                     opts["var"] = np.full((ntrials,), opts["var"])
                 else:
                     try:
-                        array_parser(opts["var"], varname=vname, hasinf=False,
-                                     hasnan=opts["hasnan"], ntype=opts["ntype"],
-                                     dims=(ntrials,))
+                        array_parser(
+                            opts["var"],
+                            varname=vname,
+                            hasinf=False,
+                            hasnan=opts["hasnan"],
+                            ntype=opts["ntype"],
+                            dims=(ntrials,),
+                        )
                     except Exception as exc:
                         raise exc
                 kwrds[vname] = opts["var"]
             else:
                 kwrds[vname] = np.full((ntrials,), opts["fillvalue"])
 
         # Prepare `trl` and convert event-codes + sample-numbers to lists
@@ -228,45 +266,45 @@
             if not np.isnan(kwrds["start"][trialno]):
                 try:
                     sidx = evtid.index(kwrds["start"][trialno])
                 except:
                     act = str(kwrds["start"][trialno])
                     vname = "start"
                     break
-                begin = evtsp[sidx]/ref.samplerate
+                begin = evtsp[sidx] / ref.samplerate
                 evtid[sidx] = -np.pi
                 idxl.append(sidx)
 
             if not np.isnan(kwrds["trigger"][trialno]):
                 try:
                     idx = evtid.index(kwrds["trigger"][trialno])
                 except:
                     act = str(kwrds["trigger"][trialno])
                     vname = "trigger"
                     break
-                t0 = evtsp[idx]/ref.samplerate
+                t0 = evtsp[idx] / ref.samplerate
                 evtid[idx] = -np.pi
                 idxl.append(idx)
 
             # Trial-begin is either `trigger - pre` or `start - pre`
             if begin is not None:
                 begin -= kwrds["pre"][trialno]
             else:
                 begin = t0 - kwrds["pre"][trialno]
 
             # Try to assign `stop`, if we got nothing, use `t0 + post`
             if not np.isnan(kwrds["stop"][trialno]):
-                evtid[:sidx] = [np.pi]*sidx
+                evtid[:sidx] = [np.pi] * sidx
                 try:
                     idx = evtid.index(kwrds["stop"][trialno])
                 except:
                     act = str(kwrds["stop"][trialno])
                     vname = "stop"
                     break
-                end = evtsp[idx]/ref.samplerate + kwrds["post"][trialno]
+                end = evtsp[idx] / ref.samplerate + kwrds["post"][trialno]
                 evtid[idx] = -np.pi
                 idxl.append(idx)
             else:
                 end = t0 + kwrds["post"][trialno]
 
             # Off-set `t0`
             t0 -= begin
@@ -279,24 +317,23 @@
 
             # Finally, write line of `trl`
             trl.append([begin, end, t0])
 
             # Update counters and end this mess when we're done
             trialno += ninc
             cnt += 1
-            evtsp = evtsp[max(idxl, default=-1) + 1:]
-            evtid = evtid[max(idxl, default=-1) + 1:]
+            evtsp = evtsp[max(idxl, default=-1) + 1 :]
+            evtid = evtid[max(idxl, default=-1) + 1 :]
             if trialno == ntrials or cnt == nevents:
                 searching = False
 
         # Abort if the above loop ran into troubles
         if len(trl) < ntrials:
             if len(act) > 0:
-                raise SPYValueError(legal="existing event-id",
-                                    varname=vname, actual=act)
+                raise SPYValueError(legal="existing event-id", varname=vname, actual=act)
 
         # Make `trl` a NumPy array
         trl = np.round(np.array(trl) * tgt.samplerate).astype(int)
 
     # If appropriate, clip `trl` to AnalogData object's bounds (if wanted)
     if clip_edges and evt:
         msk = trl[:, 0] < 0
@@ -308,42 +345,48 @@
             lgl = "non-overlapping trials"
             act = "some trials are overlapping after clipping to AnalogData object range"
             raise SPYValueError(legal=lgl, actual=act)
 
     # The triplet `sampleinfo`, `t0` and `trialinfo` works identically for
     # all data genres
     if trl.shape[1] < 3:
-        raise SPYValueError("array of shape (no. of trials, 3+)",
-                            varname="trialdefinition",
-                            actual="shape = {shp:s}".format(shp=str(trl.shape)))
+        raise SPYValueError(
+            "array of shape (no. of trials, 3+)",
+            varname="trialdefinition",
+            actual="shape = {shp:s}".format(shp=str(trl.shape)),
+        )
 
     # Finally: assign `sampleinfo`, `t0` and `trialinfo` (and potentially `trialid`)
     # use target class setter
     tgt.trialdefinition = trl
 
     # In the discrete case, we have some additinal work to do
     if any(["DiscreteData" in str(base) for base in tgt.__class__.__mro__]):
 
         # Compute trial-IDs by matching data samples with provided trial-bounds
         samples = tgt.data[:, tgt.dimord.index("sample")]
         idx = np.searchsorted(samples, tgt.sampleinfo.ravel())
         idx = idx.reshape(tgt.sampleinfo.shape)
 
-        tgt._trialslice = [slice(st,end) for st,end in idx]
+        tgt._trialslice = [slice(st, end) for st, end in idx]
         tgt.trialid = np.full((samples.shape), -1, dtype=int)
         for itrl, itrl_slice in enumerate(tgt._trialslice):
             tgt.trialid[itrl_slice] = itrl
 
     # Write log entry
     if ref == tgt:
-        ref.log = "updated trial-definition with [" \
-                  + " x ".join([str(numel) for numel in trl.shape]) \
-                  + "] element array"
+        ref.log = (
+            "updated trial-definition with ["
+            + " x ".join([str(numel) for numel in trl.shape])
+            + "] element array"
+        )
     else:
         ref_log = ref._log.replace("\n\n", "\n\t")
         tgt.log = "trial-definition extracted from EventData object: "
         tgt._log += ref_log
-        tgt.cfg = {"method" : sys._getframe().f_code.co_name,
-                   "EventData object": ref.cfg}
+        tgt.cfg = {
+            "method": sys._getframe().f_code.co_name,
+            "EventData object": ref.cfg,
+        }
         ref.log = "updated trial-defnition of {} object".format(tgt.__class__.__name__)
 
     return
```

### Comparing `esi_syncopy-2023.5/syncopy/datatype/methods/redefinetrial.py` & `esi_syncopy-2023.7/syncopy/datatype/methods/redefinetrial.py`

 * *Files 7% similar despite different names*

```diff
@@ -15,17 +15,24 @@
 from syncopy.shared.errors import SPYTypeError, SPYValueError, SPYError
 from syncopy.shared.tools import get_defaults, get_frontend_cfg
 
 __all__ = ["redefinetrial"]
 
 
 @unwrap_cfg
-def redefinetrial(data_obj, trials=None, minlength=None,
-                  offset=None, toilim=None,
-                  begsample=None, endsample=None, trl=None):
+def redefinetrial(
+    data_obj,
+    trials=None,
+    minlength=None,
+    offset=None,
+    toilim=None,
+    begsample=None,
+    endsample=None,
+    trl=None,
+):
     """
     This function allows you to adjust the time axis of your data, i.e. to
     change from stimulus-locked to response-locked. Furthermore, it allows
     you to select a time window of interest, or to resegment your long trials
     into shorter fragments.
 
     Parameters
@@ -76,15 +83,15 @@
 
     defaults = get_defaults(redefinetrial)
     lcls = locals()
     new_cfg = get_frontend_cfg(defaults, lcls, kwargs={})
 
     # -- sort out mutually exclusive parameters --
 
-    vals = [new_cfg[par] for par in ['minlength', 'toilim', 'begsample', 'trl']]
+    vals = [new_cfg[par] for par in ["minlength", "toilim", "begsample", "trl"]]
     if vals.count(None) < 3:
         msg = "either `minlength` or `begsample`/`endsample` or `trl` or `toilim`"
         raise SPYError("Incompatible input arguments, " + msg)
     # now we made sure only one of the 4 parameters above is set.
 
     # total number of samples
     scount = data_obj.data.shape[data_obj._stackingDim]
@@ -103,133 +110,145 @@
     if toilim is not None:
         array_parser(toilim, dims=(2,))
         # use latency selection mechanic
         ret_obj = spy.selectdata(ret_obj, latency=toilim)
 
     elif minlength is not None:
 
-        scalar_parser(minlength, varname='minlength', lims=[0, np.inf])
+        scalar_parser(minlength, varname="minlength", lims=[0, np.inf])
 
         min_samples = int(minlength * data_obj.samplerate)
         trl_sel = []
         for trl_idx, trial in enumerate(ret_obj.trials):
             nSamples = trial.shape[data_obj._stackingDim]
             if nSamples >= min_samples:
                 trl_sel.append(trl_idx)
 
-        spy.log(f"discarding {len(data_obj.trials) - len(trl_sel)} trials", level='INFO',
-                caller='redefinetrial')
+        spy.log(
+            f"discarding {len(data_obj.trials) - len(trl_sel)} trials",
+            level="INFO",
+            caller="redefinetrial",
+        )
 
         if len(trl_sel) == 0:
-            spy.log("No trial fits the desired `minlength`, returning empty object!",
-                    caller='redefinetrial')
+            spy.log(
+                "No trial fits the desired `minlength`, returning empty object!",
+                caller="redefinetrial",
+            )
             return data_obj.__class__()
 
         ret_obj = spy.selectdata(ret_obj, trials=trl_sel)
 
     # helper variable
     new_trldef = ret_obj.trialdefinition
 
     # -- OR manipulate sampleinfo --
 
-    if new_cfg['trl'] is not None:
-        vals = [new_cfg[par] for par in ['begsample', 'endsample', 'offset']]
+    if new_cfg["trl"] is not None:
+        vals = [new_cfg[par] for par in ["begsample", "endsample", "offset"]]
         if vals.count(None) < 3:
-            msg = ("either complete trialdefinition `trl` or  "
-                   "`begsample`/`endsample` and `offset`")
+            msg = "either complete trialdefinition `trl` or  " "`begsample`/`endsample` and `offset`"
             raise SPYError("Incompatible input arguments, " + msg)
 
         # accepts also lists
         array_parser(trl, varname="trl")
         trl = np.array(trl)
 
         # to allow simple single trial definitions
         if trl.ndim == 1:
             trl = trl[None, :]
         if trl.ndim != 2:
             lgl = "2-dimensional array"
             act = f"{trl.ndim} array"
-            raise SPYValueError(lgl, 'trl', act)
+            raise SPYValueError(lgl, "trl", act)
 
         new_trldef = trl
 
         # selecting trials and applying a new trialdefinition in one go
         # is rather dangerous, but possible..
 
     elif begsample is not None or endsample is not None:
-        vals = [new_cfg[par] for par in ['begsample', 'endsample']]
+        vals = [new_cfg[par] for par in ["begsample", "endsample"]]
         if vals.count(None) != 0:
             lgl = "both `begsample` and `endsample`"
             act = f"got [{begsample}, {endsample}]"
-            raise SPYValueError(lgl, 'begsample/endsample', act)
+            raise SPYValueError(lgl, "begsample/endsample", act)
 
         try:
             begsample = np.array(begsample, dtype=int)
         except ValueError:
-            raise SPYTypeError(begsample, 'begsample', "integer number or array")
+            raise SPYTypeError(begsample, "begsample", "integer number or array")
 
         try:
             endsample = np.array(endsample, dtype=int)
         except ValueError:
-            raise SPYTypeError(endsample, 'endsample', "scalar or array")
+            raise SPYTypeError(endsample, "endsample", "scalar or array")
 
         if np.any(begsample < 0):
             lgl = "integers >= 0"
             act = "relative `begsample` < 0"
-            raise SPYValueError(lgl, 'begsample', act)
+            raise SPYValueError(lgl, "begsample", act)
 
         if begsample.size != 1 and begsample.size != len(new_trldef):
-            raise SPYValueError(f"scalar or array of length {len(new_trldef)}", "begsample/endsample",
-                                "wrong sized `begsample`")
+            raise SPYValueError(
+                f"scalar or array of length {len(new_trldef)}",
+                "begsample/endsample",
+                "wrong sized `begsample`",
+            )
 
         if begsample.size != endsample.size:
-            raise SPYValueError("same sizes for `begsample/endsample`", '',
-                                "different sizes")
+            raise SPYValueError("same sizes for `begsample/endsample`", "", "different sizes")
 
         if np.any(new_trldef[:, 0] + endsample > scount):
             lgl = f"integers < {int(scount - new_trldef[:, 0].max())}"
             act = "out of range"
-            raise SPYValueError(lgl, 'endsample', act)
+            raise SPYValueError(lgl, "endsample", act)
 
         # this also catches negative endsample
         if np.any(endsample - begsample < 0):
-            raise SPYValueError("endsample > begsample", "begsample/endsample",
-                                "endsample < begsample")
+            raise SPYValueError("endsample > begsample", "begsample/endsample", "endsample < begsample")
 
         # construct new trialdefinition
         new_trldef[:, 1] = new_trldef[:, 0] + endsample
         new_trldef[:, 0] += begsample
 
     # -- manipulate offset --
 
     if isinstance(offset, Number):
         new_trldef[:, 2] = np.ones(len(new_trldef)) * offset
 
     elif isinstance(offset, np.ndarray):
         if len(offset) != len(new_trldef):
             lgl = f"array of length {len(new_trldef)}"
             act = f"array of length {len(offset)}"
-            raise SPYValueError(lgl, 'offset', act)
+            raise SPYValueError(lgl, "offset", act)
         new_trldef[:, 2] = offset
     elif offset is None:
         pass
     else:
-        raise SPYTypeError(offset, 'offset', "scalar, array or None")
+        raise SPYTypeError(offset, "offset", "scalar, array or None")
 
     # -- apply (new) trialdefinition --
 
     array_parser(new_trldef, varname="trl", dims=2)
 
-    array_parser(new_trldef[:, :2], varname="trl", dims=(None, 2),
-                 hasnan=False, hasinf=False, ntype="int_like", lims=[0, scount])
+    array_parser(
+        new_trldef[:, :2],
+        varname="trl",
+        dims=(None, 2),
+        hasnan=False,
+        hasinf=False,
+        ntype="int_like",
+        lims=[0, scount],
+    )
 
     # apply new trialdefinition and be done with it
     spy.definetrial(ret_obj, trialdefinition=new_trldef)
 
     # Attach potential older cfg's from the input
     # to support chained frontend calls.
     ret_obj.cfg.update(data_obj.cfg)
 
     # Attach frontend parameters for replay.
-    ret_obj.cfg.update({'redefinetrial': new_cfg})
+    ret_obj.cfg.update({"redefinetrial": new_cfg})
 
     return ret_obj
```

### Comparing `esi_syncopy-2023.5/syncopy/datatype/methods/selectdata.py` & `esi_syncopy-2023.7/syncopy/datatype/methods/selectdata.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,36 +8,42 @@
 import h5py
 
 # Local imports
 import syncopy as spy
 from syncopy.shared.tools import get_frontend_cfg, get_defaults
 from syncopy.shared.parsers import data_parser
 from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYInfo, log
-from syncopy.shared.kwarg_decorators import unwrap_cfg, process_io, detect_parallel_client
+from syncopy.shared.kwarg_decorators import (
+    unwrap_cfg,
+    process_io,
+    detect_parallel_client,
+)
 from syncopy.shared.computational_routine import ComputationalRoutine
 from syncopy.shared.latency import get_analysis_window, create_trial_selection
 
 __all__ = ["selectdata"]
 
 
 @unwrap_cfg
 @detect_parallel_client
-def selectdata(data,
-               trials=None,
-               channel=None,
-               channel_i=None,
-               channel_j=None,
-               latency=None,
-               frequency=None,
-               taper=None,
-               unit=None,
-               eventid=None,
-               inplace=False,
-               clear=False,
-               **kwargs):
+def selectdata(
+    data,
+    trials=None,
+    channel=None,
+    channel_i=None,
+    channel_j=None,
+    latency=None,
+    frequency=None,
+    taper=None,
+    unit=None,
+    eventid=None,
+    inplace=False,
+    clear=False,
+    **kwargs,
+):
     """
     Create a new Syncopy object from a selection
 
     **Usage Notice**
 
     Syncopy offers two modes for selecting data:
 
@@ -252,56 +258,59 @@
         raise SPYTypeError(inplace, varname="inplace", expected="Boolean")
     if not isinstance(clear, bool):
         raise SPYTypeError(clear, varname="clear", expected="Boolean")
 
     # there is no `@unwrap_select` decorator in place here,
     # a `select` dictionary must therefore be directly passed via ** unpacking:
     # select = {'channel': [0]}; spy.selectdata(data, **select)
-    if 'select' in kwargs:
+    if "select" in kwargs:
         lgl = "unpacked selection keywords directly, try `**select`"
         act = "`select` as explicit parameter"
         raise SPYValueError(legal=lgl, varname="selection kwargs", actual=act)
 
     # get input arguments into cfg dict
     new_cfg = get_frontend_cfg(get_defaults(selectdata), locals(), kwargs)
 
     if not inplace:
         out = data.__class__(dimord=data.dimord)
 
     # First collect all available keyword values into a dict
-    selectDict = {"trials": trials,
-                  "channel": channel,
-                  "channel_i": channel_i,
-                  "channel_j": channel_j,
-                  "latency": latency,
-                  "frequency": frequency,
-                  "taper": taper,
-                  "unit": unit,
-                  "eventid": eventid}
+    selectDict = {
+        "trials": trials,
+        "channel": channel,
+        "channel_i": channel_i,
+        "channel_j": channel_j,
+        "latency": latency,
+        "frequency": frequency,
+        "taper": taper,
+        "unit": unit,
+        "eventid": eventid,
+    }
 
     # relevant selection keywords for the type of `data`
     expected = list(data._selectionKeyWords)
 
     # filter out typos like 'trails'
     if len(kwargs) > 0:
         kwargs.pop("parallel", None)
         if any([key not in expected for key in kwargs]):
-            lgl = f"the following keywords for {data.__class__.__name__}: '" +\
-                "'".join(opt + "', " for opt in expected)[:-2]
+            lgl = (
+                f"the following keywords for {data.__class__.__name__}: '"
+                + "'".join(opt + "', " for opt in expected)[:-2]
+            )
             lgl += " and 'inplace', 'clear', 'parallel'"
-            act = "dict with keys '" +\
-                  "'".join(key + "', " for key in kwargs.keys())[:-2]
+            act = "dict with keys '" + "'".join(key + "', " for key in kwargs.keys())[:-2]
             raise SPYValueError(legal=lgl, varname="selection kwargs", actual=act)
 
     # get out if unsuitable selection keywords given, e.g. 'frequency' for AnalogData
     for key, value in selectDict.items():
         if key not in expected and value is not None:
             lgl = f"one of {data.__class__._selectionKeyWords}"
             act = f"no `{key}` selection available for {data.__class__.__name__}"
-            raise SPYValueError(lgl, 'selection arguments', act)
+            raise SPYValueError(lgl, "selection arguments", act)
 
     # now just keep going with the selection keys relevant for that particular data type
     selectDict = {key: selectDict[key] for key in data._selectionKeyWords}
 
     # First simplest case: determine whether we just need to clear an existing selection
     if clear:
         if any(value is not None for value in selectDict.values()):
@@ -313,98 +322,106 @@
             data.selection = None
             SPYInfo("In-place selection cleared")
         return
 
     # first do a selection without latency as a possible subselection
     # of trials needs to be applied before the latency digesting functions
     # can be called (if the user by himself throws out non-fitting trials)
-    selectDict.pop('latency')
+    selectDict.pop("latency")
 
     # Pass provided selections on to `Selector` class which performs error checking
     # this is an in-place selection!
 
     data.selection = selectDict
 
     # -- sort out trials if latency is set --
 
     if latency is not None:
-        if not isinstance(latency, str) or latency != 'all':
+        if not isinstance(latency, str) or latency != "all":
             # sanity check done here, converts str arguments
             # ('maxperiod' and so on) into time window [start, end] of analysis
             window = get_analysis_window(data, latency)
 
             # this respects active inplace selections and
             # might update the trial selection to exclude non-fitting trials
             selectDict, numDiscard = create_trial_selection(data, window)
 
             if numDiscard > 0:
                 msg = f"Discarded {numDiscard} trial(s) which did not fit into latency window"
                 SPYInfo(msg)
 
             # update inplace selection
-            selectDict['latency'] = window
+            selectDict["latency"] = window
             data.selection = selectDict
 
     # If an in-place selection was requested we're done.
     if inplace:
         # attach frontend parameters for replay
-        data.cfg.update({'selectdata': new_cfg})
+        data.cfg.update({"selectdata": new_cfg})
         return
 
     # Inform the user what's about to happen
     selectionSize = _get_selection_size(data)
     if selectionSize > 1000:
         selectionSize /= 1024
         sUnit = "GB"
-        msg = "Copying {dsize:3.2f} {dunit:s} of data based on selection " +\
-            "to create new {objkind:s} object on disk"
+        msg = (
+            "Copying {dsize:3.2f} {dunit:s} of data based on selection "
+            + "to create new {objkind:s} object on disk"
+        )
         SPYInfo(msg.format(dsize=selectionSize, dunit=sUnit, objkind=data.__class__.__name__))
 
     # Create inventory of all available selectors and actually provided values
     # to create a bookkeeping dict for logging
     log_dct = {"inplace": inplace, "clear": clear, "latency": latency}
     log_dct.update(selectDict)
     log_dct.update(**kwargs)
 
     # Fire up `ComputationalRoutine`-subclass to do the actual selecting/copying
     selectMethod = DataSelection()
     selectMethod.initialize(data, out._stackingDim, chan_per_worker=kwargs.get("chan_per_worker"))
-    selectMethod.compute(data, out, parallel=kwargs.get("parallel"),
-                         log_dict=log_dct)
+    selectMethod.compute(data, out, parallel=kwargs.get("parallel"), log_dict=log_dct)
 
     # Handle selection of waveform for SpikeData objects
     if type(data) == spy.SpikeData and data.waveform is not None:
         if inplace:
-            spy.log("Inplace selection of SpikeData with waveform not supported for the waveform.", level="WARNING")
+            spy.log(
+                "Inplace selection of SpikeData with waveform not supported for the waveform.",
+                level="WARNING",
+            )
         else:
             fauxTrials = [data._preview_trial(trlno) for trlno in data.selection.trial_ids]
             spikes_by_trial = [f.idx[0] for f in fauxTrials]
             spike_idx = np.concatenate([np.array(x).ravel() for x in spikes_by_trial])
 
             # Copy the proper subset of the waveform dataset to `out`, the new `SpikeData` object.
             hdf5_file_in = data._get_backing_hdf5_file_handle()
             hdf5_file_out = out._get_backing_hdf5_file_handle()
 
             # Copy the waveform dataset into the new file, trial by trial to prevent memory issues.
-            ds = hdf5_file_out.create_dataset('waveform', shape=(len(spike_idx), *data.waveform.shape[1:]), dtype=data.waveform.dtype)
+            ds = hdf5_file_out.create_dataset(
+                "waveform",
+                shape=(len(spike_idx), *data.waveform.shape[1:]),
+                dtype=data.waveform.dtype,
+            )
             cur_new_idx = 0
             for tidx, old_trial_indices in enumerate(spikes_by_trial):
                 num_spikes_this_trial = len(old_trial_indices)
-                new_indices = np.s_[cur_new_idx:cur_new_idx + num_spikes_this_trial]
-                ds[new_indices, :, :] = hdf5_file_in['/waveform'][old_trial_indices, :, :]
+                new_indices = np.s_[cur_new_idx : cur_new_idx + num_spikes_this_trial]
+                ds[new_indices, :, :] = hdf5_file_in["/waveform"][old_trial_indices, :, :]
                 cur_new_idx = new_indices.stop
 
             out.waveform = ds
 
     # Wipe data-selection slot to not alter input object
     data.selection = None
 
     # attach cfg
     out.cfg.update(data.cfg)
-    out.cfg.update({'selectdata': new_cfg})
+    out.cfg.update({"selectdata": new_cfg})
 
     # return newly created output object
     return out
 
 
 def _get_selection_size(data):
     """
```

### Comparing `esi_syncopy-2023.5/syncopy/datatype/methods/show.py` & `esi_syncopy-2023.7/syncopy/datatype/methods/show.py`

 * *Files 1% similar despite different names*

```diff
@@ -137,15 +137,15 @@
                     invalid = True
         elif isinstance(sel, slice):
             if sel.start > sel.stop:
                 invalid = True
         if invalid:
             lgl = f"unique and sorted `{sel_key}` indices"
             act = sel
-            raise SPYValueError(lgl, 'show kwargs', act)
+            raise SPYValueError(lgl, "show kwargs", act)
 
     # Leverage `selectdata` to sanitize input and perform subset picking
     data.selectdata(inplace=True, **kwargs)
 
     # Truncate info message by removing any squeezed dimensions (if necessary)
     msg = data.selection.__str__().partition("with")[-1]
     if squeeze:
@@ -155,37 +155,36 @@
         msg = "".join(selectionTxt[txtMask])
         transform_out = np.squeeze
     else:
         transform_out = lambda x: x
     SPYInfo("Showing{}".format(msg))
 
     # catch totally out of range toi selection
-    has_time = True if 'time' in data.dimord else False
+    has_time = True if "time" in data.dimord else False
 
     # Use an object's `_preview_trial` method fetch required indexing tuples
     idxList = []
     for trlno in data.selection.trial_ids:
         # each dim has an entry, list only needed for mutability
         idxs = list(data._preview_trial(trlno).idx)
 
         # time/toi is a special case, all other dims get checked
         # beforehand, e.g. foi, channel, ... but out of range toi's get mapped
         # repeatedly to the last index, causing invalid hdf5 indexing
         if has_time:
-            idx = idxs[data.dimord.index('time')]
-            if not isinstance(idx, slice) and (
-                    len(idx) != len(set(idx))):
+            idx = idxs[data.dimord.index("time")]
+            if not isinstance(idx, slice) and (len(idx) != len(set(idx))):
                 lgl = "valid `toi` selection"
                 act = sel
-                raise SPYValueError(lgl, 'show kwargs', act)
+                raise SPYValueError(lgl, "show kwargs", act)
 
-        for i, prop_idx in enumerate(idxs):            
+        for i, prop_idx in enumerate(idxs):
             if isinstance(prop_idx, list) and len(prop_idx) == 1:
                 idxs[i] = prop_idx[0]
-            
+
         idxList.append(tuple(idxs))
 
     # Reset in-place subset selection
     data.selection = None
 
     # single trial selected
     if len(idxList) == 1:
```

### Comparing `esi_syncopy-2023.5/syncopy/datatype/selector.py` & `esi_syncopy-2023.7/syncopy/datatype/selector.py`

 * *Files 2% similar despite different names*

```diff
@@ -92,15 +92,15 @@
     size. To not overflow memory and slow down computations, neither `toi`
     nor `toilim` is checked for consistency with respect to `data.time`, i.e.,
     the code does not verify that min/max of `toi`/`toilim` are within the
     bounds of `data.time` for each selected trial.
 
     For objects that have a `time` property, a suitable new `trialdefinition`
     array (accessible via the identically named `Selector` class property)
-    is automatically constructed based on the provided selection. 
+    is automatically constructed based on the provided selection.
 
     By default, each selection property tries to convert a user-provided
     selection to a contiguous slice-indexer so that simple NumPy array
     indexing can be used for best performance. However, after setting all
     selection indices appropriate for the input object, a consistency
     check is performed by :meth:`_make_consistent` to ensure that the
     calculated indices can actually be jointly used on a multi-dimensional
@@ -132,32 +132,28 @@
             raise exc
         if select is None:
             select = {}
         if isinstance(select, str):
             if select == "all":
                 select = {}
             else:
-                raise SPYValueError(
-                    legal="'all' or `None` or dict", varname="select", actual=select
-                )
+                raise SPYValueError(legal="'all' or `None` or dict", varname="select", actual=select)
         if not isinstance(select, dict):
             raise SPYTypeError(select, "select", expected="dict")
 
         # Keep list of supported selectors in sync w/supported keywords of `selectdata`
         supported = data._selectionKeyWords
         # `selectdata` already throws out not supported keywords
         # so this is just a hard check when setting a selection via assignment
         if not set(select.keys()).issubset(supported):
             lgl = (
                 "dict with one or all of the following keys: '"
                 + "'".join(opt + "', " for opt in supported)[:-2]
             )
-            act = (
-                "dict with keys '" + "'".join(key + "', " for key in select.keys())[:-2]
-            )
+            act = "dict with keys '" + "'".join(key + "', " for key in select.keys())[:-2]
             raise SPYValueError(legal=lgl, varname="select", actual=act)
 
         # Save class of input object for posterity
         self._dataClass = data.__class__.__name__
 
         # Set up lists of (a) all selectable properties (b) trial-dependent ones
         # and (c) selectors independent from trials
@@ -186,17 +182,15 @@
         self._trials = None
         self._trial_ids = None
         self._trialdefinition = None
         for prop in self._allProps:
             setattr(self, "_{}".format(prop), None)
         self._useFancy = False  # flag indicating whether fancy indexing is necessary
         self._samplerate = None  # for objects supporting time-selections
-        self._timeShuffle = (
-            False  # flag indicating whether time-points are repeated/unordered
-        )
+        self._timeShuffle = False  # flag indicating whether time-points are repeated/unordered
 
         # We first need to know which trials are of interest here (assuming
         # that any valid input object *must* have a `trials_ids` attribute)
         self.trial_ids = (data, select)
 
         # Now set any possible selection attribute (depending on type of `data`)
         # Note: `trialdefinition` is set *after* harmonizing indexing selections
@@ -227,17 +221,15 @@
         trials = select.get("trials", None)
         vname = "select: trials"
 
         if isinstance(trials, str):
             if trials == "all":
                 trials = None
             else:
-                raise SPYValueError(
-                    legal="'all' or `None` or list/array", varname=vname, actual=trials
-                )
+                raise SPYValueError(legal="'all' or `None` or list/array", varname=vname, actual=trials)
         if trials is not None:
             if np.issubdtype(type(trials), np.number):
                 trials = [trials]
             try:
                 array_parser(
                     trials,
                     varname=vname,
@@ -275,15 +267,15 @@
             # this is cheap as it just initializes a list-like object
             # with no real data and/or computations!
             return TrialIndexer(self, self.trial_ids)
         else:
             return None
 
     def create_get_trial(self, data):
-        """ Closure to allow emulation of BaseData._get_trial"""
+        """Closure to allow emulation of BaseData._get_trial"""
 
         # trl_id has to be part of selection for coherence
         def _get_trial(trl_id):
             if trl_id not in self.trial_ids:
                 lgl = "a trial part of the selection"
                 act = trl_id
                 raise SPYValueError(lgl, "Selector.trials", act)
@@ -302,15 +294,15 @@
                     if len(set(dim_idx)) != len(dim_idx):
                         lgl = "simple selections w/o repetitions"
                         act = f"fancy selection with repetitions for selector {data.dimord[i]}"
                         raise SPYValueError(lgl, "Selector.trials", act)
 
                     # DiscreteData selections inherently re-order the sample dim. idx
                     # so these we sort, all others we need ordered
-                    if 'discrete_data' in str(data.__class__):
+                    if "discrete_data" in str(data.__class__):
                         # sorts in place!
                         dim_idx.sort()
                     elif np.any(np.diff(dim_idx) < 0):
                         lgl = "simple selection in ascending order"
                         act = f"fancy non-ordered selection of selector {data.dimord[i]}"
                         raise SPYValueError(lgl, "Selector.trials", act)
             # if we landed here all is good and we take
@@ -328,17 +320,15 @@
     @channel.setter
     def channel(self, dataselect):
         data, select = dataselect
         chanSpec = select.get("channel")
         if self._dataClass == "CrossSpectralData":
             if chanSpec is not None:
                 lgl = "`channel_i` and/or `channel_j` selectors for `CrossSpectralData`"
-                raise SPYValueError(
-                    legal=lgl, varname="select: channel", actual=data.__class__.__name__
-                )
+                raise SPYValueError(legal=lgl, varname="select: channel", actual=data.__class__.__name__)
             else:
                 return
         self._selection_setter(data, select, "channel")
 
     @property
     def channel_i(self):
         """List or slice encoding principal channel-pair selection"""
@@ -372,17 +362,15 @@
         timeSpec = select.get("latency", None)
         checkInf = None
         vname = "select: latency"
 
         hasTime = hasattr(data, "time") or hasattr(data, "trialtime")
         if timeSpec is not None and hasTime is False:
             lgl = "Syncopy data object with time-dimension"
-            raise SPYValueError(
-                legal=lgl, varname=vname, actual=data.__class__.__name__
-            )
+            raise SPYValueError(legal=lgl, varname=vname, actual=data.__class__.__name__)
 
         # If `data` has a `time` property, fill up `self.time`
         if hasTime:
             if isinstance(timeSpec, str):
                 if timeSpec == "all":
                     timeSpec = None
                     select["latency"] = None
@@ -391,44 +379,36 @@
                         legal="'all' or `None` or list/array",
                         varname=vname,
                         actual=timeSpec,
                     )
             if timeSpec is not None:
                 if np.issubdtype(type(timeSpec), np.number):
                     timeSpec = [timeSpec]
-                    array_parser(
-                        timeSpec, varname=vname, hasinf=checkInf, hasnan=False, dims=1
-                    )
+                    array_parser(timeSpec, varname=vname, hasinf=checkInf, hasnan=False, dims=1)
                 # can only be 2-sequence [start, end]
                 else:
                     if len(timeSpec) != 2:
                         lgl = "`select: latency` selection with two components"
-                        act = "`select: latency` with {} components".format(
-                            len(timeSpec)
-                        )
+                        act = "`select: latency` with {} components".format(len(timeSpec))
                         raise SPYValueError(legal=lgl, varname=vname, actual=act)
                     if timeSpec[0] >= timeSpec[1]:
-                        lgl = (
-                            "`select: latency` selection with `latency[0]` < `latency[1]`"
-                        )
-                        act = "selection range from {} to {}".format(
-                            timeSpec[0], timeSpec[1]
-                        )
+                        lgl = "`select: latency` selection with `latency[0]` < `latency[1]`"
+                        act = "selection range from {} to {}".format(timeSpec[0], timeSpec[1])
                         raise SPYValueError(legal=lgl, varname=vname, actual=act)
 
             # Assign timing selection and copy over samplerate from source object
             if any(["DiscreteData" in str(base) for base in data.__class__.__mro__]):
                 # special case DiscreteData: here we need an assignable property
                 # for `_make_consistent` so we unpack the Indexer right away
-                self._time = list(SelectionTimeIndexer(data, toilim=select.get("latency"),
-                                                       idx_list=self.trial_ids))
+                self._time = list(
+                    SelectionTimeIndexer(data, toilim=select.get("latency"), idx_list=self.trial_ids)
+                )
 
             else:
-                self._time = SelectionTimeIndexer(data, toilim=select.get("latency"),
-                                                  idx_list=self.trial_ids)
+                self._time = SelectionTimeIndexer(data, toilim=select.get("latency"), idx_list=self.trial_ids)
             self._samplerate = data.samplerate
 
         else:
             return
 
     @property
     def trialdefinition(self):
@@ -483,15 +463,15 @@
 
     @sampleinfo.setter
     def sampleinfo(self, sinfo):
         raise SPYError("Cannot set sampleinfo. Use `Selector.trialdefinition` instead.")
 
     @property
     def trialintervals(self):
-        """nTrials x 2 :class:`numpy.ndarray` of [start, end] times in seconds """
+        """nTrials x 2 :class:`numpy.ndarray` of [start, end] times in seconds"""
         if self._trialdefinition is not None and self._samplerate is not None:
             # trial lengths in samples
             start_end = self.sampleinfo - self.sampleinfo[:, 0][:, None]
             start_end[:, 1] -= 1  # account for last time point
             # add offset and convert to seconds
             start_end = (start_end + self.trialdefinition[:, 2][:, None]) / self._samplerate
             return start_end
@@ -508,17 +488,15 @@
 
         # Unpack input and perform error-checking
         data, select = dataselect
         freqSpec = select.get("frequency")
         hasFreq = hasattr(data, "freq")
         if freqSpec is not None and hasFreq is False:
             lgl = "Syncopy data object with freq-dimension"
-            raise SPYValueError(
-                legal=lgl, varname="frequency", actual=data.__class__.__name__
-            )
+            raise SPYValueError(legal=lgl, varname="frequency", actual=data.__class__.__name__)
 
         # If `data` has a `freq` property, fill up `self.freq`
         if hasFreq:
             if isinstance(freqSpec, str):
                 if freqSpec == "all":
                     freqSpec = None
                     select["frequency"] = None
@@ -553,21 +531,17 @@
                         ntype="numeric",
                         varname="frequency",
                         hasnan=False,
                         lims=[data.freq.min(), data.freq.max()],
                         dims=(2,),
                     )
                     if freqSpec[0] >= freqSpec[1]:
-                        lgl = (
-                            "`select: frequency` selection with `frequency[0]` < `frequency[1]`"
-                        )
-                        act = "selection range from {} to {}".format(
-                            freqSpec[0], freqSpec[1]
-                        )
-                        raise SPYValueError(legal=lgl, varname='frequency', actual=act)
+                        lgl = "`select: frequency` selection with `frequency[0]` < `frequency[1]`"
+                        act = "selection range from {} to {}".format(freqSpec[0], freqSpec[1])
+                        raise SPYValueError(legal=lgl, varname="frequency", actual=act)
 
                     self._freq = data._get_freq(foi=None, foilim=freqSpec)
 
     @property
     def taper(self):
         """List or slice encoding taper-selection"""
         return self._taper
@@ -636,17 +610,15 @@
         # Unpack input and perform error-checking
         selection = select.get(selectkey)
         target = getattr(data, selectkey, None)
         selector = "_{}".format(selectkey)
         vname = "select: {}".format(selectkey)
         if selection is not None and target is None:
             lgl = "Syncopy data object with {}".format(selectkey)
-            raise SPYValueError(
-                legal=lgl, varname=vname, actual=data.__class__.__name__
-            )
+            raise SPYValueError(legal=lgl, varname=vname, actual=data.__class__.__name__)
 
         if target is not None:
 
             if np.issubdtype(target.dtype, np.dtype("str").type):
                 slcLims = [0, target.size]
                 arrLims = None
                 hasnan = None
@@ -682,23 +654,19 @@
                 if selection.stop is not None:
                     selLims[1] = selection.stop
                 if selLims[0] >= selLims[1]:
                     lgl = "selection range with min < max"
                     act = "selection range from {} to {}".format(selLims[0], selLims[1])
                     raise SPYValueError(legal=lgl, varname=vname, actual=act)
                 # check slice/range boundaries: take care of things like `slice(-10, -3)`
-                if np.isfinite(selLims[0]) and (
-                    selLims[0] < -slcLims[1] or selLims[0] >= slcLims[1]
-                ):
+                if np.isfinite(selLims[0]) and (selLims[0] < -slcLims[1] or selLims[0] >= slcLims[1]):
                     lgl = "selection range with min >= {}".format(slcLims[0])
                     act = "selection range starting at {}".format(selLims[0])
                     raise SPYValueError(legal=lgl, varname=vname, actual=act)
-                if np.isfinite(selLims[1]) and (
-                    selLims[1] > slcLims[1] or selLims[1] < -slcLims[1]
-                ):
+                if np.isfinite(selLims[1]) and (selLims[1] > slcLims[1] or selLims[1] < -slcLims[1]):
                     lgl = "selection range with max <= {}".format(slcLims[1])
                     act = "selection range ending at {}".format(selLims[1])
                     raise SPYValueError(legal=lgl, varname=vname, actual=act)
 
                 # The 2d-arrays in `DiscreteData` objects require some additional hand-holding
                 # performed by the respective `_get_unit` and `_get_eventid` class methods
                 if selectkey in ["unit", "eventid"]:
@@ -707,27 +675,29 @@
                     else:
                         if isinstance(selection, slice):
                             if np.issubdtype(target.dtype, np.dtype("str").type):
                                 target = np.arange(target.size)
                             selection = list(target[selection])
                         else:
                             selection = list(selection)
-                        setattr(self, selector, getattr(data, "_get_" + selectkey)(self.trial_ids, selection))
+                        setattr(
+                            self,
+                            selector,
+                            getattr(data, "_get_" + selectkey)(self.trial_ids, selection),
+                        )
 
                 else:
                     if selection.start is selection.stop is None:
                         setattr(self, selector, slice(None, None, 1))
                     else:
                         if selection.step is None:
                             step = 1
                         else:
                             step = selection.step
-                        setattr(
-                            self, selector, slice(selection.start, selection.stop, step)
-                        )
+                        setattr(self, selector, slice(selection.start, selection.stop, step))
 
             # Selection is either a valid list/array or bust
             else:
                 try:
                     array_parser(
                         selection,
                         varname=vname,
@@ -749,15 +719,19 @@
 
                 # Preserve order and duplicates of selection - don't use `np.isin` here!
                 idxList = []
                 for sel in selection:
                     idxList += list(np.where(targetArr == sel)[0])
 
                 if selectkey in ["unit", "eventid"]:
-                    setattr(self, selector, getattr(data, "_get_" + selectkey)(self.trial_ids, idxList))
+                    setattr(
+                        self,
+                        selector,
+                        getattr(data, "_get_" + selectkey)(self.trial_ids, idxList),
+                    )
                 else:
                     # if possible, convert range-arrays (`[0, 1, 2, 3]`) to slices for better performance
                     if len(idxList) > 1:
                         steps = np.diff(idxList)
                         if steps.min() == steps.max() == 1:
                             idxList = slice(idxList[0], idxList[-1] + 1, 1)
 
@@ -854,33 +828,29 @@
                 elif areShuffled:
                     combinedSelect = combinedSelect.tolist()
 
                 # The usual list -> slice conversion (if possible)
                 if len(combinedSelect) > 1:
                     selSteps = np.diff(combinedSelect)
                     if selSteps.min() == selSteps.max() == 1:
-                        combinedSelect = slice(
-                            combinedSelect[0], combinedSelect[-1] + 1, 1
-                        )
+                        combinedSelect = slice(combinedSelect[0], combinedSelect[-1] + 1, 1)
 
                 # Update selector properties
                 for selection in actualSelections:
                     getattr(self, "_{}".format(selection))[tk] = combinedSelect
 
             # Ensure that `self.channel` is compatible w/provided selections: harmonize
             # `self.channel` with what is actually available in selected trials
             if self._dataClass == "SpikeData":
                 availChannels = reduce(np.union1d, chanPerTrial)
                 chanSelection = wantedChannels[np.isin(wantedChannels, availChannels)].tolist()
                 if len(chanSelection) > 1:
                     selSteps = np.diff(chanSelection)
                     if selSteps.min() == selSteps.max() == 1:
-                        chanSelection = slice(
-                            chanSelection[0], chanSelection[-1] + 1, 1
-                        )
+                        chanSelection = slice(chanSelection[0], chanSelection[-1] + 1, 1)
                 self._channel = chanSelection
 
             # Finally, prepare new `trialdefinition` array
             self.trialdefinition = data
 
             return
 
@@ -915,28 +885,26 @@
 
     # Make selection readable from the command line
     def __str__(self):
 
         # Get list of print-worthy attributes
         ppattrs = [attr for attr in self.__dir__() if not attr.startswith("_")]
         # legacy, we have proper `Selector.trials` now
-        ppattrs.remove('trial_ids')
+        ppattrs.remove("trial_ids")
         ppattrs.sort()
 
         # Construct dict of pretty-printable property info
         ppdict = {}
         for attr in ppattrs:
             val = getattr(self, attr)
             if val is not None and attr in self._byTrialProps:
                 val = next(iter(val))
             if isinstance(val, slice):
                 if val.start is val.stop is None:
-                    ppdict[attr] = "all {}{}, ".format(
-                        attr, "s" if not attr.endswith("s") else ""
-                    )
+                    ppdict[attr] = "all {}{}, ".format(attr, "s" if not attr.endswith("s") else "")
                 elif val.start is None or val.stop is None:
                     ppdict[attr] = "{}-range, ".format(attr)
                 else:
                     ppdict[attr] = "{0:d} {1:s}{2:s}, ".format(
                         int(np.ceil((val.stop - val.start) / val.step)),
                         attr,
                         "s" if not attr.endswith("s") else "",
@@ -955,15 +923,14 @@
         for pout in ppdict.values():
             msg += pout
 
         return msg[:-2]
 
 
 class SelectionTimeIndexer:
-
     def __init__(self, data_object, toilim, idx_list):
         """
         Class to obtain an indexable iterable of time slices
         from an instantiated Syncopy data class `data_object` with an
         active time/latency selection given by `toilim`.
         Relies on the `time` property of the respective
         `data_object`.
@@ -995,20 +962,20 @@
         # trivial all time points selection
         if self.toilim is None:
             return slice(None)
 
         # continuous data
         elif not self.is_discrete:
             _, selTime = best_match(self.data_object.time[trialno], self.toilim, span=True)
-            return np.s_[selTime[0]:selTime[-1] + 1:1]
+            return np.s_[selTime[0] : selTime[-1] + 1 : 1]
         # discrete data
         else:
             trlTime = self.trialtime[self.data_object._trialslice[trialno]]
             _, selTime = best_match(trlTime, self.toilim, span=True)
-            return np.s_[selTime[0]:selTime[-1] + 1:1]
+            return np.s_[selTime[0] : selTime[-1] + 1 : 1]
 
     def __getitem__(self, trialno):
         # single trial access via index operator []
         if not isinstance(trialno, Number):
             raise SPYTypeError(trialno, "trial index", "single number to index a single trial")
         if trialno not in self.idx_set:
             lgl = "index of existing trials"
```

### Comparing `esi_syncopy-2023.5/syncopy/datatype/util.py` & `esi_syncopy-2023.7/syncopy/datatype/util.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,19 +6,18 @@
 from numbers import Number
 import numpy as np
 
 # Syncopy imports
 from syncopy import __storage__, __storagelimit__, __sessionid__
 from syncopy.shared.errors import SPYTypeError, SPYValueError
 
-__all__ = ['TrialIndexer']
+__all__ = ["TrialIndexer"]
 
 
 class TrialIndexer:
-
     def __init__(self, data_object, idx_list):
         """
         Class to obtain an indexable trials iterable from
         an instantiated Syncopy data class `data_object`.
         Relies on the `_get_trial` method of the
         respective `data_object`.
 
@@ -56,15 +55,14 @@
         return self.__str__()
 
     def __str__(self):
         return "{} element iterable".format(self._len)
 
 
 class TimeIndexer:
-
     def __init__(self, trialdefinition, samplerate, idx_list):
         """
         Class to obtain an indexable time array iterable from
         an instantiated Syncopy data class `data_object`.
         Relies on the `trialdefinition` of the respective
         `data_object`.
 
@@ -107,15 +105,15 @@
     def __repr__(self):
         return self.__str__()
 
     def __str__(self):
         return "{} element iterable".format(self._len)
 
 
-def get_dir_size(start_path = '.', out="byte"):
+def get_dir_size(start_path=".", out="byte"):
     """
     Compute size of all files in directory (and its subdirectories), in bytes or GB.
     """
     total_size_bytes = 0
     num_files = 0
     for dirpath, _, filenames in os.walk(start_path):
         for f in filenames:
```

### Comparing `esi_syncopy-2023.5/syncopy/io/__init__.py` & `esi_syncopy-2023.7/syncopy/io/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -6,21 +6,27 @@
 # Import __all__ routines from local modules
 from . import (
     utils,
     load_spy_container,
     save_spy_container,
     load_ft,
     load_tdt,
-    load_nwb
+    load_nwb,
+    nwb,
+    mne_conv,
 )
 from .utils import *
 from .load_spy_container import *
 from .save_spy_container import *
 from .load_ft import *
 from .load_tdt import *
 from .load_nwb import *
+from .nwb import *
+from .mne_conv import *
 
 # Populate local __all__ namespace
-__all__ = ['load_ft_raw', 'load_tdt', 'load_nwb']
+__all__ = ["load_ft_raw", "load_tdt", "load_nwb", "mne_conv"]
 __all__.extend(utils.__all__)
 __all__.extend(load_spy_container.__all__)
 __all__.extend(save_spy_container.__all__)
+__all__.extend(nwb.__all__)
+__all__.extend(mne_conv.__all__)
```

### Comparing `esi_syncopy-2023.5/syncopy/io/load_ft.py` & `esi_syncopy-2023.7/syncopy/io/load_ft.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,22 +14,18 @@
 from syncopy.shared.errors import SPYValueError, SPYInfo, SPYWarning
 from syncopy.shared.parsers import io_parser, sequence_parser, scalar_parser
 from syncopy.datatype import AnalogData
 
 __all__ = ["load_ft_raw"]
 
 # Required fields for the ft_datatype_raw
-req_fields_raw = ('time', 'trial', 'label')
+req_fields_raw = ("time", "trial", "label")
 
 
-def load_ft_raw(filename,
-                list_only=False,
-                select_structures=None,
-                include_fields=None,
-                mem_use=4000):
+def load_ft_raw(filename, list_only=False, select_structures=None, include_fields=None, mem_use=4000):
 
     """
     Imports raw time-series data from Field Trip
     into potentially multiple :class:`~syncopy.AnalogData` objects,
     one for each structure found within the MAT-file.
 
     The aim is to parse each FT data structure, which
@@ -122,74 +118,67 @@
     """
 
     # -- Input validation --
 
     io_parser(filename, isfile=True)
 
     if select_structures is not None:
-        sequence_parser(select_structures,
-                        varname='select_structures',
-                        content_type=str)
+        sequence_parser(select_structures, varname="select_structures", content_type=str)
     if include_fields is not None:
-        sequence_parser(include_fields,
-                        varname='include_fields',
-                        content_type=str)
+        sequence_parser(include_fields, varname="include_fields", content_type=str)
 
-    scalar_parser(mem_use, varname='mem_use', ntype="int_like", lims=[1, np.inf])
+    scalar_parser(mem_use, varname="mem_use", ntype="int_like", lims=[1, np.inf])
 
     # -- MAT-File Format --
 
     version = _get_Matlab_version(filename)
     msg = f"Reading MAT-File version {version} "
     SPYInfo(msg)
 
     # new hdf container format, use h5py
     if version >= 7.3:
 
-        h5File = h5py.File(filename, 'r')
-        struct_keys = [key for key in h5File.keys() if '#' not in key]
+        h5File = h5py.File(filename, "r")
+        struct_keys = [key for key in h5File.keys() if "#" not in key]
 
         struct_container = h5File
-        struct_reader = lambda struct: _read_hdf_structure(struct,
-                                                           h5File=h5File,
-                                                           mem_use=mem_use,
-                                                           include_fields=include_fields)
+        struct_reader = lambda struct: _read_hdf_structure(
+            struct, h5File=h5File, mem_use=mem_use, include_fields=include_fields
+        )
 
     # old format <2GB, use scipy's MAT reader
     else:
 
         if mem_use < 2000:
             msg = "MAT-File version < 7.3 does not support lazy loading"
             msg += f"\nReading {filename} might take up to 2GB of RAM, you requested only {mem_use / 1000}GB"
             SPYInfo(msg)
-            lgl = '2000 or more MB'
+            lgl = "2000 or more MB"
             actual = f"{mem_use}"
-            raise SPYValueError(lgl, varname='mem_use', actual=actual)
+            raise SPYValueError(lgl, varname="mem_use", actual=actual)
 
-        raw_dict = sio.loadmat(filename,
-                               mat_dtype=True,
-                               simplify_cells=True)
+        raw_dict = sio.loadmat(filename, mat_dtype=True, simplify_cells=True)
 
-        struct_keys = [skey for skey in raw_dict.keys() if '__' not in skey]
+        struct_keys = [skey for skey in raw_dict.keys() if "__" not in skey]
 
         struct_container = raw_dict
-        struct_reader = lambda struct: _read_dict_structure(struct,
-                                                            include_fields=include_fields)
+        struct_reader = lambda struct: _read_dict_structure(struct, include_fields=include_fields)
 
     msg = f"Found {len(struct_keys)} structure(s): {struct_keys} in {filename}"
     SPYInfo(msg)
 
     if list_only:
         return struct_keys
 
     if len(struct_keys) == 0:
-        SPYValueError(legal="At least one structure",
-                      varname=filename,
-                      actual="No structure found"
-                      )
+        SPYValueError(
+            legal="At least one structure",
+            varname=filename,
+            actual="No structure found",
+        )
 
     # -- IO Operations --
 
     out_dict = {}
 
     # load only a subset
     if select_structures is not None:
@@ -215,18 +204,15 @@
         adata.log = msg
 
         out_dict[skey] = adata
 
     return out_dict
 
 
-def _read_hdf_structure(h5Group,
-                        h5File,
-                        mem_use,
-                        include_fields=None):
+def _read_hdf_structure(h5Group, h5File, mem_use, include_fields=None):
 
     """
     Each Matlab structure contained in
     a hdf5 MAT-File is a h5py Group object.
 
     Each key of this Group corresponds to
     a field in the Matlab structure.
@@ -252,20 +238,20 @@
     AData = AnalogData(dimord=AnalogData._defaultDimord)
 
     # probably better to define an abstract mapping
     # if we want to support more FT formats in the future
 
     # these are numpy arrays holding hdf5 object references
     # i.e. one per trial, channel, time (per trial)
-    trl_refs = h5Group['trial'][:, 0]
-    time_refs = h5Group['time'][:, 0]
-    chan_refs = h5Group['label'][0, :]
+    trl_refs = h5Group["trial"][:, 0]
+    time_refs = h5Group["time"][:, 0]
+    chan_refs = h5Group["label"][0, :]
 
-    if 'fsample' in h5Group:
-        AData.samplerate = h5Group['fsample'][0, 0]
+    if "fsample" in h5Group:
+        AData.samplerate = h5Group["fsample"][0, 0]
     else:
         AData.samplerate = _infer_fsample(h5File[time_refs[0]])
 
     # -- retrieve shape information --
     nTrials = trl_refs.size
 
     # peek in 1st trial to determine the number of channels
@@ -283,37 +269,35 @@
 
     itemsize = h5File[trl_refs[0]].dtype.itemsize
     # in Mbyte
     trl_size = itemsize * mean_trl_size * nChannels / 1e6
 
     # assumption: single trial fits into RAM
     if trl_size >= 0.4 * mem_use:
-        lgl = f'{2.5 * trl_size} or more MB'
+        lgl = f"{2.5 * trl_size} or more MB"
         actual = f"{mem_use}"
-        raise SPYValueError(lgl, varname='mem_use', actual=actual)
+        raise SPYValueError(lgl, varname="mem_use", actual=actual)
 
     # -- IO process --
 
     # create new hdf5 dataset for our AnalogData
     # with the default dimord ['time', 'channel']
     # and our default data type np.float32 -> implicit casting!
     with h5py.File(AData.filename, mode="w") as h5FileOut:
-        ADset = h5FileOut.create_dataset("data",
-                                         dtype=np.float32,
-                                         shape=[nTotalSamples, nChannels])
+        ADset = h5FileOut.create_dataset("data", dtype=np.float32, shape=[nTotalSamples, nChannels])
 
         pbar = tqdm(trl_refs, desc=f"{struct_name} - loading {nTrials} trials", disable=None)
 
-        SampleCounter = 0   # trial stacking
+        SampleCounter = 0  # trial stacking
         # one swipe per trial
         for tr in pbar:
             trl_array = h5File[tr]
             # in samples
             trl_samples = trl_array.shape[0]
-            ADset[SampleCounter:SampleCounter + trl_samples, :] = trl_array
+            ADset[SampleCounter : SampleCounter + trl_samples, :] = trl_array
             SampleCounter += trl_samples
         pbar.close()
 
         AData.data = ADset
 
     AData._reopen()
 
@@ -324,36 +308,36 @@
     for time_r in time_refs:
         offsets.append(h5File[time_r][0, 0])
     offsets = np.rint(np.array(offsets) * AData.samplerate)
     trl_def = np.hstack([sampleinfo, offsets[:, None]])
 
     # check if there is a 'trialinfo'
     try:
-        trl_def = np.hstack([trl_def, h5Group['trialinfo']])
+        trl_def = np.hstack([trl_def, h5Group["trialinfo"]])
     except KeyError:
         pass
 
     AData.trialdefinition = trl_def
 
     # each channel label is an integer array with shape (X, 1),
     # where `X` is the number of ascii encoded characters
-    channels = [''.join(map(chr, h5File[cr][:, 0])) for cr in chan_refs]
+    channels = ["".join(map(chr, h5File[cr][:, 0])) for cr in chan_refs]
     AData.channel = channels
 
     # -- Additional Fields --
     if include_fields is not None:
         AData.info = {}
         # additional fields in MAT-File
         afields = [k for k in h5Group.keys() if k not in req_fields_raw]
         msg = f"Found following additional fields: {afields}"
-        SPYInfo(msg, caller='load_ft_raw')
+        SPYInfo(msg, caller="load_ft_raw")
         for field in include_fields:
             if field not in h5Group:
                 msg = f"Could not find additional field {field} in {struct_name}"
-                SPYWarning(msg, caller='load_ft_raw')
+                SPYWarning(msg, caller="load_ft_raw")
                 continue
 
             dset = h5Group[field]
             # we only support fields pointing
             # directly to a dataset containing actual data
             # and not references to larger objects
             if isinstance(dset[0], h5py.Reference):
@@ -400,86 +384,83 @@
     | cfg                | X          |
     +--------------------+------------+
 
     Each trial in FT has nChannels x nSamples ordering,
     Syncopy has nSamples x nChannels
     """
 
-
     # initialize AnalogData
-    if 'fsample' in structure:
-        samplerate = structure['fsample']
+    if "fsample" in structure:
+        samplerate = structure["fsample"]
     else:
-        samplerate = _infer_fsample(structure['time'][0])
+        samplerate = _infer_fsample(structure["time"][0])
 
     AData = AnalogData(samplerate=samplerate)
 
     # compute total hdf5 shape
     # we use fixed stacking along 1st axis
     # but channel x sample ordering in FT
-    nTotalSamples = np.sum([trl.shape[1] for trl in structure['trial']])
-    nChannels = structure['trial'][0].shape[0]
+    nTotalSamples = np.sum([trl.shape[1] for trl in structure["trial"]])
+    nChannels = structure["trial"][0].shape[0]
     sampleinfo = []
 
-    with h5py.File(AData._filename, 'w') as h5file:
+    with h5py.File(AData._filename, "w") as h5file:
 
-        dset = h5file.create_dataset("data",
-                                     dtype=np.float32,
-                                     shape=[nTotalSamples, nChannels])
+        dset = h5file.create_dataset("data", dtype=np.float32, shape=[nTotalSamples, nChannels])
 
         stack_count = 0
-        for trl in structure['trial']:
+        for trl in structure["trial"]:
             trl_size = trl.shape[1]
             # default data type np.float32 -> implicit casting!
-            dset[stack_count:stack_count + trl_size] = trl.T.astype(np.float32)
+            dset[stack_count : stack_count + trl_size] = trl.T.astype(np.float32)
 
             # construct on the fly to cover all the trials
             sampleinfo.append(np.array([stack_count, stack_count + trl_size]))
 
             stack_count += trl_size
 
         AData.data = dset
 
     AData._reopen()
 
     sampleinfo = np.array(sampleinfo)
 
     # get the channel ids
-    channels = structure['label']
+    channels = structure["label"]
     # set the channel ids
     AData.channel = list(channels.astype(str))
 
     # get the offets
-    offsets = np.array([tvec[0] for tvec in structure['time']])
+    offsets = np.array([tvec[0] for tvec in structure["time"]])
     offsets *= AData.samplerate
 
     # build trialdefinition
     trl_def = np.column_stack([sampleinfo, offsets])
 
     # check if there is a 'trialinfo'
     try:
-        trl_def = np.hstack([trl_def, structure['trialinfo']])
+        trl_def = np.hstack([trl_def, structure["trialinfo"]])
     except KeyError:
         pass
 
     AData.trialdefinition = trl_def
 
     # -- Additional Fields --
 
     if include_fields is not None:
         AData.info = {}
         # additional fields in MAT-File
         afields = [k for k in structure.keys() if k not in req_fields_raw]
         msg = f"Found following additional fields: {afields}"
-        SPYInfo(msg, caller='load_ft_raw')
+        SPYInfo(msg, caller="load_ft_raw")
 
         for field in include_fields:
             if field not in structure:
                 msg = f"Could not find additional field {field}"
-                SPYWarning(msg, caller='load_ft_raw')
+                SPYWarning(msg, caller="load_ft_raw")
                 continue
             # we only support fields pointing directly to some data
             # no nested structures!
             if np.ndim(structure[field]) != 0:
                 msg = f"Could not read additional nested field '{field}'\n"
                 msg += "Only simple fields holding str labels or 1D arrays are supported"
                 SPYWarning(msg)
@@ -494,27 +475,27 @@
 
     """
     Peeks into the 1st line of a .mat file
     and extracts the version information.
     Works for both < 7.3 and newer MAT-files.
     """
 
-    with open(filename, 'rb') as matfile:
+    with open(filename, "rb") as matfile:
         line1 = next(matfile)
         # relevant information
         header = line1[:76].decode()
 
     # matches for example 'MATLAB 5.01'
     # with the version as only capture group
     pattern = re.compile(r"^MATLAB\s(\d*\.\d*)")
     match = pattern.match(header)
 
     if not match:
-        lgl = 'recognizable .mat file'
-        actual = 'can not recognize .mat file'
+        lgl = "recognizable .mat file"
+        actual = "can not recognize .mat file"
         raise SPYValueError(lgl, filename, actual)
 
     version = float(match.group(1))
 
     return version
 
 
@@ -529,15 +510,15 @@
     new-style (hdf5 Group) MAT-file structures.
     """
 
     for key in req_fields:
         if key not in structure:
             lgl = f"{key} present in MAT structure"
             actual = f"{key} missing"
-            raise SPYValueError(lgl, 'MAT structure', actual)
+            raise SPYValueError(lgl, "MAT structure", actual)
 
 
 def _infer_fsample(time_vector):
 
     """
     Akin to `ft_datatype_raw` determine
     the sampling frequency from the sampling
@@ -561,11 +542,11 @@
     for example some labels
     """
 
     # FIXME: a simple `for in ascii_arr in dataset` might do the trick as well
     # (no need to enumerate)?
     str_seq = []
     for i, ascii_arr in enumerate(dataset[...].T):
-        string = ''.join(map(chr, ascii_arr))
+        string = "".join(map(chr, ascii_arr))
         str_seq.append(string)
 
     return np.array(str_seq)
```

### Comparing `esi_syncopy-2023.5/syncopy/io/load_nwb.py` & `esi_syncopy-2023.7/syncopy/io/load_nwb.py`

 * *Files 23% similar despite different names*

```diff
@@ -8,79 +8,91 @@
 import sys
 import h5py
 import subprocess
 import numpy as np
 from tqdm import tqdm
 
 # Local imports
-from syncopy import __nwb__
 from syncopy.datatype.continuous_data import AnalogData
-from syncopy.datatype.discrete_data import EventData
-from syncopy.shared.errors import SPYError, SPYTypeError, SPYValueError, SPYWarning, SPYInfo
+from syncopy.datatype.discrete_data import EventData, SpikeData
+from syncopy.shared.errors import (
+    SPYError,
+    SPYTypeError,
+    SPYValueError,
+    SPYWarning,
+    SPYInfo,
+)
 from syncopy.shared.parsers import io_parser, scalar_parser, filename_parser
+from syncopy import __pynwb__
 
-# Conditional imports
-if __nwb__:
+__all__ = ["load_nwb"]
+
+
+if __pynwb__:
     import pynwb
 
-# Global consistent error message if NWB is missing
-nwbErrMsg = "\nSyncopy <core> WARNING: Could not import 'pynwb'. \n" +\
-          "{} requires a working pyNWB installation. \n" +\
-          "Please consider installing 'pynwb', e.g., via conda: \n" +\
-          "\tconda install -c conda-forge pynwb\n" +\
-          "or using pip:\n" +\
-          "\tpip install pynwb"
 
-__all__ = ["load_nwb"]
+def _is_valid_nwb_file(filename):
+    try:
+        this_python = os.path.join(os.path.dirname(sys.executable), "python")
+        subprocess.run([this_python, "-m", "pynwb.validate", filename], check=True)
+        return True, None
+    except subprocess.CalledProcessError as exc:
+        err = f"NWB file validation failed. Original error message: {str(exc)}"
+        return False, err
 
 
-def load_nwb(filename, memuse=3000, container=None):
+def load_nwb(
+    filename,
+    memuse=3000,
+    container=None,
+    validate=False,
+    default_spike_data_samplerate=None,
+):
     """
     Read contents of NWB files
 
     Parameters
     ----------
     filename : str
         Name of (may include full path to) NWB file (e.g., `"/path/to/mydata.nwb"`).
     memuse : scalar
         Approximate in-memory cache size (in MB) for reading data from disk
     container : str
         Name of syncopy container folder to create the syncopy data in
+    default_spike_data_samplerate : float, optional
+        The samplerate for spike data, in Hz. If not provided, the samplerate is read from the NWB file, but
+        this is not guaranteed to work as some NWB files which contain only spike data do not store a
+        samplerate. If this is `None` and no samplerate is found in the file, this function will raise an
+        error, and you will have to provide the samplerate manually.
 
     Returns
     -------
     objdict : dict
         Any NWB `TimeSeries`-like data is imported into an :class:`~syncopy.AnalogData`
         object. If the NWB file contains TTL pulse data, an additional
         :class:`~syncopy.EventData` object is instantiated. The syncopy
         objects are returned as a dictionary whose keys are the base-names
         (sans path) of the corresponding files.
     """
-
-    # Abort if NWB is not installed
-    if not __nwb__:
-        raise SPYError(nwbErrMsg.format("read_nwb"))
+    if not __pynwb__:
+        raise SPYError("NWB support is not available. Please install the 'pynwb' package.")
 
     # Check if file exists
     nwbPath, nwbBaseName = io_parser(filename, varname="filename", isfile=True, exists=True)
     nwbFullName = os.path.join(nwbPath, nwbBaseName)
 
     # Ensure `memuse` makes sense`
-    try:
-        scalar_parser(memuse, varname="memuse", lims=[0, np.inf])
-    except Exception as exc:
-        raise exc
+    scalar_parser(memuse, varname="memuse", lims=[0, np.inf])
 
-    # First, perform some basal validation w/NWB
-    try:
-        this_python = os.path.join(os.path.dirname(sys.executable),'python')
-        subprocess.run([this_python, "-m", "pynwb.validate", nwbFullName], check=True)
-    except subprocess.CalledProcessError as exc:
-        err = "NWB file validation failed. Original error message: {}"
-        raise SPYError(err.format(str(exc)))
+    # First, perform some basal validation w/NWB if requested.
+    if validate:
+        is_valid, err = _is_valid_nwb_file(nwbFullName)
+        if not is_valid:
+            raise SPYError(err)
 
     # Load NWB meta data from disk
     nwbio = pynwb.NWBHDF5IO(nwbFullName, "r", load_namespaces=True)
     nwbfile = nwbio.read()
 
     # Allocate lists for storing temporary NWB info: IMPORTANT use lists to preserve
     # order of data chunks/channels
@@ -91,25 +103,49 @@
     angSeries = []
     ttlVals = []
     ttlChanStates = []
     ttlChans = []
     ttlDtypes = []
 
     # If the file contains `epochs`, use it to infer trial information
-    hasTrials = "epochs" in nwbfile.fields.keys()
+    hasEpochs = "epochs" in nwbfile.fields.keys()
+    hasTrials = "trials" in nwbfile.fields.keys()
+    hasSpikedata = "units" in nwbfile.fields.keys()
+    hasAcquisitions = "acquisition" in nwbfile.fields.keys()
+
+    # Access LFPs in ecephys processing module, if any.
+    try:
+        lfp = nwbfile.processing["ecephys"]["LFP"]["ElectricalSeries"]
+
+        if isinstance(lfp, pynwb.ecephys.ElectricalSeries):
+
+            channel_names = lfp.electrodes[:].location
+            if channel_names.unique().size == 1:
+                SPYWarning("No unique channel names found for LFP.")
+
+            dTypes.append(lfp.data.dtype)
+            if lfp.channel_conversion is not None:
+                dTypes.append(lfp.channel_conversion.dtype)
+
+            tStarts.append(lfp.starting_time)
+            sRates.append(lfp.rate)
+            nSamples = max(nSamples, lfp.data.shape[0])
+            angSeries.append(lfp)
+    except KeyError:
+        pass
 
     # Access all (supported) `acquisition` fields in the file
     for acqName, acqValue in nwbfile.acquisition.items():
 
         # Actual extracellular analog time-series data
         if isinstance(acqValue, pynwb.ecephys.ElectricalSeries):
 
-            channels = acqValue.electrodes[:].location
-            if channels.unique().size == 1:
-                SPYWarning("No channel names found for {}".format(acqName))
+            channel_names = acqValue.electrodes[:].location
+            if channel_names.unique().size == 1:
+                SPYWarning("No unique channel names found for {}".format(acqName))
 
             dTypes.append(acqValue.data.dtype)
             if acqValue.channel_conversion is not None:
                 dTypes.append(acqValue.channel_conversion.dtype)
 
             tStarts.append(acqValue.starting_time)
             sRates.append(acqValue.rate)
@@ -131,51 +167,92 @@
                 raise SPYValueError(lgl, varname=acqName, actual=act.format(acqValue.description))
 
             ttlDtypes.append(acqValue.data.dtype)
             ttlDtypes.append(acqValue.timestamps.dtype)
 
         # Unsupported
         else:
-            lgl = "supported NWB data class"
+            lgl = "supported NWB Acquisition data class"
             raise SPYValueError(lgl, varname=acqName, actual=str(acqValue.__class__))
 
-    # If the NWB data is split up in "trials" (i.e., epochs), ensure things don't
-    # get too wild (uniform sampling rates and timing offsets)
-    if hasTrials:
+    # Load Spike Data from units. The data gets turned into a SpikeData object later.
+    spikes_by_unit = None
+    units = None
+    if hasSpikedata:
+        units = nwbfile.units.to_dataframe()
+        spikes_by_unit = {n: units.loc[n, "spike_times"] for n in units.index}
+
+    # If the NWB data is split up in "trials" (or epochs), ensure things don't
+    # get too wild (uniform sampling rates and timing offsets).
+    if hasTrials or hasEpochs:
+        if len(tStarts) < 1 or len(sRates) < 1:
+            if (
+                hasSpikedata and not hasAcquisitions
+            ):  # There may be no samplerate read from acquisitions because there are no acquisitions, but only spike data.
+                samplerate = default_spike_data_samplerate
+                if samplerate is None:
+                    if "samplerate" in units.columns:
+                        samplerate = units.loc[:, "samplerate"].unique()[0]
+                        sRates.append(samplerate)
+                        tStarts.append(0.0)
+                    else:
+                        raise SPYError(
+                            "Could not read samplerate for spike data from NWB file. Please provide a samplerate manually via parameter 'default_spike_data_samplerate'."
+                        )
+            else:
+                raise SPYError(
+                    "Found acquisitions and trials but no valid timing/samplerate data in NWB file. Data in file not supported."
+                )
         if all(tStarts) is None or all(sRates) is None:
             lgl = "acquisition timings defined by `starting_time` and `rate`"
             act = "`starting_time` or `rate` not set"
             raise SPYValueError(lgl, varname="starting_time/rate", actual=act)
         if np.unique(tStarts).size > 1 or np.unique(sRates).size > 1:
             lgl = "acquisitions with unique `starting_time` and `rate`"
             act = "`starting_time` or `rate` different across acquisitions"
             raise SPYValueError(lgl, varname="starting_time/rate", actual=act)
-        epochs = nwbfile.epochs[:]
-        trl = np.zeros((epochs.shape[0], 3), dtype=np.intp)
-        trl[:, :2] = (epochs - tStarts[0]) * sRates[0]
+
+        if hasTrials:
+            time_intervals = nwbfile.trials[:]
+        else:
+            time_intervals = nwbfile.epochs[:]
+        if not type(time_intervals) is np.ndarray:
+            time_intervals = time_intervals.to_numpy()
+        trl = np.zeros((time_intervals.shape[0], 3), dtype=np.intp)
+        trial_start_stop = (time_intervals - tStarts[0]) * sRates[
+            0
+        ]  # use offset relative to first acquisition
+        trl[:, 0:2] = trial_start_stop[:, 0:2]
+
+        # If we found trials, we may be able to load the offset field from the trials
+        # table. This is not guaranteed to work, though, as the offset field is only present if the
+        # file was exported by Syncopy. If the field is not present, we do not do anything here, we just
+        # proceed with the default zero offset.
+        if hasTrials and "offset" in nwbfile.trials.colnames:
+            df = nwbfile.trials.to_dataframe()
+            trl[:, 2] = df["offset"] * sRates[0]
+
         msg = "Found {} trials".format(trl.shape[0])
     else:
         trl = np.array([[0, nSamples, 0]])
         msg = "No trial information found. Proceeding with single all-encompassing trial"
 
     # Print status update to inform user
-    SPYInfo(msg)
+    log_msg = "Read data from NWB file {}".format(nwbFullName)
 
     # Check for filename
     if container is not None:
         if not isinstance(container, str):
             raise SPYTypeError(container, varname="container", expected="str")
         if not os.path.splitext(container)[1] == ".spy":
             container += ".spy"
         if not os.path.isdir(container):
             os.makedirs(container)
         fileInfo = filename_parser(container)
-        filebase = os.path.join(fileInfo["folder"],
-                                fileInfo["container"],
-                                fileInfo["basename"])
+        filebase = os.path.join(fileInfo["folder"], fileInfo["container"], fileInfo["basename"])
 
     # If TTL data was found, ensure we have exactly one set of values and associated
     # channel markers
     if max(len(ttlVals), len(ttlChans)) > min(len(ttlVals), len(ttlChans)):
         lgl = "TTL pulse values and channel markers"
         act = "pulses: {}, channels: {}".format(str(ttlVals), str(ttlChans))
         raise SPYValueError(lgl, varname=ttlVals[0].name, actual=act)
@@ -191,39 +268,37 @@
         msg = "Creating separate EventData object for embedded TTL pulse data..."
         SPYInfo(msg)
         if container is not None:
             filename = filebase + ".event"
         else:
             filename = None
 
-        evtData = EventData(dimord=["sample","eventid","chans"], filename=filename)
+        evtData = EventData(dimord=["sample", "eventid", "chans"], filename=filename)
         h5evt = h5py.File(evtData.filename, mode="w")
-        evtDset = h5evt.create_dataset("data", dtype=int,
-                                       shape=(ttlVals[0].data.size, 3))
+        evtDset = h5evt.create_dataset("data", dtype=int, shape=(ttlVals[0].data.size, 3))
         # Column 1: sample indices
         # Column 2: TTL pulse values
         # Column 3: TTL channel markers
-        if 'resolution' in ttlChans[0].__nwbfields__:
+        if "resolution" in ttlChans[0].__nwbfields__:
             ts_resolution = ttlChans[0].resolution
         else:
             ts_resolution = ttlChans[0].timestamps__resolution
 
         evtDset[:, 0] = ((ttlChans[0].timestamps[()] - tStarts[0]) / ts_resolution).astype(np.intp)
         evtDset[:, 1] = ttlVals[0].data[()].astype(int)
         evtDset[:, 2] = ttlChans[0].data[()].astype(int)
         evtData.data = evtDset
         evtData.samplerate = float(1 / ts_resolution)
         if hasTrials:
             evtData.trialdefinition = trl
         else:
-            evtData.trialdefinition = np.array([[np.nanmin(evtDset[:,0]), np.nanmax(evtDset[:,0]), 0]])
+            evtData.trialdefinition = np.array([[np.nanmin(evtDset[:, 0]), np.nanmax(evtDset[:, 0]), 0]])
             msg = "No trial information found. Proceeding with single all-encompassing trial"
 
         # Write logs
-        log_msg = "Read data from NWB file {}".format(nwbFullName)
         evtData.log = log_msg
         objectDict[os.path.basename(evtData.filename)] = evtData
 
     # Compute actually available memory
     pbarDesc = "Reading data in blocks of {} GB".format(round(memuse / 1000, 2))
     memuse *= 1024**2
 
@@ -239,46 +314,97 @@
             filename = filebase + "_" + acqValue.name + ".analog"
         else:
             filename = None
 
         angData = AnalogData(dimord=AnalogData._defaultDimord, filename=filename)
         angShape = [None, None]
         angShape[angData.dimord.index("time")] = acqValue.data.shape[0]
-        angShape[angData.dimord.index("channel")] = acqValue.data.shape[1]
+        numDataChannels = acqValue.data.shape[1] if acqValue.data.ndim > 1 else 1
+        angShape[angData.dimord.index("channel")] = numDataChannels
         h5ang = h5py.File(angData.filename, mode="w")
         angDset = h5ang.create_dataset("data", dtype=np.result_type(*dTypes), shape=angShape)
 
         # If channel-specific gains are set, load them now
         if acqValue.channel_conversion is not None:
             gains = acqValue.channel_conversion[()]
-            if np.all(gains ==  gains[0]):
+            if np.all(gains == gains[0]):
                 gains = gains[0]
 
         # Given memory cap, compute how many data blocks can be grabbed per swipe:
         # `nSamp` is the no. of samples that can be loaded into memory without exceeding `memuse`
         # `rem` is the no. of remaining samples, s. t. ``nSamp + rem = angDset.shape[0]`
         # `blockList` is a list of samples to load per swipe, i.e., `[nSamp, nSamp, ..., rem]`
-        nSamp = int(memuse / (acqValue.data.shape[1] * angDset.dtype.itemsize))
+        nSamp = int(memuse / (numDataChannels * angDset.dtype.itemsize))
         rem = int(angDset.shape[0] % nSamp)
         blockList = [nSamp] * int(angDset.shape[0] // nSamp) + [rem] * int(rem > 0)
 
         for m, M in enumerate(tqdm(blockList, desc=pbarDesc, position=1, leave=False, disable=None)):
             st_samp, end_samp = m * nSamp, m * nSamp + M
-            angDset[st_samp : end_samp, :] = acqValue.data[st_samp : end_samp, :]
+            angDset[st_samp:end_samp, :] = acqValue.data[st_samp:end_samp, :]
             if acqValue.channel_conversion is not None:
-                angDset[st_samp : end_samp, :] *= gains
+                angDset[st_samp:end_samp, :] *= gains
 
         # Finalize angData
         angData.data = angDset
-        channels = acqValue.electrodes[:].location
-        if channels.unique().size == 1:
-            SPYWarning("No channel names found for {}".format(acqName))
+        channel_names = acqValue.electrodes[:].location
+
+        if channel_names.size != numDataChannels:
+            SPYWarning(
+                f"Found {channel_names.size} channel names for data with {numDataChannels} channels in NWB file. Discarding channel names."
+            )
+            angData.channel = None
+
+        if channel_names.unique().size == 1 and channel_names.size > 1:
+            SPYWarning(
+                "No unique channel names found for acquisition {}. Discarding channel names.".format(acqName)
+            )
             angData.channel = None
         else:
-            angData.channel = channels.to_list()
+            angData.channel = channel_names.to_list()
         angData.samplerate = sRates[0]
         angData.trialdefinition = trl
-        angData.info = {'starting_time' : tStarts[0]}
+        angData.info = {"starting_time": tStarts[0]}
         angData.log = log_msg
         objectDict[os.path.basename(angData.filename)] = angData
 
+    if hasSpikedata and spikes_by_unit is not None:
+        dsetname = "nwbspike"  # TODO: Can we get a name for this somwhere in the NWB file?
+        if container is not None:
+            filename = filebase + "_" + dsetname + ".spike"
+        else:
+            filename = None
+
+        spData = SpikeData(dimord=SpikeData._defaultDimord, filename=filename)
+
+        # Convert spike times to Syncopy format: load one vector for time, unit, and channel, repectively.
+        spike_times = np.sort(np.concatenate([np.array(i) for i in spikes_by_unit.values()]))
+        spike_units = np.concatenate([np.array([i] * len(spikes_by_unit[i])) for i in spikes_by_unit.keys()])
+        spike_channels = np.array([0] * len(spike_times))  # single channel, map all to channel 0.
+
+        # Try to get the samplerate from the NWB file
+        samplerate = sRates[0]
+        spike_data_sampleidx = np.column_stack(
+            (np.rint(spike_times * samplerate), spike_channels, spike_units)
+        )
+        hdf5_file = h5py.File(spData.filename, mode="w")
+
+        spDset = hdf5_file.create_dataset("data", data=spike_data_sampleidx, dtype=np.int64)
+
+        # Finally, assign the dataset to the SpikeData object.
+        spData.data = spDset
+
+        # Fill other fields
+        spData.channel = [
+            "channel0"
+        ]  # No channel information is saved in NWB files for spike data, only unit information.
+        spData.samplerate = samplerate
+        spData.trialdefinition = trl
+        spData.info = {"starting_time": tStarts[0]}
+        spData.log = log_msg
+
+        # Add loaded Syncopy data object to list of objects to return
+        objectDict[os.path.basename(spData.filename)] = spData
+
+    # Close NWB file
+    nwbio.close()
+
     return objectDict
```

### Comparing `esi_syncopy-2023.5/syncopy/io/load_spy_container.py` & `esi_syncopy-2023.7/syncopy/io/load_spy_container.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,22 +10,27 @@
 import sys
 import numpy as np
 from glob import glob
 
 # Local imports
 from syncopy.shared.filetypes import FILE_EXT
 from syncopy.shared.parsers import io_parser, data_parser, filename_parser, array_parser
-from syncopy.shared.errors import (SPYTypeError, SPYValueError, SPYIOError,
-                                   SPYError, SPYWarning)
+from syncopy.shared.errors import (
+    SPYTypeError,
+    SPYValueError,
+    SPYIOError,
+    SPYError,
+    SPYWarning,
+)
 from syncopy.io.utils import hash_file, startInfoDict
 import syncopy.datatype as spd
 import syncopy as spy
 
 # to allow loading older spy containers
-legacy_not_required = ['info']
+legacy_not_required = ["info"]
 
 __all__ = ["load"]
 
 
 def load(filename, tag=None, dataclass=None, checksum=False, mode="r+", out=None):
     """
     Load Syncopy data object(s) from disk
@@ -168,20 +173,19 @@
     if dataclass is not None:
         if isinstance(dataclass, str):
             dataclass = [dataclass]
         try:
             array_parser(dataclass, varname="dataclass", ntype=str)
         except Exception as exc:
             raise exc
-        dataclass = ["." + dclass if not dclass.startswith(".") else dclass
-                     for dclass in dataclass]
+        dataclass = ["." + dclass if not dclass.startswith(".") else dclass for dclass in dataclass]
         extensions = set(dataclass).intersection(FILE_EXT["data"])
         if len(extensions) == 0:
-                lgl = "extension(s) '" + "or '".join(ext + "' " for ext in FILE_EXT["data"])
-                raise SPYValueError(legal=lgl, varname="dataclass", actual=str(dataclass))
+            lgl = "extension(s) '" + "or '".join(ext + "' " for ext in FILE_EXT["data"])
+            raise SPYValueError(legal=lgl, varname="dataclass", actual=str(dataclass))
 
     # Avoid any misunderstandings here...
     if not isinstance(checksum, bool):
         raise SPYTypeError(checksum, varname="checksum", expected="bool")
 
     # Abuse `AnalogData.mode`-setter to check `mode`
     try:
@@ -196,18 +200,21 @@
             extensions = FILE_EXT["data"]
         container = os.path.join(fileInfo["folder"], fileInfo["container"])
         fileList = []
         for ext in extensions:
             for tag in tags:
                 fileList.extend(glob(os.path.join(container, tag + ext)))
         if len(fileList) == 0:
-            fsloc = os.path.join(container, "" + \
-                                 "or ".join(tag + " " for tag in tags) + \
-                                 "with extensions " + \
-                                 "or ".join(ext + " " for ext in extensions))
+            fsloc = os.path.join(
+                container,
+                ""
+                + "or ".join(tag + " " for tag in tags)
+                + "with extensions "
+                + "or ".join(ext + " " for ext in extensions),
+            )
             raise SPYIOError(fsloc, exists=False)
         if len(fileList) == 1:
             return _load(fileList[0], checksum, mode, out)
         if out is not None:
             msg = "When loading multiple objects, the `out` keyword is ignored"
             SPYWarning(msg)
         objectDict = {}
@@ -216,18 +223,16 @@
             objectDict[os.path.basename(obj.filename)] = obj
         return objectDict
 
     else:
 
         if dataclass is not None:
             if os.path.splitext(fileInfo["filename"])[1] not in dataclass:
-                lgl = "extension '" + \
-                    "or '".join(dclass + "' " for dclass in dataclass)
-                raise SPYValueError(legal=lgl, varname="filename",
-                                    actual=fileInfo["filename"])
+                lgl = "extension '" + "or '".join(dclass + "' " for dclass in dataclass)
+                raise SPYValueError(legal=lgl, varname="filename", actual=fileInfo["filename"])
         return _load(filename, checksum, mode, out)
 
 
 def _load(filename, checksum, mode, out):
     """
     Local helper
     """
@@ -253,29 +258,32 @@
     else:
         raise SPYError("Unknown data class {class}".format(jsonDict["dataclass"]))
 
     requiredFields = tuple(startInfoDict.keys()) + dataclass._infoFileProperties
 
     for key in requiredFields:
         if key not in jsonDict.keys() and key not in legacy_not_required:
-            raise SPYError("Required field {field} for {cls} not in {file}"
-                           .format(field=key,
-                                   cls=dataclass.__name__,
-                                   file=jsonFile))
+            raise SPYError(
+                "Required field {field} for {cls} not in {file}".format(
+                    field=key, cls=dataclass.__name__, file=jsonFile
+                )
+            )
 
     # FIXME: add version comparison (syncopy.__version__ vs jsonDict["_version"])
 
     # If wanted, perform checksum matching
     if checksum:
         hsh_msg = "hash = {hsh:s}"
         hsh = hash_file(hdfFile)
         if hsh != jsonDict["file_checksum"]:
-            raise SPYValueError(legal=hsh_msg.format(hsh=jsonDict["file_checksum"]),
-                                varname=os.path.basename(hdfFile),
-                                actual=hsh_msg.format(hsh=hsh))
+            raise SPYValueError(
+                legal=hsh_msg.format(hsh=jsonDict["file_checksum"]),
+                varname=os.path.basename(hdfFile),
+                actual=hsh_msg.format(hsh=hsh),
+            )
 
     # Parsing is done, create new or check provided object
     dimord = jsonDict.pop("dimord")
     if out is not None:
         try:
             data_parser(out, varname="out", writable=True, dataclass=jsonDict["dataclass"])
         except Exception as exc:
@@ -288,37 +296,46 @@
 
     # Access data on disk (error checking is done by setters)
     out.mode = mode
 
     # If the JSON contains `_hdfFileDatasetProperties`, load all datasets listed in there. Otherwise, load the ones
     # already defined by `out._hdfFileDatasetProperties` and defined in the respective data class.
     # This is needed to load both new files with, and legacy files without the `_hdfFileDatasetProperties` in the JSON.
-    json_hdfFileDatasetProperties = jsonDict.pop("_hdfFileDatasetProperties", None) # They may not be in there for legacy files, so allow None.
+    json_hdfFileDatasetProperties = jsonDict.pop(
+        "_hdfFileDatasetProperties", None
+    )  # They may not be in there for legacy files, so allow None.
     if json_hdfFileDatasetProperties is not None:
-        out._hdfFileDatasetProperties = tuple(json_hdfFileDatasetProperties) # It's a list in the JSON, so convert to tuple.
+        out._hdfFileDatasetProperties = tuple(
+            json_hdfFileDatasetProperties
+        )  # It's a list in the JSON, so convert to tuple.
     for datasetProperty in out._hdfFileDatasetProperties:
         targetProperty = datasetProperty if datasetProperty == "data" else "_" + datasetProperty
         try:
             setattr(out, targetProperty, h5py.File(hdfFile, mode="r")[datasetProperty])
         except KeyError:
             if datasetProperty == "data":
-                raise SPYError("Data file {file} does not contain a dataset named 'data'.".format(file=hdfFile))
+                raise SPYError(
+                    "Data file {file} does not contain a dataset named 'data'.".format(file=hdfFile)
+                )
             else:
-                spy.log(f"Dataset '{datasetProperty}' not present in HDF5 file, cannot load it. Setting to None.", level="DEBUG")
+                spy.log(
+                    f"Dataset '{datasetProperty}' not present in HDF5 file, cannot load it. Setting to None.",
+                    level="DEBUG",
+                )
                 # It is fine if an extra dataset is not present in the file, e.g., the SpikeData waveform dataset is not present when set to None.
                 setattr(out, targetProperty, None)
 
-
     # Abuse ``definetrial`` to set trial-related props
     trialdef = h5py.File(hdfFile, mode="r")["trialdefinition"][()]
     out.definetrial(trialdef)
 
     # Assign metadata
-    for key in [prop for prop in dataclass._infoFileProperties if
-                prop != "dimord" and prop in jsonDict.keys()]:
+    for key in [
+        prop for prop in dataclass._infoFileProperties if prop != "dimord" and prop in jsonDict.keys()
+    ]:
         setattr(out, key, jsonDict[key])
 
     thisMethod = sys._getframe().f_code.co_name.replace("_", "")
 
     # Write log-entry
     msg = "Read files v. {ver:s} ".format(ver=jsonDict["_version"])
     msg += "{hdf:s}\n\t" + (len(msg) + len(thisMethod) + 2) * " " + "{json:s}"
```

### Comparing `esi_syncopy-2023.5/syncopy/io/load_tdt.py` & `esi_syncopy-2023.7/syncopy/io/load_tdt.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,30 +4,28 @@
 #
 # load_tdt.py Merge separate TDT SEV files into one HDF5 file
 
 import os
 from datetime import datetime
 import re
 import numpy as np
-from getpass import getuser
 from tqdm.auto import tqdm
 import h5py
 
 # Local imports
 from syncopy.shared.parsers import io_parser, scalar_parser
 from syncopy.shared.errors import SPYWarning, SPYValueError
 from syncopy.shared.tools import StructDict
 import syncopy as spy
 
 
 # --- The user exposed function ---
 
 
-def load_tdt(data_path, start_code=None, end_code=None,
-             subtract_median=False):
+def load_tdt(data_path, start_code=None, end_code=None, subtract_median=False):
     """
     Imports TDT time series data and meta-information
     into a single :class:`~syncopy.AnalogData` object.
 
     An ad-hoc trialdefinition will be attached if
     both `start_code` and `end_code` are given.
     Otherwise a single all-to-all trialdefinition
@@ -88,32 +86,30 @@
     if end_code is not None and start_code is None:
         lgl = "trigger codes for both trial start and end"
         raise SPYValueError(lgl, "start_code", start_code)
     elif end_code is None and start_code is None:
         pass
     # both are given
     else:
-        scalar_parser(start_code, "start_code", ntype='int_like')
-        scalar_parser(end_code, "end_code", ntype='int_like')
+        scalar_parser(start_code, "start_code", ntype="int_like")
+        scalar_parser(end_code, "end_code", ntype="int_like")
 
     # initialize tdt info loader class
     TDT_Load_Info = ESI_TDTinfo(data_path)
     # this is a StructDict
     tdt_info = TDT_Load_Info.load_tdt_info()
 
     # nicely sorted by channel names
     file_paths = _get_source_paths(data_path, ".sev")
 
-    tdt_data_handler = ESI_TDTdata(
-        data_path, subtract_median=subtract_median, channels=None
-    )
+    tdt_data_handler = ESI_TDTdata(data_path, subtract_median=subtract_median, channels=None)
 
     adata = tdt_data_handler.data_aranging(file_paths, tdt_info)
     # we have to open for reading again
-    adata.data = h5py.File(adata.filename, "r")['data']
+    adata.data = h5py.File(adata.filename, "r")["data"]
 
     # Write log-entry
     msg = f"loaded TDT data from {len(file_paths)} files\n"
     msg += f"\tsource folder: {data_path}\n"
     msg += f"\tsubtract median: {subtract_median}"
     adata.log = msg
 
@@ -141,15 +137,22 @@
         self.HASDATA = int("00008000", 16)
         self.UCF = int("00000010", 16)
         self.PHANTOM = int("00000020", 16)
         self.MASK = int("0000FF0F", 16)
         self.INVALID_MASK = int("FFFF0000", 16)
         self.STARTBLOCK = int("0001", 16)
         self.STOPBLOCK = int("0002", 16)
-        self.ALLOWED_FORMATS = [np.float32, np.int32, np.int16, np.int8, np.float64, np.int64]
+        self.ALLOWED_FORMATS = [
+            np.float32,
+            np.int32,
+            np.int16,
+            np.int8,
+            np.float64,
+            np.int64,
+        ]
         self.ALLOWED_EVTYPES = ["all", "epocs", "snips", "streams", "scalars"]
 
     def code_to_type(self, code):
         # given event code, return string 'epocs', 'snips', 'streams', or 'scalars'
         strobe_types = [self.STRON, self.STROFF, self.MARK]
         scalar_types = [self.SCALAR]
         snip_types = [self.SNIP]
@@ -361,17 +364,16 @@
                 valid_ind = np.where(codes == store_code["code"])[0]
                 temp = heads[3, valid_ind].view(np.uint16)
 
                 if store_code["type_str"] != "epocs":
                     if not hasattr(header.stores[var_name], "ts"):
                         header.stores[var_name].ts = []
                     vvv = (
-                        np.reshape(
-                            heads[[[4], [5]], valid_ind].T,
-                            (-1, 1)).T.view(np.float64) - header.start_time
+                        np.reshape(heads[[[4], [5]], valid_ind].T, (-1, 1)).T.view(np.float64)
+                        - header.start_time
                     )
                     # round timestamps to the nearest sample
                     vvv = self.time2sample(vvv, to_time=True)
                     header.stores[var_name].ts.append(vvv)
                     if (not self.nodata) or (store_code["type_str"] == "streams"):
                         if not hasattr(header.stores[var_name], "data"):
                             header.stores[var_name].data = []
@@ -381,23 +383,23 @@
                     if not hasattr(header.stores[var_name], "chan"):
                         header.stores[var_name].chan = []
                     header.stores[var_name].chan.append(temp[::2])
                 else:
                     loc = epocs.name.index(store_code["name"])
                     # round timestamps to the nearest sample
                     vvv = (
-                        np.reshape(
-                            heads[[[4], [5]], valid_ind].T,
-                            (-1, 1)).T.view(np.float64) - header.start_time
+                        np.reshape(heads[[[4], [5]], valid_ind].T, (-1, 1)).T.view(np.float64)
+                        - header.start_time
                     )
                     # round timestamps to the nearest sample
                     vvv = self.time2sample(vvv, to_time=True)
                     epocs.ts[loc] = np.append(epocs.ts[loc], vvv)
                     epocs.data[loc] = np.append(
-                        epocs.data[loc], np.reshape(heads[[[6], [7]], valid_ind].T, (-1, 1)).T.view(np.float64)
+                        epocs.data[loc],
+                        np.reshape(heads[[[6], [7]], valid_ind].T, (-1, 1)).T.view(np.float64),
                     )
             last_ts = heads[[4, 5], -1].view(np.float64) - header.start_time
             last_ts = last_ts[0]
             if self.t2 > 0 and last_ts > self.t2:
                 break
             # eof reached
             if heads.size < read_size:
@@ -464,15 +466,17 @@
             if header.stores[var_name].type_str in ["streams", "snips"]:
                 if "data" in header.stores[var_name].keys():
                     header.stores[var_name].data = header.stores[var_name].data.view(np.uint64)
             if "chan" in header.stores[var_name].keys():
                 if np.max(header.stores[var_name].chan) == 1:
                     header.stores[var_name].chan = [1]
 
-        valid_time_range = np.array([[self.t1], [self.t2]]) if self.t2 > 0 else np.array([[self.t1], [np.inf]])
+        valid_time_range = (
+            np.array([[self.t1], [self.t2]]) if self.t2 > 0 else np.array([[self.t1], [np.inf]])
+        )
         ranges = None
         if hasattr(ranges, "__len__"):
             valid_time_range = ranges
 
         num_ranges = valid_time_range.shape[1]
         if num_ranges > 0:
             data.time_ranges = valid_time_range
@@ -530,15 +534,18 @@
                         if current_type_str == "streams":
                             # keep one prior for streams (for all channels)
                             if not bSkip:
                                 nchan = max(data[current_type_str][var_name].chan)
                                 temp = filter_ind[jj]
                                 if temp[0] - nchan > -1:
                                     filter_ind[jj] = np.concatenate(
-                                        [-np.arange(nchan, 0, -1) + temp[0], filter_ind[jj]]
+                                        [
+                                            -np.arange(nchan, 0, -1) + temp[0],
+                                            filter_ind[jj],
+                                        ]
                                     )
                                 temp = data[current_type_str][var_name].ts[filter_ind[jj]]
                                 data[current_type_str][var_name].start_time[jj] = temp[0]
                         else:
                             data[current_type_str][var_name].filtered_ts[jj] = data[current_type_str][
                                 var_name
                             ].ts[filter_ind[jj]]
@@ -615,15 +622,17 @@
                     start = valid_time_range[0, jj]
                     stop = valid_time_range[1, jj]
                     ind1 = data[current_type_str][var_name].onset >= start
                     ind2 = data[current_type_str][var_name].onset < stop
                     filter_ind.append(np.where(ind1 & ind2)[0])
                 filter_ind = np.concatenate(filter_ind)
                 if len(filter_ind) > 0:
-                    data[current_type_str][var_name].onset = data[current_type_str][var_name].onset[filter_ind]
+                    data[current_type_str][var_name].onset = data[current_type_str][var_name].onset[
+                        filter_ind
+                    ]
                     data[current_type_str][var_name].data = data[current_type_str][var_name].data[filter_ind]
                     data[current_type_str][var_name].offset = data[current_type_str][var_name].offset[
                         filter_ind
                     ]
 
                     if data[current_type_str][var_name].offset[0] < data[current_type_str][var_name].onset[0]:
                         if data[current_type_str][var_name].onset[0] > firstStart:
@@ -662,15 +671,16 @@
                         ind.append(np.where(data[current_type_str][current_name].chan == xx + 1)[0])
                         min_length = min(len(ind[-1]), min_length)
                         max_length = max(len(ind[-1]), max_length)
                     if min_length != max_length:
                         SPYWarning(
                             "Truncating store {0} to {1} values (from {2})".format(
                                 current_name, min_length, max_length
-                            ))
+                            )
+                        )
                         ind = [ind[xx][:min_length] for xx in range(nchan)]
                     if not self.nodata:
                         data[current_type_str][current_name].data = (
                             data[current_type_str][current_name].data[np.concatenate(ind)].reshape(nchan, -1)
                         )
                     # only use timestamps from first channel
                     data[current_type_str][current_name].ts = data[current_type_str][current_name].ts[ind[0]]
@@ -723,20 +733,26 @@
         hash = md5()
         with open(filename, "rb") as f:
             for chunk in iter(lambda: f.read(128 * hash.block_size), b""):
                 hash.update(chunk)
         return hash.hexdigest()
 
     def data_aranging(self, Files, DataInfo_loaded):
-        AData = spy.AnalogData(dimord=['time', 'channel'])
+        AData = spy.AnalogData(dimord=["time", "channel"])
         hdf_out_path = AData.filename
-        LenOfData = self.read_data(Files[0]).shape[0]  # Lenght of the data is always set to the length of the first channel
+        LenOfData = self.read_data(Files[0]).shape[
+            0
+        ]  # Lenght of the data is always set to the length of the first channel
         with h5py.File(hdf_out_path, "w") as combined_data_file:
             idxStartStop = [
-                np.clip(np.array((jj, jj + self.chan_in_chunks)), a_min=None, a_max=len(Files))
+                np.clip(
+                    np.array((jj, jj + self.chan_in_chunks)),
+                    a_min=None,
+                    a_max=len(Files),
+                )
                 for jj in range(0, len(Files), self.chan_in_chunks)
             ]
             print(
                 "Merging {0} files in {1} chunks each with {2} channels into \n   {3}".format(
                     len(Files), len(idxStartStop), self.chan_in_chunks, hdf_out_path
                 )
             )
@@ -784,23 +800,24 @@
         AData.info["Trigger_code"] = serial(DataInfo_loaded.Mark.data[0])
 
         return AData
 
 
 # --- Helpers ---
 
+
 def _mk_trialdef(adata, start_code, end_code):
     """
     Create a basic trialdefinition from the trial
     start and end trigger codes
     """
 
     # trigger codes and samples
-    trg_codes = np.array(adata.info['Trigger_code'], dtype=int)
-    trg_sample = np.array(adata.info['Trigger_sample'], dtype=int)
+    trg_codes = np.array(adata.info["Trigger_code"], dtype=int)
+    trg_sample = np.array(adata.info["Trigger_sample"], dtype=int)
 
     # boolean indexing
     trl_starts = trg_sample[trg_codes == start_code]
     trl_ends = trg_sample[trg_codes == end_code]
 
     if trl_starts.size == 0:
         lgl = "at least one occurence of trial start code"
@@ -847,15 +864,15 @@
         tdtPaths.append(os.path.join(f_path, f_name))
     tdtPaths = _natural_sort(tdtPaths)
     return tdtPaths
 
 
 def _natural_sort(file_names):
     """Sort a list of strings using numbers
-        Ch1 will be followed by Ch2 and not Ch11.
+    Ch1 will be followed by Ch2 and not Ch11.
     """
 
     def convert(text):
         return int(text) if text.isdigit() else text.lower()
 
     def alphanum_key(key):
         return [convert(c) for c in re.split("([0-9]+)", key)]
```

### Comparing `esi_syncopy-2023.5/syncopy/io/save_spy_container.py` & `esi_syncopy-2023.7/syncopy/io/save_spy_container.py`

 * *Files 2% similar despite different names*

```diff
@@ -112,31 +112,29 @@
     syncopy.load : load data created with :func:`syncopy.save`
     """
 
     # Make sure `out` is a valid Syncopy data object
     data_parser(out, varname="out", writable=None, empty=False)
 
     if filename is None and container is None:
-        raise SPYError('filename and container cannot both be `None`')
+        raise SPYError("filename and container cannot both be `None`")
 
     if container is not None and filename is None:
         # construct filename from container name
         if not isinstance(container, str):
             raise SPYTypeError(container, varname="container", expected="str")
         if not os.path.splitext(container)[1] == ".spy":
             container += ".spy"
         fileInfo = filename_parser(container)
-        filename = os.path.join(fileInfo["folder"],
-                                fileInfo["container"],
-                                fileInfo["basename"])
+        filename = os.path.join(fileInfo["folder"], fileInfo["container"], fileInfo["basename"])
         # handle tag
         if tag is not None:
             if not isinstance(tag, str):
                 raise SPYTypeError(tag, varname="tag", expected="str")
-            filename += '_' + tag
+            filename += "_" + tag
 
     elif container is not None and filename is not None:
         raise SPYError("container and filename cannot be used at the same time")
 
     if not isinstance(filename, str):
         raise SPYTypeError(filename, varname="filename", expected="str")
 
@@ -146,17 +144,22 @@
 
     if not isinstance(overwrite, bool):
         raise SPYTypeError(overwrite, varname="overwrite", expected="bool")
 
     # Parse filename for validity and construct full path to HDF5 file
     fileInfo = filename_parser(filename)
     if fileInfo["extension"] != out._classname_to_extension():
-        raise SPYError("""Extension in filename ('{ext}') does not match data
-                    class ({dclass}), expected '{exp}'.""".format(ext=fileInfo["extension"],
-                                               dclass=out.__class__.__name__, exp=out._classname_to_extension()))
+        raise SPYError(
+            """Extension in filename ('{ext}') does not match data
+                    class ({dclass}), expected '{exp}'.""".format(
+                ext=fileInfo["extension"],
+                dclass=out.__class__.__name__,
+                exp=out._classname_to_extension(),
+            )
+        )
     dataFile = os.path.join(fileInfo["folder"], fileInfo["filename"])
 
     # If `out` is to replace its own on-disk representation, be more careful
     if overwrite and dataFile == out.filename:
         replace = True
     else:
         replace = False
@@ -191,27 +194,32 @@
                     raise SPYIOError(dataFile, exists=True)
         h5f = h5py.File(dataFile, mode="w")
 
         # Save each member of `_hdfFileDatasetProperties` in target HDF file
         for datasetName in out._hdfFileDatasetProperties:
             dataset = getattr(out, "_" + datasetName)
             if dataset is not None:
-                spy.log(f"Writing dataset '{datasetName}' ({len(out._hdfFileDatasetProperties)} datasets total) to HDF5 file '{dataFile}'.", level="DEBUG")
+                spy.log(
+                    f"Writing dataset '{datasetName}' ({len(out._hdfFileDatasetProperties)} datasets total) to HDF5 file '{dataFile}'.",
+                    level="DEBUG",
+                )
                 dat = h5f.create_dataset(datasetName, data=dataset)
             else:
-                spy.log(f"Not writing 'None 'dataset '{datasetName}' ({len(out._hdfFileDatasetProperties)} datasets total) to HDF5 file '{dataFile}'.", level="DEBUG")
+                spy.log(
+                    f"Not writing 'None 'dataset '{datasetName}' ({len(out._hdfFileDatasetProperties)} datasets total) to HDF5 file '{dataFile}'.",
+                    level="DEBUG",
+                )
 
     # Now write trial-related information
     trl_arr = np.array(out.trialdefinition)
     if replace:
         trl[()] = trl_arr
         trl.flush()
     else:
-        trl = h5f.create_dataset("trialdefinition", data=trl_arr,
-                                 maxshape=(None, trl_arr.shape[1]))
+        trl = h5f.create_dataset("trialdefinition", data=trl_arr, maxshape=(None, trl_arr.shape[1]))
 
     # Write to log already here so that the entry can be exported to json
     infoFile = dataFile + FILE_EXT["info"]
     out.log = "Wrote files " + dataFile + "\n\t\t\t" + 2 * " " + infoFile
 
     # Assemble dict for JSON output: order things by their "readability"
     outDict = OrderedDict(startInfoDict)
@@ -243,47 +251,52 @@
     for key in out._hdfFileAttributeProperties:
         if outDict[key] is None:
             h5f.attrs[key] = "None"
         else:
             try:
                 h5f.attrs[key] = outDict[key]
             except RuntimeError:
-                msg = "Too many entries in `{}` - truncating HDF5 attribute. " +\
-                    "Please refer to {} for complete listing."
+                msg = (
+                    "Too many entries in `{}` - truncating HDF5 attribute. "
+                    + "Please refer to {} for complete listing."
+                )
                 info_fle = os.path.split(os.path.split(filename.format(ext=FILE_EXT["info"]))[0])[1]
-                info_fle = os.path.join(info_fle, os.path.basename(
-                    filename.format(ext=FILE_EXT["info"])))
+                info_fle = os.path.join(info_fle, os.path.basename(filename.format(ext=FILE_EXT["info"])))
                 SPYWarning(msg.format(key, info_fle))
                 h5f.attrs[key] = [outDict[key][0], "...", outDict[key][-1]]
 
     # Save the dataset names that should be loaded later into the JSON.
-    outDict['_hdfFileDatasetProperties'] = list(out._hdfFileDatasetProperties)
+    outDict["_hdfFileDatasetProperties"] = list(out._hdfFileDatasetProperties)
 
     # Re-assign filename after saving (and remove source in case it came from `__storage__`)
     if not replace:
         h5f.close()
         # points to source file path
         if __storage__ in out.filename:
             is_virtual = out.data.is_virtual
             out.data.file.close()
             try:
                 os.unlink(out.filename)
                 if is_virtual:
                     virtual_dir_path = os.path.splitext(out.filename)[0]
                     shutil.rmtree(virtual_dir_path)
             except PermissionError as ex:
-                spy.log(f"Could not delete file '{out.filename}': {str(ex)}.", level='IMPORTANT')
+                spy.log(
+                    f"Could not delete file '{out.filename}': {str(ex)}.",
+                    level="IMPORTANT",
+                )
         out.data = dataFile
 
     # Compute checksum and finally write JSON (automatically overwrites existing)
     outDict["file_checksum"] = hash_file(dataFile)
 
-    with open(infoFile, 'w') as out_json:
+    with open(infoFile, "w") as out_json:
         json.dump(outDict, out_json, indent=4)
-    spy.log(f"Wrote container to {os.path.dirname(out.filename)}", level='INFO')
+    spy.log(f"Wrote container to {os.path.dirname(out.filename)}", level="INFO")
+
 
 def _dict_converter(dct, firstrun=True):
     """
     Convert all dict values having NumPy dtypes to corresponding builtin types
 
     Also works w/ nested dict of dicts and is cycle-save, i.e., it can
     handle self-referencing dictionaires. For instance, consider a nested dict
```

### Comparing `esi_syncopy-2023.5/syncopy/io/utils.py` & `esi_syncopy-2023.7/syncopy/io/utils.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,17 +9,19 @@
 import shutil
 import inspect
 import numpy as np
 from datetime import datetime
 from glob import glob
 from collections import OrderedDict
 from tqdm import tqdm
+
 if sys.platform == "win32":
     # tqdm breaks term colors on Windows - fix that (tqdm issue #446)
     import colorama
+
     colorama.deinit()
     colorama.init(strip=False)
 
 # Local imports
 from syncopy import __storage__, __sessionid__, __checksum_algorithm__, __spydir__
 from syncopy.datatype.base_data import BaseData
 from syncopy.datatype.util import get_dir_size
@@ -77,25 +79,30 @@
 
     Examples
     --------
     >>> spy.cleanup()
     """
 
     # Make sure age-cutoff is valid
-    scalar_parser(older_than, varname="older_than", ntype="int_like",
-                  lims=[0, np.inf])
+    scalar_parser(older_than, varname="older_than", ntype="int_like", lims=[0, np.inf])
     older_than = int(older_than)
 
     # For clarification: show location of storage folder that is scanned here
     funcName = "Syncopy <{}>".format(inspect.currentframe().f_code.co_name)
     storage_size_gb, storage_num_files = get_dir_size(__storage__, out="GB")
-    dirInfo = \
-        "\n{name:s} Analyzing temporary storage folder '{dir:s}' containing {numf:d} files with total size {sizegb:.2f} GB...\n"
-    log(dirInfo.format(name=funcName, dir=__storage__, numf=storage_num_files, sizegb=storage_size_gb),
-        caller='cleanup')
+    dirInfo = "\n{name:s} Analyzing temporary storage folder '{dir:s}' containing {numf:d} files with total size {sizegb:.2f} GB...\n"
+    log(
+        dirInfo.format(
+            name=funcName,
+            dir=__storage__,
+            numf=storage_num_files,
+            sizegb=storage_size_gb,
+        ),
+        caller="cleanup",
+    )
 
     # Parse interactive keyword: if `False`, don't ask, just delete
     if not isinstance(interactive, bool):
         raise SPYTypeError(interactive, varname="interactive", expected="bool")
 
     # Also check for dangling data (not associated to any session)
     data = glob(os.path.join(__storage__, "spy_*"))
@@ -105,96 +112,106 @@
         if not only_current_session:
             dangling.append(dat)
         elif sessid == __sessionid__:
             dangling.append(dat)
 
     # Farewell if nothing's to do here
     if not dangling:
-        ext = \
-        "Did not find any dangling data or Syncopy session remains " +\
-        "older than {age:d} hours."
+        ext = "Did not find any dangling data or Syncopy session remains " + "older than {age:d} hours."
         log(ext.format(name=funcName, age=older_than), caller=cleanup)
         spydir_size_gb, spydir_num_files = get_dir_size(__spydir__, out="GB")
-        log(f"Note: {spydir_num_files} files with total size of {spydir_size_gb:.2f} GB left in spy dir '{__spydir__}'.",
-            caller=cleanup)
+        log(
+            f"Note: {spydir_num_files} files with total size of {spydir_size_gb:.2f} GB left in spy dir '{__spydir__}'.",
+            caller=cleanup,
+        )
         return
 
     # Prepare info prompt for dangling files
     if dangling:
-        dangInfo = \
-            "Found {numdang:d} dangling files not associated to any session " +\
-            "using {szdang:4.1f} GB of disk space. \n"
+        dangInfo = (
+            "Found {numdang:d} dangling files not associated to any session "
+            + "using {szdang:4.1f} GB of disk space. \n"
+        )
         numdang = 0
         szdang = 0.0
         for file in dangling:
             try:
                 if os.path.isfile(file):
-                    szdang += os.path.getsize(file)/1024**3
+                    szdang += os.path.getsize(file) / 1024**3
                     numdang += 1
                 elif os.path.isdir(file):
-                    szdang += sum(os.path.getsize(os.path.join(dirpth, fname)) / 1024**3 \
-                                           for dirpth, _, fnames in os.walk(file) \
-                                               for fname in fnames)
+                    szdang += sum(
+                        os.path.getsize(os.path.join(dirpth, fname)) / 1024**3
+                        for dirpth, _, fnames in os.walk(file)
+                        for fname in fnames
+                    )
                     numdang += 1
 
             except OSError as ex:
-                log(f"Dangling file {file} no longer exists: {ex}. (Maybe already deleted.)", caller=cleanup)
+                log(
+                    f"Dangling file {file} no longer exists: {ex}. (Maybe already deleted.)",
+                    caller=cleanup,
+                )
         dangInfo = dangInfo.format(numdang=numdang, szdang=szdang)
 
-        dangOptions = \
-            "[D]ANGLING FILE removal to delete anything not associated to sessions " +\
-            "(you will not be prompted for confirmation) \n"
+        dangOptions = (
+            "[D]ANGLING FILE removal to delete anything not associated to sessions "
+            + "(you will not be prompted for confirmation) \n"
+        )
         dangValid = ["D"]
         promptInfo = dangInfo
         promptOptions = dangOptions
         promptValid = dangValid
 
     # Put together actual prompt message message
     promptChoice = "\nPlease choose one of the following options:\n"
     abortOption = "[C]ANCEL\n"
     abortValid = ["C"]
 
     if dangling:
-        rmAllOption = \
-            "[R]EMOVE all dangling files at once " +\
-            "(you will not be prompted for confirmation)\n"
+        rmAllOption = "[R]EMOVE all dangling files at once " + "(you will not be prompted for confirmation)\n"
         rmAllValid = ["R"]
         promptInfo = dangInfo
         promptOptions = dangOptions + rmAllOption
         promptValid = dangValid + rmAllValid
 
     # By default, ask what to do; if `interactive` is `False`, remove everything
     if interactive:
-        choice = user_input(promptInfo + promptChoice + promptOptions + abortOption,
-                            valid=promptValid + abortValid)
+        choice = user_input(
+            promptInfo + promptChoice + promptOptions + abortOption,
+            valid=promptValid + abortValid,
+        )
     else:
         choice = "R"
 
     # Deleate all dangling files at once
     if choice == "D":
         for dat in tqdm(dangling, desc="Deleting dangling data...", disable=None):
             _rm_session([dat])
 
     # Delete everything
     elif choice == "R":
-        for contents in tqdm([[dat] for dat in dangling],
-                             desc="Deleting temporary data...", disable=None):
+        for contents in tqdm([[dat] for dat in dangling], desc="Deleting temporary data...", disable=None):
             _rm_session(contents)
 
     # Don't do anything for now, continue w/dangling data
     else:
         print(f"Aborting...")
 
     # Report on remaining data
     storage_size_gb, storage_num_files = get_dir_size(__storage__, out="GB")
-    log(f"{storage_num_files} files with total size of {storage_size_gb:.2f} GB left in storage dir '{__storage__}'.",
-        caller='cleanup')
+    log(
+        f"{storage_num_files} files with total size of {storage_size_gb:.2f} GB left in storage dir '{__storage__}'.",
+        caller="cleanup",
+    )
     spydir_size_gb, spydir_num_files = get_dir_size(__spydir__, out="GB")
-    log(f"{spydir_num_files} files with total size of {spydir_size_gb:.2f} GB left in spy dir '{__spydir__}'.",
-        caller='cleanup')
+    log(
+        f"{spydir_num_files} files with total size of {spydir_size_gb:.2f} GB left in spy dir '{__spydir__}'.",
+        caller="cleanup",
+    )
 
 
 def clear():
     """
     Clear Syncopy objects from memory
 
     Notes
```

### Comparing `esi_syncopy-2023.5/syncopy/plotting/_helpers.py` & `esi_syncopy-2023.7/syncopy/plotting/_helpers.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,14 +17,15 @@
     To extract 'meta-information' like time and freq axis
     for a particular plot we use (implicit from the users
     perspective) selections. To return to a clean slate
     we revert/delete it afterwards.
 
     All plotting routines must have `data` as 1st argument!
     """
+
     @functools.wraps(plotter)
     def wrapper_plot(data, *args, **kwargs):
 
         # to restore
         select_backup = None if data.selection is None else deepcopy(data.selection.select)
 
         res = plotter(data, *args, **kwargs)
@@ -146,30 +147,30 @@
 
     """
     Returns the method string from
     the log of a Syncopy data object
     """
 
     # get the method string in a capture group
-    pattern = re.compile(r'[\s\w\D]+method = (\w+)')
+    pattern = re.compile(r"[\s\w\D]+method = (\w+)")
     match = pattern.match(dataobject._log)
     if match:
         meth_str = match.group(1)
         return meth_str
 
 
 def get_output(dataobject):
 
     """
     Returns the output string from
     the log of a Syncopy data object
     """
 
     # get the output string in a capture group
-    pattern = re.compile(r'[\s\w\D]+output = (\w+)')
+    pattern = re.compile(r"[\s\w\D]+output = (\w+)")
     match = pattern.match(dataobject._log)
     if match:
         output_str = match.group(1)
         return output_str
 
 
 def calc_multi_layout(nAx):
@@ -184,26 +185,26 @@
 
     # This works as well as long
     # as `nAx` isn't prime :]
     # so we have to augment by 1 if that's the case
     if nAx % 2 != 0:
         ncols = int(np.sqrt(nAx))  # this is max pltConfig["mMaxYaxes"]
         nrows = ncols
-        while(ncols * nrows < nAx):
+        while ncols * nrows < nAx:
             ncols += 1
             nrows = int(nAx / ncols)
         # nAx was prime and too big
         # for one plotting row
         if ncols == nAx and nAx > 4:
             nAx += 1
     # no elif to capture possibly incremented nAx
     if nAx % 2 == 0 and nAx > 2:
         nrows = int(np.sqrt(nAx))  # this is max pltConfig["mMaxYaxes"]
         ncols = nAx // nrows
-        while(ncols * nrows < nAx):
+        while ncols * nrows < nAx:
             nrows -= 1
             ncols = int(nAx / nrows)
     # just two axes
     elif nAx == 2:
         nrows, ncols = 1, 2
 
     return nrows, ncols
```

### Comparing `esi_syncopy-2023.5/syncopy/plotting/_plotting.py` & `esi_syncopy-2023.7/syncopy/plotting/_plotting.py`

 * *Files 17% similar despite different names*

```diff
@@ -17,156 +17,158 @@
 
 # for the legends
 ncol_max = 3
 
 
 # -- 2d-line plots --
 @matplotlib.rc_context(rc_props)
-def mk_line_figax(xlabel='time (s)', ylabel='signal (a.u.)'):
+def mk_line_figax(xlabel="time (s)", ylabel="signal (a.u.)"):
 
     """
     Create the figure and axis for a
     standard 2d-line plot
     """
 
-    fig, ax = ppl.subplots(figsize=pltConfig['sFigSize'])
+    fig, ax = ppl.subplots(figsize=pltConfig["sFigSize"])
     # Hide the right and top spines
-    ax.spines['right'].set_visible(False)
-    ax.spines['top'].set_visible(False)
-    ax.tick_params(axis='both', which='major',
-                   labelsize=pltConfig['sTickSize'])
+    ax.spines["right"].set_visible(False)
+    ax.spines["top"].set_visible(False)
+    ax.tick_params(axis="both", which="major", labelsize=pltConfig["sTickSize"])
 
-    ax.set_xlabel(xlabel, fontsize=pltConfig['sLabelSize'])
-    ax.set_ylabel(ylabel, fontsize=pltConfig['sLabelSize'])
+    ax.set_xlabel(xlabel, fontsize=pltConfig["sLabelSize"])
+    ax.set_ylabel(ylabel, fontsize=pltConfig["sLabelSize"])
 
     return fig, ax
 
 
 @matplotlib.rc_context(rc_props)
-def mk_multi_line_figax(nrows, ncols, xlabel='time (s)', ylabel='signal (a.u.)'):
+def mk_multi_line_figax(nrows, ncols, xlabel="time (s)", ylabel="signal (a.u.)", x_size=None, y_size=None):
 
     """
     Create the figure and axes for a
     multipanel 2d-line plot
     """
 
     # ncols and nrows get
-    # restricted via the plotting frontend
-    x_size = ncols * pltConfig['mXSize']
-    y_size = nrows * pltConfig['mYSize']
+    # restricted via the plotting frontends
+    if x_size is None:
+        x_size = ncols * pltConfig["mXSize"]
+    else:
+        x_size = ncols * x_size
+    if y_size is None:
+        y_size = ncols * pltConfig["mYSize"]
+    else:
+        y_size = ncols * y_size
 
-    fig, axs = ppl.subplots(nrows, ncols, figsize=(x_size, y_size),
-                            sharex=True, sharey=True, squeeze=False)
+    fig, axs = ppl.subplots(nrows, ncols, figsize=(x_size, y_size), sharex=True, sharey=True, squeeze=False)
 
     # Hide the right and top spines
     # and remove all tick labels
     for ax in axs.flatten():
-        ax.spines['right'].set_visible(False)
-        ax.spines['top'].set_visible(False)
+        ax.spines["right"].set_visible(False)
+        ax.spines["top"].set_visible(False)
         ax.tick_params(labelsize=0)
 
     # determine axis layout
     y_left = axs[:, 0]
     x_bottom = axs[-1, :]
 
     # write tick and axis labels only on outer axes to save space
     for ax in y_left:
-        ax.tick_params(labelsize=pltConfig['mTickSize'])
-        ax.set_ylabel(ylabel, fontsize=pltConfig['mLabelSize'])
+        ax.tick_params(labelsize=pltConfig["mTickSize"])
+        ax.set_ylabel(ylabel, fontsize=pltConfig["mLabelSize"])
 
     for ax in x_bottom:
-        ax.tick_params(labelsize=pltConfig['mTickSize'])
-        ax.set_xlabel(xlabel, fontsize=pltConfig['mLabelSize'])
+        ax.tick_params(labelsize=pltConfig["mTickSize"])
+        ax.set_xlabel(xlabel, fontsize=pltConfig["mLabelSize"])
 
     return fig, axs
 
 
 @matplotlib.rc_context(rc_props)
-def plot_lines(ax, data_x, data_y,
-               leg_fontsize=pltConfig['sLegendSize'],
-               shifted=False,
-               **pkwargs):
+def plot_lines(ax, data_x, data_y, leg_fontsize=pltConfig["sLegendSize"], shifted=False, **pkwargs):
 
     if shifted:
         offsets = _helpers.shift_multichan(data_y)
         data_y = data_y + offsets
         # no colors needed
-        pkwargs['color'] = foreground
+        pkwargs["color"] = foreground
 
-    if 'alpha' not in pkwargs:
-        pkwargs['alpha'] = 0.9
+    if "alpha" not in pkwargs:
+        pkwargs["alpha"] = 0.9
 
     ax.plot(data_x, data_y, **pkwargs)
 
     # plot the legend
-    if 'label' in pkwargs:
+    if "label" in pkwargs:
         # multi-chan stacking, use labels as ticks
         if shifted and data_y.ndim > 1:
             pos = np.array(data_y.mean(axis=0))
-            ax.set_yticks(pos, pkwargs['label'])
+            ax.set_yticks(pos, pkwargs["label"])
 
         else:
-            ax.legend(ncol=ncol_max, loc='best', frameon=False,
-                      fontsize=leg_fontsize,
-                      )
+            ax.legend(
+                ncol=ncol_max,
+                loc="best",
+                frameon=False,
+                fontsize=leg_fontsize,
+            )
             # make room for the legend
             mn, mx = data_y.min(), data_y.max()
 
             stretch = lambda x, fac: np.abs((fac - 1) * x)
 
             ax.set_ylim((mn - stretch(mn, 1.1), mx + stretch(mx, 1.1)))
 
 
 # -- image plots --
 @matplotlib.rc_context(rc_props)
-def mk_img_figax(xlabel='time (s)', ylabel='frequency (Hz)'):
+def mk_img_figax(xlabel="time (s)", ylabel="frequency (Hz)"):
 
     """
     Create the figure and axes for an
     image plot with `imshow`
     """
 
-    fig, ax = ppl.subplots(figsize=pltConfig['sFigSize'])
+    fig, ax = ppl.subplots(figsize=pltConfig["sFigSize"])
 
-    ax.tick_params(axis='both', which='major',
-                   labelsize=pltConfig['sTickSize'])
-    ax.set_xlabel(xlabel, fontsize=pltConfig['sLabelSize'])
-    ax.set_ylabel(ylabel, fontsize=pltConfig['sLabelSize'])
+    ax.tick_params(axis="both", which="major", labelsize=pltConfig["sTickSize"])
+    ax.set_xlabel(xlabel, fontsize=pltConfig["sLabelSize"])
+    ax.set_ylabel(ylabel, fontsize=pltConfig["sLabelSize"])
 
     return fig, ax
 
 
 @matplotlib.rc_context(rc_props)
-def mk_multi_img_figax(nrows, ncols, xlabel='time (s)', ylabel='frequency (Hz)'):
+def mk_multi_img_figax(nrows, ncols, xlabel="time (s)", ylabel="frequency (Hz)"):
 
     """
     Create the figure and axes for an
     image plot with `imshow` for multiple
     sub plots
     """
     # ncols and nrows get
     # restricted via the plotting frontend
-    x_size = ncols * pltConfig['mXSize']
-    y_size = nrows * pltConfig['mYSize']
+    x_size = ncols * pltConfig["mXSize"]
+    y_size = nrows * pltConfig["mYSize"]
 
-    fig, axs = ppl.subplots(nrows, ncols, figsize=(x_size, y_size),
-                            sharex=True, sharey=True, squeeze=False)
+    fig, axs = ppl.subplots(nrows, ncols, figsize=(x_size, y_size), sharex=True, sharey=True, squeeze=False)
 
     # determine axis layout
     y_left = axs[:, 0]
     x_bottom = axs[-1, :]
 
     # write tick and axis labels only on outer axes to save space
     for ax in y_left:
-        ax.tick_params(labelsize=pltConfig['mTickSize'])
-        ax.set_ylabel(ylabel, fontsize=pltConfig['mLabelSize'])
+        ax.tick_params(labelsize=pltConfig["mTickSize"])
+        ax.set_ylabel(ylabel, fontsize=pltConfig["mLabelSize"])
 
     for ax in x_bottom:
-        ax.tick_params(labelsize=pltConfig['mTickSize'])
-        ax.set_xlabel(xlabel, fontsize=pltConfig['mLabelSize'])
+        ax.tick_params(labelsize=pltConfig["mTickSize"])
+        ax.set_xlabel(xlabel, fontsize=pltConfig["mLabelSize"])
 
     return fig, axs
 
 
 @matplotlib.rc_context(rc_props)
 def plot_tfreq(ax, data_yx, times, freqs, **pkwargs):
 
@@ -176,13 +178,11 @@
 
     Needs frequencies (`freqs`) and sampling rate (`fs`)
     for correct units.
     """
 
     # extent is defined in xy order
     df = freqs[1] - freqs[0]
-    extent = [times[0], times[-1],
-              freqs[0] - df / 2, freqs[-1] - df / 2]
+    extent = [times[0], times[-1], freqs[0] - df / 2, freqs[-1] - df / 2]
 
-    cmap = pkwargs.pop('cmap', pltConfig['cmap'])
-    ax.imshow(data_yx[::-1], aspect='auto', cmap=cmap,
-              extent=extent, **pkwargs)
+    cmap = pkwargs.pop("cmap", pltConfig["cmap"])
+    ax.imshow(data_yx[::-1], aspect="auto", cmap=cmap, extent=extent, **pkwargs)
```

### Comparing `esi_syncopy-2023.5/syncopy/plotting/config.py` & `esi_syncopy-2023.7/syncopy/plotting/config.py`

 * *Files 25% similar despite different names*

```diff
@@ -3,64 +3,69 @@
 # Syncopy plotting setup
 #
 
 from syncopy import __plt__
 from packaging.version import parse
 
 foreground = "#2E3440"  # nord0
-background = '#fcfcfc'  # hint of gray
+background = "#fcfcfc"  # hint of gray
 
 # dark mode
 # foreground = "#D8DEE9"  # nord4
 # background = "#2E3440"  # nord0
 
 if __plt__:
     import matplotlib as mpl
+
     # to allow both older and newer matplotlib versions
     if parse(mpl.__version__) < parse("3.6"):
-        mpl.style.use('seaborn-colorblind')
+        mpl.style.use("seaborn-colorblind")
     else:
-        mpl.style.use('seaborn-v0_8-colorblind')
+        mpl.style.use("seaborn-v0_8-colorblind")
     # a hint of gray
     rc_props = {
-        'patch.edgecolor': foreground,
-        'text.color': foreground,
-        'axes.facecolor': background,
-        'axes.facecolor': background,
-        'figure.facecolor': background,
+        "patch.edgecolor": foreground,
+        "text.color": foreground,
+        "axes.facecolor": background,
+        "axes.facecolor": background,
+        "figure.facecolor": background,
         "axes.edgecolor": foreground,
         "axes.labelcolor": foreground,
         "xtick.color": foreground,
         "ytick.color": foreground,
         "legend.framealpha": 0,
         "figure.facecolor": background,
         "figure.edgecolor": background,
         "savefig.facecolor": background,
         "savefig.edgecolor": background,
-        'ytick.color': foreground,
-        'xtick.color': foreground,
-        'text.color': foreground
+        "ytick.color": foreground,
+        "xtick.color": foreground,
+        "text.color": foreground,
     }
 
 
 # Global style settings for single-/multi-plots
-pltConfig = {"sTitleSize": 15,
-             "sLabelSize": 16,
-             "sTickSize": 12,
-             "sLegendSize": 12,
-             "sFigSize": (6.4, 4.2),
-             "mTitleSize": 12.5,
-             "mLabelSize": 12.5,
-             "mTickSize": 11,
-             "mLegendSize": 10,
-             "mXSize": 3.2,
-             "mYSize": 2.4,
-             "mMaxAxes": 25,
-             "cmap": "magma"}
+pltConfig = {
+    "sTitleSize": 15,
+    "sLabelSize": 16,
+    "sTickSize": 12,
+    "sLegendSize": 12,
+    "sFigSize": (6.4, 4.2),
+    "mTitleSize": 12.5,
+    "mLabelSize": 12.5,
+    "mTickSize": 11,
+    "mLegendSize": 10,
+    "mXSize": 3.2,
+    "mYSize": 2.4,
+    "mMaxAxes": 25,
+    "cmap": "magma",
+}
 
 # Global consistent error message if matplotlib is missing
-pltErrMsg = "\nSyncopy <core> WARNING: Could not import 'matplotlib'. \n" +\
-          "{} requires a working matplotlib installation. \n" +\
-          "Please consider installing 'matplotlib', e.g., via conda: \n" +\
-          "\tconda install matplotlib\n" +\
-          "or using pip:\n" +\
-          "\tpip install matplotlib"
+pltErrMsg = (
+    "\nSyncopy <core> WARNING: Could not import 'matplotlib'. \n"
+    + "{} requires a working matplotlib installation. \n"
+    + "Please consider installing 'matplotlib', e.g., via conda: \n"
+    + "\tconda install matplotlib\n"
+    + "or using pip:\n"
+    + "\tpip install matplotlib"
+)
```

### Comparing `esi_syncopy-2023.5/syncopy/plotting/helpers.py` & `esi_syncopy-2023.7/syncopy/plotting/helpers.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,14 +16,15 @@
     To extract 'meta-information' like time and freq axis
     for a particular plot we use (implicit from the users
     perspective) selections. To return to a clean slate
     we revert/delete it afterwards.
 
     All plotting routines must have `data` as 1st argument!
     """
+
     @functools.wraps(plotter)
     def wrapper_plot(data, *args, **kwargs):
 
         # to restore
         select_backup = None if data.selection is None else deepcopy(data.selection.select)
 
         res = plotter(data, *args, **kwargs)
@@ -145,26 +146,26 @@
 
     """
     Returns the method string from
     the cfg of a Syncopy data object
     """
 
     cfg_entry = dataobject.cfg[frontend_name]
-    return cfg_entry.get('method')
+    return cfg_entry.get("method")
 
 
 def get_output(dataobject, frontend_name):
 
     """
     Returns the output string from
     the cfg of a Syncopy data object
     """
 
     cfg_entry = dataobject.cfg[frontend_name]
-    return cfg_entry.get('output')
+    return cfg_entry.get("output")
 
 
 def calc_multi_layout(nAx):
 
     """
     Given the total numbers of
     axes `nAx` create the nrows, ncols
@@ -175,26 +176,26 @@
 
     # This works as well as long
     # as `nAx` isn't prime :]
     # so we have to augment by 1 if that's the case
     if nAx % 2 != 0:
         ncols = int(np.sqrt(nAx))  # this is max pltConfig["mMaxYaxes"]
         nrows = ncols
-        while(ncols * nrows < nAx):
+        while ncols * nrows < nAx:
             ncols += 1
             nrows = int(nAx / ncols)
         # nAx was prime and too big
         # for one plotting row
         if ncols == nAx and nAx > 4:
             nAx += 1
     # no elif to capture possibly incremented nAx
     if nAx % 2 == 0 and nAx > 2:
         nrows = int(np.sqrt(nAx))  # this is max pltConfig["mMaxYaxes"]
         ncols = nAx // nrows
-        while(ncols * nrows < nAx):
+        while ncols * nrows < nAx:
             nrows -= 1
             ncols = int(nAx / nrows)
     # just two axes
     elif nAx == 2:
         nrows, ncols = 1, 2
 
     return nrows, ncols
```

### Comparing `esi_syncopy-2023.5/syncopy/plotting/mp_plotting.py` & `esi_syncopy-2023.7/syncopy/plotting/mp_plotting.py`

 * *Files 15% similar despite different names*

```diff
@@ -32,15 +32,15 @@
 
     if not __plt__:
         SPYWarning(pltErrMsg)
         return
 
     # right now we have to enforce
     # single trial selection only
-    trl = show_kwargs.get('trials', None)
+    trl = show_kwargs.get("trials", None)
     if not isinstance(trl, Number) and len(data.trials) > 1:
         SPYWarning("Please select a single trial for plotting!")
         return
 
     # only 1 trial so no explicit selection needed
     elif len(data.trials) == 1:
         trl = 0
@@ -59,29 +59,27 @@
 
     # multiple channels?
     labels = plot_helpers.parse_channel(data, show_kwargs)
     nAx = 1 if isinstance(labels, str) else len(labels)
 
     if nAx < 2:
         SPYWarning("Please select at least two channels for a multipanelplot!")
-        return
+        return None, None
 
-    elif nAx > pltConfig['mMaxAxes']:
+    elif nAx > pltConfig["mMaxAxes"]:
         SPYWarning(f"Please select max. {pltConfig['mMaxAxes']} channels for a multipanelplot!")
-        return
-    else:
-        # determine axes layout, prefer columns over rows due to display aspect ratio
-        nrows, ncols = plot_helpers.calc_multi_layout(nAx)
+        return None, None
+
+    # determine axes layout, prefer columns over rows due to display aspect ratio
+    nrows, ncols = plot_helpers.calc_multi_layout(nAx)
 
     fig, axs = _plotting.mk_multi_line_figax(nrows, ncols)
 
     for chan_dat, ax, label in zip(data_y.T, axs.flatten(), labels):
-        _plotting.plot_lines(ax, data_x, chan_dat,
-                             label=label,
-                             leg_fontsize=pltConfig['mLegendSize'])
+        _plotting.plot_lines(ax, data_x, chan_dat, label=label, leg_fontsize=pltConfig["mLegendSize"])
 
     # delete empty plot due to grid extension
     # because of prime nAx -> can be maximally 1 plot
     if ncols * nrows > nAx:
         axs.flatten()[-1].remove()
 
     fig.tight_layout()
@@ -104,28 +102,28 @@
 
     if not __plt__:
         SPYWarning(pltErrMsg)
         return
 
     # right now we have to enforce
     # single trial selection only
-    trl = show_kwargs.get('trials', None)
+    trl = show_kwargs.get("trials", None)
     if not isinstance(trl, Number) and len(data.trials) > 1:
         SPYWarning("Please select a single trial for plotting!")
         return
     elif len(data.trials) == 1:
         trl = 0
 
     channels = plot_helpers.parse_channel(data, show_kwargs)
     nAx = 1 if isinstance(channels, str) else len(channels)
 
     if nAx < 2:
         SPYWarning("Please select at least two channels for a multipanelplot!")
         return
-    elif nAx > pltConfig['mMaxAxes']:
+    elif nAx > pltConfig["mMaxAxes"]:
         SPYWarning("Please select max. {pltConfig['mMaxAxes']} channels for a multipanelplot!")
         return
     else:
         # determine axes layout, prefer columns over rows due to display aspect ratio
         nrows, ncols = plot_helpers.calc_multi_layout(nAx)
 
     # -- check if it is a time-frequency spectrum ----------
@@ -146,130 +144,70 @@
             lgl = "Selection with non-zero size"
             act = "got zero samples"
             raise SPYValueError(lgl, varname="show_kwargs", actual=act)
 
         maxP = data_cyx.max()
         for data_yx, ax, label in zip(data_cyx, axs.flatten(), channels):
             _plotting.plot_tfreq(ax, data_yx, time, freqs, vmax=maxP)
-            ax.set_title(label, fontsize=pltConfig['mTitleSize'])
+            ax.set_title(label, fontsize=pltConfig["mTitleSize"])
         fig.tight_layout()
         fig.subplots_adjust(wspace=0.05)
 
     # just a line plot
     else:
         msg = False
-        if 'toilim' in show_kwargs:
-            show_kwargs.pop('toilim')
+        if "toilim" in show_kwargs:
+            show_kwargs.pop("toilim")
             msg = True
-        if 'toi' in show_kwargs:
-            show_kwargs.pop('toi')
+        if "toi" in show_kwargs:
+            show_kwargs.pop("toi")
             msg = True
         if msg:
-            msg = ("Line spectra don't have a time axis, "
-                   "ignoring `toi/toilim` selection!")
+            msg = "Line spectra don't have a time axis, " "ignoring `toi/toilim` selection!"
             SPYWarning(msg)
 
         # get the data to plot
         data_x = plot_helpers.parse_foi(data, show_kwargs)
-        output = plot_helpers.get_output(data, 'freqanalysis')
+        output = plot_helpers.get_output(data, "freqanalysis")
 
         # only log10 the absolute squared spectra
-        if output == 'pow':
+        if output == "pow":
             data_y = np.log10(data.show(**show_kwargs))
-            ylabel = 'power (dB)'
-        elif output in ['fourier', 'complex']:
-            SPYWarning("Can't plot complex valued spectra, choose 'real' or 'imag' as output! Aborting plotting.")
+            ylabel = "power (dB)"
+        elif output in ["fourier", "complex"]:
+            SPYWarning(
+                "Can't plot complex valued spectra, choose 'real' or 'imag' as output! Aborting plotting."
+            )
             return
         else:
             data_y = data.show(**show_kwargs)
-            ylabel = f'{output}'
+            ylabel = f"{output}"
 
         taper_labels = None
-        if len(data.taper) != 1:   
-            taper = show_kwargs.get('taper')
+        if len(data.taper) != 1:
+            taper = show_kwargs.get("taper")
             # multiple tapers are to be plotted
             if not isinstance(taper, (Number, str)):
                 taper_labels = data.taper
 
-        fig, axs = _plotting.mk_multi_line_figax(nrows, ncols, xlabel='frequency (Hz)',
-                                                 ylabel=ylabel)
+        fig, axs = _plotting.mk_multi_line_figax(nrows, ncols, xlabel="frequency (Hz)", ylabel=ylabel)
 
         for chan_dat, ax, label in zip(data_y.T, axs.flatten(), channels):
             if taper_labels is not None:
-                _plotting.plot_lines(ax, data_x, chan_dat, label=taper_labels, leg_fontsize=pltConfig['mLegendSize'])
+                _plotting.plot_lines(
+                    ax,
+                    data_x,
+                    chan_dat,
+                    label=taper_labels,
+                    leg_fontsize=pltConfig["mLegendSize"],
+                )
             else:
                 _plotting.plot_lines(ax, data_x, chan_dat)
-            ax.set_title(label, fontsize=pltConfig['mTitleSize'])
+            ax.set_title(label, fontsize=pltConfig["mTitleSize"])
 
         # delete empty plot due to grid extension
         # because of prime nAx -> can be maximally 1 plot
         if ncols * nrows > nAx:
             axs.flatten()[-1].remove()
         fig.tight_layout()
 
     return fig, axs
-
-
-@plot_helpers.revert_selection
-def plot_CrossSpectralData(data, **show_kwargs):
-    """
-    Plot 2d-line plots for the different connectivity measures.
-
-    Parameters
-    ----------
-    data : :class:`~syncopy.datatype.CrossSpectralData`
-    show_kwargs : :func:`~syncopy.datatype.methods.show.show` arguments
-    """
-
-    if not __plt__:
-        SPYWarning(pltErrMsg)
-        return
-
-    # right now we have to enforce
-    # single trial selection only
-    trl = show_kwargs.get('trials', None)
-    if not isinstance(trl, int) and len(data.trials) > 1:
-        SPYWarning("Please select a single trial for plotting!")
-        return
-    elif len(data.trials) == 1:
-        trl = 0
-
-    # what channel combination
-    if 'channel_i' not in show_kwargs or 'channel_j' not in show_kwargs:
-        SPYWarning("Please select a channel combination for plotting!")
-        return
-    chi, chj = show_kwargs['channel_i'], show_kwargs['channel_j']
-
-    # what data do we have?
-    method = plot_helpers.get_method(data, 'connectivityanalysis')
-    output = plot_helpers.get_output(data, 'connectivityanalysis')
-
-    if method == 'granger':
-        xlabel = 'frequency (Hz)'
-        ylabel = 'Granger causality'
-        label = rf"channel{chi} $\rightarrow$ channel{chj}"
-        data_x = plot_helpers.parse_foi(data, show_kwargs)
-    elif method == 'coh':
-        xlabel = 'frequency (Hz)'
-        ylabel = f'{output} coherence'
-        label = rf"channel{chi} - channel{chj}"
-        data_x = plot_helpers.parse_foi(data, show_kwargs)
-    elif method == 'corr':
-        xlabel = 'lag'
-        ylabel = 'correlation'
-        label = rf"channel{chi} - channel{chj}"
-        data_x = plot_helpers.parse_toi(data, show_kwargs)
-    # that's all the methods we got so far
-    else:
-        raise NotImplementedError
-
-    # get the data to plot
-    data_y = data.show(**show_kwargs)
-
-    # Create the axes and figure if needed.
-    # Persistent axes allow for plotting different
-    # channel combinations into the same figure.
-    if not hasattr(data, 'ax'):
-        fig, data.ax = _plotting.mk_line_figax(xlabel, ylabel)
-    _plotting.plot_lines(data.ax, data_x, data_y, label=label)
-
-    return fig, data.ax
```

### Comparing `esi_syncopy-2023.5/syncopy/plotting/sp_plotting.py` & `esi_syncopy-2023.7/syncopy/plotting/sp_plotting.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 # -*- coding: utf-8 -*-
 #
 # The singlepanel plotting functions for Syncopy
 # data types
-# 1st argument **must** be `data` to revert the (plotting-)selections
+# 1st argument **must** be `spy_data`
 #
 
 # Builtin/3rd party package imports
 import numpy as np
 from numbers import Number
 
 # Syncopy imports
@@ -14,75 +14,77 @@
 from syncopy.shared.errors import SPYWarning, SPYValueError
 from syncopy.plotting import _plotting
 from syncopy.plotting import helpers as plot_helpers
 from syncopy.plotting.config import pltErrMsg, pltConfig
 
 
 @plot_helpers.revert_selection
-def plot_AnalogData(data, shifted=True, **show_kwargs):
+def plot_AnalogData(spy_data, shifted=True, **show_kwargs):
     """
     Simple 2d-line plot of selected channels.
 
     Parameters
     ----------
-    data : :class:`~syncopy.datatype.AnalogData`
+    spy_data : :class:`~syncopy.datatype.AnalogData`
+    shifted : bool
+        Stacks the signals on top of each other if `True` by
+        extending the y-axis
     show_kwargs : :func:`~syncopy.datatype.methods.show.show` arguments
 
     Returns
     -------
     fig : `matplotlib.figure.Figure` instance (or `None` in case of errors), the plot figure.
     ax  : `matplotlib.axes.Axes` instance (or `None` in case of errors), the plot axes.
     """
 
     if not __plt__:
         SPYWarning(pltErrMsg)
         return None, None
 
     # right now we have to enforce
     # single trial selection only
-    trl = show_kwargs.get('trials', None)
-    if not isinstance(trl, Number) and len(data.trials) > 1:
+    trl = show_kwargs.get("trials", None)
+    if not isinstance(trl, Number) and len(spy_data.trials) > 1:
         SPYWarning("Please select a single trial for plotting.")
         return None, None
     # only 1 trial so no explicit selection needed
-    elif len(data.trials) == 1:
+    elif len(spy_data.trials) == 1:
         trl = 0
 
     # get the data to plot
-    data_y = data.show(**show_kwargs)
+    data_y = spy_data.show(**show_kwargs)
     # 'time' and 'channel' are the only axes
-    if data._defaultDimord != data.dimord:
+    if spy_data._defaultDimord != spy_data.dimord:
         data_y = data_y.T
     if data_y.size == 0:
         lgl = "Selection with non-zero size"
         act = "got zero samples"
         raise SPYValueError(lgl, varname="show_kwargs", actual=act)
 
-    data_x = plot_helpers.parse_toi(data, trl, show_kwargs)
+    data_x = plot_helpers.parse_toi(spy_data, trl, show_kwargs)
 
     # multiple channels?
-    labels = plot_helpers.parse_channel(data, show_kwargs)
+    labels = plot_helpers.parse_channel(spy_data, show_kwargs)
 
-    fig, ax = _plotting.mk_line_figax(ylabel='')
-    _plotting.plot_lines(ax, data_x, data_y,
-                         label=labels, shifted=shifted)
+    fig, ax = _plotting.mk_line_figax(ylabel="")
+    _plotting.plot_lines(ax, data_x, data_y, label=labels, shifted=shifted)
     fig.tight_layout()
     return fig, ax
 
 
 @plot_helpers.revert_selection
-def plot_SpectralData(data, logscale=True, **show_kwargs):
+def plot_SpectralData(spy_data, logscale=True, **show_kwargs):
     """
     Plot either a 2d-line plot in case of
     singleton time axis or an image plot
     for time-frequency spectra.
 
     Parameters
     ----------
-    data : :class:`~syncopy.datatype.SpectralData`
+    spy_data : :class:`~syncopy.datatype.SpectralData`
     logscale : bool
         If `True` the log10 of the power spectra (output='pow') values
         is plotted.
     show_kwargs : :func:`~syncopy.datatype.methods.show.show` arguments
 
     Returns
     -------
@@ -92,150 +94,146 @@
 
     if not __plt__:
         SPYWarning(pltErrMsg)
         return None, None
 
     # right now we have to enforce
     # single trial selection only
-    trl = show_kwargs.get('trials', None)
-    if not isinstance(trl, Number) and len(data.trials) > 1:
+    trl = show_kwargs.get("trials", None)
+    if not isinstance(trl, Number) and len(spy_data.trials) > 1:
         SPYWarning("Please select a single trial for plotting.")
         return None, None
-    elif len(data.trials) == 1:
+    elif len(spy_data.trials) == 1:
         trl = 0
 
-    is_tf = plot_helpers.check_if_time_freq(data)
+    is_tf = plot_helpers.check_if_time_freq(spy_data)
 
     if is_tf:
         # multiple channels?
-        label = plot_helpers.parse_channel(data, show_kwargs)
+        label = plot_helpers.parse_channel(spy_data, show_kwargs)
         # only relevant for mtmconvol
-        if 'taper' in show_kwargs:
+        if "taper" in show_kwargs:
             SPYWarning("Taper selection not supported for time-frequency spectra!\nSkipping plot..")
             return None, None
 
         if not isinstance(label, str):
             SPYWarning("Please select a single channel for plotting!\nSkipping plot..")
             return None, None
 
         # here we always need a new axes
         fig, ax = _plotting.mk_img_figax()
 
-        time = plot_helpers.parse_toi(data, trl, show_kwargs)
-        freqs = plot_helpers.parse_foi(data, show_kwargs)
+        time = plot_helpers.parse_toi(spy_data, trl, show_kwargs)
+        freqs = plot_helpers.parse_foi(spy_data, show_kwargs)
 
         # custom dimords for SpectralData not supported atm
         # dimord is time x taper x freq x channel
         # need freq x time for plotting
-        data_yx = data.show(**show_kwargs).T
+        data_yx = spy_data.show(**show_kwargs).T
         _plotting.plot_tfreq(ax, data_yx, time, freqs)
-        ax.set_title(label, fontsize=pltConfig['sTitleSize'])
+        ax.set_title(label, fontsize=pltConfig["sTitleSize"])
         fig.tight_layout()
 
     # just a line plot
     else:
 
         msg = False
-        if 'toilim' in show_kwargs:
-            show_kwargs.pop('toilim')
-            msg = True
-        if 'toi' in show_kwargs:
-            show_kwargs.pop('toi')
+        if "latency" in show_kwargs:
+            show_kwargs.pop("latency")
             msg = True
         if msg:
-            msg = ("Line spectra don't have a time axis, "
-                   "ignoring `toi/toilim` selection!")
+            msg = "Line spectra don't have a time axis, " "ignoring `toi/toilim` selection!"
             SPYWarning(msg)
 
         # multiple channels?
-        channels = plot_helpers.parse_channel(data, show_kwargs)
+        channels = plot_helpers.parse_channel(spy_data, show_kwargs)
 
         # just multiple tapers or multiple channels in one plot
-        if len(data.taper) != 1:
-            taper = show_kwargs.get('taper')
+        if len(spy_data.taper) != 1:
+            taper = show_kwargs.get("taper")
             if not isinstance(taper, (Number, str)) and not isinstance(channels, str):
                 msg = "Please select a single taper or a single channel \nfor plotting multi-taper spectra.. aborting plotting\n"
                 SPYWarning(msg)
                 return None, None
             # single channel, multiple tapers
             elif isinstance(channels, str):
-                labels = data.taper
+                labels = spy_data.taper
             # single taper, multiple channels
             elif isinstance(taper, (Number, str)):
                 labels = channels
         else:
             labels = channels
         # get the data to plot
-        data_x = plot_helpers.parse_foi(data, show_kwargs)
-        output = plot_helpers.get_output(data, 'freqanalysis')
+        data_x = plot_helpers.parse_foi(spy_data, show_kwargs)
+        output = plot_helpers.get_output(spy_data, "freqanalysis")
+
+        pow_or_fooof = "fooof" in output or output == "pow"
 
         # only log10 the absolute squared spectra
-        if output == 'pow' and logscale:
-            data_y = np.log10(data.show(**show_kwargs))
-            ylabel = 'power (dB)'
-        elif output == 'pow' and not logscale:
-            data_y = data.show(**show_kwargs)
-            ylabel = r'power (mV^2)'
-        elif output in ['fourier', 'complex']:
-            SPYWarning("Can't plot complex valued spectra, choose 'real' or 'imag' as freqanalysis output.. aborting plotting")
+        if pow_or_fooof and logscale:
+            data_y = np.log10(spy_data.show(**show_kwargs))
+            ylabel = "power (dB)"
+        elif output in ["fourier", "complex"]:
+            SPYWarning(
+                "Can't plot complex valued spectra, choose 'real' or 'imag' as freqanalysis output.. aborting plotting"
+            )
             return None, None
         else:
-            data_y = data.show(**show_kwargs)
-            ylabel = f'{output}'
+            data_y = spy_data.show(**show_kwargs)
+            ylabel = f"{output} (a.u.)"
 
         # for itc.. needs to be improved
         if output is None:
-            ylabel = ''
+            ylabel = ""
 
         # flip if required
         if data_y.ndim > 1:
             if data_y.shape[1] == len(data_x):
                 data_y = data_y.T
 
-        fig, ax = _plotting.mk_line_figax(xlabel='frequency (Hz)',
-                                          ylabel=ylabel)
+        fig, ax = _plotting.mk_line_figax(xlabel="frequency (Hz)", ylabel=ylabel)
 
-        _plotting.plot_lines(ax, data_x, data_y, label=labels)
+        _plotting.plot_lines(ax, data_x, data_y, label=labels, lw=1.5, alpha=0.8)
         fig.tight_layout()
 
     return fig, ax
 
 
 @plot_helpers.revert_selection
-def plot_CrossSpectralData(data, **show_kwargs):
+def plot_CrossSpectralData(spy_data, **show_kwargs):
     """
     Plot 2d-line plots for the different connectivity measures.
 
     Parameters
     ----------
-    data : :class:`~syncopy.datatype.CrossSpectralData`
+    spy_data : :class:`~syncopy.datatype.CrossSpectralData`
     show_kwargs : :func:`~syncopy.datatype.methods.show.show` arguments
 
     Returns
     -------
     fig : `matplotlib.figure.Figure` instance (or `None` in case of errors), the plot figure.
     ax  : `matplotlib.axes.Axes` instance (or `None` in case of errors), the plot axes.
     """
 
     if not __plt__:
         SPYWarning(pltErrMsg)
         return None, None
 
     # right now we have to enforce
     # single trial selection only
-    trl = show_kwargs.get('trials', 0)
-    if not isinstance(trl, int) and len(data.trials) > 1:
+    trl = show_kwargs.get("trials", 0)
+    if not isinstance(trl, int) and len(spy_data.trials) > 1:
         SPYWarning("Please select a single trial for plotting.")
         return None, None
 
     # what channel combination
-    if 'channel_i' not in show_kwargs or 'channel_j' not in show_kwargs:
+    if "channel_i" not in show_kwargs or "channel_j" not in show_kwargs:
         SPYWarning("Please select a channel combination `channel_i` and `channel_j` for plotting.")
         return None, None
-    chi, chj = show_kwargs['channel_i'], show_kwargs['channel_j']
+    chi, chj = show_kwargs["channel_i"], show_kwargs["channel_j"]
     # parse labels
     if isinstance(chi, str):
         chi_label = chi
     # must be int
     else:
         chi_label = f"channel{chi}"
     # parse labels
@@ -245,78 +243,78 @@
         chi_label = chi
     if isinstance(chj, int):
         chj_label = f"channel{chj + 1}"
     else:
         chj_label = chj
 
     # what data do we have?
-    method = plot_helpers.get_method(data, 'connectivityanalysis')
-    output = plot_helpers.get_output(data, 'connectivityanalysis')
+    method = plot_helpers.get_method(spy_data, "connectivityanalysis")
+    output = plot_helpers.get_output(spy_data, "connectivityanalysis")
 
-    if method == 'granger':
-        xlabel = 'frequency (Hz)'
-        ylabel = 'Granger causality'
+    if method == "granger":
+        xlabel = "frequency (Hz)"
+        ylabel = "Granger causality"
         label = rf"{chi_label} $\rightarrow$ {chj_label}"
-        data_x = plot_helpers.parse_foi(data, show_kwargs)
-    elif method == 'coh':
-        xlabel = 'frequency (Hz)'
-        ylabel = f'{output} coherence'
+        data_x = plot_helpers.parse_foi(spy_data, show_kwargs)
+    elif method == "coh":
+        xlabel = "frequency (Hz)"
+        ylabel = f"{output} coherence"
         label = rf"{chi_label} - {chj_label}"
-        data_x = plot_helpers.parse_foi(data, show_kwargs)
-    elif method == 'ppc':
-        xlabel = 'frequency (Hz)'
-        ylabel = 'PPC'
+        data_x = plot_helpers.parse_foi(spy_data, show_kwargs)
+    elif method == "ppc":
+        xlabel = "frequency (Hz)"
+        ylabel = "PPC"
         label = rf"{chi_label} - {chj_label}"
-        data_x = plot_helpers.parse_foi(data, show_kwargs)
-    elif method == 'corr':
-        xlabel = 'lag'
-        ylabel = 'correlation'
+        data_x = plot_helpers.parse_foi(spy_data, show_kwargs)
+    elif method == "corr":
+        xlabel = "lag"
+        ylabel = "correlation"
         label = rf"{chi_label} - {chj_label}"
-        data_x = plot_helpers.parse_toi(data, trl, show_kwargs)
+        data_x = plot_helpers.parse_toi(spy_data, trl, show_kwargs)
     # that's all the methods we got so far
     else:
         raise NotImplementedError
 
-    is_tf = plot_helpers.check_if_time_freq(data)
+    is_tf = plot_helpers.check_if_time_freq(spy_data)
 
     # time dependent coherence
-    if method in ['coh', 'ppc'] and is_tf:
+    if method in ["coh", "ppc"] and is_tf:
         # here we always need a new axes
         fig, ax = _plotting.mk_img_figax()
 
-        time = plot_helpers.parse_toi(data, trl, show_kwargs)
-        freqs = plot_helpers.parse_foi(data, show_kwargs)
+        time = plot_helpers.parse_toi(spy_data, trl, show_kwargs)
+        freqs = plot_helpers.parse_foi(spy_data, show_kwargs)
 
         # custom dimords for SpectralData not supported atm
         # dimord is time x freq x channel_i x channel_j
         # need freq x time for plotting
-        data_yx = data.show(**show_kwargs).T
-        _plotting.plot_tfreq(ax, data_yx, time, freqs, cmap='cividis')
-        ax.set_title(f"{method}: " + label, fontsize=pltConfig['sTitleSize'])
+        data_yx = spy_data.show(**show_kwargs).T
+        _plotting.plot_tfreq(ax, data_yx, time, freqs, cmap="cividis")
+        ax.set_title(f"{method}: " + label, fontsize=pltConfig["sTitleSize"])
         fig.tight_layout()
 
         return fig, ax
 
     else:
         # get the data to plot
-        data_y = data.show(**show_kwargs)
+        data_y = spy_data.show(**show_kwargs)
         if data_y.size == 0:
             lgl = "Selection with non-zero size"
             act = f"{show_kwargs}, got zero samples"
             raise SPYValueError(lgl, varname="show_kwargs", actual=act)
 
         # create the axes and figure if needed
         # persistent axes allows for plotting different
         # channel combinations into the same figure
-        if not hasattr(data, 'fig') or not _plotting.ppl.fignum_exists(data.fig.number):
-            data.fig, data.ax = _plotting.mk_line_figax(xlabel, ylabel)
-        _plotting.plot_lines(data.ax, data_x, data_y, label=label)
+        if not hasattr(spy_data, "fig") or not _plotting.ppl.fignum_exists(spy_data.fig.number):
+            spy_data.fig, spy_data.ax = _plotting.mk_line_figax(xlabel, ylabel)
+        _plotting.plot_lines(spy_data.ax, data_x, data_y, label=label)
         # format axes
-        if method in ['granger', 'coh'] and output in ['pow', 'abs']:
-            data.ax.set_ylim((-.02, 1.02))
-        elif method == 'corr':
-            data.ax.set_ylim((-1.02, 1.02))
-        data.ax.legend(ncol=1)
+        if method in ["granger", "coh"] and output in ["pow", "abs"]:
+            spy_data.ax.set_ylim((-0.02, 1.02))
+        elif method == "corr":
+            spy_data.ax.set_ylim((-1.02, 1.02))
+        spy_data.ax.legend(ncol=1)
 
-        data.fig.tight_layout()
+        spy_data.fig.tight_layout()
 
-        return data.fig, data.ax
+        return spy_data.fig, spy_data.ax
```

### Comparing `esi_syncopy-2023.5/syncopy/plotting/spy_plotting.py` & `esi_syncopy-2023.7/syncopy/plotting/spy_plotting.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 # Top-level interfaces for the plotting functionality
 #
 
 from syncopy import __plt__
 from syncopy.plotting.config import pltErrMsg
 from syncopy.shared.errors import SPYWarning
 
-__all__ = ['singlepanelplot', 'multipanelplot']
+__all__ = ["singlepanelplot", "multipanelplot"]
 
 
 def singlepanelplot(data, **show_kwargs):
 
     """
     Plot Syncopy data in a single panel
```

### Comparing `esi_syncopy-2023.5/syncopy/preproc/compRoutines.py` & `esi_syncopy-2023.7/syncopy/preproc/compRoutines.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,36 +7,40 @@
 # Builtin/3rd party package imports
 import numpy as np
 import scipy.signal as sci
 import logging, platform
 from inspect import signature
 
 # syncopy imports
-from syncopy.shared.computational_routine import ComputationalRoutine, propagate_properties
+from syncopy.shared.computational_routine import (
+    ComputationalRoutine,
+    propagate_properties,
+)
 from syncopy.shared.const_def import spectralConversions, spectralDTypes
 from syncopy.shared.kwarg_decorators import process_io
 
 # backend imports
 from .firws import design_wsinc, apply_fir, minphaserceps
 from .resampling import downsample, resample
 
 
 @process_io
-def sinc_filtering_cF(dat,
-                      samplerate=1,
-                      filter_type='lp',
-                      freq=None,
-                      order=None,
-                      window="hamming",
-                      direction='onepass',
-                      polyremoval=None,
-                      timeAxis=0,
-                      noCompute=False,
-                      chunkShape=None
-                      ):
+def sinc_filtering_cF(
+    dat,
+    samplerate=1,
+    filter_type="lp",
+    freq=None,
+    order=None,
+    window="hamming",
+    direction="onepass",
+    polyremoval=None,
+    timeAxis=0,
+    noCompute=False,
+    chunkShape=None,
+):
     """
     Provides basic filtering of signals with FIR (windowed sinc)
     filters. Supported are low-pass, high-pass,
     band-pass and band-stop (Notch) filtering.
 
     dat : (N, K) :class:`numpy.ndarray`
         Uniformly sampled multi-channel time-series data
@@ -90,55 +94,55 @@
     Consequently, this function does **not** perform any error checking and operates
     under the assumption that all inputs have been externally validated and cross-checked.
 
     """
 
     # Re-arrange array if necessary and get dimensional information
     if timeAxis != 0:
-        dat = dat.T       # does not copy but creates view of `dat`
+        dat = dat.T  # does not copy but creates view of `dat`
     else:
         dat = dat
 
     # filtering does not change the shape
     outShape = dat.shape
     if noCompute:
         return outShape, np.float32
 
     # detrend
     if polyremoval == 0:
-        dat = sci.detrend(dat, type='constant', axis=0, overwrite_data=True)
+        dat = sci.detrend(dat, type="constant", axis=0, overwrite_data=True)
     elif polyremoval == 1:
-        dat = sci.detrend(dat, type='linear', axis=0, overwrite_data=True)
+        dat = sci.detrend(dat, type="linear", axis=0, overwrite_data=True)
 
     # max order is signal length
     if order is None:
         order = dat.shape[0]
 
     # construct the filter
     fkernel = design_wsinc(window, order, freq / samplerate, filter_type)
 
     # switch to time-domain convolutions if NaNs present
     if np.any(np.isnan(dat)):
-        method = 'direct'
+        method = "direct"
     else:
-        method = 'fft'
+        method = "fft"
     # to pass info to user
-    metadata = {'has_nan': np.array(method == 'direct')}
+    metadata = {"has_nan": np.array(method == "direct")}
 
     # filtering by convolution
-    if direction == 'onepass':
+    if direction == "onepass":
         filtered = apply_fir(dat, fkernel, method)
 
     # for symmetric filters actual
     # filter direction does NOT matter
-    elif direction == 'twopass':
+    elif direction == "twopass":
         filtered = apply_fir(dat, fkernel, method)
         filtered = apply_fir(filtered, fkernel, method)
 
-    elif direction == 'onepass-minphase':
+    elif direction == "onepass-minphase":
         # 0-phase transform
         fkernel = minphaserceps(fkernel)
         filtered = apply_fir(dat, fkernel, method)
 
     return filtered, metadata
 
 
@@ -164,25 +168,26 @@
 
     def process_metadata(self, data, out):
 
         propagate_properties(data, out)
 
 
 @process_io
-def but_filtering_cF(dat,
-                     samplerate=1,
-                     filter_type='lp',
-                     freq=None,
-                     order=6,
-                     direction='twopass',
-                     polyremoval=None,
-                     timeAxis=0,
-                     noCompute=False,
-                     chunkShape=None
-                     ):
+def but_filtering_cF(
+    dat,
+    samplerate=1,
+    filter_type="lp",
+    freq=None,
+    order=6,
+    direction="twopass",
+    polyremoval=None,
+    timeAxis=0,
+    noCompute=False,
+    chunkShape=None,
+):
     """
     Provides basic filtering of signals with IIR (Butterworth)
     filters. Supported are low-pass, high-pass,
     band-pass and band-stop (Notch) filtering.
 
     dat : (N, K) :class:`numpy.ndarray`
         Uniformly sampled multi-channel time-series data
@@ -233,42 +238,42 @@
     --------
     `Scipy butterworth documentation <https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html>`_
 
     """
 
     # Re-arrange array if necessary and get dimensional information
     if timeAxis != 0:
-        dat = dat.T       # does not copy but creates view of `dat`
+        dat = dat.T  # does not copy but creates view of `dat`
     else:
         dat = dat
 
     # filtering does not change the shape
     outShape = dat.shape
     if noCompute:
         return outShape, np.float32
 
     # we can't do anything here, but at least
     # collect this information to pass back to user
-    metadata = {'has_nan': np.array(np.any(np.isnan(dat)))}
+    metadata = {"has_nan": np.array(np.any(np.isnan(dat)))}
 
     # detrend
     if polyremoval == 0:
-        dat = sci.detrend(dat, type='constant', axis=0, overwrite_data=True)
+        dat = sci.detrend(dat, type="constant", axis=0, overwrite_data=True)
     elif polyremoval == 1:
-        dat = sci.detrend(dat, type='linear', axis=0, overwrite_data=True)
+        dat = sci.detrend(dat, type="linear", axis=0, overwrite_data=True)
 
     # design the butterworth filter with "second-order-sections" output
-    sos = sci.butter(order, freq, filter_type, fs=samplerate, output='sos')
+    sos = sci.butter(order, freq, filter_type, fs=samplerate, output="sos")
 
     # do the filtering
-    if direction == 'twopass':
+    if direction == "twopass":
         filtered = sci.sosfiltfilt(sos, dat, axis=0)
         return filtered, metadata
 
-    elif direction == 'onepass':
+    elif direction == "onepass":
         filtered = sci.sosfilt(sos, dat, axis=0)
         return filtered, metadata
 
 
 class ButFiltering(ComputationalRoutine):
 
     """
@@ -353,15 +358,15 @@
 
     def process_metadata(self, data, out):
 
         propagate_properties(data, out)
 
 
 @process_io
-def hilbert_cF(dat, output='abs', timeAxis=0, noCompute=False, chunkShape=None):
+def hilbert_cF(dat, output="abs", timeAxis=0, noCompute=False, chunkShape=None):
 
     """
     Provides Hilbert transformation with various outputs, band-pass filtering
     beforehand highly recommended.
 
     dat : (N, K) :class:`numpy.ndarray`
         Uniformly sampled multi-channel time-series data
@@ -389,22 +394,22 @@
     Consequently, this function does **not** perform any error checking and operates
     under the assumption that all inputs have been externally validated and cross-checked.
 
     """
 
     # Re-arrange array if necessary and get dimensional information
     if timeAxis != 0:
-        dat = dat.T       # does not copy but creates view of `dat`
+        dat = dat.T  # does not copy but creates view of `dat`
     else:
         dat = dat
 
     # operation does not change the shape
     # but may change the number format
     outShape = dat.shape
-    fmt = spectralDTypes["fourier"] if output == 'complex' else spectralDTypes["abs"]
+    fmt = spectralDTypes["fourier"] if output == "complex" else spectralDTypes["abs"]
     if noCompute:
         return outShape, fmt
 
     logger = logging.getLogger("syncopy_" + platform.node())
     logger.debug(f"Computing Hilbert transformation on data chunk with shape {dat.shape} along axis 0.")
 
     trafo = sci.hilbert(dat, axis=0)
@@ -434,21 +439,15 @@
 
     def process_metadata(self, data, out):
 
         propagate_properties(data, out)
 
 
 @process_io
-def downsample_cF(dat,
-                  samplerate=1,
-                  new_samplerate=1,
-                  timeAxis=0,
-                  chunkShape=None,
-                  noCompute=False
-                  ):
+def downsample_cF(dat, samplerate=1, new_samplerate=1, timeAxis=0, chunkShape=None, noCompute=False):
     """
     Provides basic downsampling of signals. The `new_samplerate` should be
     an integer division of the original `samplerate`.
 
     dat : (N, K) :class:`numpy.ndarray`
         Uniformly sampled multi-channel time-series data
         The 1st dimension is interpreted as the time axis,
@@ -474,27 +473,29 @@
     Consequently, this function does **not** perform any error checking and operates
     under the assumption that all inputs have been externally validated and cross-checked.
 
     """
 
     # Re-arrange array if necessary and get dimensional information
     if timeAxis != 0:
-        dat = dat.T       # does not copy but creates view of `dat`
+        dat = dat.T  # does not copy but creates view of `dat`
     else:
         dat = dat
 
     if noCompute:
         # we need integers for slicing
         skipped = int(samplerate // new_samplerate)
         outShape = list(dat.shape)
         outShape[0] = int(np.ceil(dat.shape[0] / skipped))
         return tuple(outShape), dat.dtype
 
     logger = logging.getLogger("syncopy_" + platform.node())
-    logger.debug(f"Downsampling data chunk with shape {dat.shape} from samplerate {samplerate} to {new_samplerate}.")
+    logger.debug(
+        f"Downsampling data chunk with shape {dat.shape} from samplerate {samplerate} to {new_samplerate}."
+    )
 
     resampled = downsample(dat, samplerate, new_samplerate)
 
     return resampled
 
 
 class Downsample(ComputationalRoutine):
@@ -517,39 +518,40 @@
     # 1st argument,the data, gets omitted
     valid_kws = list(signature(downsample_cF).parameters.keys())[1:]
 
     def process_metadata(self, data, out):
 
         # we need to re-calculate the downsampling factor
         # that it actually is an 1 / integer gets checked in the frontend
-        factor = self.cfg['new_samplerate'] / data.samplerate
+        factor = self.cfg["new_samplerate"] / data.samplerate
 
         if data.selection is not None:
             chanSec = data.selection.channel
             trl = _resampling_trl_definition(data.selection.trialdefinition, factor)
         else:
             chanSec = slice(None)
             trl = _resampling_trl_definition(data.trialdefinition, factor)
 
         out.trialdefinition = trl
         # now set new samplerate
-        out.samplerate = self.cfg['new_samplerate']
+        out.samplerate = self.cfg["new_samplerate"]
         out.channel = np.array(data.channel[chanSec])
 
 
 @process_io
-def resample_cF(dat,
-                samplerate=1,
-                new_samplerate=1,
-                lpfreq=None,
-                order=None,
-                timeAxis=0,
-                chunkShape=None,
-                noCompute=False
-                ):
+def resample_cF(
+    dat,
+    samplerate=1,
+    new_samplerate=1,
+    lpfreq=None,
+    order=None,
+    timeAxis=0,
+    chunkShape=None,
+    noCompute=False,
+):
     """
     Provides resampling of signals. The `new_samplerate` can be
     any (rational) factor of the original `samplerate`.
 
     For the anti-aliasing an explicit low-pass firws filter
     is constructed. Either implicitly with `lpfreq=None`
     which takes the new Nyquist (new_samplerate / 2) as cut-off
@@ -587,33 +589,31 @@
     Consequently, this function does **not** perform any error checking and operates
     under the assumption that all inputs have been externally validated and cross-checked.
 
     """
 
     # Re-arrange array if necessary and get dimensional information
     if timeAxis != 0:
-        dat = dat.T       # does not copy but creates view of `dat`
+        dat = dat.T  # does not copy but creates view of `dat`
     else:
         dat = dat
 
     nSamples = dat.shape[0]
     fs_ratio = new_samplerate / samplerate
 
     if noCompute:
         new_nSamples = int(np.ceil(nSamples * fs_ratio))
         return (new_nSamples, dat.shape[1]), dat.dtype
 
     logger = logging.getLogger("syncopy_" + platform.node())
-    logger.debug(f"Resampling data chunk with shape {dat.shape} from samplerate {samplerate} to {new_samplerate} with lpfreq={lpfreq}, order={order}.")
+    logger.debug(
+        f"Resampling data chunk with shape {dat.shape} from samplerate {samplerate} to {new_samplerate} with lpfreq={lpfreq}, order={order}."
+    )
 
-    resampled = resample(dat,
-                         samplerate,
-                         new_samplerate,
-                         lpfreq=lpfreq,
-                         order=order)
+    resampled = resample(dat, samplerate, new_samplerate, lpfreq=lpfreq, order=order)
 
     return resampled
 
 
 class Resample(ComputationalRoutine):
 
     """
@@ -633,28 +633,28 @@
 
     # 1st argument,the data, gets omitted
     valid_kws = list(signature(downsample_cF).parameters.keys())[1:]
 
     def process_metadata(self, data, out):
 
         # we need to re-calculate the resampling factor
-        factor = self.cfg['new_samplerate'] / data.samplerate
+        factor = self.cfg["new_samplerate"] / data.samplerate
         trafo_trl = _resampling_trl_definition
 
         if data.selection is not None:
             chanSec = data.selection.channel
             trl = trafo_trl(data.selection.trialdefinition, factor)
         else:
             chanSec = slice(None)
             trl = trafo_trl(data.trialdefinition, factor)
 
         out.trialdefinition = trl
 
         # now set new samplerate
-        out.samplerate = self.cfg['new_samplerate']
+        out.samplerate = self.cfg["new_samplerate"]
         out.channel = np.array(data.channel[chanSec])
 
 
 @process_io
 def detrending_cF(dat, polyremoval=None, timeAxis=0, noCompute=False, chunkShape=None):
 
     """
@@ -697,43 +697,43 @@
 
     # should be captured in frontend
     if polyremoval is None:
         return dat
 
     # Re-arrange array if necessary and get dimensional information
     if timeAxis != 0:
-        dat = dat.T       # does not copy but creates view of `dat`
+        dat = dat.T  # does not copy but creates view of `dat`
     else:
         dat = dat
 
     # detrending does not change the shape
     outShape = dat.shape
     if noCompute:
         return outShape, np.float32
 
     logger = logging.getLogger("syncopy_" + platform.node())
     logger.debug(f"Detrending data chunk with shape {dat.shape} with polyremoval={polyremoval}.")
 
     # we can't do anything here, but at least
     # collect this information to pass back to user
     has_nan = np.array(np.any(np.isnan(dat)))
-    metadata = {'has_nan': has_nan}
+    metadata = {"has_nan": has_nan}
 
     # demeaning, this 'works' with NaNs (all nan come back)
     if polyremoval == 0:
-        dat = sci.detrend(dat, type='constant', axis=0, overwrite_data=True)
+        dat = sci.detrend(dat, type="constant", axis=0, overwrite_data=True)
     elif polyremoval == 1 and not has_nan:
-        dat = sci.detrend(dat, type='linear', axis=0, overwrite_data=True)
+        dat = sci.detrend(dat, type="linear", axis=0, overwrite_data=True)
     # here we have to nan-all the offending channels ourselves
     elif polyremoval == 1 and has_nan:
         nan_col_sum = np.sum(np.isnan(dat), axis=0)
         nan_cols = np.where(nan_col_sum)[0]
         ok_cols = np.where(nan_col_sum == 0)[0]
         dat[:, nan_cols] = np.nan
-        dat[:, ok_cols] = sci.detrend(dat[:, ok_cols], type='linear', axis=0, overwrite_data=True)
+        dat[:, ok_cols] = sci.detrend(dat[:, ok_cols], type="linear", axis=0, overwrite_data=True)
 
     # renaming
     detrended = dat
     return detrended, metadata
 
 
 class Detrending(ComputationalRoutine):
@@ -800,28 +800,28 @@
     Thus, input parameters are presumed to be forwarded from a parent metafunction.
     Consequently, this function does **not** perform any error checking and operates
     under the assumption that all inputs have been externally validated and cross-checked.
     """
 
     # Re-arrange array if necessary and get dimensional information
     if timeAxis != 0:
-        dat = dat.T       # does not copy but creates view of `dat`
+        dat = dat.T  # does not copy but creates view of `dat`
     else:
         dat = dat
 
     # cF does not change the shape
     outShape = dat.shape
     if noCompute:
         return outShape, np.float32
 
     # detrend
     if polyremoval == 0:
-        dat = sci.detrend(dat, type='constant', axis=0, overwrite_data=True)
+        dat = sci.detrend(dat, type="constant", axis=0, overwrite_data=True)
     elif polyremoval == 1:
-        dat = sci.detrend(dat, type='linear', axis=0, overwrite_data=True)
+        dat = sci.detrend(dat, type="linear", axis=0, overwrite_data=True)
 
     logger = logging.getLogger("syncopy_" + platform.node())
     logger.debug(f"Standardizing data chunk with shape {dat.shape} (prior polyremoval was {polyremoval}).")
 
     # standardize
     dat = (dat - np.mean(dat, axis=0)) / np.std(dat, axis=0)
```

### Comparing `esi_syncopy-2023.5/syncopy/preproc/firws.py` & `esi_syncopy-2023.7/syncopy/preproc/firws.py`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 
 # Builtin/3rd party package imports
 import numpy as np
 import scipy.signal.windows as sci_win
 from scipy.signal import convolve
 
 
-def apply_fir(data, fkernel, method='fft'):
+def apply_fir(data, fkernel, method="fft"):
 
     """
     Convolution of the input `data` with a FIR filter.
     The filter's impulse response is given by `fkernel`.
 
     Parameters
     ----------
@@ -180,31 +180,27 @@
        https://ccrma.stanford.edu/~jos/fp/Matlab_listing_mps_m.html
     .. [2] Vetter, K. (2013, Nov 11). Long FIR filters with low latency.
        Retrieved Nov 11 2013, from
        http://www.katjaas.nl/minimumphase/minimumphase.html
     """
 
     nSamples = len(fkernel)
-    upsamplingFactor = (
-        1e3  # Impulse response upsampling/zero padding to reduce time-aliasing
-    )
+    upsamplingFactor = 1e3  # Impulse response upsampling/zero padding to reduce time-aliasing
     nFFT = int(2 ** np.ceil(np.log2(nSamples * upsamplingFactor)))  # Power of 2
     clipThresh = 1e-8  # -160 dB
 
     # Spectrum
     specC = np.abs(np.fft.fft(fkernel, nFFT))
     specC[specC < clipThresh] = clipThresh  # Clip spectrum to reduce time-aliasing
 
     # Real cepstrum
     specR = np.real(np.fft.ifft(np.log(specC)))
 
     # Convolve
-    ires = np.hstack([specR[1 : nFFT // 2], 0]) + np.conj(
-        specR[nFFT // 2 : nFFT + 1][::-1]
-    )
+    ires = np.hstack([specR[1 : nFFT // 2], 0]) + np.conj(specR[nFFT // 2 : nFFT + 1][::-1])
     specR = np.hstack([specR[0], ires, np.zeros(nFFT // 2 - 2)])
 
     # Minimum phase
     MinPhase = np.real(np.fft.ifft(np.exp(np.fft.fft(specR))))
 
     # Remove zero-padding
     return MinPhase[:nSamples]
```

### Comparing `esi_syncopy-2023.5/syncopy/preproc/preprocessing.py` & `esi_syncopy-2023.7/syncopy/preproc/preprocessing.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,15 +24,15 @@
 
 from .compRoutines import (
     ButFiltering,
     SincFiltering,
     Rectify,
     Hilbert,
     Detrending,
-    Standardize
+    Standardize,
 )
 
 availableFilters = ("but", "firws")
 availableFilterTypes = ("lp", "hp", "bp", "bs")
 availableDirections = ("twopass", "onepass", "onepass-minphase")
 availableWindows = ("hamming", "hann", "blackman")
 
@@ -44,15 +44,15 @@
 @detect_parallel_client
 def preprocessing(
     data,
     filter_class="but",
     filter_type="lp",
     freq=None,
     order=None,
-    direction='twopass',
+    direction="twopass",
     window="hamming",
     polyremoval=None,
     zscore=False,
     rectify=False,
     hilbert=False,
     **kwargs,
 ):
@@ -120,17 +120,15 @@
 
     """
 
     # -- Basic input parsing --
 
     # Make sure our one mandatory input object can be processed
     try:
-        data_parser(
-            data, varname="data", dataclass="AnalogData", writable=None, empty=False
-        )
+        data_parser(data, varname="data", dataclass="AnalogData", writable=None, empty=False)
     except Exception as exc:
         raise exc
     timeAxis = data.dimord.index("time")
 
     # Get everything of interest in local namespace
     defaults = get_defaults(preprocessing)
     lcls = locals()
@@ -216,34 +214,29 @@
         if hilbert not in hilbert_outputs:
             lgl = f"one of {hilbert_outputs}"
             raise SPYValueError(lgl, varname="hilbert", actual=hilbert)
 
     # -- Method calls
 
     # Prepare keyword dict for logging
-    log_dict = {
-        "polyremoval": polyremoval,
-        "zscore": zscore
-    }
+    log_dict = {"polyremoval": polyremoval, "zscore": zscore}
 
     # pre-processing
     if zscore:
 
         std_data = AnalogData(dimord=data.dimord)
         stdCR = Standardize(polyremoval=polyremoval, timeAxis=timeAxis)
         stdCR.initialize(
             data,
             data._stackingDim,
             chan_per_worker=kwargs.get("chan_per_worker"),
             keeptrials=True,
         )
 
-        stdCR.compute(
-            data, std_data, parallel=kwargs.get("parallel"), log_dict=log_dict
-        )
+        stdCR.compute(data, std_data, parallel=kwargs.get("parallel"), log_dict=log_dict)
 
         data = std_data
 
     if filter_class == "but":
 
         if window != defaults["window"] and window is not None:
             lgl = "no `window` setting for IIR filtering"
@@ -266,18 +259,15 @@
 
         log_dict["order"] = order
         log_dict["direction"] = direction
         log_dict["filter_class"] = filter_class
         log_dict["filter_type"] = filter_type
         log_dict["freq"] = freq
 
-        check_effective_parameters(
-            ButFiltering, defaults, lcls, besides=("hilbert", "rectify",
-                                                   "zscore")
-        )
+        check_effective_parameters(ButFiltering, defaults, lcls, besides=("hilbert", "rectify", "zscore"))
 
         filterMethod = ButFiltering(
             samplerate=data.samplerate,
             filter_type=filter_type,
             freq=freq,
             order=order,
             direction=direction,
@@ -357,71 +347,65 @@
         # Perform actual computation
         filterMethod.initialize(
             data,
             data._stackingDim,
             chan_per_worker=kwargs.get("chan_per_worker"),
             keeptrials=True,
         )
-        filterMethod.compute(
-            data, filtered, parallel=kwargs.get("parallel"), log_dict=log_dict
-        )
+        filterMethod.compute(data, filtered, parallel=kwargs.get("parallel"), log_dict=log_dict)
 
         # give warnings if NaNs were present
         nan_trials = []
         for key, value in metadata_from_hdf5_file(filtered.filename).items():
-            if 'has_nan' in key and value:
+            if "has_nan" in key and value:
                 # try to also record the trial numbers
-                trl_num = key.split('__')[-1].split('_')[0]
+                trl_num = key.split("__")[-1].split("_")[0]
                 nan_trials.append(int(trl_num))
 
         if len(nan_trials) != 0:
             msg = "Data contains NaNs! See `.info['nan_trials']` for the offending trials"
-            if filter_class == 'but':
+            if filter_class == "but":
                 msg += "\n\t\t try using a 'onepass' FIR filter of low order.."
             SPYWarning(msg)
-        filtered.info['nan_trials'] = nan_trials
+        filtered.info["nan_trials"] = nan_trials
 
     # -- check for post-processing flags --
 
     if rectify:
         log_dict["rectify"] = rectify
         rectified = AnalogData(dimord=data.dimord)
         rectCR = Rectify()
         rectCR.initialize(
             filtered,
             data._stackingDim,
             chan_per_worker=kwargs.get("chan_per_worker"),
             keeptrials=True,
         )
-        rectCR.compute(
-            filtered, rectified, parallel=kwargs.get("parallel"), log_dict=log_dict
-        )
+        rectCR.compute(filtered, rectified, parallel=kwargs.get("parallel"), log_dict=log_dict)
         del filtered
         rectified.cfg.update(data.cfg)
-        rectified.cfg.update({'preprocessing': new_cfg})
+        rectified.cfg.update({"preprocessing": new_cfg})
         return rectified
 
     elif hilbert:
         log_dict["hilbert"] = hilbert
         htrafo = AnalogData(dimord=data.dimord)
         hilbertCR = Hilbert(output=hilbert, timeAxis=timeAxis)
         hilbertCR.initialize(
             filtered,
             data._stackingDim,
             chan_per_worker=kwargs.get("chan_per_worker"),
             keeptrials=True,
         )
-        hilbertCR.compute(
-            filtered, htrafo, parallel=kwargs.get("parallel"), log_dict=log_dict
-        )
+        hilbertCR.compute(filtered, htrafo, parallel=kwargs.get("parallel"), log_dict=log_dict)
         del filtered
         htrafo.cfg.update(data.cfg)
-        htrafo.cfg.update({'preprocessing': new_cfg})
+        htrafo.cfg.update({"preprocessing": new_cfg})
         return htrafo
 
     # no post-processing
     else:
         # attach potential older cfg's from the input
         # to support chained frontend calls..
         filtered.cfg.update(data.cfg)
-        filtered.cfg.update({'preprocessing': new_cfg})
+        filtered.cfg.update({"preprocessing": new_cfg})
         return filtered
```

### Comparing `esi_syncopy-2023.5/syncopy/preproc/resampledata.py` & `esi_syncopy-2023.7/syncopy/preproc/resampledata.py`

 * *Files 5% similar despite different names*

```diff
@@ -24,20 +24,15 @@
 
 availableMethods = ("downsample", "resample")
 
 
 @unwrap_cfg
 @unwrap_select
 @detect_parallel_client
-def resampledata(data,
-                 resamplefs=1.,
-                 method="resample",
-                 lpfreq=None,
-                 order=None,
-                 **kwargs):
+def resampledata(data, resamplefs=1.0, method="resample", lpfreq=None, order=None, **kwargs):
     """
     Performs resampling or downsampling of :class:`~syncopy.AnalogData` objects,
     representing uniformly sampled time series data.
 
     Two methods are supported:
 
     "downsample" : Take every nth sample
@@ -97,17 +92,15 @@
 
     if method not in availableMethods:
         lgl = "'" + "or '".join(opt + "' " for opt in availableMethods)
         raise SPYValueError(legal=lgl, varname="method", actual=method)
 
     # Make sure our one mandatory input object can be processed
     try:
-        data_parser(
-            data, varname="data", dataclass="AnalogData", writable=None, empty=False
-        )
+        data_parser(data, varname="data", dataclass="AnalogData", writable=None, empty=False)
     except Exception as exc:
         raise exc
     timeAxis = data.dimord.index("time")
 
     # if a subset selection is present
     # get sampleinfo and check for equidistancy
     if data.selection is not None:
@@ -128,17 +121,18 @@
     # check resampling frequency
     scalar_parser(resamplefs, varname="resamplefs", lims=[1, data.samplerate])
 
     # filter order
     if order is not None:
         scalar_parser(order, varname="order", lims=[0, np.inf], ntype="int_like")
         if order < 100:
-            msg = ("You have chosen an anti-alias filter of very low "
-                   f"`order={order}`, expect a slow roll-off!"
-                   )
+            msg = (
+                "You have chosen an anti-alias filter of very low "
+                f"`order={order}`, expect a slow roll-off!"
+            )
             SPYWarning(msg)
 
     # set default
     else:
         order = int(lenTrials.min()) if lenTrials.min() < 1000 else 1000
 
     # check for anti-alias low-pass filter settings
@@ -146,101 +140,98 @@
     if lpfreq is not None:
         scalar_parser(lpfreq, varname="lpfreq", lims=[0, resamplefs / 2])
 
     # -- downsampling --
     if method == "downsample":
 
         if data.samplerate % resamplefs != 0:
-            lgl = (
-                "integer division of the original sampling rate "
-                "for `method='downsample'`"
-            )
+            lgl = "integer division of the original sampling rate " "for `method='downsample'`"
             raise SPYValueError(lgl, varname="resamplefs", actual=resamplefs)
 
         # explicit low-pass filtering on the fly
         if lpfreq is not None:
             AntiAliasFilter = SincFiltering(
                 samplerate=data.samplerate,
-                filter_type='lp',
+                filter_type="lp",
                 freq=lpfreq,
                 order=order,
-                direction='twopass',
+                direction="twopass",
                 timeAxis=timeAxis,
             )
             # keyword dict for logging
-            aa_log_dict = {"filter_type": 'lp',
-                           "lpfreq": lpfreq,
-                           "order": order,
-                           "direction": 'twopass'}
+            aa_log_dict = {
+                "filter_type": "lp",
+                "lpfreq": lpfreq,
+                "order": order,
+                "direction": "twopass",
+            }
 
         else:
             AntiAliasFilter = None
 
-        resampleMethod = Downsample(
-            samplerate=data.samplerate, new_samplerate=resamplefs, timeAxis=timeAxis
-        )
+        resampleMethod = Downsample(samplerate=data.samplerate, new_samplerate=resamplefs, timeAxis=timeAxis)
         # keyword dict for logging
-        log_dict = {"method": method,
-                    "resamplefs": resamplefs,
-                    "origfs": data.samplerate}
+        log_dict = {
+            "method": method,
+            "resamplefs": resamplefs,
+            "origfs": data.samplerate,
+        }
 
     # -- resampling --
     elif method == "resample":
 
         if data.samplerate % resamplefs == 0:
-            msg = ("New sampling rate is integeger division of the "
-                   "original sampling rate, consider using `method='downsample'`"
-                   )
+            msg = (
+                "New sampling rate is integeger division of the "
+                "original sampling rate, consider using `method='downsample'`"
+            )
             SPYWarning(msg)
 
         # has anti-alias filtering included
         # configured by lpfreq and order
         resampleMethod = Resample(
             samplerate=data.samplerate,
             new_samplerate=resamplefs,
             lpfreq=lpfreq,
             order=order,
-            timeAxis=timeAxis
+            timeAxis=timeAxis,
         )
         # keyword dict for logging
-        log_dict = {"method": method,
-                    "resamplefs": resamplefs,
-                    "origfs": data.samplerate,
-                    "lpfreq": lpfreq,
-                    "order": order}
+        log_dict = {
+            "method": method,
+            "resamplefs": resamplefs,
+            "origfs": data.samplerate,
+            "lpfreq": lpfreq,
+            "order": order,
+        }
 
     # ------------------------------------
     # Call the chosen ComputationalRoutine
     # ------------------------------------
 
     resampled = AnalogData(dimord=data.dimord)
 
-    if method == 'downsample' and AntiAliasFilter is not None:
+    if method == "downsample" and AntiAliasFilter is not None:
         filtered = AnalogData(dimord=data.dimord)
         AntiAliasFilter.initialize(
             data,
             filtered._stackingDim,
             chan_per_worker=kwargs.get("chan_per_worker"),
-            keeptrials=True
+            keeptrials=True,
         )
 
-        AntiAliasFilter.compute(data,
-                                filtered,
-                                parallel=kwargs.get("parallel"),
-                                log_dict=aa_log_dict)
+        AntiAliasFilter.compute(data, filtered, parallel=kwargs.get("parallel"), log_dict=aa_log_dict)
         target = filtered
     else:
         target = data  # just rebinds the name
 
     resampleMethod.initialize(
         target,
         resampled._stackingDim,
         chan_per_worker=kwargs.get("chan_per_worker"),
         keeptrials=True,
     )
-    resampleMethod.compute(
-        target, resampled, parallel=kwargs.get("parallel"), log_dict=log_dict
-    )
+    resampleMethod.compute(target, resampled, parallel=kwargs.get("parallel"), log_dict=log_dict)
 
     resampled.cfg.update(data.cfg)
-    resampled.cfg.update({'resampledata': new_cfg})
+    resampled.cfg.update({"resampledata": new_cfg})
     return resampled
```

### Comparing `esi_syncopy-2023.5/syncopy/preproc/resampling.py` & `esi_syncopy-2023.7/syncopy/preproc/resampling.py`

 * *Files 2% similar despite different names*

```diff
@@ -74,30 +74,28 @@
     if order is None:
         order = nSamples * up
         # limit maximal order
         order = 10000 if order > 10000 else order
 
     if f_c:
         # filter has to be applied to the upsampled data
-        window = firws.design_wsinc("hamming",
-                                    order=order,
-                                    f_c=f_c / up)
+        window = firws.design_wsinc("hamming", order=order, f_c=f_c / up)
     else:
-        window = ('kaiser', 5.0)  # triggers SciPy default filter design
+        window = ("kaiser", 5.0)  # triggers SciPy default filter design
 
     resampled = sci_sig.resample_poly(data, up, down, window=window, axis=0)
 
     return resampled
 
 
 def downsample(
     dat,
     samplerate=1,
     new_samplerate=1,
-    ):
+):
     """
     Provides basic downsampling of signals. The `new_samplerate` should be
     an integer division of the original `samplerate`.
 
     Parameters
     ----------
     dat : (N, K) :class:`numpy.ndarray`
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/__init__.py` & `esi_syncopy-2023.7/syncopy/shared/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 # -*- coding: utf-8 -*-
 #
 # Import utility functions mainly used internally
 #
 
 # Import __all__ routines from local modules
-from . import (queries, errors, parsers, kwarg_decorators,
-               computational_routine, tools)
+from . import queries, errors, parsers, kwarg_decorators, computational_routine, tools
 from .queries import *
 from .errors import *
 from .parsers import *
 from .kwarg_decorators import *
 from .computational_routine import *
 from .tools import *
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/computational_routine.py` & `esi_syncopy-2023.7/syncopy/shared/computational_routine.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,31 +9,40 @@
 import psutil
 import h5py
 import numpy as np
 from itertools import chain
 from abc import ABC, abstractmethod
 from copy import copy
 from tqdm.auto import tqdm
+
 if sys.platform == "win32":
     # tqdm breaks term colors on Windows - fix that (tqdm issue #446)
     import colorama
+
     colorama.deinit()
     colorama.init(strip=False)
 
 import dask.distributed as dd
 import dask_jobqueue as dj
 
 # Local imports
 import syncopy as spy
 from .tools import get_defaults
 from .dask_helpers import check_slurm_available
 from syncopy import __storage__, __acme__
-from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYParallelError, SPYWarning
+from syncopy.shared.errors import (
+    SPYValueError,
+    SPYTypeError,
+    SPYParallelError,
+    SPYWarning,
+)
+
 if __acme__:
     from acme import ParallelMap
+
     # # In case of problems w/worker-stealing, uncomment the following lines
     # import dask
     # dask.config.set(distributed__scheduler__work_stealing=False)
 
 from syncopy.shared.metadata import parse_cF_returns, h5_add_metadata
 
 __all__ = []
@@ -282,20 +291,19 @@
         dryRunKwargs = copy(self.cfg)
         dryRunKwargs["noCompute"] = True
         chk_list = []
         dtp_list = []
         trials = []
         for tk, trialno in enumerate(self.trialList):
             trial = data._preview_trial(trialno)
-            trlArg = tuple(arg[tk] if isinstance(arg, (list, tuple, np.ndarray)) and
-                           len(arg) == self.numTrials
-                           else arg for arg in self.argv)
-            chunkShape, dtype = self.computeFunction(trial,
-                                                     *trlArg,
-                                                     **dryRunKwargs)
+            trlArg = tuple(
+                arg[tk] if isinstance(arg, (list, tuple, np.ndarray)) and len(arg) == self.numTrials else arg
+                for arg in self.argv
+            )
+            chunkShape, dtype = self.computeFunction(trial, *trlArg, **dryRunKwargs)
             chk_list.append(list(chunkShape))
             dtp_list.append(dtype)
             trials.append(trial)
 
         # Determine trial stacking dimension and compute aggregate shape of output
         stackingDim = out_stackingdim
         totalSize = sum(cShape[stackingDim] for cShape in chk_list)
@@ -328,24 +336,26 @@
             chan_per_worker = None
         if chan_per_worker is not None and self.keeptrials is False:
             msg = "trial-averaging does not support channel-block parallelization!"
             SPYWarning(msg)
             chan_per_worker = None
         if data.selection is not None:
             if chan_per_worker is not None and data.selection.channel != slice(None, None, 1):
-                msg = "channel selection and simultaneous channel-block " +\
-                    "parallelization not yet supported!"
+                msg = (
+                    "channel selection and simultaneous channel-block " + "parallelization not yet supported!"
+                )
                 SPYWarning(msg)
                 chan_per_worker = None
 
         # Allocate control variables
         trial = trials[0]
-        trlArg0 = tuple(arg[0] if isinstance(arg, (list, tuple, np.ndarray)) and
-                        len(arg) == self.numTrials
-                        else arg for arg in self.argv)
+        trlArg0 = tuple(
+            arg[0] if isinstance(arg, (list, tuple, np.ndarray)) and len(arg) == self.numTrials else arg
+            for arg in self.argv
+        )
         chunkShape0 = tuple(chk_arr[0, :])
         lyt = [slice(0, stop) for stop in chunkShape0]
         sourceLayout = []
         sourceShapes = []
         targetLayout = []
         targetShapes = []
         c_blocks = [1]
@@ -402,17 +412,18 @@
             sourceLayout.append(trial.idx)
             sourceShapes.append(trial.shape)
 
         # Construct dimensional layout of output and append remaining trials to input layout
         stacking = targetLayout[0][stackingDim].stop
         for tk in range(1, self.numTrials):
             trial = trials[tk]
-            trlArg = tuple(arg[tk] if isinstance(arg, (list, tuple, np.ndarray)) and
-                           len(arg) == self.numTrials
-                           else arg for arg in self.argv)
+            trlArg = tuple(
+                arg[tk] if isinstance(arg, (list, tuple, np.ndarray)) and len(arg) == self.numTrials else arg
+                for arg in self.argv
+            )
             chkshp = chk_list[tk]
             lyt = [slice(0, stop) for stop in chkshp]
             lyt[stackingDim] = slice(stacking, stacking + chkshp[stackingDim])
             stacking += chkshp[stackingDim]
             if chan_per_worker is None:
                 targetLayout.append(tuple(lyt))
                 targetShapes.append(tuple([slc.stop - slc.start for slc in lyt]))
@@ -424,15 +435,15 @@
                 for block in c_blocks:
                     shp = list(trial.shape)
                     idx = list(trial.idx)
                     shp[inchanidx] = block
                     idx[inchanidx] = slice(blockstack, blockstack + block)
                     trial.shape = tuple(shp)
                     trial.idx = tuple(idx)
-                    res, _ = self.computeFunction(trial, *trlArg, **dryRunKwargs)   # FauxTrial
+                    res, _ = self.computeFunction(trial, *trlArg, **dryRunKwargs)  # FauxTrial
                     lyt[outchanidx] = slice(chanstack, chanstack + res[outchanidx])
                     targetLayout.append(tuple(lyt))
                     targetShapes.append(tuple([slc.stop - slc.start for slc in lyt]))
                     sourceLayout.append(trial.idx)
                     sourceShapes.append(trial.shape)
                     chanstack += res[outchanidx]
                     blockstack += block
@@ -448,29 +459,30 @@
         # In this case `sourceLayout` uses ABSOLUTE indices (indices wrt to size
         # of ENTIRE DATASET) that are SORTED W/O REPS to extract a NumPy array
         # of appropriate size from HDF5.
         # Then `sourceSelectors` uses RELATIVE indices (indices wrt to size of CURRENT
         # TRIAL) that can be UNSORTED W/REPS to actually perform the requested
         # selection on the NumPy array extracted w/`sourceLayout`.
         for grd in sourceLayout:
-            if any([np.diff(sel).min() <= 0 if isinstance(sel, list)
-                    and len(sel) > 1 else False for sel in grd]):
+            if any(
+                [np.diff(sel).min() <= 0 if isinstance(sel, list) and len(sel) > 1 else False for sel in grd]
+            ):
                 self.useFancyIdx = True
                 break
         if self.useFancyIdx:
             sourceSelectors = []
             for gk, grd in enumerate(sourceLayout):
                 ingrid = list(grd)
                 sigrid = []
                 for sk, sel in enumerate(grd):
                     if np.issubdtype(type(sel), np.number):
                         sel = [sel]
                     if isinstance(sel, list):
                         selarr = np.array(sel, dtype=np.intp)
-                    else:   # sel is a slice
+                    else:  # sel is a slice
                         step = sel.step
                         if sel.step is None:
                             step = 1
                         selarr = np.array(list(range(sel.start, sel.stop, step)), dtype=np.intp)
                     if selarr.size > 0:
                         sigrid.append(np.array(selarr) - selarr.min())
                         ingrid[sk] = slice(selarr.min(), selarr.max() + 1, 1)
@@ -494,16 +506,25 @@
             self.chunkMem = np.prod(self.cfg["chunkShape"]) * self.dtype.itemsize
         else:
             self.chunkMem = max([np.prod(shp) for shp in self.targetShapes]) * self.dtype.itemsize
 
         # Get data access mode (only relevant for parallel reading access)
         self.dataMode = data.mode
 
-    def compute(self, data, out, parallel=False, parallel_store=None,
-                method=None, mem_thresh=0.5, log_dict=None, parallel_debug=False):
+    def compute(
+        self,
+        data,
+        out,
+        parallel=False,
+        parallel_store=None,
+        method=None,
+        mem_thresh=0.5,
+        log_dict=None,
+        parallel_debug=False,
+    ):
         """
         Central management and processing method
 
         Parameters
         ----------
         data : syncopy data object
            Syncopy data object to be processed (has to be the same object
@@ -615,38 +636,47 @@
             # Construct list of dicts that will be passed on to workers: in the
             # parallel case, `trl_dat` is a dictionary!
 
             # Use the trial IDs from the selection, so we have absolute trial indices.
             trial_ids = data.selection.trial_ids if data.selection is not None else range(self.numTrials)
 
             # Use trial_ids and chunk_ids to turn into a unique index.
-            if self.numBlocksPerTrial == 1:  # The simple case: 1 call per trial. We add a chunk id (the trailing `_0` to be consistent with
-                                             # the more complex case, but the chunk index is always `0`).
+            if (
+                self.numBlocksPerTrial == 1
+            ):  # The simple case: 1 call per trial. We add a chunk id (the trailing `_0` to be consistent with
+                # the more complex case, but the chunk index is always `0`).
                 unique_key = ["__" + str(trial_id) + "_0" for trial_id in trial_ids]
             else:  # The more complex case: channel parallelization is active, we need to add the chunk to the
-                   # trial ID for the key to be unique, as a trial will be split into several chunks.
+                # trial ID for the key to be unique, as a trial will be split into several chunks.
                 trial_ids = np.repeat(trial_ids, self.numBlocksPerTrial)
                 chunk_ids = np.tile(np.arange(self.numBlocksPerTrial), self.numTrials)
-                unique_key = ["__" + str(trial_id) + "_" + str(chunk_id) for trial_id, chunk_id in zip(trial_ids, chunk_ids)]
-
-            workerDicts = [{"keeptrials": self.keeptrials,
-                            "infile": data.filename,
-                            "indset": data.data.name,
-                            "ingrid": self.sourceLayout[chk],
-                            "inshape": self.sourceShapes[chk],
-                            "sigrid": self.sourceSelectors[chk],
-                            "fancy": self.useFancyIdx,
-                            "vdsdir": self.virtualDatasetDir,
-                            "outfile": self.outFileName.format(chk),
-                            "outdset": self.tmpDsetName,
-                            "outgrid": self.targetLayout[chk],
-                            "outshape": self.targetShapes[chk],
-                            "dtype": self.dtype,
-                            "call_id": unique_key[chk] } for chk in range(self.numCalls)]
-
+                unique_key = [
+                    "__" + str(trial_id) + "_" + str(chunk_id)
+                    for trial_id, chunk_id in zip(trial_ids, chunk_ids)
+                ]
+
+            workerDicts = [
+                {
+                    "keeptrials": self.keeptrials,
+                    "infile": data.filename,
+                    "indset": data.data.name,
+                    "ingrid": self.sourceLayout[chk],
+                    "inshape": self.sourceShapes[chk],
+                    "sigrid": self.sourceSelectors[chk],
+                    "fancy": self.useFancyIdx,
+                    "vdsdir": self.virtualDatasetDir,
+                    "outfile": self.outFileName.format(chk),
+                    "outdset": self.tmpDsetName,
+                    "outgrid": self.targetLayout[chk],
+                    "outshape": self.targetShapes[chk],
+                    "dtype": self.dtype,
+                    "call_id": unique_key[chk],
+                }
+                for chk in range(self.numCalls)
+            ]
 
             # If channel-block parallelization has been set up, positional args of
             # `computeFunction` need to be massaged: any list whose elements represent
             # trial-specific args, needs to be expanded (so that each channel-block
             # per trial receives the correct number of pos. args)
             ArgV = list(self.argv)
             if self.numBlocksPerTrial > 1:
@@ -656,20 +686,22 @@
                             unrolled = chain.from_iterable([[ag] * self.numBlocksPerTrial for ag in arg])
                             if isinstance(arg, list):
                                 ArgV[ak] = list(unrolled)
                             else:
                                 ArgV[ak] = tuple(unrolled)
                     elif isinstance(arg, np.ndarray):
                         if len(arg.squeeze().shape) == 1 and arg.squeeze().size == self.numTrials:
-                            ArgV[ak] = np.array(chain.from_iterable([[ag] * self.numBlocksPerTrial for ag in arg]))
+                            ArgV[ak] = np.array(
+                                chain.from_iterable([[ag] * self.numBlocksPerTrial for ag in arg])
+                            )
 
             # Positional args for `process_io` wrapped computeFunctions consist of `trl_dat` + others
             # (stored in `ArgV`). Account for this when seeting up `ParallelMap`
             if len(ArgV) == 0:
-                self.inargs = (workerDicts, )
+                self.inargs = (workerDicts,)
             else:
                 self.inargs = (workerDicts, *ArgV)
             # Store provided debugging state for ACME
             self.parallelDebug = parallel_debug
 
             # store mem_thresh
             self.mem_thresh = mem_thresh
@@ -678,17 +710,19 @@
         else:
 
             # We only check memory
             memSize = psutil.virtual_memory().available
             if self.chunkMem >= mem_thresh * memSize:
                 self.chunkMem /= 1024**3
                 memSize /= 1024**3
-                msg = ("Single-trial processing requires {0:2.2f} GB of memory "
-                       "which is larger than the available "
-                       "memory ({1:2.2f} GB)")
+                msg = (
+                    "Single-trial processing requires {0:2.2f} GB of memory "
+                    "which is larger than the available "
+                    "memory ({1:2.2f} GB)"
+                )
                 raise SPYParallelError(msg.format(2 * self.chunkMem, memSize))
 
         # The `method` keyword can be used to override the `parallel` flag
         if method is None:
             if parallel:
                 computeMethod = self.compute_parallel
             else:
@@ -745,30 +779,31 @@
             os.mkdir(self.virtualDatasetDir)
 
             layout = h5py.VirtualLayout(shape=self.outputShape, dtype=self.dtype)
             for k, idx in enumerate(self.targetLayout):
                 fname = os.path.join(self.virtualDatasetDir, "{0:d}.h5".format(k))
                 # Catch empty selections: don't map empty sources into the layout of the VDS
                 if all([sel for sel in self.sourceLayout[k]]):
-                    layout[idx] = h5py.VirtualSource(fname, self.virtualDatasetNames, shape=self.targetShapes[k])
+                    layout[idx] = h5py.VirtualSource(
+                        fname, self.virtualDatasetNames, shape=self.targetShapes[k]
+                    )
             self.VirtualDatasetLayout = layout
             self.outFileName = os.path.join(self.virtualDatasetDir, "{0:d}.h5")
             self.tmpDsetName = self.virtualDatasetNames
 
         # Create regular HDF5 dataset for sequential writing
         else:
 
             # The shape of the target depends on trial-averaging
             if not self.keeptrials:
                 shp = self.cfg["chunkShape"]
             else:
                 shp = self.outputShape
             with h5py.File(out.filename, mode="w") as h5f:
-                h5f.create_dataset(name=self.outDatasetName,
-                                   dtype=self.dtype, shape=shp)
+                h5f.create_dataset(name=self.outDatasetName, dtype=self.dtype, shape=shp)
             self.outFileName = out.filename
             self.tmpDsetName = self.outDatasetName
 
     def compute_parallel(self, data, out):
         """
         Concurrent computing kernel
 
@@ -797,28 +832,30 @@
         compute_sequential : serial processing counterpart of this method
         """
 
         # Let ACME take care of argument distribution and memory checks: note
         # that `cfg` is trial-independent, i.e., we can simply throw it in here!
         if __acme__ and check_slurm_available():
 
-            self.pmap = ParallelMap(self.computeFunction,
-                                    *self.inargs,
-                                    n_inputs=self.numCalls,
-                                    write_worker_results=False,
-                                    write_pickle=False,
-                                    partition="auto",
-                                    n_workers="auto",
-                                    mem_per_worker="auto",
-                                    setup_timeout=60,
-                                    setup_interactive=False,
-                                    stop_client="auto",
-                                    verbose=None,
-                                    logfile=None,
-                                    **self.cfg)
+            self.pmap = ParallelMap(
+                self.computeFunction,
+                *self.inargs,
+                n_inputs=self.numCalls,
+                write_worker_results=False,
+                write_pickle=False,
+                partition="auto",
+                n_workers="auto",
+                mem_per_worker="auto",
+                setup_timeout=60,
+                setup_interactive=False,
+                stop_client="auto",
+                verbose=None,
+                logfile=None,
+                **self.cfg
+            )
 
             # Edge-case correction: if by chance, any array-like element `x` of `cfg`
             # satisfies `len(x) = numCalls`, `ParallelMap` attempts to tear open `x` and
             # distribute its elements across workers. Prevent this!
             if self.numCalls > 1:
                 for key, value in self.pmap.kwargv.items():
                     if isinstance(value, (list, tuple)):
@@ -845,17 +882,19 @@
             workerMem = [w["memory_limit"] for w in client.cluster.scheduler_info["workers"].values()]
             if len(workerMem) == 0:
                 raise SPYParallelError("no online workers found", client=client)
             workerMemMax = max(workerMem)
             if self.chunkMem >= self.mem_thresh * workerMemMax:
                 self.chunkMem /= 1024**3
                 workerMemMax /= 1000**3
-                msg = ("Single-trial processing requires {0:2.2f} GB of memory "
-                       "which is larger than the available "
-                       "worker memory ({1:2.2f} GB)")
+                msg = (
+                    "Single-trial processing requires {0:2.2f} GB of memory "
+                    "which is larger than the available "
+                    "worker memory ({1:2.2f} GB)"
+                )
                 raise SPYParallelError(msg.format(2 * self.chunkMem, workerMemMax))
 
         # --- trigger actual computation ---
 
         if self.pmap is not None:
             # Let ACME do the heavy lifting
             with self.pmap as pm:
@@ -867,18 +906,22 @@
             # we have a tuple like (wdict[n], argv1_seq[n], argv2) passed to the cF
             # by the Dask client mapping
             workerDicts = self.inargs[0]
             if len(self.inargs) > 1:
                 iterables = []
                 ArgV = self.inargs[1:]
                 for nblock, wdict in enumerate(workerDicts):
-                    argv = tuple(arg[nblock]
-                                 if isinstance(arg, (list, tuple, np.ndarray)) and  # these are the argv_seq
-                                 len(arg) == len(workerDicts)  # each call gets one element of a sequence type argv
-                                 else arg for arg in ArgV)
+                    argv = tuple(
+                        arg[nblock]
+                        if isinstance(arg, (list, tuple, np.ndarray))
+                        and len(arg)  # these are the argv_seq
+                        == len(workerDicts)  # each call gets one element of a sequence type argv
+                        else arg
+                        for arg in ArgV
+                    )
                     iterables.append((wdict, *argv))
             # no *args for the cF
             else:
                 iterables = workerDicts
 
             futures = client.map(self.computeFunction, iterables, **self.cfg)
             # similar to tqdm progress bar
@@ -935,18 +978,20 @@
 
             for nblock in tqdm(range(self.numTrials), bar_format=self.tqdmFormat, disable=None):
 
                 # Extract respective indexing tuples from constructed lists
                 ingrid = self.sourceLayout[nblock]
                 sigrid = self.sourceSelectors[nblock]
                 outgrid = self.targetLayout[nblock]
-                argv = tuple(arg[nblock]
-                             if isinstance(arg, (list, tuple, np.ndarray)) and
-                             len(arg) == self.numTrials
-                             else arg for arg in self.argv)
+                argv = tuple(
+                    arg[nblock]
+                    if isinstance(arg, (list, tuple, np.ndarray)) and len(arg) == self.numTrials
+                    else arg
+                    for arg in self.argv
+                )
 
                 # Catch empty source-array selections; this workaround is not
                 # necessary for h5py version 2.10+ (see https://github.com/h5py/h5py/pull/1174)
                 if any([not sel for sel in ingrid]):
                     res = np.empty(self.targetShapes[nblock], dtype=self.dtype)
                 else:
                     # Get source data as NumPy array
@@ -1024,17 +1069,18 @@
             cfg.pop(key)
 
         # Write log and store `cfg` constructed above in corresponding prop of `out`
         if log_dict is None:
             log_dict = cfg
         logOpts = ""
         for k, v in log_dict.items():
-            logOpts += "\t{key:s} = {value:s}\n".format(key=k,
-                                                        value=str(v) if len(str(v)) < 80
-                                                        else str(v)[:30] + ", ..., " + str(v)[-30:])
+            logOpts += "\t{key:s} = {value:s}\n".format(
+                key=k,
+                value=str(v) if len(str(v)) < 80 else str(v)[:30] + ", ..., " + str(v)[-30:],
+            )
         out.log = logHead + logOpts
 
     @abstractmethod
     def process_metadata(self, data, out):
         """
         Meta-information manager
 
@@ -1060,14 +1106,15 @@
         write_log : Logging of calculation parameters
         """
         pass
 
 
 # --- metadata helper functions ---
 
+
 def propagate_properties(in_data, out_data, keeptrials=True, time_axis=False):
 
     """
     Propagating data class properties (channels, trials, time, ...)
     from the input object `in_data` to the output
     object `out_data` and respecting selections.
 
@@ -1172,8 +1219,8 @@
             out_data.trialdefinition = in_data.selection.trialdefinition[0, :][None, :]
 
         # this might bite us
         out_data.samplerate = in_data.samplerate
 
     if selection_cleanup:
         in_data.selection = None
-        in_data.cfg.pop('selectdata')
+        in_data.cfg.pop("selectdata")
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/dask_helpers.py` & `esi_syncopy-2023.7/syncopy/shared/dask_helpers.py`

 * *Files 16% similar despite different names*

```diff
@@ -14,17 +14,15 @@
 def check_slurm_available():
     """
     Returns `True` if a SLURM instance could be reached via
     a `sinfo` call, `False` otherwise.
     """
 
     # Check if SLURM's `sinfo` can be accessed
-    proc = subprocess.Popen("sinfo",
-                            stdout=subprocess.PIPE, stderr=subprocess.PIPE,
-                            text=True, shell=True)
+    proc = subprocess.Popen("sinfo", stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, shell=True)
     _, err = proc.communicate()
     # Any non-zero return-code means SLURM is not available
     # so we disable ACME
     if proc.returncode != 0:
         has_slurm = False
     else:
         has_slurm = True
@@ -38,23 +36,25 @@
     until at least ``n_workers`` workers are available.
     """
 
     logger = get_logger()
     totalWorkers = len(client.cluster.requested)
 
     # dictionary of workers
-    workers = client.cluster.scheduler_info['workers']
+    workers = client.cluster.scheduler_info["workers"]
 
     # some small initial wait
-    sleep(.25)
+    sleep(0.25)
 
     if len(workers) < n_workers:
-        logger.important(f"waiting for at least {n_workers}/{totalWorkers} workers being available, timeout after {timeout} seconds..")
+        logger.important(
+            f"waiting for at least {n_workers}/{totalWorkers} workers being available, timeout after {timeout} seconds.."
+        )
     client.wait_for_workers(n_workers, timeout=timeout)
 
-    sleep(.25)
+    sleep(0.25)
 
     # report what we have
     logger.important(f"{len(workers)}/{totalWorkers} workers available, starting computation..")
 
     # wait a little more to get consistent client print out
-    sleep(.25)
+    sleep(0.25)
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/errors.py` & `esi_syncopy-2023.7/syncopy/shared/errors.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,26 +8,26 @@
 import traceback
 import logging
 from collections import OrderedDict
 
 # Local imports
 from syncopy import __tbcount__
 from syncopy.shared.log import get_logger, get_parallel_logger, loglevels
-import syncopy
 
 # Custom definition of bold ANSI for formatting errors/warnings in iPython/Jupyter
 ansiBold = "\033[1m"
 
 __all__ = []
 
 
 class SPYError(Exception):
     """
     Base class for SynCoPy errors
     """
+
     pass
 
 
 class SPYTypeError(SPYError):
     """
     SynCoPy-specific version of a TypeError
 
@@ -44,17 +44,19 @@
     def __init__(self, var, varname="", expected=""):
         self.found = str(type(var).__name__)
         self.varname = str(varname)
         self.expected = str(expected)
 
     def __str__(self):
         msg = "Wrong type{vn:s}{ex:s}{fd:s}"
-        return msg.format(vn=" of `" + self.varname + "`:" if len(self.varname) else ":",
-                          ex=" expected " + self.expected if len(self.expected) else "",
-                          fd=" found " + self.found)
+        return msg.format(
+            vn=" of `" + self.varname + "`:" if len(self.varname) else ":",
+            ex=" expected " + self.expected if len(self.expected) else "",
+            fd=" found " + self.found,
+        )
 
 
 class SPYValueError(SPYError):
     """
     SynCoPy-specific version of a ValueError
 
     Attributes
@@ -70,17 +72,19 @@
     def __init__(self, legal, varname="", actual=""):
         self.legal = str(legal)
         self.varname = str(varname)
         self.actual = str(actual)
 
     def __str__(self):
         msg = "Invalid value{vn:s}{fd:s} expected {ex:s}"
-        return msg.format(vn=" of `" + self.varname + "`:" if len(self.varname) else ":",
-                          fd=" '" + self.actual + "';" if len(self.actual) else "",
-                          ex=self.legal)
+        return msg.format(
+            vn=" of `" + self.varname + "`:" if len(self.varname) else ":",
+            fd=" '" + self.actual + "';" if len(self.actual) else "",
+            ex=self.legal,
+        )
 
 
 class SPYIOError(SPYError):
     """
     SynCoPy-specific version of an IO/OSError
 
     Attributes
@@ -94,18 +98,23 @@
 
     def __init__(self, fs_loc, exists=None):
         self.fs_loc = str(fs_loc)
         self.exists = exists
 
     def __str__(self):
         msg = "Cannot {op:s} {fs_loc:s}{ex:s}"
-        return msg.format(op="access" if self.exists is None else "write" if self.exists else "read",
-                          fs_loc=self.fs_loc,
-                          ex=": object already exists" if self.exists is True \
-                          else ": object does not exist" if self.exists is False else "")
+        return msg.format(
+            op="access" if self.exists is None else "write" if self.exists else "read",
+            fs_loc=self.fs_loc,
+            ex=": object already exists"
+            if self.exists is True
+            else ": object does not exist"
+            if self.exists is False
+            else "",
+        )
 
 
 class SPYParallelError(SPYError):
     """
     Syncopy-specific error intended for concurrent processing routines
 
     Attributes
@@ -140,143 +149,148 @@
     # Depending on the number of input arguments, we're either in Jupyter/iPython
     # or "regular" Python - this matters for coloring error messages
     if len(excargs) == 3:
         isipy = False
         etype, evalue, etb = excargs
     else:
         etype, evalue, etb = sys.exc_info()
-        try:                            # careful: if iPython is used to launch a script, ``get_ipython`` is not defined
+        try:  # careful: if iPython is used to launch a script, ``get_ipython`` is not defined
             ipy = get_ipython()
             isipy = True
             cols = ipy.InteractiveTB.Colors
             cols.filename = cols.filenameEm
             cols.bold = ansiBold
-            sys.last_traceback = etb    # smartify ``sys``
+            sys.last_traceback = etb  # smartify ``sys``
         except NameError:
             isipy = False
 
     # Pass ``KeyboardInterrupt`` on to regular excepthook so that CTRL + C
     # can still be used to abort program execution (only relevant in "regular"
     # Python prompts)
     if issubclass(etype, KeyboardInterrupt) and not isipy:
         sys.__excepthook__(etype, evalue, etb)
         return
 
     # Starty by putting together first line of error message
-    emsg = "{}\nSyNCoPy encountered an error in{} \n\n".format(cols.topline if isipy else "",
-                                                               cols.Normal if isipy else "")
+    emsg = "{}\nSyNCoPy encountered an error in{} \n\n".format(
+        cols.topline if isipy else "", cols.Normal if isipy else ""
+    )
 
     # If we're dealing with a `SyntaxError`, show it and getta outta here
     if issubclass(etype, SyntaxError):
 
         # Just format exception, don't mess around w/ traceback
         exc_fmt = traceback.format_exception_only(etype, evalue)
         for eline in exc_fmt:
             if "File" in eline:
                 eline = eline.split("File ")[1]
                 fname, lineno = eline.split(", line ")
-                emsg += "{}{}{}".format(cols.filename if isipy else "",
-                                        fname,
-                                        cols.Normal if isipy else "")
-                emsg += ", line {}{}{}".format(cols.lineno if isipy else "",
-                                               lineno,
-                                               cols.Normal if isipy else "")
+                emsg += "{}{}{}".format(cols.filename if isipy else "", fname, cols.Normal if isipy else "")
+                emsg += ", line {}{}{}".format(
+                    cols.lineno if isipy else "", lineno, cols.Normal if isipy else ""
+                )
             elif "SyntaxError" in eline:
                 smsg = eline.split("SyntaxError: ")[1]
-                emsg += "{}{}SyntaxError{}: {}{}{}".format(cols.excName if isipy else "",
-                                                           cols.bold if isipy else "",
-                                                           cols.Normal if isipy else "",
-                                                           cols.bold if isipy else "",
-                                                           smsg,
-                                                           cols.Normal if isipy else "")
+                emsg += "{}{}SyntaxError{}: {}{}{}".format(
+                    cols.excName if isipy else "",
+                    cols.bold if isipy else "",
+                    cols.Normal if isipy else "",
+                    cols.bold if isipy else "",
+                    smsg,
+                    cols.Normal if isipy else "",
+                )
             else:
-                emsg += "{}{}{}".format(cols.line if isipy else "",
-                                        eline,
-                                        cols.Normal if isipy else "")
+                emsg += "{}{}{}".format(cols.line if isipy else "", eline, cols.Normal if isipy else "")
 
         # Show generated message and leave (or kick-off debugging in Jupyer/iPython if %pdb is on)
         logger = get_parallel_logger()
         logger.critical(emsg)
         if isipy:
             if ipy.call_pdb:
                 ipy.InteractiveTB.debugger()
         return
 
     # Build an ordered(!) dictionary that encodes separators for traceback components
-    sep = OrderedDict({"filename": ", line ",
-                       "lineno": " in ",
-                       "name": "\n\t",
-                       "line": "\n"})
+    sep = OrderedDict({"filename": ", line ", "lineno": " in ", "name": "\n\t", "line": "\n"})
 
     # Find "root" of traceback tree (and remove outer-most frames)
     keepgoing = True
     while keepgoing:
         frame = traceback.extract_tb(etb)[0]
         etb = etb.tb_next
-        if frame.filename.find("site-packages") < 0 or \
-           (frame.filename.find("site-packages") >= 0 and \
-            frame.filename.find("syncopy") >= 0):
+        if frame.filename.find("site-packages") < 0 or (
+            frame.filename.find("site-packages") >= 0 and frame.filename.find("syncopy") >= 0
+        ):
             tb_entry = ""
             for attr in sep.keys():
-                tb_entry += "{}{}{}{}".format(getattr(cols, attr) if isipy else "",
-                                              getattr(frame, attr),
-                                              cols.Normal if isipy else "",
-                                              sep.get(attr))
+                tb_entry += "{}{}{}{}".format(
+                    getattr(cols, attr) if isipy else "",
+                    getattr(frame, attr),
+                    cols.Normal if isipy else "",
+                    sep.get(attr),
+                )
             emsg += tb_entry
             keepgoing = False
 
     # Format the exception-part of the traceback - the resulting list usually
     # contains only a single string - if we find more just use everything
     exc_fmt = traceback.format_exception_only(etype, evalue)
     if len(exc_fmt) == 1:
         exc_msg = exc_fmt[0]
         idx = exc_msg.rfind(etype.__name__)
         if idx >= 0:
-            exc_msg = exc_msg[idx + len(etype.__name__):]
-        exc_name = "{}{}{}{}".format(cols.excName if isipy else "",
-                                     cols.bold if isipy else "",
-                                     etype.__name__,
-                                     cols.Normal if isipy else "")
+            exc_msg = exc_msg[idx + len(etype.__name__) :]
+        exc_name = "{}{}{}{}".format(
+            cols.excName if isipy else "",
+            cols.bold if isipy else "",
+            etype.__name__,
+            cols.Normal if isipy else "",
+        )
     else:
         exc_msg = "".join(exc_fmt)
         exc_name = ""
 
     # Now go through traceback and put together a list of strings for printing
     if __tbcount__ and etb is not None:
-        emsg += "\n" + "-"*80 + "\nAbbreviated traceback:\n\n"
+        emsg += "\n" + "-" * 80 + "\nAbbreviated traceback:\n\n"
         tb_count = 0
         tb_list = []
         for frame in traceback.extract_tb(etb):
-            if frame.filename.find("site-packages") < 0 or \
-               (frame.filename.find("site-packages") >= 0 and \
-                frame.filename.find("syncopy") >= 0):
+            if frame.filename.find("site-packages") < 0 or (
+                frame.filename.find("site-packages") >= 0 and frame.filename.find("syncopy") >= 0
+            ):
                 tb_entry = ""
                 for attr in sep.keys():
-                    tb_entry += "{}{}{}{}".format("", # placeholder for color if wanted
-                                                  getattr(frame, attr),
-                                                  "", # placeholder for color if wanted
-                                                  sep.get(attr))
+                    tb_entry += "{}{}{}{}".format(
+                        "",  # placeholder for color if wanted
+                        getattr(frame, attr),
+                        "",  # placeholder for color if wanted
+                        sep.get(attr),
+                    )
                 tb_list.append(tb_entry)
                 tb_count += 1
                 if tb_count == __tbcount__:
                     break
         emsg += "".join(tb_list)
 
     # Finally, another info message
     if etb is not None:
-        emsg += "\nUse `import traceback; import sys; traceback.print_tb(sys.last_traceback)` " + \
-                "for full error traceback.\n"
+        emsg += (
+            "\nUse `import traceback; import sys; traceback.print_tb(sys.last_traceback)` "
+            + "for full error traceback.\n"
+        )
 
     # Glue actual Exception name + message to output string
-    emsg += "{}{}{}{}{}".format("\n" if isipy else "",
-                                exc_name,
-                                cols.bold if isipy else "",
-                                exc_msg,
-                                cols.Normal if isipy else "",)
-
+    emsg += "{}{}{}{}{}".format(
+        "\n" if isipy else "",
+        exc_name,
+        cols.bold if isipy else "",
+        exc_msg,
+        cols.Normal if isipy else "",
+    )
 
     # Show generated message and get outta here
     logger = get_parallel_logger()
     logger.critical(emsg)
 
     # Kick-start debugging in case %pdb is enabled in Jupyter/iPython
     if isipy:
@@ -318,19 +332,23 @@
         boldEm = ""
 
     # Plug together message string and print it
     if caller is None:
         caller = sys._getframe().f_back.f_code.co_name
     PrintMsg = "{coloron:s}{bold:s}Syncopy{caller:s} WARNING: {msg:s}{coloroff:s}"
     logger = get_logger()
-    logger.warning(PrintMsg.format(coloron=warnCol,
-                          bold=boldEm,
-                          caller=" <" + caller + ">" if len(caller) else caller,
-                          msg=msg,
-                          coloroff=normCol))
+    logger.warning(
+        PrintMsg.format(
+            coloron=warnCol,
+            bold=boldEm,
+            caller=_get_caller(caller),
+            msg=msg,
+            coloroff=normCol,
+        )
+    )
 
 
 def SPYParallelLog(msg, loglevel="INFO", caller=None):
     """Log a message in parallel code run via slurm.
 
     This uses the parallel logger and one file per machine.
 
@@ -342,16 +360,24 @@
     if not isinstance(numeric_level, int):  # Invalid string was set.
         raise SPYValueError(legal=f"one of: {loglevels}", varname="loglevel", actual=loglevel)
     if caller is None:
         caller = sys._getframe().f_back.f_code.co_name
     PrintMsg = "{caller:s} {msg:s}"
     logger = get_parallel_logger()
     logfunc = getattr(logger, loglevel.lower())
-    logfunc(PrintMsg.format(caller=" <" + caller + ">" if len(caller) else caller,
-                          msg=msg))
+    logfunc(PrintMsg.format(caller=_get_caller(caller), msg=msg))
+
+
+def _get_caller(caller):
+    try:
+        ret_caller = " <" + caller + ">" if len(caller) else caller
+    except TypeError:
+        ret_caller = caller.__name__
+    return ret_caller
+
 
 def SPYLog(msg, loglevel="INFO", caller=None):
     """Log a message in sequential code.
 
     This uses the standard logger that logs to console and a local log file by default.
 
     Returns
@@ -362,16 +388,16 @@
     if not isinstance(numeric_level, int):  # Invalid string was set.
         raise SPYValueError(legal=f"one of: {loglevels}", varname="loglevel", actual=loglevel)
     if caller is None:
         caller = sys._getframe().f_back.f_code.co_name
     PrintMsg = "{caller:s} {msg:s}"
     logger = get_logger()
     logfunc = getattr(logger, loglevel.lower())
-    logfunc(PrintMsg.format(caller=" <" + caller + ">" if len(caller) else caller,
-                          msg=msg))
+    logfunc(PrintMsg.format(caller=_get_caller(caller), msg=msg))
+
 
 def log(msg, level="IMPORTANT", par=False, caller=None):
     """
     Log a message using the Syncopy logging setup.
 
     Parameters
     ----------
@@ -415,33 +441,38 @@
     """
 
     # If Syncopy's running in Jupyter/iPython colorize warning message
     # Use the following chart (enter FG color twice b/w ';') to change:
     # https://en.wikipedia.org/wiki/ANSI_escape_code#Colors
     try:
         cols = get_ipython().InteractiveTB.Colors
-        infoCol = cols.Normal # infos are fine with just bold text
+        infoCol = cols.Normal  # infos are fine with just bold text
         normCol = cols.Normal
         boldEm = ansiBold
     except NameError:
         infoCol = ""
         normCol = ""
         boldEm = ""
 
     # Plug together message string and print it
     if caller is None:
         caller = sys._getframe().f_back.f_code.co_name
     PrintMsg = "{coloron:s}{bold:s}Syncopy{caller:s} {tag}: {msg:s}{coloroff:s}"
     logger = get_logger()
-    logger.info(PrintMsg.format(coloron=infoCol,
-                          bold=boldEm,
-                          caller=" <" + caller + ">" if len(caller) else caller,
-                          tag=tag,
-                          msg=msg,
-                          coloroff=normCol))
+    logger.info(
+        PrintMsg.format(
+            coloron=infoCol,
+            bold=boldEm,
+            caller=_get_caller(caller),
+            tag=tag,
+            msg=msg,
+            coloroff=normCol,
+        )
+    )
+
 
 def SPYDebug(msg, caller=None):
     """
     Log a standardized Syncopy debug message.
 
     .. note::
         Depending on the currently active log level, this may or may not produce any output.
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/filetypes.py` & `esi_syncopy-2023.7/syncopy/shared/filetypes.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,20 +1,24 @@
 # -*- coding: utf-8 -*-
 #
 # Supported Syncopy classes and file extensions
 #
 
+
 def _data_classname_to_extension(name):
-    return "." + name.split('Data')[0].lower()
+    return "." + name.split("Data")[0].lower()
+
 
 # data file extensions are first word of data class name in lower-case
-supportedClasses = ('AnalogData', 'SpectralData', 'CrossSpectralData', # ContinousData
-                    'SpikeData', 'EventData',  # DiscreteData
-                    'TimelockData', ) # StatisticalData
+supportedClasses = (
+    "AnalogData",
+    "SpectralData",
+    "CrossSpectralData",  # ContinousData
+    "SpikeData",
+    "EventData",  # DiscreteData
+    "TimelockData",
+)  # StatisticalData
 
-supportedDataExtensions = tuple([_data_classname_to_extension(cls)
-                                 for cls in supportedClasses])
+supportedDataExtensions = tuple([_data_classname_to_extension(cls) for cls in supportedClasses])
 
 # Define SynCoPy's general file-/directory-naming conventions
-FILE_EXT = {"dir" : ".spy",
-            "info" : ".info",
-            "data" : supportedDataExtensions}
+FILE_EXT = {"dir": ".spy", "info": ".info", "data": supportedDataExtensions}
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/input_processors.py` & `esi_syncopy-2023.7/syncopy/shared/input_processors.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,15 +12,19 @@
 import numbers
 from inspect import signature
 from scipy.signal import windows
 
 from syncopy.specest.mtmfft import _get_dpss_pars
 from syncopy.shared.errors import SPYValueError, SPYWarning, SPYInfo
 from syncopy.shared.parsers import scalar_parser, array_parser
-from syncopy.shared.const_def import availableTapers, generalParameters, availablePaddingOpt
+from syncopy.shared.const_def import (
+    availableTapers,
+    generalParameters,
+    availablePaddingOpt,
+)
 
 
 def process_padding(pad, lenTrials, samplerate):
 
     """
     Simplified padding interface, for all taper based methods
     padding has to be done **after** tapering!
@@ -63,25 +67,23 @@
         lgl = "'maxperlen', 'nextpow2' or a float number"
         actual = f"{pad}"
         raise SPYValueError(legal=lgl, varname="pad", actual=actual)
 
     # zero padding of ALL trials the same way
     if isinstance(pad, numbers.Number):
 
-        scalar_parser(pad,
-                      varname='pad',
-                      lims=[lenTrials.max() / samplerate, np.inf])
+        scalar_parser(pad, varname="pad", lims=[lenTrials.max() / samplerate, np.inf])
         abs_pad = int(pad * samplerate)
 
     # or pad to optimal FFT lengths
-    elif pad == 'nextpow2':
+    elif pad == "nextpow2":
         abs_pad = _nextpow2(int(lenTrials.max()))
 
     # no padding in case of equal length trials
-    elif pad == 'maxperlen':
+    elif pad == "maxperlen":
         abs_pad = int(lenTrials.max())
         if lenTrials.min() != lenTrials.max():
             msg = f"Unequal trial lengths present, padding all trials to {abs_pad} samples"
             SPYInfo(msg)
 
     # `abs_pad` is now the (soon to be padded) signal length in samples
 
@@ -122,56 +124,72 @@
         raise SPYValueError(legal=lgl, varname="foi/foilim", actual=act)
 
     if foi is not None:
         if isinstance(foi, str):
             if foi == "all":
                 foi = None
             else:
-                raise SPYValueError(legal="'all' or `None` or list/array",
-                                    varname="foi", actual=foi)
+                raise SPYValueError(legal="'all' or `None` or list/array", varname="foi", actual=foi)
         else:
             try:
-                array_parser(foi, varname="foi", hasinf=False, hasnan=False,
-                             lims=[0, samplerate / 2], dims=(None,))
+                array_parser(
+                    foi,
+                    varname="foi",
+                    hasinf=False,
+                    hasnan=False,
+                    lims=[0, samplerate / 2],
+                    dims=(None,),
+                )
             except Exception as exc:
                 raise exc
             foi = np.array(foi, dtype="float")
 
     if foilim is not None:
         if isinstance(foilim, str):
             if foilim == "all":
                 foilim = None
             else:
-                raise SPYValueError(legal="'all' or `None` or `[fmin, fmax]`",
-                                    varname="foilim", actual=foilim)
+                raise SPYValueError(
+                    legal="'all' or `None` or `[fmin, fmax]`",
+                    varname="foilim",
+                    actual=foilim,
+                )
         else:
-            array_parser(foilim, varname="foilim", hasinf=False, hasnan=False,
-                         lims=[0, samplerate / 2], dims=(2,))
+            array_parser(
+                foilim,
+                varname="foilim",
+                hasinf=False,
+                hasnan=False,
+                lims=[0, samplerate / 2],
+                dims=(2,),
+            )
 
             # QUICKFIX for #392
             foilim = [float(f) for f in foilim]
 
             # foilim is of shape (2,)
             if foilim[0] > foilim[1]:
                 msg = "Sorting foilim low to high.."
                 SPYInfo(msg)
                 foilim = np.sort(foilim)
 
     return foi, foilim
 
 
-def process_taper(taper,
-                  taper_opt,
-                  tapsmofrq,
-                  nTaper,
-                  keeptapers,
-                  foimax,
-                  samplerate,
-                  nSamples,
-                  output):
+def process_taper(
+    taper,
+    taper_opt,
+    tapsmofrq,
+    nTaper,
+    keeptapers,
+    foimax,
+    samplerate,
+    nSamples,
+    output,
+):
 
     """
     General taper validation and Slepian/dpss input sanitization.
 
     For multi-tapering with slepian tapers the default is to max out
     `nTaper` to achieve the desired frequency smoothing bandwidth.
     For details about the Slepian settings see
@@ -210,17 +228,17 @@
     taper_opt : dict
         For multi-tapering contains the
         keys `NW` and `Kmax` for `scipy.signal.windows.dpss`.
         For other tapers these are the additional parameters or
         an empty dictionary in case selected taper has no further args.
     """
 
-    if taper == 'dpss':
+    if taper == "dpss":
         lgl = "set `tapsmofrq` parameter directly for multi-tapering"
-        raise SPYValueError(legal=lgl, varname='taper', actual=taper)
+        raise SPYValueError(legal=lgl, varname="taper", actual=taper)
 
     # no tapering at all
     if taper is None and tapsmofrq is None:
         return None, {}
 
     # See if taper choice is supported
     if taper not in availableTapers:
@@ -242,116 +260,121 @@
             SPYWarning(msg)
 
         # availableTapers are given by windows.__all__
         parameters = signature(getattr(windows, taper)).parameters
         supported_kws = list(parameters.keys())
         # 'M' is the kw for the window length
         # for all of scipy's windows
-        supported_kws.remove('M')
-        supported_kws.remove('sym')
+        supported_kws.remove("M")
+        supported_kws.remove("sym")
 
         if taper_opt is not None:
 
             if len(supported_kws) == 0:
                 lgl = f"`None`, taper '{taper}' has no additional parameters"
-                raise SPYValueError(lgl, varname='taper_opt', actual=taper_opt)
+                raise SPYValueError(lgl, varname="taper_opt", actual=taper_opt)
 
             for key in taper_opt:
                 if key not in supported_kws:
                     lgl = f"one of {supported_kws} for `taper='{taper}'`"
                     raise SPYValueError(lgl, "taper_opt key", key)
             for key in supported_kws:
                 if key not in taper_opt:
                     lgl = f"additional parameter '{key}' for `taper='{taper}'`"
                     raise SPYValueError(lgl, "taper_opt", None)
             # all supplied keys are fine
             return taper, taper_opt
 
         elif len(supported_kws) > 0:
             lgl = f"additional parameters for taper '{taper}': {supported_kws}"
-            raise SPYValueError(lgl, varname='taper_opt', actual=taper_opt)
+            raise SPYValueError(lgl, varname="taper_opt", actual=taper_opt)
         else:
             # taper_opt was None and taper needs no additional parameters
             return taper, {}
 
     # -- multi-tapering --
     else:
-        if taper != 'hann':
+        if taper != "hann":
             lgl = "`None` for multi-tapering, just set `tapsmofrq`"
-            raise SPYValueError(lgl, varname='taper', actual=taper)
+            raise SPYValueError(lgl, varname="taper", actual=taper)
 
         if taper_opt is not None:
             msg = "For multi-tapering use `tapsmofrq` and `nTaper` to control frequency smoothing, `taper_opt` has no effect"
             SPYWarning(msg)
 
         # direct mtm estimate (averaging) only valid for spectral power
         if not keeptapers and output != "pow":
-            lgl = (f"'pow'|False or '{output}'|True, set either keeptapers=True "
-                   "or `output='pow'`!")
-            raise SPYValueError(legal=lgl, varname="output|keeptapers", actual=f"'{output}'|{keeptapers}")
+            lgl = f"'pow'|False or '{output}'|True, set either keeptapers=True " "or `output='pow'`!"
+            raise SPYValueError(
+                legal=lgl,
+                varname="output|keeptapers",
+                actual=f"'{output}'|{keeptapers}",
+            )
 
         # --- minimal smoothing bandwidth ---
         # --- such that Kmax/nTaper is at least 1
-        minBw = 2 * samplerate / nSamples
+        minBw = samplerate / nSamples
         # -----------------------------------
 
         # --- maximal smoothing bandwidth ---
         # --- such that Kmax < nSamples and NW < nSamples / 2
-        maxBw = np.min([samplerate / 2 - 1 / nSamples,
-                        samplerate * (nSamples + 1) / (2 * nSamples)])
+        maxBw = np.min(
+            [
+                samplerate / 2 - 1 / nSamples,
+                samplerate * (nSamples + 1) / (2 * nSamples),
+            ]
+        )
         # -----------------------------------
 
         try:
             scalar_parser(tapsmofrq, varname="tapsmofrq", lims=[0, np.inf])
         except Exception:
             lgl = "smoothing bandwidth in Hz, typical values are in the range 1-10Hz"
             raise SPYValueError(legal=lgl, varname="tapsmofrq", actual=tapsmofrq)
 
         if tapsmofrq < minBw:
-            msg = f'Setting tapsmofrq to the minimal attainable bandwidth of {minBw:.2f}Hz'
+            msg = f"Setting tapsmofrq to the minimal attainable bandwidth of {minBw:.2f}Hz"
             SPYInfo(msg)
             tapsmofrq = minBw
 
         if tapsmofrq > maxBw:
-            msg = f'Setting tapsmofrq to the maximal attainable bandwidth of {maxBw:.2f}Hz'
+            msg = f"Setting tapsmofrq to the maximal attainable bandwidth of {maxBw:.2f}Hz"
             SPYInfo(msg)
             tapsmofrq = maxBw
 
         # --------------------------------------------------------------
         # set parameters for scipy.signal.windows.dpss
         NW, Kmax = _get_dpss_pars(tapsmofrq, nSamples, samplerate)
         # --------------------------------------------------------------
 
         # tapsmofrq too large
         # if Kmax > nSamples or NW > nSamples / 2:
 
         # the recommended way:
         # set nTaper automatically to achieve exact effective smoothing bandwidth
         if nTaper is None:
-            msg = f'Using {Kmax} taper(s) for multi-tapering'
+            msg = f"Using {Kmax} taper(s) for multi-tapering"
             SPYInfo(msg)
-            dpss_opt = {'NW': NW, 'Kmax': Kmax}
-            return 'dpss', dpss_opt
+            dpss_opt = {"NW": NW, "Kmax": Kmax}
+            return "dpss", dpss_opt
 
         elif nTaper is not None:
 
-            scalar_parser(nTaper,
-                          varname="nTaper",
-                          ntype="int_like", lims=[1, np.inf])
+            scalar_parser(nTaper, varname="nTaper", ntype="int_like", lims=[1, np.inf])
 
             if nTaper != Kmax:
-                msg = f'''
+                msg = f"""
                 Manually setting the number of tapers is not recommended
                 and may (strongly) distort the effective smoothing bandwidth!\n
                 The optimal number of tapers is {Kmax}, you have chosen to use {nTaper}.
-                '''
+                """
                 SPYWarning(msg)
 
-            dpss_opt = {'NW': NW, 'Kmax': nTaper}
-            return 'dpss', dpss_opt
+            dpss_opt = {"NW": NW, "Kmax": nTaper}
+            return "dpss", dpss_opt
 
 
 def check_effective_parameters(CR, defaults, lcls, besides=None):
 
     """
     For a given ComputationalRoutine, compare set parameters
     (*lcls*) with the accepted parameters and the frontend
@@ -375,15 +398,15 @@
         expected += besides
 
     relevant = [name for name in defaults if name not in generalParameters]
 
     for name in relevant:
         if name not in expected and (lcls[name] != defaults[name]):
             msg = f"option `{name}` has no effect for `{CR.__name__}`!"
-            SPYWarning(msg, caller=__name__.split('.')[-1])
+            SPYWarning(msg, caller=__name__.split(".")[-1])
 
 
 def check_passed_kwargs(lcls, defaults, frontend_name):
     """
     Catch additional kwargs passed to the frontends
     which have no effect
     """
@@ -393,20 +416,20 @@
     kw_dict = lcls.get("kwargs")
 
     # nothing to do..
     if not kw_dict:
         return
 
     relevant = list(kw_dict.keys())
-    expected = [name for name in defaults] + ['chan_per_worker']
+    expected = [name for name in defaults] + ["chan_per_worker"]
 
     for name in relevant:
         if name not in expected:
             msg = f"option `{name}` has no effect in `{frontend_name}`!"
-            SPYWarning(msg, caller=__name__.split('.')[-1])
+            SPYWarning(msg, caller=__name__.split(".")[-1])
 
 
 def _nextpow2(number):
     """Find integer power of 2 greater than or equal to number."""
     n = 1
     while n < number:
         n *= 2
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/kwarg_decorators.py` & `esi_syncopy-2023.7/syncopy/shared/kwarg_decorators.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,16 +8,21 @@
 import h5py
 import inspect
 import numpy as np
 import dask.distributed as dd
 
 
 import syncopy as spy
-from syncopy.shared.errors import (SPYTypeError, SPYValueError,
-                                   SPYError, SPYWarning, SPYInfo)
+from syncopy.shared.errors import (
+    SPYTypeError,
+    SPYValueError,
+    SPYError,
+    SPYWarning,
+    SPYInfo,
+)
 from syncopy.shared.tools import StructDict
 from syncopy.shared.metadata import h5_add_metadata, parse_cF_returns
 
 # Local imports
 from .dask_helpers import check_slurm_available, check_workers_available
 from .log import get_logger
 
@@ -139,17 +144,19 @@
                 k += 1
 
         # If a dict was found, assume it's a `cfg` dict and extract it from
         # the positional argument list; if more than one dict was found, abort
         if k == 1:
             cfg = args.pop(cfgidx)
         elif k > 1:
-            raise SPYValueError(legal="single `cfg` input",
-                                varname="cfg",
-                                actual="{0:d} `cfg` objects in input arguments".format(k))
+            raise SPYValueError(
+                legal="single `cfg` input",
+                varname="cfg",
+                actual="{0:d} `cfg` objects in input arguments".format(k),
+            )
 
         # Now parse provided keywords for `cfg` entry - if `cfg` was already
         # provided as positional argument, abort
         if kwargs.get("cfg") is not None:
             if cfg:
                 lgl = "`cfg` either as positional or keyword argument, not both"
                 raise SPYValueError(legal=lgl, varname="cfg")
@@ -171,22 +178,20 @@
             cfg = StructDict(cfg)
 
             # If a meta-function is called using `cfg`, any (not only non-default) values for
             # keyword arguments must *either* be provided via `cfg` or via standard kw
             # NOTE: the frontend defaults not set by the user do NOT appear in `kwargs`!
             for key in kwargs:
                 # these get special treatment below
-                if key in ['data', 'dataset']:
+                if key in ["data", "dataset"]:
                     continue
                 elif key in cfg:
                     lgl = f"parameter set either via `cfg.{key}=...` or directly via keyword"
                     act = f"parameter `{key}` set in both `cfg` and via explicit keyword"
-                    raise SPYValueError(legal=lgl,
-                                        varname=f"cfg/{key}",
-                                        actual=act)
+                    raise SPYValueError(legal=lgl, varname=f"cfg/{key}", actual=act)
                 # now attach the explicit set keywords to `cfg`
                 # to be passed to the func call
                 else:
                     cfg[key] = kwargs[key]
 
             # Translate any existing "yes" and "no" fields to `True` and `False`
             for key in cfg.keys():
@@ -223,77 +228,77 @@
 
         # If Syncopy data object(s) were provided convert single objects to one-element
         # lists, ensure positional args do *not* contain add'l objects; ensure keyword
         # args (besides `cfg`) do *not* contain add'l objects; ensure `data` exclusively
         # contains Syncopy data objects. Finally, rename remaining positional arguments
         if data:
             if any([isinstance(arg, spy.datatype.base_data.BaseData) for arg in args]):
-                lgl = "Syncopy data object provided either via `cfg`/keyword or " +\
-                    "positional arguments, not both"
+                lgl = (
+                    "Syncopy data object provided either via `cfg`/keyword or "
+                    + "positional arguments, not both"
+                )
                 raise SPYValueError(legal=lgl, varname="cfg/data")
             if kwargs.get("data") or kwargs.get("dataset"):
-                lgl = "Syncopy data object provided either via `cfg` or as " +\
-                    "keyword argument, not both"
+                lgl = "Syncopy data object provided either via `cfg` or as " + "keyword argument, not both"
                 raise SPYValueError(legal=lgl, varname="cfg.data")
             if not isinstance(data, spy.datatype.base_data.BaseData):
                 raise SPYError("`data` must be Syncopy data object!")
             posargs = args
 
         # If `data` was not provided via `cfg` or as kw-arg, parse positional arguments
         if data is None:
             posargs = []
             while args:
                 arg = args.pop(0)
                 if data is not None and isinstance(arg, spy.datatype.base_data.BaseData):
                     lgl = "only one Syncopy data object"
-                    raise SPYValueError(lgl, varname='data')
+                    raise SPYValueError(lgl, varname="data")
                 if isinstance(arg, spy.datatype.base_data.BaseData):
                     data = arg
                 else:
                     posargs.append(arg)
 
         # if there was no Syncopy data found at this point,
         # we call the wrapped function without it
         if data is None:
             return func(*posargs, **cfg)
         else:
             # Call function with unfolded `data` + modified positional/keyword args
             return func(data, *posargs, **cfg)
 
     # Append two-liner to docstring header mentioning the use of `cfg`
-    introEntry = \
-    "    \n" +\
-    "    The parameters listed below can be provided as is or a via a `cfg`\n" +\
-    "    configuration 'structure', see Notes for details. \n"
-    wrapper_cfg.__doc__ = _append_docstring(wrapper_cfg,
-                                            introEntry,
-                                            insert_in="Header",
-                                            at_end=True)
+    introEntry = (
+        "    \n"
+        + "    The parameters listed below can be provided as is or a via a `cfg`\n"
+        + "    configuration 'structure', see Notes for details. \n"
+    )
+    wrapper_cfg.__doc__ = _append_docstring(wrapper_cfg, introEntry, insert_in="Header", at_end=True)
 
     # Append a paragraph explaining the use of `cfg` by an example that explicitly
     # mentions `func`'s name and input parameters
-    notesEntry = \
-    "    This function can be either called providing its input arguments directly\n" +\
-    "    or via a `cfg` configuration 'structure'. For instance, the following function\n" +\
-    "    calls are equivalent\n" +\
-    "    \n" +\
-    "    >>> spy.{fname:s}({arg0:s}, {kwarg0:s}=...)\n" +\
-    "    >>> cfg = spy.StructDict()\n" +\
-    "    >>> cfg.{kwarg0:s} = ...\n" +\
-    "    >>> spy.{fname:s}(cfg, {arg0:s})\n" +\
-    "    >>> cfg.{arg0:s} = {arg0:s}\n" +\
-    "    >>> spy.{fname:s}(cfg)\n" +\
-    "    \n" +\
-    "    Please refer to :doc:`/user/fieldtrip` for further details. \n\n"
-    wrapper_cfg.__doc__ = _append_docstring(wrapper_cfg,
-                                            notesEntry.format(fname=func.__name__,
-                                                              arg0=arg0,
-                                                              kwarg0=kwarg0),
-                                            insert_in="Notes",
-                                            at_end=False)
+    notesEntry = (
+        "    This function can be either called providing its input arguments directly\n"
+        + "    or via a `cfg` configuration 'structure'. For instance, the following function\n"
+        + "    calls are equivalent\n"
+        + "    \n"
+        + "    >>> spy.{fname:s}({arg0:s}, {kwarg0:s}=...)\n"
+        + "    >>> cfg = spy.StructDict()\n"
+        + "    >>> cfg.{kwarg0:s} = ...\n"
+        + "    >>> spy.{fname:s}(cfg, {arg0:s})\n"
+        + "    >>> cfg.{arg0:s} = {arg0:s}\n"
+        + "    >>> spy.{fname:s}(cfg)\n"
+        + "    \n"
+        + "    Please refer to :doc:`/user/fieldtrip` for further details. \n\n"
+    )
+    wrapper_cfg.__doc__ = _append_docstring(
+        wrapper_cfg,
+        notesEntry.format(fname=func.__name__, arg0=arg0, kwarg0=kwarg0),
+        insert_in="Notes",
+        at_end=False,
+    )
 
     return wrapper_cfg
 
 
 def unwrap_select(func):
     """
     Decorator for handling in-place data selections via `select` keyword
@@ -376,33 +381,35 @@
                 if obj.selection is None and select is not None:
                     obj.selection = select
                     attached_selection = True
                     # we have one and only one input data object
                     break
                 else:
                     if select is not None:
-                        raise SPYError(f"Selection found both in kwarg 'selection' ({select}) and in \npassed Syncopy Data object of type '{type(obj)}' ({obj.selection})")
-
+                        raise SPYError(
+                            f"Selection found both in kwarg 'selection' ({select}) and in \npassed Syncopy Data object of type '{type(obj)}' ({obj.selection})"
+                        )
 
         # Call function with modified data object(s)
         res = func(*args, **kwargs)
 
         # Wipe data-selection slot to not alter user objects
         # if the selection got attached by this wrapper here
         for obj in args:
             if hasattr(obj, "selection") and attached_selection:
                 obj.selection = None
 
         return res
 
     # Append `select` keyword entry to wrapped function's docstring and signature
-    selectDocEntry = \
-    "    select : dict or :class:`~syncopy.shared.tools.StructDict` or str\n" +\
-    "        In-place selection of subset of input data for processing. Please refer\n" +\
-    "        to :func:`syncopy.selectdata` for further usage details."
+    selectDocEntry = (
+        "    select : dict or :class:`~syncopy.shared.tools.StructDict` or str\n"
+        + "        In-place selection of subset of input data for processing. Please refer\n"
+        + "        to :func:`syncopy.selectdata` for further usage details."
+    )
     wrapper_select.__doc__ = _append_docstring(func, selectDocEntry)
     wrapper_select.__signature__ = _append_signature(func, "select")
 
     return wrapper_select
 
 
 def detect_parallel_client(func):
@@ -482,18 +489,19 @@
         # if acme is around, let it manage everything assuming we are on the ESI cluster
         if spy.__acme__ and parallel is not False:
             try:
                 client = dd.get_client()
                 parallel = True
             except ValueError:
                 if parallel:
-                    msg = (f"Could not find a running dask cluster!\n"
-                           "Try `esi_cluster_setup` from ACME to set up a cluster on the ESI HPC\n"
-                           "..computing sequentially"
-                           )
+                    msg = (
+                        f"Could not find a running dask cluster!\n"
+                        "Try `esi_cluster_setup` from ACME to set up a cluster on the ESI HPC\n"
+                        "..computing sequentially"
+                    )
                     logger.important(msg)
                 parallel = False
 
         # This effectively searches for a global dask cluster, and sets
         # parallel=True if one was found. If no cluster was found, parallel is set to False,
         # so no automatic spawning of a LocalCluster this needs explicit `parallel=True`.
         elif parallel is None:
@@ -519,32 +527,32 @@
                 check_workers_available(client, timeout=dask_timeout, n_workers=1)
                 msg = f"..attaching to running Dask client:\n{client}"
                 logger.important(msg)
             except ValueError:
                 # we are on a HPC but ACME and/or Dask client are missing,
                 # LocalCluster still gets created
                 if has_slurm and not spy.__acme__:
-                    slurm_msg = ("We are apparently on a slurm cluster but\n"
-                                 "Syncopy could not find a Dask client.\n"
-                                 "Syncopy does not provide an "
-                                 "automatic Dask SLURMCluster on its own!"
-                                 "\nPlease consider configuring your own dask cluster "
-                                 "via `dask_jobqueue.SLURMCluster()`"
-                                 "\n\nCreating a LocalCluster as fallback.."
-                           )
+                    slurm_msg = (
+                        "We are apparently on a slurm cluster but\n"
+                        "Syncopy could not find a Dask client.\n"
+                        "Syncopy does not provide an "
+                        "automatic Dask SLURMCluster on its own!"
+                        "\nPlease consider configuring your own dask cluster "
+                        "via `dask_jobqueue.SLURMCluster()`"
+                        "\n\nCreating a LocalCluster as fallback.."
+                    )
                     SPYWarning(slurm_msg)
 
                 # -- spawn fallback local cluster --
 
                 cluster = dd.LocalCluster()
                 # attaches to local cluster residing in global namespace
                 dd.Client(cluster)
                 kill_spawn = True
-                msg = ("No running Dask cluster found, created a local instance:\n"
-                       f"\t{cluster.scheduler}")
+                msg = "No running Dask cluster found, created a local instance:\n" f"\t{cluster.scheduler}"
                 logger.important(msg)
 
         # Add/update `parallel` to/in keyword args
         kwargs["parallel"] = parallel
 
         results = func(*args, **kwargs)
 
@@ -557,22 +565,23 @@
         # print again in case it got drowned
         if slurm_msg:
             SPYWarning(slurm_msg)
 
         return results
 
     # Append `parallel` keyword entry to wrapped function's docstring and signature
-    parallelDocEntry = \
-    "    parallel : None or bool\n" +\
-    "        If `None` (recommended), processing is automatically performed in \n" +\
-    "        parallel (i.e., concurrently across trials/channel-groups), provided \n" +\
-    "        a dask parallel processing client is running and available. \n" +\
-    "        Parallel processing can be manually disabled by setting `parallel` \n" +\
-    "        to `False`. If `parallel` is `True` but no parallel processing client\n" +\
-    "        is running, computing will be performed sequentially."
+    parallelDocEntry = (
+        "    parallel : None or bool\n"
+        + "        If `None` (recommended), processing is automatically performed in \n"
+        + "        parallel (i.e., concurrently across trials/channel-groups), provided \n"
+        + "        a dask parallel processing client is running and available. \n"
+        + "        Parallel processing can be manually disabled by setting `parallel` \n"
+        + "        to `False`. If `parallel` is `True` but no parallel processing client\n"
+        + "        is running, computing will be performed sequentially."
+    )
     parallel_client_detector.__doc__ = _append_docstring(func, parallelDocEntry)
     parallel_client_detector.__signature__ = _append_signature(func, "parallel")
 
     return parallel_client_detector
 
 
 def process_io(func):
@@ -707,15 +716,15 @@
             with h5py.File(outfilename, "w") as h5fout:
                 h5fout.create_dataset(outdset, data=res)
                 h5_add_metadata(h5fout, details, unique_key_suffix=call_id)
                 h5fout.flush()
         else:
 
             # Create distributed lock (use unique name so it's synced across workers)
-            lock = dd.lock.Lock(name='sequential_write')
+            lock = dd.lock.Lock(name="sequential_write")
             # Either (continue to) compute average or write current chunk
             lock.acquire()
             with h5py.File(outfilename, "r+") as h5fout:
                 main_dset = h5fout[outdset]
                 if keeptrials:
                     main_dset[outgrid] = res
                 else:
@@ -770,15 +779,15 @@
     _append_signature : extend a function's signature
     """
 
     if func.__doc__ is None:
         return
 
     # these are the 4 whitespaces right in front of every doc string line
-    space4 = '    '
+    space4 = "    "
 
     # "Header" insertions always work (an empty docstring is enough to do this).
     # Otherwise ensure the provided `insert_in` section already exists, i.e.,
     # partitioned `sectionHeading` == queried `sectionTitle`
     if insert_in == "Header":
         sectionText, sectionDivider, rest = func.__doc__.partition("Parameters\n")
         textBefore = ""
@@ -795,29 +804,25 @@
         insertAtLine = -1
         while sectionTextList[insertAtLine].isspace():
             insertAtLine -= 1
         insertAtLine = min(-1, insertAtLine + 1)
 
         # to avoid clipping the last line of a parameter description
         if sectionTextList[-1] != space4:
-            sectionTextList.append('\n')
+            sectionTextList.append("\n")
             sectionTextList.append(space4)
     else:
         # this is the 1st line break or the '    --------'
         insertAtLine = 1
 
     sectionText = "".join(sectionTextList[:insertAtLine])
     sectionText += supplement
     sectionText += "".join(sectionTextList[insertAtLine:])
 
-    newDocString = textBefore +\
-                   sectionHeading +\
-                   sectionText +\
-                   sectionDivider +\
-                   rest
+    newDocString = textBefore + sectionHeading + sectionText + sectionDivider + rest
 
     return newDocString
 
 
 def _append_signature(func, kwname, kwdefault=None):
     """
     Local helper to automate keyword argument insertions in function signatures
@@ -858,15 +863,14 @@
     """
 
     funcSignature = inspect.signature(func)
     if kwname in list(funcSignature.parameters):
         newSignature = funcSignature
     else:
         paramList = list(funcSignature.parameters.values())
-        keyword = inspect.Parameter(kwname, inspect.Parameter.POSITIONAL_OR_KEYWORD,
-                                    default=kwdefault)
+        keyword = inspect.Parameter(kwname, inspect.Parameter.POSITIONAL_OR_KEYWORD, default=kwdefault)
         if paramList[-1].name == "kwargs":
             paramList.insert(-1, keyword)
         else:
             paramList.append(keyword)
         newSignature = inspect.Signature(parameters=paramList)
     return newSignature
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/latency.py` & `esi_syncopy-2023.7/syncopy/shared/latency.py`

 * *Files 9% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 import numpy as np
 from copy import deepcopy
 
 # Syncopy imports
 from syncopy.shared.parsers import array_parser
 from syncopy.shared.errors import SPYValueError
 
-available_latencies = ['maxperiod', 'minperiod', 'prestim', 'poststim']
+available_latencies = ["maxperiod", "minperiod", "prestim", "poststim"]
 
 
 def get_analysis_window(data, latency):
     """
     Given the frontend `latency` parameter determine the
     analysis time window [start, end] in seconds
 
@@ -28,65 +28,68 @@
     -------
     window : list
         [start, end] in seconds
     """
 
     # beginnings and ends of all (selected) trials in trigger-relative time in seconds
     if data.selection is not None:
-        trl_starts, trl_ends = data.selection.trialintervals[:, 0], data.selection.trialintervals[:, 1]
+        trl_starts, trl_ends = (
+            data.selection.trialintervals[:, 0],
+            data.selection.trialintervals[:, 1],
+        )
     else:
         trl_starts, trl_ends = data.trialintervals[:, 0], data.trialintervals[:, 1]
 
     if isinstance(latency, str):
         if latency not in available_latencies:
             lgl = f"one of {available_latencies}"
             act = latency
-            raise SPYValueError(lgl, varname='latency', actual=act)
+            raise SPYValueError(lgl, varname="latency", actual=act)
 
         # find overlapping window (timelocked time axis borders) for all trials
-        if latency == 'minperiod':
+        if latency == "minperiod":
             # latest start and earliest finish
             window = [np.max(trl_starts), np.min(trl_ends)]
             if window[0] > window[1]:
-                lgl = 'overlapping trials'
+                lgl = "overlapping trials"
                 act = f"{latency} - no common time window for all trials"
-                raise SPYValueError(lgl, 'latency', act)
+                raise SPYValueError(lgl, "latency", act)
 
         # cover maximal time window where
         # there is still some data in at least 1 trial
-        elif latency == 'maxperiod':
+        elif latency == "maxperiod":
             window = [np.min(trl_starts), np.max(trl_ends)]
 
-        elif latency == 'prestim':
+        elif latency == "prestim":
             if not np.any(trl_starts < 0):
                 lgl = "pre-stimulus recordings"
                 act = "no pre-stimulus (t < 0) events"
-                raise SPYValueError(lgl, 'latency', act)
+                raise SPYValueError(lgl, "latency", act)
             window = [np.min(trl_starts), 0]
 
-        elif latency == 'poststim':
+        elif latency == "poststim":
             if not np.any(trl_ends > 0):
                 lgl = "post-stimulus recordings"
                 act = "no post-stimulus (t > 0) events"
-                raise SPYValueError(lgl, 'latency', act)
+                raise SPYValueError(lgl, "latency", act)
             window = [0, np.max(trl_ends)]
 
     # explicit time window in seconds
     else:
         array_parser(latency, lims=[-np.inf, np.inf], dims=(2,))
         # check that at least some events are covered
         if latency[0] > trl_ends.max():
             lgl = f"start of latency window < {trl_ends.max()}s"
             act = latency[0]
-            raise SPYValueError(lgl, 'latency[0]', act)
+            raise SPYValueError(lgl, "latency[0]", act)
 
         if latency[1] < trl_starts.min():
             lgl = f"end of latency window > {trl_starts.min()}s"
             act = latency[1]
-            raise SPYValueError(lgl, 'latency[1]', act)
+            raise SPYValueError(lgl, "latency[1]", act)
 
         if latency[0] > latency[1]:
             lgl = "start < end latency window"
             act = f"start={latency[0]}, end={latency[1]}"
             raise SPYValueError(lgl, "latency", act)
         window = list(latency)
 
@@ -116,46 +119,48 @@
         selecting only trials which completely cover the analysis ``window``
     numDiscard : int
         Number of to be discarded trials
     """
 
     # beginnings and ends of all (selected) trials in trigger-relative time in seconds
     if data.selection is not None:
-        trl_starts, trl_ends = data.selection.trialintervals[:, 0], data.selection.trialintervals[:, 1]
+        trl_starts, trl_ends = (
+            data.selection.trialintervals[:, 0],
+            data.selection.trialintervals[:, 1],
+        )
         trl_idx = np.array(data.selection.trial_ids)
     else:
         trl_starts, trl_ends = data.trialintervals[:, 0], data.trialintervals[:, 1]
         trl_idx = np.arange(len(data.trials))
 
     # trial idx for whole dataset
     bmask = (trl_starts <= window[0]) & (trl_ends >= window[1])
 
     # trials which fit completely into window
     fit_trl_idx = trl_idx[bmask]
     if fit_trl_idx.size == 0:
-        lgl = 'at least one trial covering the latency window'
-        act = 'no trial that completely covers the latency window'
-        raise SPYValueError(lgl, varname='latency/vartriallen', actual=act)
+        lgl = "at least one trial covering the latency window"
+        act = "no trial that completely covers the latency window"
+        raise SPYValueError(lgl, varname="latency/vartriallen", actual=act)
 
     # the easy part, no selection so we make one
     if data.selection is None:
-        select = {'trials': fit_trl_idx}
+        select = {"trials": fit_trl_idx}
         numDiscard = len(trl_idx) - len(fit_trl_idx)
     else:
         sel_ids = np.array(data.selection.trial_ids)[bmask]
         # match fitting trials with selected ones
         intersection = np.intersect1d(data.selection.trial_ids, sel_ids)
         # intersect result is sorted, restore original selection order
-        fit_trl_idx = np.array(
-            [trl_id for trl_id in data.selection.trial_ids if trl_id in intersection])
+        fit_trl_idx = np.array([trl_id for trl_id in data.selection.trial_ids if trl_id in intersection])
         numDiscard = len(data.selection.trial_ids) - len(fit_trl_idx)
 
         if fit_trl_idx.size == 0:
-            lgl = 'at least one trial covering the latency window'
-            act = 'no trial that completely covers the latency window'
-            raise SPYValueError(lgl, varname='latency/vartriallen', actual=act)
+            lgl = "at least one trial covering the latency window"
+            act = "no trial that completely covers the latency window"
+            raise SPYValueError(lgl, varname="latency/vartriallen", actual=act)
 
-        # now modify 
+        # now modify
         select = deepcopy(data.selection.select)
-        select['trials'] = fit_trl_idx
+        select["trials"] = fit_trl_idx
 
     return select, numDiscard
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/log.py` & `esi_syncopy-2023.7/syncopy/shared/log.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,38 +10,46 @@
 import socket
 import syncopy
 import warnings
 import datetime
 import platform
 
 
-loggername = "syncopy"  # Since this is a library, we should not use the root logger (see Python logging docs).
-loglevels = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']
+loggername = (
+    "syncopy"  # Since this is a library, we should not use the root logger (see Python logging docs).
+)
+loglevels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
 
 
 def setup_logging(spydir=None, session=""):
     """Setup logging on module initialization (in the module root level '__init__.py' file). Should not be called elsewhere."""
 
-    _addLoggingLevel('IMPORTANT', logging.WARNING - 5)  # Add a new custom log level named 'IMPORTANT' between DEBUG and INFO (int value = 25).
+    _addLoggingLevel(
+        "IMPORTANT", logging.WARNING - 5
+    )  # Add a new custom log level named 'IMPORTANT' between DEBUG and INFO (int value = 25).
 
     if os.environ.get("SPYLOGDIR"):
         syncopy.__logdir__ = os.path.abspath(os.path.expanduser(os.environ["SPYLOGDIR"]))
     else:
         if spydir is not None:
             syncopy.__logdir__ = os.path.join(spydir, "logs")
         else:
             syncopy.__logdir__ = os.path.join(os.path.expanduser("~"), ".spy", "logs")
 
     if not os.path.exists(syncopy.__logdir__):
         os.makedirs(syncopy.__logdir__, exist_ok=True)
 
     loglevel = os.getenv("SPYLOGLEVEL", "IMPORTANT")
     numeric_level = getattr(logging, loglevel.upper(), None)
-    if not isinstance(numeric_level, int):  # An invalid string was set as the env variable, default to IMPORTANT.
-        warnings.warn("Invalid log level set in environment variable 'SPYLOGLEVEL', ignoring and using IMPORTANT instead. Hint: Set one of 'DEBUG', 'IMPORTANT', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'.")
+    if not isinstance(
+        numeric_level, int
+    ):  # An invalid string was set as the env variable, default to IMPORTANT.
+        warnings.warn(
+            "Invalid log level set in environment variable 'SPYLOGLEVEL', ignoring and using IMPORTANT instead. Hint: Set one of 'DEBUG', 'IMPORTANT', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'."
+        )
         loglevel = "IMPORTANT"
 
     class HostnameFilter(logging.Filter):
         hostname = platform.node()
 
         def filter(self, record):
             record.hostname = HostnameFilter.hostname
@@ -51,34 +59,40 @@
         def filter(self, record):
             record.session = session
             return True
 
     # The logger for local/sequential stuff -- goes to terminal and to a file.
     spy_logger = logging.getLogger(loggername)
 
-    datefmt_interactive = '%H:%M:%S'
+    datefmt_interactive = "%H:%M:%S"
     datefmt_file = "%Y-%m-%d %H:%M:%S"
 
     # Interactive formatter: no hostname and session info (less clutter on terminal).
-    fmt_interactive = logging.Formatter('%(asctime)s - %(levelname)s: %(message)s', datefmt_interactive)
+    fmt_interactive = logging.Formatter("%(asctime)s - %(levelname)s: %(message)s", datefmt_interactive)
     # Log file formatter: with hostname and session info.
-    fmt_with_hostname = logging.Formatter('%(asctime)s - %(levelname)s - %(hostname)s - %(session)s: %(message)s',
-                                          datefmt_file)
+    fmt_with_hostname = logging.Formatter(
+        "%(asctime)s - %(levelname)s - %(hostname)s - %(session)s: %(message)s",
+        datefmt_file,
+    )
 
     # Allow users to set env variable SPYLOGMSECS to have millisecond precision in logs. Useful for performance profiling.
     if os.environ.get("SPYLOGMSECS"):
-        fmt_interactive = logging.Formatter('%(asctime)s.%(msecs)03d - %(levelname)s: %(message)s', datefmt_interactive)
-        fmt_with_hostname = logging.Formatter('%(asctime)s.%(msecs)03d - %(levelname)s - %(hostname)s - %(session)s: %(message)s',
-                                          datefmt_file)
+        fmt_interactive = logging.Formatter(
+            "%(asctime)s.%(msecs)03d - %(levelname)s: %(message)s", datefmt_interactive
+        )
+        fmt_with_hostname = logging.Formatter(
+            "%(asctime)s.%(msecs)03d - %(levelname)s - %(hostname)s - %(session)s: %(message)s",
+            datefmt_file,
+        )
 
     sh = logging.StreamHandler(sys.stdout)
     sh.setFormatter(fmt_interactive)
     spy_logger.addHandler(sh)
 
-    logfile = os.path.join(syncopy.__logdir__, f'syncopy.log')
+    logfile = os.path.join(syncopy.__logdir__, f"syncopy.log")
     fh = logging.FileHandler(logfile)  # The default mode is 'append'.
     fh.addFilter(HostnameFilter())
     fh.addFilter(SessionFilter())
     fh.setFormatter(fmt_with_hostname)
     spy_logger.addHandler(fh)
 
     spy_logger.setLevel(loglevel)
@@ -86,33 +100,37 @@
     spy_logger.debug(f"Syncopy logger '{loggername}' setup to log to file '{logfile}' at level {loglevel}.")
 
     # Log to per-host files in parallel code by default.
     # Note that this setup handles only the logger of the current host.
     parloglevel = os.getenv("SPYPARLOGLEVEL", "IMPORTANT")
     numeric_level = getattr(logging, parloglevel.upper(), None)
     if not isinstance(numeric_level, int):  # An invalid string was set as the env variable, use default.
-        warnings.warn("Invalid log level set in environment variable 'SPYPARLOGLEVEL', ignoring and using IMPORTANT instead. Hint: Set one of 'DEBUG', 'IMPORTANT', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'.")
+        warnings.warn(
+            "Invalid log level set in environment variable 'SPYPARLOGLEVEL', ignoring and using IMPORTANT instead. Hint: Set one of 'DEBUG', 'IMPORTANT', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'."
+        )
         parloglevel = "IMPORTANT"
     host = platform.node()
     parallel_logger_name = "syncopy_" + host
     spy_parallel_logger = logging.getLogger(parallel_logger_name)
 
-    logfile_par = os.path.join(syncopy.__logdir__, f'syncopy_{host}.log')
+    logfile_par = os.path.join(syncopy.__logdir__, f"syncopy_{host}.log")
     fhp = logging.FileHandler(logfile_par)  # The default mode is 'append'.
     fhp.addFilter(HostnameFilter())
     fhp.addFilter(SessionFilter())
     spy_parallel_logger.setLevel(parloglevel)
 
     fhp.setFormatter(fmt_with_hostname)
     spy_parallel_logger.addHandler(fhp)
     sh = logging.StreamHandler(sys.stdout)
     sh.setFormatter(fmt_interactive)
 
     spy_parallel_logger.addHandler(sh)
-    spy_parallel_logger.debug(f"Syncopy parallel logger '{parallel_logger_name}' setup to log to file '{logfile_par}' at level {parloglevel}.")
+    spy_parallel_logger.debug(
+        f"Syncopy parallel logger '{parallel_logger_name}' setup to log to file '{logfile_par}' at level {parloglevel}."
+    )
 
 
 # See https://stackoverflow.com/questions/2183233/how-to-add-a-custom-loglevel-to-pythons-logging-facility/35804945#35804945
 def _addLoggingLevel(levelName, levelNum, methodName=None):
     """
     Comprehensively adds a new logging level to the `logging` module and the
     currently configured logging class.
@@ -136,23 +154,27 @@
     >>> logging.TRACE
     5
 
     """
     if not methodName:
         methodName = levelName.lower()
 
-    if hasattr(logging, levelName) and hasattr(logging, methodName) and hasattr(logging.getLoggerClass(), methodName):
+    if (
+        hasattr(logging, levelName)
+        and hasattr(logging, methodName)
+        and hasattr(logging.getLoggerClass(), methodName)
+    ):
         return  # Setup already complete.
 
     if hasattr(logging, levelName):
-        raise AttributeError('{} already defined in logging module'.format(levelName))
+        raise AttributeError("{} already defined in logging module".format(levelName))
     if hasattr(logging, methodName):
-        raise AttributeError('{} already defined in logging module'.format(methodName))
+        raise AttributeError("{} already defined in logging module".format(methodName))
     if hasattr(logging.getLoggerClass(), methodName):
-        raise AttributeError('{} already defined in logger class'.format(methodName))
+        raise AttributeError("{} already defined in logger class".format(methodName))
 
     # This method was inspired by the answers to Stack Overflow post
     # http://stackoverflow.com/q/2183233/2988730, especially
     # http://stackoverflow.com/a/13638084/2988730
     def logForLevel(self, message, *args, **kwargs):
         if self.isEnabledFor(levelNum):
             self._log(levelNum, message, args, **kwargs)
@@ -166,17 +188,25 @@
     setattr(logging, methodName, logToRoot)
 
 
 def get_logger():
     """Get the syncopy root logger.
 
     Logs to console by default. To be used in everything that runs on the local computer."""
+
     return logging.getLogger(loggername)
 
 
+def set_loglevel(level):
+    """Set the syncopy root logger level."""
+
+    logger = get_logger()
+    logger.setLevel(level)
+
+
 def get_parallel_logger():
     """
     Get a logger for stuff that is run in parallel.
 
     Logs to a machine-specific file in the SPYLOGDIR by default. To be used in computational routines.
 
     The log directory used is `syncopy.__logdir__`. It can be changed by setting the environment variable SPYLOGDIR before running an application that uses Syncopy.
@@ -204,15 +234,15 @@
     """Delete all '.log' files in the Syncopy logging directory.
 
     The log directory that will be emptied is `syncopy.__logdir__`.
     """
     logdir = syncopy.__logdir__
     num_deleted = 0
     if os.path.isdir(logdir):
-        filelist = [ f for f in os.listdir(logdir) if f.endswith(".log") ]
+        filelist = [f for f in os.listdir(logdir) if f.endswith(".log")]
         for f in filelist:
             logfile = os.path.join(logdir, f)
             try:
                 os.remove(logfile)
                 num_deleted += 1
             except Exception as ex:
                 warnings.warn(f"Could not delete log file '{logfile}': {str(ex)}")
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/metadata.py` & `esi_syncopy-2023.7/syncopy/shared/metadata.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 #  Function for handling additional return values from compute functions
 
 import h5py
 from hmac import compare_digest
 from numbers import Number
 import numpy as np
 
-from syncopy.shared.errors import (SPYInfo, SPYTypeError, SPYValueError, SPYWarning)
+from syncopy.shared.errors import SPYInfo, SPYTypeError, SPYValueError, SPYWarning
 
 
 def metadata_from_hdf5_file(h5py_filename, delete_afterwards=True):
     """
     Extract metadata from h5py file.
 
     This extracts metadata as a standard dictionary from the 'metadata' group of a (virtual or standard)
@@ -31,36 +31,39 @@
     metadata None or dict
         If a dict, that dict contains two more dictionaries at keys `'dsets'` and `'attrs'`. Both
         sub dicts are of type `(str, np.ndarray)`.
     """
     metadata = None
     open_mode = "a" if delete_afterwards else "r"
     with h5py.File(h5py_filename, mode=open_mode) as h5f:
-        if 'data' in h5f:
-            main_dset = h5f['data']
+        if "data" in h5f:
+            main_dset = h5f["data"]
             if main_dset.is_virtual:
                 metadata_list = list()  # A list of dicts.
 
                 # Now open the virtual sources and check there for the metadata.
                 for source_tpl in main_dset.virtual_sources():
                     with h5py.File(source_tpl.file_name, mode=open_mode) as h5f_virtual_part:
-                        if 'metadata' in h5f_virtual_part:
-                            virtual_metadata_grp = h5f_virtual_part['metadata']
+                        if "metadata" in h5f_virtual_part:
+                            virtual_metadata_grp = h5f_virtual_part["metadata"]
                             metadata_list.append(extract_md_group(virtual_metadata_grp))
                             if delete_afterwards:
-                                del h5f_virtual_part['metadata']
+                                del h5f_virtual_part["metadata"]
                 metadata = _merge_md_list(metadata_list)
             else:
                 # the main_dset is not virtual, so just grab the metadata group from the file root.
-                if 'metadata' in h5f:
-                    metadata = extract_md_group(h5f['metadata'])
+                if "metadata" in h5f:
+                    metadata = extract_md_group(h5f["metadata"])
                     if delete_afterwards:
-                        del h5f['metadata']
+                        del h5f["metadata"]
         else:
-            raise SPYValueError("'data' dataset in hd5f file {of}.".format(of=h5py_filename), actual="no such dataset")
+            raise SPYValueError(
+                "'data' dataset in hd5f file {of}.".format(of=h5py_filename),
+                actual="no such dataset",
+            )
     return metadata
 
 
 def _merge_md_list(md_list):
     """
     Merge a list of dictionaries as returned by `extract_md_group()` into a single dictionary.
 
@@ -114,18 +117,22 @@
         if not isinstance(v, np.ndarray):
             raise SPYTypeError(v, varname="value in metadata", expected="np.ndarray")
         if isinstance(k, str):
             attribs[k] = v
         else:
             raise SPYValueError("keys in metadata must be strings", varname="details")
     if check_attr_dsize:
-        for k,v in attribs.items():
-            dsize_kb = np.prod(v.shape) * v.dtype.itemsize / 1024.
+        for k, v in attribs.items():
+            dsize_kb = np.prod(v.shape) * v.dtype.itemsize / 1024.0
             if dsize_kb > 64:
-                SPYWarning("cF details: attribute '{attr}' has size {attr_size} kb, which is > the allowed 64 kb limit.".format(attr=k, attr_size=dsize_kb))
+                SPYWarning(
+                    "cF details: attribute '{attr}' has size {attr_size} kb, which is > the allowed 64 kb limit.".format(
+                        attr=k, attr_size=dsize_kb
+                    )
+                )
     return attribs
 
 
 def parse_cF_returns(res):
     """
     Split the first and second return value of user-supplied cF, if a second one exists.
 
@@ -135,29 +142,41 @@
     -------
     res: np.ndarray
     details: dict or None
     """
     details = None  # This holds the 2nd return value from a cF, if any.
     if isinstance(res, tuple):  # The cF has a 2nd return value.
         if len(res) != 2:
-            raise SPYValueError("user-supplied compute function must return a single ndarray or a tuple with length exactly 2", actual="tuple with length {tl}".format(tl=len(res)))
+            raise SPYValueError(
+                "user-supplied compute function must return a single ndarray or a tuple with length exactly 2",
+                actual="tuple with length {tl}".format(tl=len(res)),
+            )
         else:
             res, details = res
-        if details is not None: # Accept and silently ignore a 2nd return value of None.
+        if details is not None:  # Accept and silently ignore a 2nd return value of None.
             if isinstance(details, dict):
                 for _, v in details.items():
                     if not isinstance(v, np.ndarray):
-                        raise SPYValueError("the second return value of user-supplied compute functions must be a dict containing np.ndarrays")
+                        raise SPYValueError(
+                            "the second return value of user-supplied compute functions must be a dict containing np.ndarrays"
+                        )
                     if v.dtype == object:
-                        raise SPYValueError("the second return value of user-supplied compute functions must be a dict containing np.ndarrays with datatype other than 'np.object'")
+                        raise SPYValueError(
+                            "the second return value of user-supplied compute functions must be a dict containing np.ndarrays with datatype other than 'np.object'"
+                        )
             else:
-                raise SPYValueError("the second return value of user-supplied compute functions must be a dict")
+                raise SPYValueError(
+                    "the second return value of user-supplied compute functions must be a dict"
+                )
     else:
         if not isinstance(res, np.ndarray):
-            raise SPYValueError("user-supplied compute function must return a single ndarray or a tuple with length exactly 2", actual="neither tuple nor np.ndarray")
+            raise SPYValueError(
+                "user-supplied compute function must return a single ndarray or a tuple with length exactly 2",
+                actual="neither tuple nor np.ndarray",
+            )
     return res, details
 
 
 def h5_add_metadata(h5fout, metadata, unique_key_suffix=""):
     """
     Add details, the second return value of user-supplied cF, after parsing with `_parse_backend_metadata`,
     as a 'metadata' group to an existing hdf5 file.
@@ -177,15 +196,15 @@
         integer (i.e., it will be treated as a trial index for which only a single chunk exists).
     """
     if metadata is None:
         return
 
     close_file = False
     if isinstance(h5fout, str):
-        close_file = True # We openend it, we close it.
+        close_file = True  # We openend it, we close it.
         h5fout = h5py.File(h5fout, mode="w")
 
     if isinstance(unique_key_suffix, Number):
         unique_key_suffix = "__" + str(unique_key_suffix) + "_0"
 
     grp = h5fout.require_group("metadata")
     attribs = _parse_backend_metadata(metadata)
@@ -196,15 +215,15 @@
 
     if close_file:
         h5fout.close()
 
 
 def encode_unique_md_label(label, trial_idx, call_idx=0):
     """Assemble something like `test`, `2` and `0` into `test__2_0`."""
-    return(label + "__" + str(trial_idx) + "_" + str(call_idx))
+    return label + "__" + str(trial_idx) + "_" + str(call_idx)
 
 
 def decode_unique_md_label(unique_label):
     """
     Splits something like `test__2_0` into `test`, `2` and `0`.
 
     Parameters
@@ -218,17 +237,19 @@
     try:
         lab_ind = unique_label.rsplit("__")
         label = lab_ind[0]
         trialidx_callidx = lab_ind[1].rsplit("_")
         trialidx = trialidx_callidx[0]
         callidx = trialidx_callidx[1]
     except Exception as ex:
-        raise SPYValueError(f"Could not decode metadata key '{unique_label}' into label, trial_index and chunk index. Expected input string in format `<label>__<trial_idx>_<chunk_idx>', e.g. 'pp__0_0': '{str(ex)}'")
+        raise SPYValueError(
+            f"Could not decode metadata key '{unique_label}' into label, trial_index and chunk index. Expected input string in format `<label>__<trial_idx>_<chunk_idx>', e.g. 'pp__0_0': '{str(ex)}'"
+        )
 
-    return label, trialidx ,callidx
+    return label, trialidx, callidx
 
 
 def extract_md_group(md):
     """
     Extract metadata from h5py 'metadata' group and return a standard dict.
 
     Parameters
@@ -249,19 +270,20 @@
 
     """
     Helper routine to "unpack" hdf5 0-dim attribute arrays,
     as even though they are effectively scalar,
     they can't be directly serialized to go into .info
     """
 
-    rules = {'float': lambda x: float(x),
-             'int': lambda x: int(x),
-             'bool': lambda x: bool(x),
-             'str': lambda x: str(x)
-             }
+    rules = {
+        "float": lambda x: float(x),
+        "int": lambda x: int(x),
+        "bool": lambda x: bool(x),
+        "str": lambda x: str(x),
+    }
 
     if rule not in rules:
         lgl = f"one of {rules.keys()}"
         raise SPYValueError(lgl, "rule", rule)
 
     if arr.ndim != 0:
         lgl = "0-dim numpy array"
@@ -283,19 +305,21 @@
             ref_hash = fhash
             ref_id = trl_id
         else:
             if not compare_digest(ref_hash, fhash):
                 trl_mismatches.append(trl_id)
     # some freq axis were different
     if trl_mismatches:
-        msg = (f"Frequency axes hashes mismatched for {len(trl_mismatches)} trials: "
-               f"{trl_mismatches} against reference hash from first trial {ref_id}.")
+        msg = (
+            f"Frequency axes hashes mismatched for {len(trl_mismatches)} trials: "
+            f"{trl_mismatches} against reference hash from first trial {ref_id}."
+        )
         SPYWarning(msg)
         out.log = msg
-        out.info['mismatched freq. axis trial ids'] = trl_mismatches
+        out.info["mismatched freq. axis trial ids"] = trl_mismatches
 
 
 def metadata_nest(metadata):
     """
     Nest md dictionary keys with identical label prefixes into sub dictionaries.
 
     Put another way, this will add a layer of new dictionaries, which are the unique label
@@ -347,15 +371,21 @@
     See also
     --------
     metadata_nest: performs the reverse operation of this function.
     """
     metadata_unnested = dict()
     for nested_category_name, nested_dict in metadata.items():
         if not isinstance(nested_dict, dict):
-            raise SPYValueError(legal="Dict containing only other dictionaries at first level.", varname="metadata", actual=f"Value at key '{nested_category_name}' is not a dict.")
+            raise SPYValueError(
+                legal="Dict containing only other dictionaries at first level.",
+                varname="metadata",
+                actual=f"Value at key '{nested_category_name}' is not a dict.",
+            )
         for unique_attr_label, nested_value in nested_dict.items():
             if unique_attr_label in metadata_unnested:  # It's already in there, from a previous dict!
-                raise SPYValueError(legal="Dict containing no duplicated keys in nested sub dictionaries at first level.", varname="metadata", actual=f"Duplicate key '{unique_attr_label}': cannot unnest without losing data.")
+                raise SPYValueError(
+                    legal="Dict containing no duplicated keys in nested sub dictionaries at first level.",
+                    varname="metadata",
+                    actual=f"Duplicate key '{unique_attr_label}': cannot unnest without losing data.",
+                )
             metadata_unnested[unique_attr_label] = nested_value
     return metadata_unnested
-
-
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/parsers.py` & `esi_syncopy-2023.7/syncopy/shared/parsers.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,16 +5,15 @@
 
 # Builtin/3rd party package imports
 import os
 import numpy as np
 
 # Local imports
 from syncopy.shared.filetypes import FILE_EXT
-from syncopy.shared.errors import (SPYIOError, SPYTypeError, SPYValueError,
-                                   SPYWarning)
+from syncopy.shared.errors import SPYIOError, SPYTypeError, SPYValueError, SPYWarning
 
 __all__ = []
 
 
 def io_parser(fs_loc, varname="", isfile=True, ext="", exists=True):
     """
     Parse file-system location strings for reading/writing files/directories
@@ -88,17 +87,17 @@
         raise SPYIOError(fs_loc, exists=False)
     if not exists and os.path.exists(fs_loc):
         raise SPYIOError(fs_loc, exists=True)
 
     # First, take care of directories...
     if not isfile:
         isdir = os.path.isdir(fs_loc)
-        if (isdir and not exists):
-            raise SPYIOError (fs_loc, exists=isdir)
-        elif (not isdir and exists):
+        if isdir and not exists:
+            raise SPYIOError(fs_loc, exists=isdir)
+        elif not isdir and exists:
             raise SPYValueError(legal="directory", actual="file")
         else:
             return fs_loc
 
     # ...now files
     else:
 
@@ -119,17 +118,17 @@
             if file_ext not in str(ext) or error:
                 if isinstance(ext, (list, np.ndarray)):
                     ext = "'" + "or '".join(ex + "' " for ex in ext)
                 raise SPYValueError(ext, varname="filename-extension", actual=file_ext)
 
         # Now make sure file does or does not exist
         isfile = os.path.isfile(fs_loc)
-        if (isfile and not exists):
+        if isfile and not exists:
             raise SPYIOError(fs_loc, exists=isfile)
-        elif (not isfile and exists):
+        elif not isfile and exists:
             raise SPYValueError(legal="file", actual="directory")
         else:
             return fs_loc.split(file_name)[0], file_name
 
 
 def scalar_parser(var, varname="", ntype=None, lims=None):
     """
@@ -210,22 +209,33 @@
             val = np.array([var.real, var.imag])
             legal = "both real and imaginary part to be "
         else:
             val = np.array([var])
             legal = "value to be "
         if np.any(val < lims[0]) or np.any(val > lims[1]) or not np.isfinite(var):
             legal += "greater or equals {lb:s} and less or equals {ub:s}"
-            raise SPYValueError(legal.format(lb=str(lims[0]), ub=str(lims[1])),
-                                varname=varname, actual=str(var))
+            raise SPYValueError(
+                legal.format(lb=str(lims[0]), ub=str(lims[1])),
+                varname=varname,
+                actual=str(var),
+            )
 
     return
 
 
-def array_parser(var, varname="", ntype=None, hasinf=None, hasnan=None,
-                 lims=None, dims=None, issorted=None):
+def array_parser(
+    var,
+    varname="",
+    ntype=None,
+    hasinf=None,
+    hasnan=None,
+    lims=None,
+    dims=None,
+    issorted=None,
+):
     """
     Parse array-like objects
 
     Parameters
     ----------
     var : array_like
         Array object to verify
@@ -360,30 +370,36 @@
 
     # If array-element order parsing is requested by `ntype` and/or `dims` are not
     # set, use sane defaults to ensure array is numeric and one-dimensional
     if issorted is not None:
         if ntype is None:
             ntype = "numeric"
         if dims is None:
-            dims = (None, )
+            dims = (None,)
 
     # If required, parse type (handle "int_like" and "numeric" separately)
     if ntype is not None:
         msg = "dtype = {dt:s}"
         if ntype in ["numeric", "int_like"]:
             if not np.issubdtype(arr.dtype, np.number):
-                raise SPYValueError(msg.format(dt="numeric"), varname=varname,
-                                    actual=msg.format(dt=str(arr.dtype)))
+                raise SPYValueError(
+                    msg.format(dt="numeric"),
+                    varname=varname,
+                    actual=msg.format(dt=str(arr.dtype)),
+                )
             if ntype == "int_like":
                 if not np.array_equal(arr, np.round(arr)):
                     raise SPYValueError(msg.format(dt=ntype), varname=varname)
         else:
             if not np.issubdtype(arr.dtype, np.dtype(ntype).type):
-                raise SPYValueError(msg.format(dt=ntype), varname=varname,
-                                    actual=msg.format(dt=str(arr.dtype)))
+                raise SPYValueError(
+                    msg.format(dt=ntype),
+                    varname=varname,
+                    actual=msg.format(dt=str(arr.dtype)),
+                )
 
     # If required, parse finiteness of array-elements
     if hasinf is not None:
         if not hasinf and np.isinf(arr).any():
             lgl = "finite numerical array"
             act = "array with {} `inf` entries".format(str(np.isinf(arr).sum()))
             raise SPYValueError(legal=lgl, varname=varname, actual=act)
@@ -410,16 +426,19 @@
             amin = min(fi_arr.real.min(), fi_arr.imag.min())
             amax = max(fi_arr.real.max(), fi_arr.imag.max())
         else:
             amin = fi_arr.min()
             amax = fi_arr.max()
         if amin < lims[0] or amax > lims[1]:
             legal = "all array elements to be bounded by {lb:s} and {ub:s}"
-            raise SPYValueError(legal.format(lb=str(lims[0]), ub=str(lims[1])),
-                                varname=varname, actual=f"array with range {amin} to {amax}")
+            raise SPYValueError(
+                legal.format(lb=str(lims[0]), ub=str(lims[1])),
+                varname=varname,
+                actual=f"array with range {amin} to {amax}",
+            )
 
     # If required parse dimensional layout of array
     if dims is not None:
 
         # Account for the special case of 1d character arrays (that
         # collapse to 0d-arrays when squeezed)
         ischar = int(np.issubdtype(arr.dtype, np.dtype("str").type))
@@ -431,25 +450,30 @@
             else:
                 if arr.size == 1:
                     ashape = arr.shape
                 else:
                     ashape = max((ischar,), arr.squeeze().shape)
             if len(dims) != len(ashape):
                 msg = "{}-dimensional array"
-                raise SPYValueError(legal=msg.format(len(dims)), varname=varname,
-                                    actual=msg.format(len(ashape)))
+                raise SPYValueError(
+                    legal=msg.format(len(dims)),
+                    varname=varname,
+                    actual=msg.format(len(ashape)),
+                )
             for dk, dim in enumerate(dims):
                 if dim is not None and ashape[dk] != dim:
-                    raise SPYValueError("array of shape " + str(dims),
-                                        varname=varname, actual="shape = " + str(arr.shape))
+                    raise SPYValueError(
+                        "array of shape " + str(dims),
+                        varname=varname,
+                        actual="shape = " + str(arr.shape),
+                    )
         else:
             ndim = max(ischar, arr.ndim)
             if ndim != dims:
-                raise SPYValueError(str(dims) + "d-array", varname=varname,
-                                    actual=str(ndim) + "d-array")
+                raise SPYValueError(str(dims) + "d-array", varname=varname, actual=str(ndim) + "d-array")
 
     # If required check if array elements are orderd by magnitude
     if issorted is not None:
         if not np.all(np.isreal(arr)):
             lgl = "real-valued array"
             act = "array containing complex elements"
             raise SPYValueError(legal=lgl, varname=varname, actual=act)
@@ -524,42 +548,43 @@
             msg = "Syncopy {} object".format(dataclass)
             raise SPYTypeError(data, varname=varname, expected=msg)
 
     # If requested, ensure object contains data (or not)
     if empty is not None:
         legal = "{status:s} Syncopy data object"
         if empty and not data._is_empty():
-            raise SPYValueError(legal=legal.format(status="empty"),
-                                varname=varname,
-                                actual="non-empty")
+            raise SPYValueError(legal=legal.format(status="empty"), varname=varname, actual="non-empty")
         elif not empty and data._is_empty():
-            raise SPYValueError(legal=legal.format(status="non-empty"),
-                                varname=varname,
-                                actual="empty")
+            raise SPYValueError(legal=legal.format(status="non-empty"), varname=varname, actual="empty")
 
     # If requested, ensure proper access to object
     if writable is not None:
         legal = "{access:s} to Syncopy data object"
         actual = "mode = {mode:s}"
         if writable and data.mode == "r":
-            raise SPYValueError(legal=legal.format(access="write-access"),
-                                varname=varname,
-                                actual=actual.format(mode=data.mode))
+            raise SPYValueError(
+                legal=legal.format(access="write-access"),
+                varname=varname,
+                actual=actual.format(mode=data.mode),
+            )
         elif not writable and data.mode != "r":
-            raise SPYValueError(legal=legal.format(access="read-only-access"),
-                                varname=varname,
-                                actual=actual.format(mode=data.mode))
+            raise SPYValueError(
+                legal=legal.format(access="read-only-access"),
+                varname=varname,
+                actual=actual.format(mode=data.mode),
+            )
 
     # If requested, check integrity of dimensional information (if non-empty)
     if dimord is not None:
         base = "Syncopy {diminfo:s} data object"
         if data.dimord != dimord:
             legal = base.format(diminfo="'" + "' x '".join(str(dim) for dim in dimord) + "'")
-            actual = base.format(diminfo="'" + "' x '".join(str(dim) for dim in data.dimord)
-                                 + "' " if data.dimord else "empty")
+            actual = base.format(
+                diminfo="'" + "' x '".join(str(dim) for dim in data.dimord) + "' " if data.dimord else "empty"
+            )
             raise SPYValueError(legal=legal, varname=varname, actual=actual)
 
     return
 
 
 def filename_parser(filename, is_in_valid_container=None):
     """
@@ -620,93 +645,100 @@
     See also
     --------
     io_parser : check file and folder names for existence
 
     """
     if filename is None:
         return {
-        "filename": None,
-        "container": None,
-        "folder": None,
-        "tag": None,
-        "basename": None,
-        "extension": None
+            "filename": None,
+            "container": None,
+            "folder": None,
+            "tag": None,
+            "basename": None,
+            "extension": None,
         }
 
     filename = os.path.abspath(os.path.expanduser(filename))
 
     folder, filename = os.path.split(filename)
     container = folder.split(os.path.sep)[-1]
     basename, ext = os.path.splitext(filename)
 
     if filename.count(".") > 2:
-        raise SPYValueError(legal="single extension, found {}".format(filename.count(".")),
-                            actual=filename, varname="filename")
+        raise SPYValueError(
+            legal="single extension, found {}".format(filename.count(".")),
+            actual=filename,
+            varname="filename",
+        )
     if ext == FILE_EXT["dir"] and basename.count(".") > 0:
-        raise SPYValueError(legal="no extension, found {}".format(basename.count(".")),
-                            actual=basename, varname="container")
+        raise SPYValueError(
+            legal="no extension, found {}".format(basename.count(".")),
+            actual=basename,
+            varname="container",
+        )
 
     if ext == FILE_EXT["info"]:
         filename = basename
         basename, ext = os.path.splitext(filename)
     elif ext == FILE_EXT["dir"]:
         return {
-        "filename": None,
-        "container": filename,
-        "folder": folder,
-        "tag": None,
-        "basename": basename,
-        "extension": ext
+            "filename": None,
+            "container": filename,
+            "folder": folder,
+            "tag": None,
+            "basename": basename,
+            "extension": ext,
         }
 
     if ext not in FILE_EXT["data"] + (FILE_EXT["dir"],):
-        raise SPYValueError(legal=FILE_EXT["data"],
-                            actual=ext, varname="filename extension")
+        raise SPYValueError(legal=FILE_EXT["data"], actual=ext, varname="filename extension")
 
     folderExtIsSpy = os.path.splitext(container)[1] == FILE_EXT["dir"]
     if is_in_valid_container is not None:
         if not folderExtIsSpy and is_in_valid_container:
-            raise SPYValueError(legal=FILE_EXT["dir"],
-                                actual=os.path.splitext(container)[1],
-                                varname="folder extension")
+            raise SPYValueError(
+                legal=FILE_EXT["dir"],
+                actual=os.path.splitext(container)[1],
+                varname="folder extension",
+            )
         elif folderExtIsSpy and not is_in_valid_container:
-            raise SPYValueError(legal='not ' + FILE_EXT["dir"],
-                                actual=os.path.splitext(container)[1],
-                                varname="folder extension")
-
+            raise SPYValueError(
+                legal="not " + FILE_EXT["dir"],
+                actual=os.path.splitext(container)[1],
+                varname="folder extension",
+            )
 
     if folderExtIsSpy:
         containerBasename = os.path.splitext(container)[0]
         if not basename.startswith(containerBasename):
-            raise SPYValueError(legal=containerBasename,
-                                actual=filename,
-                                varname='start of filename')
+            raise SPYValueError(legal=containerBasename, actual=filename, varname="start of filename")
         tag = basename.partition(containerBasename)[-1]
         if tag == "":
             tag = None
         else:
-            if tag[0] == '_': tag = tag[1:]
+            if tag[0] == "_":
+                tag = tag[1:]
         basename = containerBasename
     else:
         container = None
         tag = None
 
     return {
         "filename": filename,
         "container": container,
         "folder": folder,
         "tag": tag,
         "basename": basename,
-        "extension": ext
+        "extension": ext,
     }
 
 
 def sequence_parser(sequence, content_type=None, varname=""):
 
-    '''
+    """
     Check if input is of sequence (list, tuple, array..)
     type. Intended for function arguments like
     `add_fields = ['fieldA', 'fieldB']`. For numeric
     sequences (aka arrays) better to use the `array_parser`.
 
     Parameters
     ----------
@@ -732,31 +764,25 @@
     sequence_parser(seq1)
 
     This will raise a `SPYTypeError` as the
     actual content type is `str`
 
     sequence_parser(seq1, content_type=int)
 
-    '''
+    """
 
     # this does NOT capture str and dict
     try:
         iter(sequence)
     except TypeError:
-        expected = 'sequence'
-        raise SPYTypeError(sequence,
-                           varname=varname,
-                           expected=expected)
+        expected = "sequence"
+        raise SPYTypeError(sequence, varname=varname, expected=expected)
 
     if isinstance(sequence, str) or isinstance(sequence, dict):
-        expected = 'sequence'
-        raise SPYTypeError(sequence,
-                           varname=varname,
-                           expected=expected)
+        expected = "sequence"
+        raise SPYTypeError(sequence, varname=varname, expected=expected)
 
     if content_type is not None:
         for element in sequence:
             if not isinstance(element, content_type):
                 expected = content_type.__name__
-                raise SPYTypeError(element,
-                                   varname=f"element of {varname}",
-                                   expected=expected)
+                raise SPYTypeError(element, varname=f"element of {varname}", expected=expected)
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/queries.py` & `esi_syncopy-2023.7/syncopy/shared/queries.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 # -*- coding: utf-8 -*-
 #
 # Auxiliary functions for querying things/people
-# 
+#
 
 __all__ = []
 
 
 def user_yesno(msg, default=None):
     """
     Docstring
     """
 
     # Parse optional `default` answer
-    valid = {"yes": True, "y": True, "ye":True, "no":False, "n":False}
+    valid = {"yes": True, "y": True, "ye": True, "no": False, "n": False}
     if default is None:
         suffix = " [y/n] "
     elif default == "yes":
         suffix = " [Y/n] "
     elif default == "no":
         suffix = " [y/N] "
 
@@ -40,23 +40,20 @@
     default = str (default option, same as above)
     """
 
     # Add trailing whitespace to `msg` if not already present and append
     # default reply (if provided)
     suffix = "" + " " * (not msg.endswith(" "))
     if default is not None:
-        default = default.replace("[", "").replace("]","")
+        default = default.replace("[", "").replace("]", "")
         assert default in valid
         suffix = "[Default: '{}'] ".format(default)
 
     # Wait for valid user input and return choice upon receipt
     while True:
         choice = input(msg + suffix)
         if default is not None and choice == "":
             return default
         elif choice in valid:
             return choice
         else:
-            print("Please respond with '" + \
-                  "or '".join(opt + "' " for opt in valid) + "\n")
-
-
+            print("Please respond with '" + "or '".join(opt + "' " for opt in valid) + "\n")
```

### Comparing `esi_syncopy-2023.5/syncopy/shared/tools.py` & `esi_syncopy-2023.7/syncopy/shared/tools.py`

 * *Files 2% similar despite different names*

```diff
@@ -61,15 +61,15 @@
         deep: bool
             Whether to produce a deep copy. Defaults to `True`.
 
         Returns
         -------
         Copy of StructDict.
         """
-        if(deep):
+        if deep:
             return self.deepcopy()
         else:
             obj = type(self).__new__(self.__class__)
             obj.__dict__.update(self.__dict__)
             return obj
 
     def deepcopy(self):
@@ -133,24 +133,24 @@
         value = value.tolist()
 
     if isinstance(value, range):
         value = list(value)
 
     # unpack the list, if ppl mix types this will go wrong
     if isinstance(value, list) and len(value) != 0:
-        if hasattr(value[0], 'is_integer'):
+        if hasattr(value[0], "is_integer"):
             value = [float(v) for v in value]
         # should only be the integers
         elif isinstance(value[0], Number) and not isinstance(value[0], bool):
             value = [int(v) for v in value]
 
     # singleton/non-sequence type entries
     if isinstance(value, Number) and not isinstance(value, bool):
         # all floating types have this method
-        if hasattr(value, 'is_integer'):
+        if hasattr(value, "is_integer"):
             # get rid of np.int64 or np.float32
             value = int(value) if value.is_integer() else float(value)
         else:
             value = int(value)
 
     return value
 
@@ -190,21 +190,21 @@
         # check only set parameters
         if par_name in lcls:
             value = _serialize_value(lcls[par_name])
             new_cfg[par_name] = value
 
     # 'select' only allowed dictionary parameter within kwargs
     # we can 'pop' here as selection got digested beforehand by @unwrap_select
-    sdict = kwargs.pop('select', None)
+    sdict = kwargs.pop("select", None)
     if sdict is not None:
         # serialized selection dict
         ser_sdict = dict()
         for sel_key in sdict:
             ser_sdict[sel_key] = _serialize_value(sdict[sel_key])
-        new_cfg['select'] = ser_sdict
+        new_cfg["select"] = ser_sdict
 
     # should only be 'parallel' and 'chan_per_worker'
     for key in kwargs:
         new_cfg[key] = _serialize_value(kwargs[key])
 
     # use instantiation for a final check
     SerializableDict(new_cfg)
@@ -293,38 +293,36 @@
     if np.issubdtype(type(selection), np.number):
         selection = [selection]
 
     # Ensure selection is within `tol` bounds from `source`
     if tol is not None:
         if not np.all([np.all((np.abs(source - value)) < tol) for value in selection]):
             lgl = "all elements of `selection` to be within a {0:2.4f}-band around `source`"
-            act = "values in `selection` deviating further than given tolerance " +\
-                "of {0:2.4f} from source"
-            raise SPYValueError(legal=lgl.format(tol),
-                                varname="selection",
-                                actual=act.format(tol))
+            act = "values in `selection` deviating further than given tolerance " + "of {0:2.4f} from source"
+            raise SPYValueError(legal=lgl.format(tol), varname="selection", actual=act.format(tol))
 
     # Do not perform O(n) potentially unnecessary sort operations...
     issorted = True
 
     # Interval-selections are a lot easier than discrete time-points...
     if span:
-        idx = np.intersect1d(np.where(source >= selection[0])[0],
-                             np.where(source <= selection[1])[0])
+        idx = np.intersect1d(np.where(source >= selection[0])[0], np.where(source <= selection[1])[0])
     else:
         issorted = True
         if source.size > 1 and np.diff(source).min() < 0:
             issorted = False
             orig = np.array(source, copy=True)
             idx_orig = np.argsort(orig)
             source = orig[idx_orig]
         idx = np.searchsorted(source, selection, side="left")
         leftNbrs = np.abs(selection - source[np.maximum(idx - 1, np.zeros(idx.shape, dtype=np.intp))])
-        rightNbrs = np.abs(selection - source[np.minimum(idx, np.full(idx.shape, source.size - 1, dtype=np.intp))])
-        shiftLeft = ((idx == source.size) | (leftNbrs < rightNbrs))
+        rightNbrs = np.abs(
+            selection - source[np.minimum(idx, np.full(idx.shape, source.size - 1, dtype=np.intp))]
+        )
+        shiftLeft = (idx == source.size) | (leftNbrs < rightNbrs)
         idx[shiftLeft] -= 1
 
     # Account for potentially unsorted selections (and thus unordered `idx`)
     if squash_duplicates:
         _, xdi = np.unique(idx.astype(np.intp), return_index=True)
         idx = idx[np.sort(xdi)]
 
@@ -357,13 +355,13 @@
     To see the default input arguments of :meth:`syncopy.freqanalysis` use
 
     >>> spy.get_defaults(spy.freqanalysis)
     """
 
     if not callable(obj):
         raise SPYTypeError(obj, varname="obj", expected="SyNCoPy function or class")
-    dct = {k: v.default for k, v in inspect.signature(obj).parameters.items()\
-           if v.default != v.empty and v.name != "cfg"}
+    dct = {
+        k: v.default
+        for k, v in inspect.signature(obj).parameters.items()
+        if v.default != v.empty and v.name != "cfg"
+    }
     return StructDict(dct)
-
-
-
```

### Comparing `esi_syncopy-2023.5/syncopy/specest/README.md` & `esi_syncopy-2023.7/syncopy/specest/README.md`

 * *Files identical despite different names*

### Comparing `esi_syncopy-2023.5/syncopy/specest/_norm_spec.py` & `esi_syncopy-2023.7/syncopy/specest/_norm_spec.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,25 +2,25 @@
 #
 # Helper routines to normalize Fourier spectra
 #
 
 import numpy as np
 
 
-def _norm_spec(ftr, nSamples, fs, mode='bins'):
+def _norm_spec(ftr, nSamples, fs, mode="bins"):
 
     """
     Normalizes the complex Fourier transform to
     power spectral density or 1Hz-bin units.
     """
 
     # frequency bins
-    if mode == 'density':
+    if mode == "density":
         delta_f = fs / nSamples
-    elif mode == 'bins':
+    elif mode == "bins":
         delta_f = 1
 
     ftr *= np.sqrt(2) / (nSamples * np.sqrt(delta_f))
 
     return ftr
 
 
@@ -30,17 +30,17 @@
     Helper function to normalize tapers such
     that the resulting spectra are normalized
     with respect to the sum of the window. Meaning
     that the total original (untapered) power gets
     distributed over the spectral window response.
     """
 
-    if taper == 'dpss':
+    if taper == "dpss":
         windows *= np.sqrt(nSamples)
-    elif taper == 'boxcar':
+    elif taper == "boxcar":
         windows *= np.sqrt(nSamples / windows.sum())
     # weird 3 point normalization,
     # checks out exactly for 'hann' though
     else:
         windows *= np.sqrt(4 / 3) * np.sqrt(nSamples / windows.sum())
 
     return windows
```

### Comparing `esi_syncopy-2023.5/syncopy/specest/compRoutines.py` & `esi_syncopy-2023.7/syncopy/specest/compRoutines.py`

 * *Files 4% similar despite different names*

```diff
@@ -32,38 +32,46 @@
 from .wavelet import wavelet
 from .fooofspy import fooofspy
 
 
 # Local imports
 from syncopy.shared.errors import SPYValueError, SPYWarning, SPYParallelLog
 from syncopy.shared.tools import best_match
-from syncopy.shared.computational_routine import ComputationalRoutine, propagate_properties
+from syncopy.shared.computational_routine import (
+    ComputationalRoutine,
+    propagate_properties,
+)
 from syncopy.shared.kwarg_decorators import process_io
 from syncopy.shared.metadata import (
     encode_unique_md_label,
     decode_unique_md_label,
     metadata_from_hdf5_file,
     check_freq_hashes,
-    metadata_nest
+    metadata_nest,
 )
 
-from syncopy.shared.const_def import (
-    spectralConversions,
-    spectralDTypes
-)
+from syncopy.shared.const_def import spectralConversions, spectralDTypes
 
 # -----------------------
 # MultiTaper FFT
 # -----------------------
 
 
 @process_io
-def mtmfft_cF(trl_dat, foi=None, timeAxis=0, keeptapers=True,
-              polyremoval=None, output="pow",
-              noCompute=False, chunkShape=None, method_kwargs=None):
+def mtmfft_cF(
+    trl_dat,
+    foi=None,
+    timeAxis=0,
+    keeptapers=True,
+    polyremoval=None,
+    output="pow",
+    noCompute=False,
+    chunkShape=None,
+    method_kwargs=None,
+):
 
     """
     Compute (multi-)tapered Fourier transform of multi-channel time series data
 
     Parameters
     ----------
     trl_dat : 2D :class:`numpy.ndarray`
@@ -129,54 +137,54 @@
     syncopy.freqanalysis : parent metafunction
     MultiTaperFFT : :class:`~syncopy.shared.computational_routine.ComputationalRoutine` instance
                      that calls this method as :meth:`~syncopy.shared.computational_routine.ComputationalRoutine.computeFunction`
     numpy.fft.rfft : NumPy's FFT implementation
     """
     # Re-arrange array if necessary and get dimensional information
     if timeAxis != 0:
-        dat = trl_dat.T       # does not copy but creates view of `trl_dat`
+        dat = trl_dat.T  # does not copy but creates view of `trl_dat`
     else:
         dat = trl_dat
 
-    if method_kwargs['nSamples'] is None:
+    if method_kwargs["nSamples"] is None:
         nSamples = dat.shape[0]
     else:
-        nSamples = method_kwargs['nSamples']
+        nSamples = method_kwargs["nSamples"]
 
     nChannels = dat.shape[1]
 
     # Determine frequency band and shape of output
     # (time=1 x taper x freq x channel)
     freqs = np.fft.rfftfreq(nSamples, 1 / method_kwargs["samplerate"])
     _, freq_idx = best_match(freqs, foi, squash_duplicates=True)
     nFreq = freq_idx.size
-    nTaper = method_kwargs["taper_opt"].get('Kmax', 1)
+    nTaper = method_kwargs["taper_opt"].get("Kmax", 1)
     outShape = (1, max(1, nTaper * keeptapers), nFreq, nChannels)
 
     # For initialization of computational routine,
     # just return output shape and dtype
     if noCompute:
         return outShape, spectralDTypes[output]
 
     # detrend, does not work with 'FauxTrial' data..
     if polyremoval == 0:
-        dat = signal.detrend(dat, type='constant', axis=0, overwrite_data=True)
+        dat = signal.detrend(dat, type="constant", axis=0, overwrite_data=True)
     elif polyremoval == 1:
-        dat = signal.detrend(dat, type='linear', axis=0, overwrite_data=True)
+        dat = signal.detrend(dat, type="linear", axis=0, overwrite_data=True)
 
     # call actual specest method
     res, freqs = mtmfft(dat, **method_kwargs)
 
     # attach time-axis and convert to output
     spec = res[np.newaxis, :, freq_idx, :]
     spec = spectralConversions[output](spec)
 
     # Hash the freqs and add to second return value.
-    freqs_hash = blake2b(freqs).hexdigest().encode('utf-8')
-    metadata = {'freqs_hash': np.array(freqs_hash)}  # Will have dtype='|S128'
+    freqs_hash = blake2b(freqs).hexdigest().encode("utf-8")
+    metadata = {"freqs_hash": np.array(freqs_hash)}  # Will have dtype='|S128'
 
     # Average across tapers if wanted
     # averaging is only valid spectral estimate
     # if output == 'pow'! (gets checked in parent meta)
     if not keeptapers:
         return spec.mean(axis=1, keepdims=True), metadata
 
@@ -198,32 +206,32 @@
 
     computeFunction = staticmethod(mtmfft_cF)
 
     # 1st argument,the data, gets omitted
     valid_kws = list(signature(mtmfft).parameters.keys())[1:]
     valid_kws += list(signature(mtmfft_cF).parameters.keys())[1:]
     # hardcode some parameter names which got digested from the frontend
-    valid_kws += ['tapsmofrq', 'nTaper', 'pad', 'fooof_opt']
+    valid_kws += ["tapsmofrq", "nTaper", "pad", "fooof_opt"]
 
     def process_metadata(self, data, out):
 
         # General-purpose loading of metadata.
         metadata = metadata_from_hdf5_file(out.filename)
 
         check_freq_hashes(metadata, out)
 
         # channels and trialdefinition
         propagate_properties(data, out, self.keeptrials)
 
         taper_kw = self.cfg["method_kwargs"]["taper"]
         if taper_kw is None:
-            out.taper = np.array(['None'])
+            out.taper = np.array(["None"])
         # multi-tapering
-        elif taper_kw == 'dpss':
-            nTaper =  self.outputShape[out.dimord.index("taper")]
+        elif taper_kw == "dpss":
+            nTaper = self.outputShape[out.dimord.index("taper")]
             out.taper = np.array([taper_kw + str(i) for i in range(nTaper)])
         # just a single taper
         else:
             out.taper = np.array([taper_kw])
         out.freq = self.cfg["foi"]
 
 
@@ -231,23 +239,30 @@
 # MultiTaper Windowed FFT
 # -----------------------
 
 
 # Local workhorse that performs the computational heavy lifting
 @process_io
 def mtmconvol_cF(
-        trl_dat,
-        soi,
-        postselect,
-        equidistant=True,
-        toi=None,
-        foi=None,
-        nTaper=1, tapsmofrq=None, timeAxis=0,
-        keeptapers=True, polyremoval=0, output="pow",
-        noCompute=False, chunkShape=None, method_kwargs=None):
+    trl_dat,
+    soi,
+    postselect,
+    equidistant=True,
+    toi=None,
+    foi=None,
+    nTaper=1,
+    tapsmofrq=None,
+    timeAxis=0,
+    keeptapers=True,
+    polyremoval=0,
+    output="pow",
+    noCompute=False,
+    chunkShape=None,
+    method_kwargs=None,
+):
     """
     Perform time-frequency analysis on multi-channel time series data using a sliding window FFT
 
     Parameters
     ----------
     trl_dat : 2D :class:`numpy.ndarray`
         Uniformly sampled multi-channel time-series
@@ -331,60 +346,58 @@
                           instance that calls this method as
                           :meth:`~syncopy.shared.computational_routine.ComputationalRoutine.computeFunction`
     scipy.signal.stft : SciPy's STFT implementation
     """
 
     # Re-arrange array if necessary and get dimensional information
     if timeAxis != 0:
-        dat = trl_dat.T       # does not copy but creates view of `trl_dat`
+        dat = trl_dat.T  # does not copy but creates view of `trl_dat`
     else:
         dat = trl_dat
 
     # Get shape of output for dry-run phase
     nChannels = dat.shape[1]
-    if isinstance(toi, np.ndarray):     # `toi` is an array of time-points
+    if isinstance(toi, np.ndarray):  # `toi` is an array of time-points
         nTime = toi.size
         stftBdry = None
         stftPad = False
-    else:                               # `toi` is either 'all' or a percentage
-        nTime = np.ceil(dat.shape[0] / (method_kwargs['nperseg'] - method_kwargs['noverlap'])).astype(np.intp)
+    else:  # `toi` is either 'all' or a percentage
+        nTime = np.ceil(dat.shape[0] / (method_kwargs["nperseg"] - method_kwargs["noverlap"])).astype(np.intp)
         stftBdry = "zeros"
         stftPad = True
     nFreq = foi.size
-    taper_opt = method_kwargs['taper_opt']
+    taper_opt = method_kwargs["taper_opt"]
     if taper_opt:
         nTaper = taper_opt.get("Kmax", 1)
     outShape = (nTime, max(1, nTaper * keeptapers), nFreq, nChannels)
     if noCompute:
         return outShape, spectralDTypes[output]
 
     # detrending options for each segment
     if polyremoval == 0:
-        detrend = 'constant'
+        detrend = "constant"
     elif polyremoval == 1:
-        detrend = 'linear'
+        detrend = "linear"
     else:
         detrend = False
 
     # additional keyword args for `stft` in dictionary
-    method_kwargs.update({"boundary": stftBdry,
-                          "padded": stftPad,
-                          "detrend": detrend})
+    method_kwargs.update({"boundary": stftBdry, "padded": stftPad, "detrend": detrend})
 
     if equidistant:
         ftr, freqs = mtmconvol(dat[soi, :], **method_kwargs)
         _, fIdx = best_match(freqs, foi, squash_duplicates=True)
         spec = ftr[postselect, :, fIdx, :]
         spec = spectralConversions[output](spec)
 
     else:
         # in this case only a single window gets centered on
         # every individual soi, so we can use mtmfft!
-        samplerate = method_kwargs['samplerate']
-        taper = method_kwargs['taper']
+        samplerate = method_kwargs["samplerate"]
+        taper = method_kwargs["taper"]
 
         # In case tapers aren't preserved allocate `spec` "too big"
         # and average afterwards
         spec = np.full((nTime, nTaper, nFreq, nChannels), np.nan, dtype=spectralDTypes[output])
 
         ftr, freqs = mtmfft(dat[soi[0], :], samplerate, taper=taper, taper_opt=taper_opt)
         _, fIdx = best_match(freqs, foi, squash_duplicates=True)
@@ -396,14 +409,15 @@
 
     # Average across tapers if wanted
     # only valid if output='pow' !
     if not keeptapers:
         return np.nanmean(spec, axis=1, keepdims=True)
     return spec
 
+
 class MultiTaperFFTConvol(ComputationalRoutine):
     """
     Compute class that performs time-frequency analysis of :class:`~syncopy.AnalogData` objects
 
     Sub-class of :class:`~syncopy.shared.computational_routine.ComputationalRoutine`,
     see :doc:`/developer/compute_kernels` for technical details on Syncopy's compute
     classes and metafunctions.
@@ -415,15 +429,15 @@
 
     computeFunction = staticmethod(mtmconvol_cF)
 
     # 1st argument,the data, gets omitted
     valid_kws = list(signature(mtmconvol).parameters.keys())[1:]
     valid_kws += list(signature(mtmconvol_cF).parameters.keys())[1:]
     # hardcode some parameter names which got digested from the frontend
-    valid_kws += ['tapsmofrq', 't_ftimwin', 'nTaper']
+    valid_kws += ["tapsmofrq", "t_ftimwin", "nTaper"]
 
     def process_metadata(self, data, out):
 
         # Get trialdef array + channels from source
         if data.selection is not None:
             chanSec = data.selection.channel
             trl = data.selection.trialdefinition
@@ -444,18 +458,18 @@
         # Attach meta-data
         out.trialdefinition = trl
         out.samplerate = srate
         out.channel = np.array(data.channel[chanSec])
 
         taper_kw = self.cfg["method_kwargs"]["taper"]
         if taper_kw is None:
-            out.taper = np.array(['None'])
+            out.taper = np.array(["None"])
         # multi-tapering
-        elif taper_kw == 'dpss':
-            nTaper =  self.outputShape[out.dimord.index("taper")]
+        elif taper_kw == "dpss":
+            nTaper = self.outputShape[out.dimord.index("taper")]
             out.taper = np.array([taper_kw + str(i) for i in range(nTaper)])
         # just a single taper
         else:
             out.taper = np.array([taper_kw])
 
         out.freq = self.cfg["foi"]
 
@@ -562,17 +576,17 @@
     nScales = method_kwargs["scales"].size
     outShape = (nTime, 1, nScales, nChannels)
     if noCompute:
         return outShape, spectralDTypes[output]
 
     # detrend, does not work with 'FauxTrial' data..
     if polyremoval == 0:
-        dat = signal.detrend(dat, type='constant', axis=0, overwrite_data=True)
+        dat = signal.detrend(dat, type="constant", axis=0, overwrite_data=True)
     elif polyremoval == 1:
-        dat = signal.detrend(dat, type='linear', axis=0, overwrite_data=True)
+        dat = signal.detrend(dat, type="linear", axis=0, overwrite_data=True)
 
     # ------------------
     # actual method call
     # ------------------
     # Compute wavelet transform with given data/time-selection
     spec = wavelet(dat[preselect, :], **method_kwargs)
     # the cwt stacks the scales on the 1st axis, move to 2nd
@@ -625,15 +639,15 @@
         # Attach meta-data
         out.trialdefinition = trl
         out.samplerate = srate
         out.channel = np.array(data.channel[chanSec])
         out.freq = 1 / self.cfg["method_kwargs"]["wavelet"].fourier_period(
             self.cfg["method_kwargs"]["scales"]
         )
-        out.taper = np.array(['None'])
+        out.taper = np.array(["None"])
 
 
 # -----------------
 # SuperletTransform
 # -----------------
 
 
@@ -730,17 +744,17 @@
     nScales = method_kwargs["scales"].size
     outShape = (nTime, 1, nScales, nChannels)
     if noCompute:
         return outShape, spectralDTypes[output]
 
     # detrend, does not work with 'FauxTrial' data..
     if polyremoval == 0:
-        dat = signal.detrend(dat, type='constant', axis=0, overwrite_data=True)
+        dat = signal.detrend(dat, type="constant", axis=0, overwrite_data=True)
     elif polyremoval == 1:
-        dat = signal.detrend(dat, type='linear', axis=0, overwrite_data=True)
+        dat = signal.detrend(dat, type="linear", axis=0, overwrite_data=True)
 
     # ------------------
     # actual method call
     # ------------------
     gmean_spec = superlet(dat[preselect, :], **method_kwargs)
     # the cwtSL stacks the scales on the 1st axis
     gmean_spec = gmean_spec.transpose(1, 0, 2)[postselect, :, :]
@@ -789,15 +803,15 @@
 
         # Attach meta-data
         out.trialdefinition = trl
         out.samplerate = srate
         out.channel = np.array(data.channel[chanSec])
         # for the SL Morlets the conversion is straightforward
         out.freq = 1 / (2 * np.pi * self.cfg["method_kwargs"]["scales"])
-        out.taper = np.array(['None'])
+        out.taper = np.array(["None"])
 
 
 def _make_trialdef(cfg, trialdefinition, samplerate):
     """
     Local helper to construct trialdefinition arrays for time-frequency
     :class:`~syncopy.SpectralData` objects
 
@@ -861,15 +875,15 @@
 
         # Reconstruct trigger-onset based on provided time-point array
         trialdefinition[:, 2] = toi[0] * samplerate
 
     # If `toi` was a percentage, some cumsum/winSize algebra is required
     # Note: if `toi` was "all", simply use provided `trialdefinition` and `samplerate`
     elif np.issubdtype(type(toi), np.number):
-        mKw = cfg['method_kwargs']
+        mKw = cfg["method_kwargs"]
         winSize = mKw["nperseg"] - mKw["noverlap"]
         trialdefinitionLens = np.ceil(np.diff(trialdefinition[:, :2]) / winSize)
         sumLens = np.cumsum(trialdefinitionLens).reshape(trialdefinitionLens.shape)
         trialdefinition[:, 0] = np.ravel(sumLens - trialdefinitionLens)
         trialdefinition[:, 1] = sumLens.ravel()
         trialdefinition[:, 2] = trialdefinition[:, 2] / winSize
         samplerate = np.round(samplerate / winSize, 2)
@@ -884,17 +898,26 @@
     return trialdefinition, samplerate
 
 
 # -----------------------
 # FOOOF
 # -----------------------
 
+
 @process_io
-def fooofspy_cF(trl_dat, foi=None, timeAxis=0,
-                output='fooof', fooof_settings=None, noCompute=False, chunkShape=None, method_kwargs=None):
+def fooofspy_cF(
+    trl_dat,
+    foi=None,
+    timeAxis=0,
+    output="fooof",
+    fooof_settings=None,
+    noCompute=False,
+    chunkShape=None,
+    method_kwargs=None,
+):
     """
     Run FOOOF
 
     Parameters
     ----------
     trl_dat : 2D :class:`numpy.ndarray`
         Uniformly sampled multi-channel time-series
@@ -934,30 +957,36 @@
     under the assumption that all inputs have been externally validated and cross-checked.
 
     See also
     --------
     syncopy.freqanalysis : parent metafunction
     """
     if timeAxis != 0:
-        raise SPYValueError("timeaxis of input spectral data to be 0. Non-standard axes not supported with FOOOF.", actual=timeAxis)
+        raise SPYValueError(
+            "timeaxis of input spectral data to be 0. Non-standard axes not supported with FOOOF.",
+            actual=timeAxis,
+        )
 
     outShape = trl_dat.shape
     # For initialization of computational routine,
     # just return output shape and dtype
     if noCompute:
-        return outShape, spectralDTypes['pow']
-
-
+        return outShape, spectralDTypes["pow"]
 
     # Call actual fooof method
-    res, metadata = fooofspy(trl_dat[0, 0, :, :], in_freqs=fooof_settings['in_freqs'], freq_range=fooof_settings['freq_range'], out_type=output,
-                      fooof_opt=method_kwargs)
+    res, metadata = fooofspy(
+        trl_dat[0, 0, :, :],
+        in_freqs=fooof_settings["in_freqs"],
+        freq_range=fooof_settings["freq_range"],
+        out_type=output,
+        fooof_opt=method_kwargs,
+    )
 
-    if 'settings_used' in metadata:
-        del metadata['settings_used']  # We like to keep this in the return value of the
+    if "settings_used" in metadata:
+        del metadata["settings_used"]  # We like to keep this in the return value of the
     # backend functions for now (the vast majority of unit tests rely on it), but
     # nested dicts are not allowed in the additional return value of cFs, so we remove
     # it before passing the return value on.
 
     metadata = FooofSpy.encode_singletrial_metadata_fooof_for_hdf5(metadata)
 
     res = res[np.newaxis, np.newaxis, :, :]  # Re-add omitted axes.
@@ -987,36 +1016,51 @@
 
     #: The keys available in the metadata returned by this function. These come from `fooof` and correspond
     #: to the attributes of the `fooof.FOOOF` instance (with an `'_'` suffix in `fooof`, e.g., `aperiodic_params` corresponds
     #: to `fooof.FOOOF.aperiodic_params_`).
     #: Please
     #: refer to the `FOOOF docs <https://fooof-tools.github.io/fooof/generated/fooof.FOOOF.html#fooof.FOOOF>`_
     #: for the meanings.
-    metadata_keys = ('aperiodic_params', 'error', 'gaussian_params', 'n_peaks', 'peak_params', 'r_squared',)
-
+    metadata_keys = (
+        "aperiodic_params",
+        "error",
+        "gaussian_params",
+        "n_peaks",
+        "peak_params",
+        "r_squared",
+    )
 
     # To attach metadata to the output of the CF
     def process_metadata(self, data, out):
 
-        SPYParallelLog(f"Fetching FOOOF output metadata from file '{out.filename}'.", loglevel="DEBUG")
+        SPYParallelLog(
+            f"Fetching FOOOF output metadata from file '{out.filename}'.",
+            loglevel="DEBUG",
+        )
 
         # General-purpose loading of metadata.
         mdata = metadata_from_hdf5_file(out.filename)
 
         # Note that FOOOF never sees absolute trial indices if a selection was
         # made in the call to `freqanalysis`, because the mtmfft run before will have
         # consumed them. So the trial indices are always relative.
 
-        SPYParallelLog(f"Decoding FOOOF output metadata from HDF5 datastructures.", loglevel="DEBUG")
+        SPYParallelLog(
+            f"Decoding FOOOF output metadata from HDF5 datastructures.",
+            loglevel="DEBUG",
+        )
 
         # Backend-specific post-processing. May or may not be needed, depending on what
         # you need to do in the cF to fit the return values into hdf5.
         out.metadata = metadata_nest(FooofSpy.decode_metadata_fooof_alltrials_from_hdf5(mdata))
 
-        SPYParallelLog(f"Copying recording information to output syncopy data instance.", loglevel="DEBUG")
+        SPYParallelLog(
+            f"Copying recording information to output syncopy data instance.",
+            loglevel="DEBUG",
+        )
 
         # Some index gymnastics to get trial begin/end "samples"
         if data.selection is not None:
             chanSec = data.selection.channel
         else:
             chanSec = slice(None)
 
@@ -1024,27 +1068,27 @@
         out.samplerate = data.samplerate
         out.channel = np.array(data.channel[chanSec])
         out.freq = data.freq
         out._trialdefinition = data._trialdefinition
 
     @staticmethod
     def encode_singletrial_metadata_fooof_for_hdf5(metadata_fooof_backend):
-        """ Reformat the gaussian and peak params for inclusion in the 2nd return value and hdf5 file.
+        """Reformat the gaussian and peak params for inclusion in the 2nd return value and hdf5 file.
 
         For several channels, the number of peaks may differ, and thus we cannot simply
         call something like `np.array(gaussian_params)` in that case, as that will create
         an array of dtype 'object', which is not supported by hdf5. We could use one return
         value (entry in the `'metadata_fooof_backend'` dict below) per channel to solve that, but in this
         case, we decided to `vstack` the arrays instead. When extracting the data again
         (in `process_metadata()`), we need to revert this. That is possible because we can
         see from the `n_peaks` return value how many (and thus which) rows belong to
         which channel.
         """
-        metadata_fooof_backend['gaussian_params'] = np.vstack(metadata_fooof_backend['gaussian_params'])
-        metadata_fooof_backend['peak_params'] = np.vstack(metadata_fooof_backend['peak_params'])
+        metadata_fooof_backend["gaussian_params"] = np.vstack(metadata_fooof_backend["gaussian_params"])
+        metadata_fooof_backend["peak_params"] = np.vstack(metadata_fooof_backend["peak_params"])
         return metadata_fooof_backend
 
     @staticmethod
     def decode_metadata_fooof_alltrials_from_hdf5(metadata_fooof_hdf5):
         """This reverts the special packaging applied to the fooof backend
         function return values to fit them into the hdf5 container.
 
@@ -1062,20 +1106,25 @@
         been pre-processed by the general-purpose metadata extraction
         function `metadata_from_hdf5_file()`.
         """
         for unique_attr_label, v in metadata_fooof_hdf5.items():
             label, trial_idx, call_idx = decode_unique_md_label(unique_attr_label)
             if label == "n_peaks":
                 n_peaks = v
-                SPYParallelLog(f"FOOOF detected {n_peaks} peaks in data of trial {trial_idx} call {call_idx}.", loglevel="DEBUG")
+                SPYParallelLog(
+                    f"FOOOF detected {n_peaks} peaks in data of trial {trial_idx} call {call_idx}.",
+                    loglevel="DEBUG",
+                )
                 gaussian_params_out = list()
                 peak_params_out = list()
                 start_idx = 0
-                unique_attr_label_gaussian_params = encode_unique_md_label('gaussian_params', trial_idx, call_idx)
-                unique_attr_label_peak_params = encode_unique_md_label('peak_params', trial_idx, call_idx)
+                unique_attr_label_gaussian_params = encode_unique_md_label(
+                    "gaussian_params", trial_idx, call_idx
+                )
+                unique_attr_label_peak_params = encode_unique_md_label("peak_params", trial_idx, call_idx)
                 gaussian_params_in = metadata_fooof_hdf5[unique_attr_label_gaussian_params]
                 peak_params_in = metadata_fooof_hdf5[unique_attr_label_peak_params]
                 for trial_idx in range(len(n_peaks)):
                     end_idx = start_idx + n_peaks[trial_idx]
                     gaussian_params_out.append(gaussian_params_in[start_idx:end_idx, :])
                     peak_params_out.append(peak_params_in[start_idx:end_idx, :])
```

### Comparing `esi_syncopy-2023.5/syncopy/specest/fooofspy.py` & `esi_syncopy-2023.7/syncopy/specest/fooofspy.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,24 +8,27 @@
 # Builtin/3rd party package imports
 import numpy as np
 from fooof import FOOOF
 import logging
 import platform
 
 # Constants
-available_fooof_out_types = ['fooof', 'fooof_aperiodic', 'fooof_peaks']
-default_fooof_opt = {'peak_width_limits': (0.5, 12.0), 'max_n_peaks': np.inf,
-                     'min_peak_height': 0.0, 'peak_threshold': 2.0,
-                     'aperiodic_mode': 'fixed', 'verbose': False}
+available_fooof_out_types = ["fooof", "fooof_aperiodic", "fooof_peaks"]
+default_fooof_opt = {
+    "peak_width_limits": (0.5, 12.0),
+    "max_n_peaks": np.inf,
+    "min_peak_height": 0.0,
+    "peak_threshold": 2.0,
+    "aperiodic_mode": "fixed",
+    "verbose": False,
+}
 available_fooof_options = list(default_fooof_opt)
 
 
-def fooofspy(data_arr, in_freqs, freq_range=None,
-             fooof_opt=None,
-             out_type='fooof'):
+def fooofspy(data_arr, in_freqs, freq_range=None, fooof_opt=None, out_type="fooof"):
     """
     Parameterization of neural power spectra using
     the FOOOF mothod by Donoghue et al: fitting oscillations & one over f.
 
     Parameters
     ----------
     data_arr : 2D :class:`numpy.ndarray`
@@ -90,87 +93,118 @@
 
     if fooof_opt is None:
         fooof_opt = default_fooof_opt
     else:
         fooof_opt = {**default_fooof_opt, **fooof_opt}
 
     if in_freqs is None:
-        raise ValueError('infreqs: The input frequencies are required and must not be None.')
+        raise ValueError("infreqs: The input frequencies are required and must not be None.")
 
     logger = logging.getLogger("syncopy_" + platform.node())
     logger.debug(f"Running FOOOF backend function on data chunk with shape {data_arr.shape}.")
 
     invalid_fooof_opts = [i for i in fooof_opt.keys() if i not in available_fooof_options]
     if invalid_fooof_opts:
-        raise ValueError("fooof_opt: invalid keys: '{inv}', allowed keys are: '{lgl}'.".format(inv=invalid_fooof_opts, lgl=fooof_opt.keys()))
+        raise ValueError(
+            "fooof_opt: invalid keys: '{inv}', allowed keys are: '{lgl}'.".format(
+                inv=invalid_fooof_opts, lgl=fooof_opt.keys()
+            )
+        )
 
     if out_type not in available_fooof_out_types:
-        raise ValueError("out_type: invalid value '{inv}', expected one of '{lgl}'.".format(inv=out_type, lgl=available_fooof_out_types))
+        raise ValueError(
+            "out_type: invalid value '{inv}', expected one of '{lgl}'.".format(
+                inv=out_type, lgl=available_fooof_out_types
+            )
+        )
 
     if in_freqs.size != data_arr.shape[0]:
-        raise ValueError("data_arr/in_freqs: The signal length {sl} must match the number of frequency labels {ll}.".format(sl=data_arr.shape[0], ll=in_freqs.size))
+        raise ValueError(
+            "data_arr/in_freqs: The signal length {sl} must match the number of frequency labels {ll}.".format(
+                sl=data_arr.shape[0], ll=in_freqs.size
+            )
+        )
 
     if in_freqs[0] == 0:
-        raise ValueError("in_freqs: invalid frequency range {minf} to {maxf}, expected a frequency range that does not include zero.".format(minf=min(in_freqs), maxf=max(in_freqs)))
+        raise ValueError(
+            "in_freqs: invalid frequency range {minf} to {maxf}, expected a frequency range that does not include zero.".format(
+                minf=min(in_freqs), maxf=max(in_freqs)
+            )
+        )
 
     num_channels = data_arr.shape[1]
 
     fm = FOOOF(**fooof_opt)
 
     # Prepare output data structures
     out_spectra = np.zeros_like(data_arr, data_arr.dtype)
-    if fm.aperiodic_mode == 'knee':
+    if fm.aperiodic_mode == "knee":
         aperiodic_params = np.zeros(shape=(3, num_channels), dtype=np.float64)
     else:
         aperiodic_params = np.zeros(shape=(2, num_channels), dtype=np.float64)
-    n_peaks = np.zeros(shape=(num_channels), dtype=np.int32)    # helper: number of peaks fit.
+    n_peaks = np.zeros(shape=(num_channels), dtype=np.int32)  # helper: number of peaks fit.
     r_squared = np.zeros(shape=(num_channels), dtype=np.float64)  # helper: R squared of fit.
-    error = np.zeros(shape=(num_channels), dtype=np.float64)      # helper: model error.
+    error = np.zeros(shape=(num_channels), dtype=np.float64)  # helper: model error.
     gaussian_params = list()  # Gaussian fit parameters of peaks
     peak_params = list()  # Peak fit parameters, a modified version of gaussian_parameters. See FOOOF docs.
 
     # Run fooof and store results.
     for channel_idx in range(num_channels):
         spectrum = data_arr[:, channel_idx]
         fm.fit(in_freqs, spectrum, freq_range=freq_range)
 
         # compute aperiodic fit
         offset = fm.aperiodic_params_[0]
-        if fm.aperiodic_mode == 'fixed':
+        if fm.aperiodic_mode == "fixed":
             exp = fm.aperiodic_params_[1]
             aperiodic_spec = offset - np.log10(in_freqs**exp)
         else:  # fm.aperiodic_mode == 'knee':
             knee = fm.aperiodic_params_[1]
             exp = fm.aperiodic_params_[2]
             aperiodic_spec = offset - np.log10(knee + in_freqs**exp)
 
-        if out_type == 'fooof':
-            out_spectrum = 10 ** fm.fooofed_spectrum_  # The powers. Need to undo log10, which is used internally by fooof.
+        if out_type == "fooof":
+            out_spectrum = (
+                10**fm.fooofed_spectrum_
+            )  # The powers. Need to undo log10, which is used internally by fooof.
         elif out_type == "fooof_aperiodic":
-            out_spectrum = 10 ** aperiodic_spec
+            out_spectrum = 10**aperiodic_spec
         elif out_type == "fooof_peaks":
-            out_spectrum = (10 ** fm.fooofed_spectrum_) - (10 ** aperiodic_spec)
+            out_spectrum = (10**fm.fooofed_spectrum_) - (10**aperiodic_spec)
             out_spectrum += 1e-16  # Prevent zero values in areas without peaks/periodic parts. These would result in log plotting issues.
         else:
-            raise ValueError("out_type: invalid value '{inv}', expected one of '{lgl}'.".format(inv=out_type, lgl=available_fooof_out_types))
+            raise ValueError(
+                "out_type: invalid value '{inv}', expected one of '{lgl}'.".format(
+                    inv=out_type, lgl=available_fooof_out_types
+                )
+            )
 
         out_spectra[:, channel_idx] = out_spectrum
         aperiodic_params[:, channel_idx] = fm.aperiodic_params_
         n_peaks[channel_idx] = fm.n_peaks_
         r_squared[channel_idx] = fm.r_squared_
         error[channel_idx] = fm.error_
         gaussian_params.append(fm.gaussian_params_)
         peak_params.append(fm.peak_params_)
 
-    settings_used = {'fooof_opt': fooof_opt, 'out_type': out_type, 'freq_range': freq_range}
+    settings_used = {
+        "fooof_opt": fooof_opt,
+        "out_type": out_type,
+        "freq_range": freq_range,
+    }
     #  Note: we add the 'settings_used' here in the backend, but they get stripped in the middle layer
     #       (in the 'compRoutines.py/fooofspy_cF()'), so they do not reach the frontend.
     #        The reason for removing them there is that we/h5py do not support nested dicts as
     #        dataset/group attributes, and thus we cannot encode them in hdf5. We could work around
     #        that, but due to our log, we do not really need to.
     #        Returning them from here still has the benefit that we can test for them in backend tests.
-    metadata = {'aperiodic_params': aperiodic_params, 'gaussian_params': gaussian_params,
-               'peak_params': peak_params, 'n_peaks': n_peaks, 'r_squared': r_squared,
-               'error': error, 'settings_used': settings_used}
+    metadata = {
+        "aperiodic_params": aperiodic_params,
+        "gaussian_params": gaussian_params,
+        "peak_params": peak_params,
+        "n_peaks": n_peaks,
+        "r_squared": r_squared,
+        "error": error,
+        "settings_used": settings_used,
+    }
 
     return out_spectra, metadata
-
```

### Comparing `esi_syncopy-2023.5/syncopy/specest/freqanalysis.py` & `esi_syncopy-2023.7/syncopy/specest/freqanalysis.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,27 +6,36 @@
 # Builtin/3rd party package imports
 import numpy as np
 
 # Syncopy imports
 from syncopy.shared.parsers import data_parser, scalar_parser, array_parser
 from syncopy.shared.tools import get_defaults, get_frontend_cfg
 from syncopy.datatype import SpectralData
-from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYWarning, SPYInfo, SPYLog
-from syncopy.shared.kwarg_decorators import (unwrap_cfg, unwrap_select,
-                                             detect_parallel_client)
+from syncopy.shared.errors import (
+    SPYValueError,
+    SPYTypeError,
+    SPYWarning,
+    SPYInfo,
+    SPYLog,
+)
+from syncopy.shared.kwarg_decorators import (
+    unwrap_cfg,
+    unwrap_select,
+    detect_parallel_client,
+)
 from syncopy.shared.tools import best_match
 from syncopy.shared.const_def import spectralConversions
 import syncopy as spy
 
 from syncopy.shared.input_processors import (
     process_taper,
     process_foi,
     process_padding,
     check_effective_parameters,
-    check_passed_kwargs
+    check_passed_kwargs,
 )
 
 # method specific imports - they should go!
 from syncopy.specest.fooofspy import default_fooof_opt
 import syncopy.specest.wavelets as spywave
 import syncopy.specest.superlet as superlet
 from .wavelet import get_optimal_wavelet_scales
@@ -34,33 +43,55 @@
 # Local imports
 
 from .compRoutines import (
     SuperletTransform,
     WaveletTransform,
     MultiTaperFFT,
     MultiTaperFFTConvol,
-    FooofSpy
+    FooofSpy,
 )
 
-availableFooofOutputs = ['fooof', 'fooof_aperiodic', 'fooof_peaks']
+availableFooofOutputs = ["fooof", "fooof_aperiodic", "fooof_peaks"]
 availableOutputs = tuple(spectralConversions.keys())
 availableWavelets = ("Morlet", "Paul", "DOG", "Ricker", "Marr", "Mexican_hat")
 availableMethods = ("mtmfft", "mtmconvol", "wavelet", "superlet", "welch")
 
 
 @unwrap_cfg
 @unwrap_select
 @detect_parallel_client
-def freqanalysis(data, method='mtmfft', output='pow',
-                 keeptrials=True, foi=None, foilim=None,
-                 pad='maxperlen', polyremoval=0, taper="hann", demean_taper=False,
-                 taper_opt=None, tapsmofrq=None, nTaper=None, keeptapers=False,
-                 toi="all", t_ftimwin=None, wavelet="Morlet", width=6, order=None,
-                 order_max=None, order_min=1, c_1=3, adaptive=False,
-                 out=None, fooof_opt=None, ft_compat=False, **kwargs):
+def freqanalysis(
+    data,
+    method="mtmfft",
+    output="pow",
+    keeptrials=True,
+    foi=None,
+    foilim=None,
+    pad="maxperlen",
+    polyremoval=0,
+    taper="hann",
+    demean_taper=False,
+    taper_opt=None,
+    tapsmofrq=None,
+    nTaper=None,
+    keeptapers=False,
+    toi="all",
+    t_ftimwin=None,
+    wavelet="Morlet",
+    width=6,
+    order=None,
+    order_max=None,
+    order_min=1,
+    c_1=3,
+    adaptive=False,
+    out=None,
+    fooof_opt=None,
+    ft_compat=False,
+    **kwargs,
+):
     """
     Perform (time-)frequency analysis of Syncopy :class:`~syncopy.AnalogData` objects
 
     **Usage Summary**
 
     Options available in all analysis methods:
 
@@ -349,16 +380,15 @@
     syncopy.specest.wavelet.wavelet : time-frequency analysis of multi-channel time series data using a wavelet transform
     syncopy.specest.fooofspy.fooofspy : parameterization of neural power spectra with the 'fitting oscillations & one over f' method
     numpy.fft.fft : NumPy's reference FFT implementation
     scipy.signal.stft : SciPy's Short Time Fourier Transform
     """
     # Make sure our one mandatory input object can be processed
     try:
-        data_parser(data, varname="data", dataclass="AnalogData",
-                    writable=None, empty=False)
+        data_parser(data, varname="data", dataclass="AnalogData", writable=None, empty=False)
     except Exception as exc:
         raise exc
     timeAxis = data.dimord.index("time")
 
     # Get everything of interest in local namespace
     defaults = get_defaults(freqanalysis)
 
@@ -415,20 +445,22 @@
     # check polyremoval
     if polyremoval is not None:
         scalar_parser(polyremoval, varname="polyremoval", ntype="int_like", lims=[0, 1])
 
     # --- Padding ---
 
     # Sliding window FFT does not support "fancy" padding
-    if method in ["mtmconvol", "welch"] and isinstance(pad, str) and pad != defaults['pad']:
-        msg = "methods 'mtmconvol' and 'welch' only support in-place padding for windows " +\
-            "exceeding trial boundaries. Your choice of `pad = '{}'` will be ignored. "
+    if method in ["mtmconvol", "welch"] and isinstance(pad, str) and pad != defaults["pad"]:
+        msg = (
+            "methods 'mtmconvol' and 'welch' only support in-place padding for windows "
+            + "exceeding trial boundaries. Your choice of `pad = '{}'` will be ignored. "
+        )
         SPYWarning(msg.format(pad))
 
-    if method == 'mtmfft':
+    if method == "mtmfft":
         # the actual number of samples in case of later padding
         minSampleNum = process_padding(pad, lenTrials, data.samplerate)
     else:
         minSampleNum = lenTrials.min()
 
     # Compute length (in seconds) of shortest trial
     minTrialLength = minSampleNum / data.samplerate
@@ -443,30 +475,30 @@
         try:
             scalar_parser(polyremoval, varname="polyremoval", lims=[0, 1], ntype="int_like")
         except Exception as exc:
             raise exc
 
     # Prepare keyword dict for logging (use `lcls` to get actually provided
     # keyword values, not defaults set above)
-    log_dct = {"method": method,
-               "output": output_fooof if is_fooof else output,
-               "keeptapers": keeptapers,
-               "keeptrials": keeptrials,
-               "polyremoval": polyremoval,
-               "pad": pad}
+    log_dct = {
+        "method": method,
+        "output": output_fooof if is_fooof else output,
+        "keeptapers": keeptapers,
+        "keeptrials": keeptrials,
+        "polyremoval": polyremoval,
+        "pad": pad,
+    }
 
     SPYLog(f"Running specest method '{method}'.", loglevel="DEBUG")
 
     # --------------------------------
     # 1st: Check time-frequency inputs
     # to prepare/sanitize `toi`
     # --------------------------------
 
-
-
     if method in ["mtmconvol", "wavelet", "superlet", "welch"]:
 
         # Get start/end timing info respecting potential in-place selection
         if toi is None:
             raise SPYTypeError(toi, varname="toi", expected="scalar or array-like or 'all'")
         if data.selection is not None:
             tStart = data.selection.trialdefinition[:, 2] / data.samplerate
@@ -492,16 +524,22 @@
 
         elif not iter(toi):
             valid = False
 
         # this is the sequence type - can only be an interval!
         else:
             try:
-                array_parser(toi, varname="toi", hasinf=False, hasnan=False,
-                             lims=[tStart.min(), tEnd.max()], dims=(None,))
+                array_parser(
+                    toi,
+                    varname="toi",
+                    hasinf=False,
+                    hasnan=False,
+                    lims=[tStart.min(), tEnd.max()],
+                    dims=(None,),
+                )
             except Exception as exc:
                 raise exc
             toi = np.array(toi)
             # check for equidistancy
             if not np.allclose(np.diff(toi, 2), np.zeros(len(toi) - 2)):
                 valid = False
             # trim (preSelect) and subsample output (postSelect)
@@ -509,16 +547,15 @@
                 preSelect = []
                 postSelect = []
                 # get sample intervals and relative indices from toi
                 for tk in range(numTrials):
                     start = int(data.samplerate * (toi[0] - tStart[tk]))
                     stop = int(data.samplerate * (toi[-1] - tStart[tk]) + 1)
                     preSelect.append(slice(max(0, start), max(stop, stop - start)))
-                    smpIdx = np.minimum(lenTrials[tk] - 1,
-                                        data.samplerate * (toi - tStart[tk]) - start)
+                    smpIdx = np.minimum(lenTrials[tk] - 1, data.samplerate * (toi - tStart[tk]) - start)
                     postSelect.append(smpIdx.astype(np.intp))
 
         # get out if sth wasn't right
         if not valid:
             lgl = "array of equidistant time-points or 'all' for wavelet based methods"
             raise SPYValueError(legal=lgl, varname="toi", actual=toi)
 
@@ -532,68 +569,75 @@
     # --------------------------------------------
 
     if "mtm" in method or method == "welch":
 
         if method in ["mtmconvol", "welch"]:
             # get the sliding window size
             try:
-                scalar_parser(t_ftimwin, varname="t_ftimwin",
-                              lims=[dt, minTrialLength])
+                scalar_parser(t_ftimwin, varname="t_ftimwin", lims=[dt, minTrialLength])
             except Exception as exc:
                 SPYInfo("Please specify 't_ftimwin' parameter.. exiting!")
                 raise exc
 
             # this is the effective sliding window FFT sample size
             minSampleNum = int(t_ftimwin * data.samplerate)
 
             if method == "welch":
                 if keeptapers:
-                    raise SPYValueError(legal="keeptapers='False' with method='welch'", varname="keeptapers", actual=keeptapers)
+                    raise SPYValueError(
+                        legal="keeptapers='False' with method='welch'",
+                        varname="keeptapers",
+                        actual=keeptapers,
+                    )
 
                 if output != "pow":
-                    raise SPYValueError(legal="output='pow' with method='welch'", varname="output", actual=output)
-
+                    raise SPYValueError(
+                        legal="output='pow' with method='welch'",
+                        varname="output",
+                        actual=output,
+                    )
 
         # Construct array of maximally attainable frequencies
         freqs = np.fft.rfftfreq(minSampleNum, dt)
 
         # Match desired frequencies as close as possible to
         # actually attainable freqs
         # these are the frequencies attached to the SpectralData by the CR!
         if foi is not None:
             foi, _ = best_match(freqs, foi, squash_duplicates=True)
         elif foilim is not None:
             foi, _ = best_match(freqs, foilim, span=True, squash_duplicates=True)
         else:
-            msg = (f"Automatic FFT frequency selection from {freqs[0]:.1f}Hz to "
-                   f"{freqs[-1]:.1f}Hz")
+            msg = f"Automatic FFT frequency selection from {freqs[0]:.1f}Hz to " f"{freqs[-1]:.1f}Hz"
             SPYInfo(msg)
             foi = freqs
         log_dct["foi"] = foi
 
         # Abort if desired frequency selection is empty
         if foi.size == 0:
             lgl = "non-empty frequency specification"
             act = "empty frequency selection"
             raise SPYValueError(legal=lgl, varname="foi/foilim", actual=act)
 
         # sanitize taper selection and/or retrieve dpss settings
-        taper, taper_opt = process_taper(taper,
-                                         taper_opt,
-                                         tapsmofrq,
-                                         nTaper,
-                                         keeptapers,
-                                         foimax=foi.max(),
-                                         samplerate=data.samplerate,
-                                         nSamples=minSampleNum,
-                                         output=output)
+        taper, taper_opt = process_taper(
+            taper,
+            taper_opt,
+            tapsmofrq,
+            nTaper,
+            keeptapers,
+            foimax=foi.max(),
+            samplerate=data.samplerate,
+            nSamples=lenTrials.mean(),   # best we can do here
+            output=output,
+        )
 
         # Update `log_dct` w/method-specific options
         log_dct["taper"] = taper
-        if taper_opt and taper == 'dpss':
+        if taper_opt and taper == "dpss":
             log_dct["nTaper"] = taper_opt["Kmax"]
             log_dct["tapsmofrq"] = tapsmofrq
         elif taper_opt:
             log_dct["taper_opt"] = taper_opt
 
     # -------------------------------------------------------
     # Now, prepare explicit compute-classes for chosen method
@@ -601,30 +645,31 @@
 
     if method == "mtmfft":
 
         check_effective_parameters(MultiTaperFFT, defaults, lcls)
 
         # method specific parameters
         method_kwargs = {
-            'samplerate': data.samplerate,
-            'taper': taper,
-            'taper_opt': taper_opt,
-            'nSamples': minSampleNum,
-            'demean_taper': demean_taper,
-            'ft_compat': ft_compat
+            "samplerate": data.samplerate,
+            "taper": taper,
+            "taper_opt": taper_opt,
+            "nSamples": minSampleNum,
+            "demean_taper": demean_taper,
+            "ft_compat": ft_compat,
         }
 
         # Set up compute-class
         specestMethod = MultiTaperFFT(
             foi=foi,
             timeAxis=timeAxis,
             keeptapers=keeptapers,
             polyremoval=polyremoval,
             output=output,
-            method_kwargs=method_kwargs)
+            method_kwargs=method_kwargs,
+        )
 
     elif method in ["mtmconvol", "welch"]:
 
         check_effective_parameters(MultiTaperFFTConvol, defaults, lcls)
 
         # Process `toi` for sliding window multi taper fft,
         # we have to account for three scenarios: (1) center sliding
@@ -652,16 +697,22 @@
         # this captures all other cases, e.i. toi is of sequence type
         else:
             if method == "welch":
                 lgl = "toi to be a float in range [0, 1] for method='welch'"
                 raise SPYValueError(legal=lgl, varname="toi", actual=toi)
 
             overlap = -1
-            array_parser(toi, varname="toi", hasinf=False, hasnan=False,
-                             lims=[tStart.min(), tEnd.max()], dims=(None,))
+            array_parser(
+                toi,
+                varname="toi",
+                hasinf=False,
+                hasnan=False,
+                lims=[tStart.min(), tEnd.max()],
+                dims=(None,),
+            )
             toi = np.array(toi)
             tSteps = np.diff(toi)
             if (tSteps < 0).any():
                 lgl = "ordered list/array of time-points"
                 act = "unsorted list/array"
                 raise SPYValueError(legal=lgl, varname="toi", actual=act)
             # Account for round-off errors: if toi spacing is almost at sample interval
@@ -676,16 +727,18 @@
             # show several entries here - use `allclose` to identify "even" spacings
             equidistant = np.allclose(tSteps, [tSteps[0]] * tSteps.size)
 
         # If `toi` was 'all' or a percentage, use entire time interval of (selected)
         # trials and check if those trials have *approximately* equal length
         if toi is None:
             if not np.allclose(lenTrials, [minSampleNum] * lenTrials.size):
-                msg = "processing trials of different lengths (min = {}; max = {} samples)" +\
-                    " with `toi = 'all'`"
+                msg = (
+                    "processing trials of different lengths (min = {}; max = {} samples)"
+                    + " with `toi = 'all'`"
+                )
                 SPYWarning(msg.format(int(minSampleNum), int(lenTrials.max())))
 
         # number of samples per window
         nperseg = int(t_ftimwin * data.samplerate)
         halfWin = int(nperseg / 2)
         postSelect = slice(None)  # select all is the default
 
@@ -740,45 +793,50 @@
                     # has exactly one entry for each soi
 
         # `toi` is percentage or "all"
         else:
             soi = [slice(None)] * numTrials
 
         # Collect keyword args for `mtmconvol` in dictionary
-        method_kwargs = {"samplerate": data.samplerate,
-                         "nperseg": nperseg,
-                         "noverlap": noverlap,
-                         "taper": taper,
-                         "taper_opt": taper_opt}
+        method_kwargs = {
+            "samplerate": data.samplerate,
+            "nperseg": nperseg,
+            "noverlap": noverlap,
+            "taper": taper,
+            "taper_opt": taper_opt,
+        }
 
         # Set up compute-class
         specestMethod = MultiTaperFFTConvol(
             soi,
             postSelect,
             equidistant=equidistant,
             toi=toi,
             foi=foi,
             timeAxis=timeAxis,
             keeptapers=keeptapers,
             polyremoval=polyremoval,
             output=output,
-            method_kwargs=method_kwargs)
+            method_kwargs=method_kwargs,
+        )
 
     elif method == "wavelet":
 
         check_effective_parameters(WaveletTransform, defaults, lcls)
 
         # Check wavelet selection
         if wavelet not in availableWavelets:
             lgl = "'" + "or '".join(opt + "' " for opt in availableWavelets)
             raise SPYValueError(legal=lgl, varname="wavelet", actual=wavelet)
         if wavelet not in ["Morlet", "Paul"]:
-            msg = "the chosen wavelet '{}' is real-valued and does not provide " +\
-                "any information about amplitude or phase of the data. This wavelet function " +\
-                "may be used to isolate peaks or discontinuities in the signal. "
+            msg = (
+                "the chosen wavelet '{}' is real-valued and does not provide "
+                + "any information about amplitude or phase of the data. This wavelet function "
+                + "may be used to isolate peaks or discontinuities in the signal. "
+            )
             SPYWarning(msg.format(wavelet))
 
         # Check for consistency of `width`, `order` and `wavelet`
         if wavelet == "Morlet":
             try:
                 scalar_parser(width, varname="width", lims=[1, np.inf])
             except Exception as exc:
@@ -808,18 +866,18 @@
             wfun = getattr(spywave, wavelet)()
 
         # automatic frequency selection
         if foi is None and foilim is None:
             scales = get_optimal_wavelet_scales(
                 wfun.scale_from_period,  # all availableWavelets sport one!
                 int(minTrialLength * data.samplerate),
-                dt)
+                dt,
+            )
             foi = 1 / wfun.fourier_period(scales)
-            msg = (f"Setting frequencies of interest to {foi[0]:.1f}-"
-                   f"{foi[-1]:.1f}Hz")
+            msg = f"Setting frequencies of interest to {foi[0]:.1f}-" f"{foi[-1]:.1f}Hz"
             SPYInfo(msg)
         else:
             if foilim is not None:
                 foi = np.arange(foilim[0], foilim[1] + 1, dtype=float)
             # 0 frequency is not valid
             foi[foi < 0.01] = 0.01
             scales = wfun.scale_from_period(1 / foi)
@@ -829,70 +887,59 @@
         log_dct["foi"] = foi
         log_dct["wavelet"] = lcls["wavelet"]
         log_dct["width"] = lcls["width"]
         log_dct["order"] = lcls["order"]
 
         # method specific parameters
         method_kwargs = {
-            'samplerate': data.samplerate,
-            'scales': scales,
-            'wavelet': wfun
+            "samplerate": data.samplerate,
+            "scales": scales,
+            "wavelet": wfun,
         }
 
         # Set up compute-class
         specestMethod = WaveletTransform(
             preSelect,
             postSelect,
             toi=toi,
             timeAxis=timeAxis,
             polyremoval=polyremoval,
             output=output,
-            method_kwargs=method_kwargs)
+            method_kwargs=method_kwargs,
+        )
 
     elif method == "superlet":
 
         check_effective_parameters(SuperletTransform, defaults, lcls)
 
         # check and parse superlet specific arguments
         if order_max is None:
             lgl = "Positive integer needed for order_max"
-            raise SPYValueError(legal=lgl, varname="order_max",
-                                actual=None)
+            raise SPYValueError(legal=lgl, varname="order_max", actual=None)
         else:
-            scalar_parser(
-                order_max,
-                varname="order_max",
-                lims=[1, np.inf],
-                ntype="int_like"
-            )
+            scalar_parser(order_max, varname="order_max", lims=[1, np.inf], ntype="int_like")
 
-        scalar_parser(
-            order_min, varname="order_min",
-            lims=[1, order_max],
-            ntype="int_like"
-        )
+        scalar_parser(order_min, varname="order_min", lims=[1, order_max], ntype="int_like")
         scalar_parser(c_1, varname="c_1", lims=[1, np.inf], ntype="int_like")
 
         # if no frequencies are user selected, take a sensitive default
         if foi is None and foilim is None:
             scales = get_optimal_wavelet_scales(
-                superlet.scale_from_period,
-                int(minTrialLength * data.samplerate),
-                dt)
+                superlet.scale_from_period, int(minTrialLength * data.samplerate), dt
+            )
             foi = 1 / superlet.fourier_period(scales)
-            msg = (f"Setting frequencies of interest to {foi[0]:.1f}-"
-                   f"{foi[-1]:.1f}Hz")
+            msg = f"Setting frequencies of interest to {foi[0]:.1f}-" f"{foi[-1]:.1f}Hz"
             SPYInfo(msg)
         else:
             if foilim is not None:
                 # frequency range in 1Hz steps
                 foi = np.arange(foilim[0], foilim[1] + 1, dtype=float)
             # 0 frequency is not valid
             foi[foi < 0.01] = 0.01
-            scales = superlet.scale_from_period(1. / foi)
+            scales = superlet.scale_from_period(1.0 / foi)
 
         # FASLT needs ordered frequencies low - high
         # meaning the scales have to go high - low
         if adaptive:
             if len(scales) < 2:
                 lgl = "A range of frequencies"
                 act = "Single frequency"
@@ -905,43 +952,46 @@
         log_dct["foi"] = foi
         log_dct["c_1"] = lcls["c_1"]
         log_dct["order_max"] = lcls["order_max"]
         log_dct["order_min"] = lcls["order_min"]
 
         # method specific parameters
         method_kwargs = {
-            'samplerate': data.samplerate,
-            'scales': scales,
-            'order_max': order_max,
-            'order_min': order_min,
-            'c_1': c_1,
-            'adaptive': adaptive
+            "samplerate": data.samplerate,
+            "scales": scales,
+            "order_max": order_max,
+            "order_min": order_min,
+            "c_1": c_1,
+            "adaptive": adaptive,
         }
 
         # Set up compute-class
         specestMethod = SuperletTransform(
             preSelect,
             postSelect,
             toi=toi,
             timeAxis=timeAxis,
             polyremoval=polyremoval,
             output=output,
-            method_kwargs=method_kwargs)
+            method_kwargs=method_kwargs,
+        )
 
     # -------------------------------------------------
     # Sanitize output and call the ComputationalRoutine
     # -------------------------------------------------
 
     out = SpectralData(dimord=SpectralData._defaultDimord)
 
     # Perform actual computation
-    specestMethod.initialize(data,
-                             out._stackingDim,
-                             chan_per_worker=kwargs.get("chan_per_worker"),
-                             keeptrials=keeptrials)
+    specestMethod.initialize(
+        data,
+        out._stackingDim,
+        chan_per_worker=kwargs.get("chan_per_worker"),
+        keeptrials=keeptrials,
+    )
     specestMethod.compute(data, out, parallel=kwargs.get("parallel"), log_dict=log_dct)
 
     # FOOOF is a post-processing method of MTMFFT output, so we handle it here, once
     # the MTMFFT has finished.
     if is_fooof:
 
         # Use the output of the MTMFFMT method as the new data and create new output data.
@@ -949,53 +999,66 @@
         fooof_out = SpectralData(dimord=SpectralData._defaultDimord)
 
         # method specific parameters
         if fooof_opt is None:
             fooof_opt = default_fooof_opt
 
         # These go into the FOOOF constructor, so we keep them separate from the fooof_settings below.
-        fooof_kwargs = {**default_fooof_opt, **fooof_opt}  # Join the ones from fooof_opt (the user) into the default fooof_kwargs.
+        fooof_kwargs = {
+            **default_fooof_opt,
+            **fooof_opt,
+        }  # Join the ones from fooof_opt (the user) into the default fooof_kwargs.
 
         # Settings used during the FOOOF analysis (that are NOT passed to FOOOF constructor).
         # The user cannot influence these: in_freqs is derived from mtmfft output, freq_range is always None (=full mtmfft output spectrum).
         # We still define them here, and they are passed through to the backend and actually used there.
         fooof_settings = {
-            'in_freqs': fooof_data.freq,
-            'freq_range': None  # or something like [2, 40] to limit frequency range (post processing). Currently not exposed to user.
+            "in_freqs": fooof_data.freq,
+            "freq_range": None,  # or something like [2, 40] to limit frequency range (post processing). Currently not exposed to user.
         }
 
         if fooof_data.freq[0] == 0:
             # FOOOF does not work with input frequency zero in the data.
-            raise SPYValueError(legal="a frequency range that does not include zero. Use 'foi' or 'foilim' to restrict.", varname="foi/foilim", actual="Frequency range from {} to {}.".format(min(fooof_data.freq), max(fooof_data.freq)))
+            raise SPYValueError(
+                legal="a frequency range that does not include zero. Use 'foi' or 'foilim' to restrict.",
+                varname="foi/foilim",
+                actual="Frequency range from {} to {}.".format(min(fooof_data.freq), max(fooof_data.freq)),
+            )
 
         # Set up compute-class
         #  - the output must be one of 'fooof', 'fooof_aperiodic',
         #    or 'fooof_peaks'.
         #  - everything passed as method_kwargs is passed as arguments
         #    to the fooof.FOOOF() constructor or functions, the other args are
         #    used elsewhere.
-        fooofMethod = FooofSpy(output=output_fooof, fooof_settings=fooof_settings, method_kwargs=fooof_kwargs)
+        fooofMethod = FooofSpy(
+            output=output_fooof,
+            fooof_settings=fooof_settings,
+            method_kwargs=fooof_kwargs,
+        )
 
         # Update `log_dct` w/method-specific options
         log_dct["fooof_method"] = output_fooof
         log_dct["fooof_opt"] = fooof_kwargs
 
         # Perform actual computation
-        fooofMethod.initialize(fooof_data,
-                               fooof_out._stackingDim,
-                               chan_per_worker=kwargs.get("chan_per_worker"),
-                               keeptrials=keeptrials)
+        fooofMethod.initialize(
+            fooof_data,
+            fooof_out._stackingDim,
+            chan_per_worker=kwargs.get("chan_per_worker"),
+            keeptrials=keeptrials,
+        )
         fooofMethod.compute(fooof_data, fooof_out, parallel=kwargs.get("parallel"), log_dict=log_dct)
         out = fooof_out
 
     # Perform mtmconvolv post-processing for `method='welch'`.
     if method == "welch":
         welch_data = out
-        out = spy.mean(welch_data, dim='time')
+        out = spy.mean(welch_data, dim="time")
 
     # Attach potential older cfg's from the input
     # to support chained frontend calls.
     out.cfg.update(data.cfg)
 
     # Attach frontend parameters for replay.
-    out.cfg.update({'freqanalysis': new_cfg})
+    out.cfg.update({"freqanalysis": new_cfg})
     return out
```

### Comparing `esi_syncopy-2023.5/syncopy/specest/mtmconvol.py` & `esi_syncopy-2023.7/syncopy/specest/mtmconvol.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,16 +10,25 @@
 from scipy import signal
 
 # local imports
 from .stft import stft
 from ._norm_spec import _norm_taper
 
 
-def mtmconvol(data_arr, samplerate, nperseg, noverlap=None, taper="hann",
-              taper_opt=None, boundary='zeros', padded=True, detrend=False):
+def mtmconvol(
+    data_arr,
+    samplerate,
+    nperseg,
+    noverlap=None,
+    taper="hann",
+    taper_opt=None,
+    boundary="zeros",
+    padded=True,
+    detrend=False,
+):
 
     """
     (Multi-)tapered short time fast Fourier transform. Returns
     full complex Fourier transform for each taper.
     Multi-tapering only supported with Slepian windwows (`taper="dpss"`).
 
     Parameters
@@ -83,27 +92,27 @@
     # FFT frequencies from the window size
     freqs = np.fft.rfftfreq(nperseg, 1 / samplerate)
     nFreq = freqs.size
     # frequency bins
     dFreq = freqs[1] - freqs[0]
 
     if taper is None:
-        taper = 'boxcar'
+        taper = "boxcar"
 
     taper_func = getattr(signal.windows, taper)
 
     if taper_opt is None:
         taper_opt = {}
 
     # this parameter mitigates the sum-to-zero problem for the odd slepians
     # as signal.stft has hardcoded scaling='spectrum'
     # -> normalizes with win.sum() :/
     # see also https://github.com/scipy/scipy/issues/14740
-    if taper == 'dpss':
-        taper_opt['sym'] = False
+    if taper == "dpss":
+        taper_opt["sym"] = False
 
     # only truly 2d for multi-taper "dpss"
     windows = np.atleast_2d(taper_func(nperseg, **taper_opt))
 
     # normalize window(s)
     windows = _norm_taper(taper, windows, nperseg)
 
@@ -113,22 +122,31 @@
         nTime = int(np.ceil(nSamples / (nperseg - noverlap))) - nperseg
     else:
         # the signal is padded on each side as to cover
         # the whole signal
         nTime = int(np.ceil(nSamples / (nperseg - noverlap)))
 
     # Short time Fourier transforms (nTime x nTapers x nFreq x nChannels)
-    ftr = np.zeros((nTime, windows.shape[0], nFreq, nChannels), dtype='complex64')
+    ftr = np.zeros((nTime, windows.shape[0], nFreq, nChannels), dtype="complex64")
 
     logger = logging.getLogger("syncopy_" + platform.node())
-    logger.debug(f"Running mtmconvol on {len(windows)} windows, data chunk has {nSamples} samples and {nChannels} channels.")
+    logger.debug(
+        f"Running mtmconvol on {len(windows)} windows, data chunk has {nSamples} samples and {nChannels} channels."
+    )
 
     for taperIdx, win in enumerate(windows):
         # ftr has shape (nFreq, nChannels, nTime)
-        pxx, _, _ = stft(data_arr, samplerate, window=win,
-                         nperseg=nperseg, noverlap=noverlap,
-                         boundary=boundary, padded=padded,
-                         axis=0, detrend=detrend)
+        pxx, _, _ = stft(
+            data_arr,
+            samplerate,
+            window=win,
+            nperseg=nperseg,
+            noverlap=noverlap,
+            boundary=boundary,
+            padded=padded,
+            axis=0,
+            detrend=detrend,
+        )
 
         ftr[:, taperIdx, ...] = pxx.transpose(2, 0, 1)[:nTime, ...]
 
     return ftr, freqs
```

### Comparing `esi_syncopy-2023.5/syncopy/specest/mtmfft.py` & `esi_syncopy-2023.7/syncopy/specest/mtmfft.py`

 * *Files 13% similar despite different names*

```diff
@@ -9,21 +9,23 @@
 import logging
 import platform
 
 # local imports
 from ._norm_spec import _norm_spec, _norm_taper
 
 
-def mtmfft(data_arr,
-           samplerate,
-           nSamples=None,
-           taper="hann",
-           taper_opt=None,
-           demean_taper=False,
-           ft_compat=False):
+def mtmfft(
+    data_arr,
+    samplerate,
+    nSamples=None,
+    taper="hann",
+    taper_opt=None,
+    demean_taper=False,
+    ft_compat=False,
+):
     """
     (Multi-)tapered fast Fourier transform. Returns
     full complex Fourier transform for each taper.
     Multi-tapering only supported with Slepian windwows (`taper="dpss"`).
 
     Parameters
     ----------
@@ -82,52 +84,64 @@
     nChannels = data_arr.shape[1]
 
     freqs = np.fft.rfftfreq(nSamples, 1 / samplerate)
     nFreq = freqs.size
 
     # no taper is boxcar
     if taper is None:
-        taper = 'boxcar'
+        taper = "boxcar"
 
     if taper_opt is None:
         taper_opt = {}
 
     taper_func = getattr(signal.windows, taper)
     # only really 2d if taper='dpss' with Kmax > 1
     # here we take the actual signal lengths!
     windows = np.atleast_2d(taper_func(signal_length, **taper_opt))
     # normalize window with total (after padding) length
     windows = _norm_taper(taper, windows, nSamples)
 
     # Fourier transforms (nTapers x nFreq x nChannels)
-    ftr = np.zeros((windows.shape[0], nFreq, nChannels), dtype='complex64')
+    ftr = np.zeros((windows.shape[0], nFreq, nChannels), dtype="complex64")
 
     logger = logging.getLogger("syncopy_" + platform.node())
-    logger.debug(f"Running mtmfft on {len(windows)} windows, data chunk has {nSamples} samples and {nChannels} channels.")
+    logger.debug(
+        f"Running mtmfft on {len(windows)} windows, data chunk has {nSamples} samples and {nChannels} channels."
+    )
 
     for taperIdx, win in enumerate(windows):
         win = np.tile(win, (nChannels, 1)).T
         win *= data_arr
         # de-mean again after tapering - needed for Granger!
         if demean_taper:
             win -= win.mean(axis=0)
         ftr[taperIdx] = np.fft.rfft(win, n=nSamples, axis=0)
         # FT uses potentially padded length `nSamples`, which dilutes the power
-        if ft_compat:            
+        if ft_compat:
             ftr[taperIdx] = _norm_spec(ftr[taperIdx], nSamples, samplerate)
         # here the normalization adapts such that padding is NOT changing power
         else:
-            ftr[taperIdx] = _norm_spec(ftr[taperIdx], signal_length * np.sqrt(nSamples / signal_length), samplerate)
+            ftr[taperIdx] = _norm_spec(
+                ftr[taperIdx],
+                signal_length * np.sqrt(nSamples / signal_length),
+                samplerate,
+            )
 
     return ftr, freqs
 
 
 def _get_dpss_pars(tapsmofrq, nSamples, samplerate):
 
-    """ Helper function to retrieve dpss parameters from tapsmofrq """
+    """Helper function to retrieve dpss parameters from tapsmofrq"""
 
     # taper width parameter in sample units
     NW = tapsmofrq * nSamples / samplerate
-    # from the minBw setting NW always is at least 1
+
+    # from the minBw formula in `input_processors.process_taper`
+    # Kmax is at least 1!
     Kmax = int(2 * NW - 1)  # optimal number of tapers
 
+    # ..but NW can be 0.9999999999999999..
+    # catch those floating point issues
+    Kmax = Kmax if Kmax > 1 else 1
+
     return NW, Kmax
```

### Comparing `esi_syncopy-2023.5/syncopy/specest/stft.py` & `esi_syncopy-2023.7/syncopy/specest/stft.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,23 +9,25 @@
 import logging
 import platform
 
 # local imports
 from ._norm_spec import _norm_spec
 
 
-def stft(dat,
-         fs=1.,
-         window=None,
-         nperseg=256,
-         noverlap=None,
-         boundary='zeros',
-         detrend=False,
-         padded=True,
-         axis=0):
+def stft(
+    dat,
+    fs=1.0,
+    window=None,
+    nperseg=256,
+    noverlap=None,
+    boundary="zeros",
+    detrend=False,
+    padded=True,
+    axis=0,
+):
 
     """
     Implements the short-time (or windowed) Fourier transform
 
     The interface is designed to be close to SciPy's implementation: :func: `~scipy.signal.stft`
 
     Parameters
@@ -106,44 +108,44 @@
     if noverlap is None:
         noverlap = nperseg // 2
     nstep = nperseg - noverlap
 
     if padded:
         # Pad to integer number of windowed segments
         # I.e make x.shape[-1] = nperseg + (nseg-1)*nstep, with integer nseg
-        nadd = (-(dat.shape[-1]-nperseg) % nstep) % nperseg
+        nadd = (-(dat.shape[-1] - nperseg) % nstep) % nperseg
         zeros_shape = list(dat.shape[:-1]) + [nadd]
         dat = np.concatenate((dat, np.zeros(zeros_shape)), axis=-1)
 
     # Create strided array of data segments
     if nperseg == 1 and noverlap == 0:
         dat = dat[..., np.newaxis]
     else:
         # https://stackoverflow.com/a/5568169
         step = nperseg - noverlap
         shape = dat.shape[:-1] + ((dat.shape[-1] - noverlap) // step, nperseg)
         strides = dat.strides[:-1] + (step * dat.strides[-1], dat.strides[-1])
-        dat = np.lib.stride_tricks.as_strided(dat, shape=shape,
-                                              strides=strides)
+        dat = np.lib.stride_tricks.as_strided(dat, shape=shape, strides=strides)
     # dat now has shape (nChannels, nSamples, nperseg)
 
     # detrend each segment separately
     if detrend:
         dat = sci_sig.detrend(dat, type=detrend, overwrite_data=True)
 
     if window is not None:
         # Apply window by multiplication
         dat = dat * window
 
     logger = logging.getLogger("syncopy_" + platform.node())
     pad_status = "with padding" if padded else "without padding"
-    logger.debug(f"Running short time Fourier transform {pad_status}, detrend={detrend} and overlap of {noverlap}.")
+    logger.debug(
+        f"Running short time Fourier transform {pad_status}, detrend={detrend} and overlap of {noverlap}."
+    )
 
-    times = np.arange(nperseg / 2, dat.shape[-1] - nperseg / 2 + 1,
-                      nperseg - noverlap) / fs
+    times = np.arange(nperseg / 2, dat.shape[-1] - nperseg / 2 + 1, nperseg - noverlap) / fs
     if boundary is not None:
         times -= (nperseg / 2) / fs
 
     freqs = np.fft.rfftfreq(nperseg, 1 / fs)
 
     # the complex transforms
     ftr = np.fft.rfft(dat, axis=-1)
```

### Comparing `esi_syncopy-2023.5/syncopy/specest/superlet.py` & `esi_syncopy-2023.7/syncopy/specest/superlet.py`

 * *Files 4% similar despite different names*

```diff
@@ -83,79 +83,63 @@
 
     """
     logger = logging.getLogger("syncopy_" + platform.node())
 
     # adaptive SLT
     if adaptive:
 
-        logger.debug(f"Running fractional adaptive superlet transform with order_min={order_min}, order_max={order_max} and c_1={c_1} on data with shape {data_arr.shape}.")
+        logger.debug(
+            f"Running fractional adaptive superlet transform with order_min={order_min}, order_max={order_max} and c_1={c_1} on data with shape {data_arr.shape}."
+        )
 
-        gmean_spec = FASLT(data_arr,
-                           samplerate,
-                           scales,
-                           order_max,
-                           order_min,
-                           c_1)
+        gmean_spec = FASLT(data_arr, samplerate, scales, order_max, order_min, c_1)
 
     # multiplicative SLT
     else:
 
-        logger.debug(f"Running multiplicative superlet transform with order_min={order_min}, order_max={order_max} and c_1={c_1} on data with shape {data_arr.shape}.")
+        logger.debug(
+            f"Running multiplicative superlet transform with order_min={order_min}, order_max={order_max} and c_1={c_1} on data with shape {data_arr.shape}."
+        )
 
-        gmean_spec = multiplicativeSLT(data_arr,
-                                       samplerate,
-                                       scales,
-                                       order_max,
-                                       order_min,
-                                       c_1)
+        gmean_spec = multiplicativeSLT(data_arr, samplerate, scales, order_max, order_min, c_1)
 
     return gmean_spec
 
 
-def multiplicativeSLT(data_arr,
-                      samplerate,
-                      scales,
-                      order_max,
-                      order_min=1,
-                      c_1=3):
+def multiplicativeSLT(data_arr, samplerate, scales, order_max, order_min=1, c_1=3):
 
     dt = 1 / samplerate
     # create the complete multiplicative set spanning
     # order_min - order_max
     cycles = c_1 * np.arange(order_min, order_max + 1)
-    order_num = order_max + 1 - order_min # number of different orders
+    order_num = order_max + 1 - order_min  # number of different orders
     SL = [MorletSL(c) for c in cycles]
 
     # lowest order
     gmean_spec = cwtSL(data_arr, SL[0], scales, dt)
     gmean_spec = np.power(gmean_spec, 1 / order_num)
 
     for wavelet in SL[1:]:
 
         spec = cwtSL(data_arr, wavelet, scales, dt)
         gmean_spec *= np.power(spec, 1 / order_num)
 
     return gmean_spec
 
 
-def FASLT(data_arr,
-          samplerate,
-          scales,
-          order_max,
-          order_min=1,
-          c_1=3):
+def FASLT(data_arr, samplerate, scales, order_max, order_min=1, c_1=3):
 
-    ''' Fractional adaptive SL transform
+    """Fractional adaptive SL transform
 
     For non-integer orders fractional SLTs are
     calculated in the interval [order, order+1) via:
 
     R(o_f) = R_1 * R_2 * ... * R_i * R_i+1 ** alpha
     with o_f = o_i + alpha
-    '''
+    """
 
     dt = 1 / samplerate
     # frequencies of interest
     # from the scales for the SL Morlet
     fois = 1 / (2 * np.pi * scales)
     orders = compute_adaptive_order(fois, order_min, order_max)
 
@@ -196,39 +180,33 @@
         # order + 1 spec
         next_spec = cwtSL(data_arr, SL[i + 1], scales_o, dt)
 
         # which fractions for the current next_spec
         # in the interval [order, order+1)
         scale_span = slice(last_jump, jump + 1)
         gmean_spec[scale_span, :] *= np.power(
-            next_spec[:jump - last_jump + 1].T,
-            alphas[scale_span] * exponents[scale_span]).T
+            next_spec[: jump - last_jump + 1].T,
+            alphas[scale_span] * exponents[scale_span],
+        ).T
 
         # multiply non-fractional next_spec for
         # all remaining scales/frequencies
-        gmean_spec[jump + 1 :] *= np.power(
-            next_spec[jump - last_jump + 1:].T,
-            exponents[jump + 1 :]).T
+        gmean_spec[jump + 1 :] *= np.power(next_spec[jump - last_jump + 1 :].T, exponents[jump + 1 :]).T
 
         # go to the next [order, order+1) interval
         last_jump = jump + 1
 
     return gmean_spec
 
 
-def adaptiveSLT(data_arr,
-                samplerate,
-                scales,
-                order_max,
-                order_min=1,
-                c_1=3):
+def adaptiveSLT(data_arr, samplerate, scales, order_max, order_min=1, c_1=3):
 
-    '''This function is not used atm, it implements
+    """This function is not used atm, it implements
     the non-fractional adaptive SLT. Kept here for
-    reference/comparisons if ever needed'''
+    reference/comparisons if ever needed"""
 
     dt = 1 / samplerate
     # frequencies of interest
     # from the scales for the SL Morlet
     # for len(orders) < len(scales)
     # multiple scales have the same order/wavelet set (discrete banding)
     fois = 1 / (2 * np.pi * scales)
@@ -273,15 +251,15 @@
 
     return gmean_spec
 
 
 class MorletSL:
     def __init__(self, c_i=3, k_sd=5):
 
-        """ The Morlet formulation according to
+        """The Morlet formulation according to
         Moca et al. shifts the admissability criterion from
         the central frequency to the number of cycles c_i
         within the Gaussian envelope which has a constant
         standard deviation of k_sd.
         """
 
         self.c_i = c_i
@@ -373,15 +351,15 @@
     slices[0] = slice(None)
 
     # compute in time
     for ind, scale in enumerate(scales):
 
         t = _get_superlet_support(scale, dt, wavelet.c_i)
         # sample wavelet and normalise
-        norm = dt ** 0.5 / (4 * np.pi)
+        norm = dt**0.5 / (4 * np.pi)
         wavelet_data = norm * wavelet(t, scale)  # this is an 1d array for sure!
 
         # np.convolve only works if support is capped
         # at signal lengths, as its output has shape
         # max(len(data), len(wavelet_data)
         output[ind, :] = fftconvolve(data, wavelet_data[tuple(slices)], mode="same")
     return output
```

### Comparing `esi_syncopy-2023.5/syncopy/specest/wavelet.py` & `esi_syncopy-2023.7/syncopy/specest/wavelet.py`

 * *Files 1% similar despite different names*

```diff
@@ -36,15 +36,17 @@
     -------
     spec : :class:`numpy.ndarray`
         Complex time-frequency representation of the input data.
         Shape is (len(scales),) + data_arr.shape
     """
 
     logger = logging.getLogger("syncopy_" + platform.node())
-    logger.debug(f"Running wavelet transform on data with shape {data_arr.shape} and samplerate {samplerate}.")
+    logger.debug(
+        f"Running wavelet transform on data with shape {data_arr.shape} and samplerate {samplerate}."
+    )
 
     spec = cwt(data_arr, wavelet=wavelet, widths=scales, dt=1 / samplerate, axis=0)
 
     return spec
 
 
 def get_optimal_wavelet_scales(scale_from_period, nSamples, dt, dj=0.25, s0=None):
```

### Comparing `esi_syncopy-2023.5/syncopy/specest/wavelets/transform.py` & `esi_syncopy-2023.7/syncopy/specest/wavelets/transform.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 import scipy
 import scipy.signal
 import scipy.optimize
 import scipy.special
 
 from .wavelets import Morlet
 
-__all__ = ['cwt', 'WaveletAnalysis', 'WaveletTransform']
+__all__ = ["cwt", "WaveletAnalysis", "WaveletTransform"]
 
 
 def cwt(data, wavelet=None, widths=None, dt=1, frequency=False, axis=-1):
     """Continuous wavelet transform using the Fourier transform
     convolution as used in Terrence and Compo.
 
     (as opposed to the direct convolution method used by
@@ -70,18 +70,18 @@
     Returns
     -------
     cwt: (M, N) ndarray
         Will have shape of (len(data), len(widths)).
 
     """
     if widths is None:
-        raise UserWarning('Have to specify some widths (scales)')
+        raise UserWarning("Have to specify some widths (scales)")
 
     if not wavelet:
-        raise UserWarning('Have to specify a wavelet function')
+        raise UserWarning("Have to specify a wavelet function")
 
     if frequency:
         return cwt_freq(data, wavelet, widths, dt, axis)
     elif not frequency:
         return cwt_time(data, wavelet, widths, dt, axis)
 
 
@@ -93,24 +93,22 @@
     slices = [None for _ in data.shape]
     slices[axis] = slice(None)
     slices = tuple(slices)
     for ind, width in enumerate(widths):
         # number of points needed to capture wavelet
         M = 10 * width / dt
         # times to use, centred at zero
-        t = np.arange((-M + 1) / 2., (M + 1) / 2.) * dt
+        t = np.arange((-M + 1) / 2.0, (M + 1) / 2.0) * dt
         # sample wavelet and normalise to harmonic amplitude
-        norm = dt ** .5  / (width * 8 * np.pi)
+        norm = dt**0.5 / (width * 8 * np.pi)
         wavelet_data = norm * wavelet(t, width)
 
         # support might be longer than data, but fftconvolve
         # returns result of 1st input argument lengths!
-        output[ind, :] = scipy.signal.fftconvolve(data,
-                                                  wavelet_data[slices],
-                                                  mode='same')
+        output[ind, :] = scipy.signal.fftconvolve(data, wavelet_data[slices], mode="same")
     return output
 
 
 def cwt_freq(data, wavelet, widths, dt, axis):
     # compute in frequency
     # next highest power of two for padding
     N = data.shape[axis]
@@ -118,27 +116,26 @@
     # N.B. padding in fft adds zeros to the *end* of the array,
     # not equally either end.
     fft_data = scipy.fft(data, n=pN, axis=axis)
     # frequencies
     w_k = np.fft.fftfreq(pN, d=dt) * 2 * np.pi
 
     # sample wavelet and normalise
-    norm = (2 * np.pi * widths / dt) ** .5
+    norm = (2 * np.pi * widths / dt) ** 0.5
     wavelet_data = norm[:, None] * wavelet(w_k, widths[:, None])
 
     # Convert negative axis. Add one to account for
     # inclusion of widths axis above.
     axis = (axis % data.ndim) + 1
 
     # perform the convolution in frequency space
     slices = [slice(None)] + [None for _ in data.shape]
     slices[axis] = slice(None)
 
-    out = scipy.ifft(fft_data[None] * wavelet_data.conj()[slices],
-                     n=pN, axis=axis)
+    out = scipy.ifft(fft_data[None] * wavelet_data.conj()[slices], n=pN, axis=axis)
 
     # remove zero padding
     slices = [slice(None) for _ in out.shape]
     slices[axis] = slice(None, N)
 
     if data.ndim == 1:
         return out[slices].squeeze()
@@ -203,44 +200,53 @@
     If we wish to compare wavelet spectra at different scales with
     each other and with Fourier modes, we need a common set of
     units.
 
     The equivalent Fourier period is defined as where the wavelet
     power spectrum reaches its maximum and can be found analytically.
     """
-    def __init__(self, data=None, time=None, dt=1,
-                 dj=0.125, wavelet=Morlet(), unbias=False,
-                 mask_coi=False, frequency=False, axis=-1):
+
+    def __init__(
+        self,
+        data=None,
+        time=None,
+        dt=1,
+        dj=0.125,
+        wavelet=Morlet(),
+        unbias=False,
+        mask_coi=False,
+        frequency=False,
+        axis=-1,
+    ):
         """Arguments:
-            data - 1 dimensional input signal
-            time - corresponding times for the input signal
-                   not essential, but the COI will be calculated
-                   for time starting at zero.
-            dt - sample spacing
-            dj - scale resolution
-            wavelet - wavelet class to use, must have an attribute
-                      `time`, giving a wavelet function that takes (t, s)
-                      as arguments and, if frequency is True, an
-                      attribute `frequency`, giving a wavelet function
-                      that takes (w, s) as arguments.
-            unbias - boolean, whether to unbias the power spectrum, as
-                     in Liu et al. 2007 (default False)
-            frequency - boolean, compute the cwt in frequency space?
-                        (default False)
-            mask_coi - disregard wavelet power outside the cone of
-                       influence when computing global wavelet spectrum
-                       (default False)
-            axis - axis of the input data to transform over (default -1)
+        data - 1 dimensional input signal
+        time - corresponding times for the input signal
+               not essential, but the COI will be calculated
+               for time starting at zero.
+        dt - sample spacing
+        dj - scale resolution
+        wavelet - wavelet class to use, must have an attribute
+                  `time`, giving a wavelet function that takes (t, s)
+                  as arguments and, if frequency is True, an
+                  attribute `frequency`, giving a wavelet function
+                  that takes (w, s) as arguments.
+        unbias - boolean, whether to unbias the power spectrum, as
+                 in Liu et al. 2007 (default False)
+        frequency - boolean, compute the cwt in frequency space?
+                    (default False)
+        mask_coi - disregard wavelet power outside the cone of
+                   influence when computing global wavelet spectrum
+                   (default False)
+        axis - axis of the input data to transform over (default -1)
         """
         self.data = data
         if time is None:
             time = np.indices((data.shape[axis],)).squeeze() * dt
         self.time = time
-        self.anomaly_data = self.data - self.data.mean(axis=axis,
-                                                       keepdims=True)
+        self.anomaly_data = self.data - self.data.mean(axis=axis, keepdims=True)
         self.N = data.shape[axis]
         self.data_variance = self.data.var(axis=axis, keepdims=True)
         self.dt = dt
         self.dj = dj
         self.wavelet = wavelet
         # which continuous wavelet transform to use
         self.cwt = cwt
@@ -250,22 +256,22 @@
         self.axis = axis
 
     @property
     def fourier_period(self):
         """Return a function that calculates the equivalent Fourier
         period as a function of scale.
         """
-        return getattr(self.wavelet, 'fourier_period')
+        return getattr(self.wavelet, "fourier_period")
 
     @property
     def scale_from_period(self):
         """Return a function that calculates the wavelet scale
         from the fourier period
         """
-        return getattr(self.wavelet, 'scale_from_period')
+        return getattr(self.wavelet, "scale_from_period")
 
     @property
     def fourier_periods(self):
         """Return the equivalent Fourier periods for the scales used."""
         return self.fourier_period(self.scales)
 
     @fourier_periods.setter
@@ -287,44 +293,45 @@
         Set the scales based on a list of fourier periods.
         This is equivalent to self.fourier_periods = 1.0 / frequencies
         """
         self.fourier_periods = np.reciprocal(frequencies)
 
     @property
     def s0(self):
-        if not hasattr(self, '_s0'):
+        if not hasattr(self, "_s0"):
             return self.find_s0()
         else:
             return self._s0
 
     @s0.setter
     def s0(self, value):
-        setattr(self, '_s0', value)
+        setattr(self, "_s0", value)
 
     def find_s0(self):
         """Find the smallest resolvable scale by finding where the
         equivalent Fourier period is equal to 2 * dt. For a Morlet
         wavelet, this is roughly 1.
         """
         dt = self.dt
 
         def f(s):
             return self.fourier_period(s) - 2 * dt
+
         return scipy.optimize.fsolve(f, 1)[0]
 
     @property
     def scales(self):
-        if not hasattr(self, '_scales'):
+        if not hasattr(self, "_scales"):
             return self.compute_optimal_scales()
         else:
             return self._scales
 
     @scales.setter
     def scales(self, value):
-        setattr(self, '_scales', value)
+        setattr(self, "_scales", value)
 
     def compute_optimal_scales(self):
         """Form a set of scales to use in the wavelet transform.
 
         For non-orthogonal wavelet analysis, one can use an
         arbitrary set of scales.
 
@@ -375,20 +382,22 @@
         widths = self.scales
 
         if self.frequency:
             wavelet = self.wavelet.frequency
         else:
             wavelet = self.wavelet.time
 
-        return self.cwt(self.anomaly_data,
-                        wavelet=wavelet,
-                        widths=widths,
-                        dt=self.dt,
-                        frequency=self.frequency,
-                        axis=self.axis)
+        return self.cwt(
+            self.anomaly_data,
+            wavelet=wavelet,
+            widths=widths,
+            dt=self.dt,
+            frequency=self.frequency,
+            axis=self.axis,
+        )
 
     @property
     def wavelet_power(self):
         """Calculate the wavelet power spectrum, optionally using
         the bias correction factor introduced by Liu et al. 2007,
         which is to divide by the scale.
         """
@@ -427,16 +436,16 @@
         s = self.scales
         W_n = self.wavelet_transform
 
         if scales is not None:
             self.scales = old_scales
 
         # use the transpose to allow broadcasting
-        real_sum = np.sum(W_n.real.T / s ** .5, axis=-1).T
-        x_n = real_sum * (dj * dt ** .5 / (C_d * Y_00))
+        real_sum = np.sum(W_n.real.T / s**0.5, axis=-1).T
+        x_n = real_sum * (dj * dt**0.5 / (C_d * Y_00))
 
         # add the mean back on (x_n is anomaly time series)
         x_n += self.data.mean(axis=self.axis, keepdims=True)
 
         return x_n
 
     @property
@@ -483,15 +492,15 @@
 
             C_d = (dj * dt^(1/2)) / Y_0(0) \
                     * Sum_(j=0)^J { Re(W_d(s_j)) / s_j^(1/2) }
 
         C_d is scale independent and a constant for each wavelet
         function.
         """
-        if hasattr(self.wavelet, 'C_d'):
+        if hasattr(self.wavelet, "C_d"):
             return self.wavelet.C_d
         else:
             return self.compute_Cdelta()
 
     def compute_Cdelta(self):
         """Compute the parameter C_delta (see self.C_d), used in
         reconstruction. See section 3.i of TC98.
@@ -503,30 +512,30 @@
         dt = self.dt
         s = self.scales
         W_d = self.wavelet_transform_delta
 
         # value of the wavelet function at t=0
         Y_00 = self.wavelet.time(0)
 
-        real_sum = np.sum(W_d.real / s ** .5)
-        C_d = real_sum * (dj * dt ** .5 / Y_00)
+        real_sum = np.sum(W_d.real / s**0.5)
+        C_d = real_sum * (dj * dt**0.5 / Y_00)
         return C_d
 
     @property
     def wavelet_transform_delta(self):
         """Calculate the delta wavelet transform.
 
         Returns an array of the transform computed over the scales.
         """
         Y_0 = self.wavelet.frequency  # wavelet as f(w_k, s)
 
         WK, S = np.meshgrid(self.w_k, self.scales)
 
         # compute Y_ over all s, w_k and sum over k
-        norm = (2 * np.pi * S / self.dt) ** .5
+        norm = (2 * np.pi * S / self.dt) ** 0.5
         W_d = (1 / self.N) * np.sum(norm * Y_0(WK, S), axis=1)
 
         # N.B This W_d is 1D (defined only at n=0)
         return W_d
 
     @property
     def wavelet_variance(self):
@@ -557,17 +566,15 @@
 
         Return a tuple (T, S) that describes the edge of the cone
         of influence as a single line in (time, scale).
         """
         Tmin = self.time.min()
         Tmax = self.time.max()
         Tmid = Tmin + (Tmax - Tmin) / 2
-        s = np.logspace(np.log10(self.scales.min()),
-                        np.log10(self.scales.max()),
-                        100)
+        s = np.logspace(np.log10(self.scales.min()), np.log10(self.scales.max()), 100)
         c1 = Tmin + self.wavelet.coi(s)
         c2 = Tmax - self.wavelet.coi(s)
 
         C = np.hstack((c1[np.where(c1 < Tmid)], c2[np.where(c2 > Tmid)]))
         S = np.hstack((s[np.where(c1 < Tmid)], s[np.where(c2 > Tmid)]))
 
         # sort w.r.t time
@@ -588,24 +595,20 @@
 
         if not ax:
             fig, ax = plt.subplots()
 
         Time, Scale = np.meshgrid(self.time, self.scales)
         ax.contourf(Time, Scale, self.wavelet_power, 100)
 
-        ax.set_yscale('log')
+        ax.set_yscale("log")
         ax.grid(True)
 
         if coi:
             coi_time, coi_scale = self.coi
-            ax.fill_between(x=coi_time,
-                            y1=coi_scale,
-                            y2=self.scales.max(),
-                            color='gray',
-                            alpha=0.3)
+            ax.fill_between(x=coi_time, y1=coi_scale, y2=self.scales.max(), color="gray", alpha=0.3)
 
         ax.set_xlim(self.time.min(), self.time.max())
 
         return ax
 
 
 WaveletAnalysis = WaveletTransform
```

### Comparing `esi_syncopy-2023.5/syncopy/specest/wavelets/wavelets.py` & `esi_syncopy-2023.7/syncopy/specest/wavelets/wavelets.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 import numpy as np
 import scipy
 import scipy.signal
 import scipy.optimize
 import scipy.special
 from scipy.special import factorial
 
-__all__ = ['Morlet', 'Paul', 'DOG', 'Ricker', 'Marr', 'Mexican_hat']
+__all__ = ["Morlet", "Paul", "DOG", "Ricker", "Marr", "Mexican_hat"]
 
 
 class Morlet(object):
     def __init__(self, w0=6):
         """w0 is the nondimensional frequency constant. If this is
         set too low then the wavelet does not sample very well: a
         value over 5 should be ok; Terrence and Compo set it to 6.
@@ -75,34 +75,34 @@
         w = self.w0
 
         x = t / s
 
         output = np.exp(1j * w * x)
 
         if complete:
-            output -= np.exp(-0.5 * (w ** 2))
+            output -= np.exp(-0.5 * (w**2))
 
-        output *= np.exp(-0.5 * (x ** 2)) * np.pi ** (-0.25)
+        output *= np.exp(-0.5 * (x**2)) * np.pi ** (-0.25)
 
         return output
 
     # Fourier wavelengths
     def fourier_period(self, s):
         """Equivalent Fourier period of Morlet"""
-        return 4 * np.pi * s / (self.w0 + (2 + self.w0 ** 2) ** .5)
+        return 4 * np.pi * s / (self.w0 + (2 + self.w0**2) ** 0.5)
 
     def scale_from_period(self, period):
         """
         Compute the scale from the fourier period.
         Returns the scale
         """
         # Solve 4 * np.pi * scale / (w0 + (2 + w0 ** 2) ** .5)
         #  for s to obtain this formula
         coeff = np.sqrt(self.w0 * self.w0 + 2)
-        return (period * (coeff + self.w0)) / (4. * np.pi)
+        return (period * (coeff + self.w0)) / (4.0 * np.pi)
 
     # Frequency representation
     def frequency(self, w, s=1.0):
         """Frequency representation of Morlet.
 
         Parameters
         ----------
@@ -119,32 +119,31 @@
             Value of the Morlet wavelet at the given frequency
         """
         x = w * s
         # Heaviside mock
         Hw = np.array(w)
         Hw[w <= 0] = 0
         Hw[w > 0] = 1
-        return np.pi ** -.25 * Hw * np.exp((-(x - self.w0) ** 2) / 2)
+        return np.pi**-0.25 * Hw * np.exp((-((x - self.w0) ** 2)) / 2)
 
     def coi(self, s):
         """The e folding time for the autocorrelation of wavelet
         power at each scale, i.e. the timescale over which an edge
         effect decays by a factor of 1/e^2.
 
         This can be worked out analytically by solving
 
             |Y_0(T)|^2 / |Y_0(0)|^2 = 1 / e^2
         """
-        return 2 ** .5 * s
+        return 2**0.5 * s
 
 
 class Paul(object):
     def __init__(self, m=4):
-        """Initialise a Paul wavelet function of order `m`.
-        """
+        """Initialise a Paul wavelet function of order `m`."""
         self.m = m
 
     def __call__(self, *args, **kwargs):
         return self.time(*args, **kwargs)
 
     def time(self, t, s=1.0):
         """
@@ -168,16 +167,15 @@
             (2 ** m * i ** m * m!) / (pi * (2 * m)!) \
                     * (1 - i * t / s) ** -(m + 1)
 
         """
         m = self.m
         x = t / s
 
-        const = (2 ** m * 1j ** m * factorial(m)) \
-            / (np.pi * factorial(2 * m)) ** .5
+        const = (2**m * 1j**m * factorial(m)) / (np.pi * factorial(2 * m)) ** 0.5
         functional_form = (1 - 1j * x) ** -(m + 1)
 
         output = const * functional_form
 
         return output
 
     # Fourier wavelengths
@@ -187,15 +185,15 @@
 
     def scale_from_period(self, period):
         """
         Compute the scale from the fourier period.
         Returns the scale
         """
         # Solve 4 * np.pi * scale / (2 * m + 1) for s
-        return period * (2 * self.m + 1) / (4 * np.pi)        
+        return period * (2 * self.m + 1) / (4 * np.pi)
 
     # Frequency representation
     def frequency(self, w, s=1.0):
         """Frequency representation of Paul.
 
         Parameters
         ----------
@@ -214,15 +212,15 @@
         """
         m = self.m
         x = w * s
         # Heaviside mock
         Hw = 0.5 * (np.sign(x) + 1)
 
         # prefactor
-        const = 2 ** m / (m * factorial(2 * m - 1)) ** .5
+        const = 2**m / (m * factorial(2 * m - 1)) ** 0.5
 
         functional_form = Hw * (x) ** m * np.exp(-x)
 
         output = const * functional_form
 
         return output
 
@@ -231,15 +229,15 @@
         power at each scale, i.e. the timescale over which an edge
         effect decays by a factor of 1/e^2.
 
         This can be worked out analytically by solving
 
             |Y_0(T)|^2 / |Y_0(0)|^2 = 1 / e^2
         """
-        return s / 2 ** .5
+        return s / 2**0.5
 
 
 class DOG(object):
     def __init__(self, m=2):
         """Initialise a Derivative of Gaussian wavelet of order `m`."""
         if m == 2:
             # value of C_d from TC98
@@ -304,22 +302,22 @@
         m = self.m
 
         # compute the Hermite polynomial (used to evaluate the
         # derivative of a Gaussian)
         He_n = scipy.special.hermitenorm(m)
         gamma = scipy.special.gamma
 
-        const = (-1) ** (m + 1) / gamma(m + 0.5) ** .5
-        function = He_n(x) * np.exp(-x ** 2 / 2)
+        const = (-1) ** (m + 1) / gamma(m + 0.5) ** 0.5
+        function = He_n(x) * np.exp(-(x**2) / 2)
 
         return const * function
 
     def fourier_period(self, s):
         """Equivalent Fourier period of derivative of Gaussian"""
-        return 2 * np.pi * s / (self.m + 0.5) ** .5
+        return 2 * np.pi * s / (self.m + 0.5) ** 0.5
 
     def scale_from_period(self, period):
         """
         Compute the scale from the fourier period.
         Returns the scale
         """
         # Solve 2 * np.pi * s / (np.sqrt(m + 1/2)) for s
@@ -342,28 +340,28 @@
         out : complex
             Value of the derivative of Gaussian wavelet at the
             given time
         """
         m = self.m
         x = s * w
         gamma = scipy.special.gamma
-        const = -1j ** m / gamma(m + 0.5) ** .5
-        function = x ** m * np.exp(-x ** 2 / 2)
+        const = -(1j**m) / gamma(m + 0.5) ** 0.5
+        function = x**m * np.exp(-(x**2) / 2)
         return const * function
 
     def coi(self, s):
         """The e folding time for the autocorrelation of wavelet
         power at each scale, i.e. the timescale over which an edge
         effect decays by a factor of 1/e^2.
 
         This can be worked out analytically by solving
 
             |Y_0(T)|^2 / |Y_0(0)|^2 = 1 / e^2
         """
-        return 2 ** .5 * s
+        return 2**0.5 * s
 
 
 class Ricker(DOG):
     def __init__(self):
         """The Ricker, aka Marr / Mexican Hat, wavelet is a
         derivative of Gaussian order 2.
         """
```

### Comparing `esi_syncopy-2023.5/syncopy/statistics/compRoutines.py` & `esi_syncopy-2023.7/syncopy/statistics/compRoutines.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,20 +8,23 @@
 import numpy as np
 from numpy.lib import stride_tricks
 
 # backend method imports
 from .psth import psth
 
 # syncopy imports
-from syncopy.shared.computational_routine import ComputationalRoutine, propagate_properties
+from syncopy.shared.computational_routine import (
+    ComputationalRoutine,
+    propagate_properties,
+)
 from syncopy.shared.kwarg_decorators import process_io
 
 
 @process_io
-def npstats_cF(trl_dat, operation='mean', axis=0, noCompute=False, chunkShape=None):
+def npstats_cF(trl_dat, operation="mean", axis=0, noCompute=False, chunkShape=None):
 
     """
     Numpy summary statistics on single-trial arrays along indicated `axis`.
 
     Parameters
     ----------
     trl_dat : :class:`numpy.ndarray`
@@ -44,15 +47,15 @@
     Consequently, this function does **not** perform any error checking and operates
     under the assumption that all inputs have been externally validated and cross-checked.
     """
 
     if noCompute:
         # initialize result array
         out_shape = list(trl_dat.shape)
-        out_shape[axis] = 1   # this axis will be summed over
+        out_shape[axis] = 1  # this axis will be summed over
 
         return out_shape, trl_dat.dtype
 
     return NumpyStatDim.methods[operation](trl_dat, axis=axis, keepdims=True)
 
 
 class NumpyStatDim(ComputationalRoutine):
@@ -69,47 +72,47 @@
     as in any other CR. For a standalone trial average use the _trial_statistics function.
 
     See also
     --------
     _trial_statistics: Sequential computation of statistics over trials
     """
 
-    methods = {'mean': np.nanmean,
-               'std': np.nanstd,
-               'var': np.nanvar,
-               'median': np.nanmedian
-               }
+    methods = {
+        "mean": np.nanmean,
+        "std": np.nanstd,
+        "var": np.nanvar,
+        "median": np.nanmedian,
+    }
 
     computeFunction = staticmethod(npstats_cF)
 
     def process_metadata(self, in_data, out_data):
 
         # the dimension over which the statistic got computed
-        dim = in_data.dimord[self.cfg['axis']]
+        dim = in_data.dimord[self.cfg["axis"]]
 
         out_data.samplerate = in_data.samplerate
 
         # Get/set timing-related selection modifiers
         # We've set a fallback all-to-all selection in any case
 
         # time axis really gone, only one trial and time got averaged out
-        if dim == 'time' and not self.keeptrials:
+        if dim == "time" and not self.keeptrials:
             trldef = np.array([[0, 1, 0]])
 
         # trial average, needs equal trial lengths.. just copy from 1st
-        elif dim != 'time' and not self.keeptrials:
+        elif dim != "time" and not self.keeptrials:
             trldef = in_data.selection.trialdefinition[0, :][None, :]
 
         # each trial has empty time axis, so we attach trivial trialdefinition:
         # 1 sample per trial for stacking
-        elif dim == 'time' and self.keeptrials:
+        elif dim == "time" and self.keeptrials:
             nTrials = len(in_data.selection.trials)
             stacking_time = np.arange(nTrials)[:, None]
-            trldef = np.hstack((stacking_time, stacking_time + 1,
-                               np.zeros((nTrials, 1))))
+            trldef = np.hstack((stacking_time, stacking_time + 1, np.zeros((nTrials, 1))))
 
         # nothing happened on the time axis
         else:
             trldef = in_data.selection.trialdefinition
 
         out_data.trialdefinition = trldef
 
@@ -123,25 +126,22 @@
                 if dim not in prop:
                     if np.issubdtype(type(selection), np.number):
                         selection = [selection]
                     setattr(out_data, prop, getattr(in_data, prop)[selection])
                 # set to singleton or None
                 else:
                     # numerical freq axis is gone after averaging
-                    if dim == 'freq':
+                    if dim == "freq":
                         out_data.freq = None
                     else:
-                        setattr(out_data, prop, [self.cfg['operation']])
+                        setattr(out_data, prop, [self.cfg["operation"]])
+
 
 @process_io
-def cov_cF(trl_dat,
-           ddof=None,
-           statAxis=0,
-           noCompute=False,
-           chunkShape=None):
+def cov_cF(trl_dat, ddof=None, statAxis=0, noCompute=False, chunkShape=None):
 
     """
     Covariance between channels via ``np.cov``
 
     Parameters
     ----------
     trl_dat : :class:`numpy.ndarray`
@@ -183,15 +183,15 @@
     see :doc:`/developer/compute_kernels` for technical details on Syncopy's compute
     classes and metafunctions.
 
     Notes
     -----
     Outputs a :class:`~syncopy.CrossSpectralData` object with singleton time and freq
     axis. The backing hdf5 dataset then gets stripped of the empty axes and attached
-    as additional ``.cov`` dataset to a :class:`~syncopy.TimeLockData` object in 
+    as additional ``.cov`` dataset to a :class:`~syncopy.TimeLockData` object in
     the respective frontend.
 
     See also
     --------
     syncopy.timelockanalysis : parent metafunction
     """
 
@@ -207,41 +207,48 @@
             trldef = data.selection.trialdefinition
             for row in range(trldef.shape[0]):
                 trldef[row, :2] = [row, row + 1]
         else:
             chanSec = slice(None)
             time = np.arange(len(data.trials))
             time = time.reshape((time.size, 1))
-            trldef = np.hstack((time, time + 1,
-                                np.zeros((len(data.trials), 1)),
-                                np.array(data.trialinfo)))
+            trldef = np.hstack(
+                (
+                    time,
+                    time + 1,
+                    np.zeros((len(data.trials), 1)),
+                    np.array(data.trialinfo),
+                )
+            )
 
-        # Attach constructed trialdef-array, time axis is gone 
+        # Attach constructed trialdef-array, time axis is gone
         if self.keeptrials:
             out.trialdefinition = trldef
         else:
             out.trialdefinition = np.array([[0, 1, 0]])
 
         # Attach remaining meta-data
         out.samplerate = data.samplerate
         out.channel_i = np.array(data.channel[chanSec])
         out.channel_j = np.array(data.channel[chanSec])
 
 
 @process_io
-def psth_cF(trl_dat,
-            trl_start,
-            onset,
-            trl_end,
-            chan_unit_combs=None,
-            tbins=None,
-            output='rate',
-            samplerate=1000,
-            noCompute=False,
-            chunkShape=None):
+def psth_cF(
+    trl_dat,
+    trl_start,
+    onset,
+    trl_end,
+    chan_unit_combs=None,
+    tbins=None,
+    output="rate",
+    samplerate=1000,
+    noCompute=False,
+    chunkShape=None,
+):
 
     """
     Peristimulus time histogram
 
     Backend :func:`~syncopy.spikes.psth.psth` `method_kwargs`:
 
         {'trl_start', 'onset', 'bins', 'samplerate'}
@@ -304,17 +311,24 @@
     # For initialization of computational routine,
     # just return output shape and dtype
     if noCompute:
         outShape = (nBins, nChanUnit)
         return outShape, np.float32
 
     # call backend method
-    counts, bins = psth(trl_dat, trl_start, onset, trl_end,
-                        chan_unit_combs=chan_unit_combs,
-                        tbins=tbins, samplerate=samplerate, output=output)
+    counts, bins = psth(
+        trl_dat,
+        trl_start,
+        onset,
+        trl_end,
+        chan_unit_combs=chan_unit_combs,
+        tbins=tbins,
+        samplerate=samplerate,
+        output=output,
+    )
 
     return counts
 
 
 class PSTH(ComputationalRoutine):
     """
     Compute class that performs psth analysis of :class:`~syncopy.SpikeData` objects
@@ -332,15 +346,15 @@
 
     # 1st argument,the data, gets omitted
     valid_kws = list(signature(psth).parameters.keys())[1:]
     valid_kws += list(signature(psth_cF).parameters.keys())[1:-1]
 
     def process_metadata(self, data, out):
 
-        tbins = self.cfg['tbins']
+        tbins = self.cfg["tbins"]
         # compute new time axis / samplerate
         bin_midpoints = stride_tricks.sliding_window_view(tbins, (2,)).mean(axis=1)
         srate = 1 / np.diff(bin_midpoints).mean()
 
         # each trial has the same length
         # for "timelocked" (same bins) psth data
         trl_len = len(tbins) - 1
@@ -365,14 +379,14 @@
             out.trialdefinition = trl
         else:
             out.trialdefinition = trl[[0], :]
 
         out.samplerate = srate
         # join labels for final unitX_channelY channel labels
         chan_str = "channel{}_unit{}"
-        out.channel = [chan_str.format(c, u) for c, u in self.cfg['chan_unit_combs']]
+        out.channel = [chan_str.format(c, u) for c, u in self.cfg["chan_unit_combs"]]
 
         if not self.keeptrials:
             # the ad-hoc averaging does not work well here because of NaNs
             # so we rather delete the data to 'not keep the trials'
             out.data = None
             # TODO: add real average operator here
```

### Comparing `esi_syncopy-2023.5/syncopy/statistics/jackknifing.py` & `esi_syncopy-2023.7/syncopy/statistics/jackknifing.py`

 * *Files 2% similar despite different names*

```diff
@@ -47,30 +47,28 @@
     nTrials = len(all_trials)
 
     # -- set up output object --
 
     # each of the loo averages fills one single trial
     # slot of the `replicates` dataset, hence it has the same shape
     # as the input
-    replicates = trl_ensemble.__class__(samplerate=trl_ensemble.samplerate,
-                                        dimord=trl_ensemble.dimord)
+    replicates = trl_ensemble.__class__(samplerate=trl_ensemble.samplerate, dimord=trl_ensemble.dimord)
 
-    with h5py.File(replicates._filename, mode='w') as h5file:
-        dset = h5file.create_dataset('data', shape=trl_ensemble.data.shape,
-                                     dtype=trl_ensemble.data.dtype)
+    with h5py.File(replicates._filename, mode="w") as h5file:
+        dset = h5file.create_dataset("data", shape=trl_ensemble.data.shape, dtype=trl_ensemble.data.dtype)
         replicates.data = dset
 
     # we still need to write into it
     replicates._reopen()
 
     # -- replicate computations --
 
     # first calculate the standard trial average
     # this will also catch non-equal trials in the input
-    trl_avg = spy.mean(trl_ensemble, dim='trials')
+    trl_avg = spy.mean(trl_ensemble, dim="trials")
 
     # all loo replicates have the same shape as
     # the original single trial results, so the stepping
     # along the stacking dim is fixed
     stack_step = int(np.diff(trl_ensemble.sampleinfo[0])[0])
     stack_dim = trl_ensemble._stackingDim
     # stacking index template
@@ -82,25 +80,29 @@
         # trial average is 'single trial', so this is memory safe
         # this is the simple loo average - the 'replicate'
         loo_avg = nTrials * trl_avg.data[()] - trl_ensemble.trials[loo_idx]
         # normalize
         loo_avg /= nTrials - 1
 
         # stack along stacking dim
-        stack_idx[stack_dim] = np.s_[loo_idx * stack_step:(loo_idx + 1) * stack_step]
+        stack_idx[stack_dim] = np.s_[loo_idx * stack_step : (loo_idx + 1) * stack_step]
         replicates.data[tuple(stack_idx)] = loo_avg
 
     # attach properties like channel labels etc.
     propagate_properties(trl_ensemble, replicates)
 
     # create proper trialdefinition
     # FIXME: not clear how to handle offsets (3rd column), set to 0 for now
-    trl_def = np.column_stack([np.arange(len(all_trials)) * stack_step,
-                               np.arange(len(all_trials)) * stack_step + stack_step,
-                               np.zeros(len(all_trials))])
+    trl_def = np.column_stack(
+        [
+            np.arange(len(all_trials)) * stack_step,
+            np.arange(len(all_trials)) * stack_step + stack_step,
+            np.zeros(len(all_trials)),
+        ]
+    )
     replicates.trialdefinition = trl_def
 
     # revert selection state of the input
     if selection_cleanup:
         trl_ensemble.selection = None
 
     return replicates
@@ -132,51 +134,51 @@
     variance : syncopy data object, e.g. :class:`~syncopy.SpectralData`
         The sample variance of the jackknife replicates
     """
 
     if len(direct_estimate.trials) != 1:
         lgl = "original trial statistic with one remaining trial"
         act = f"{len(direct_estimate.trials)} trials"
-        raise SPYValueError(lgl, 'direct_estimate', act)
+        raise SPYValueError(lgl, "direct_estimate", act)
 
     if len(replicates.trials) <= 1:
         lgl = "jackknife replicates with at least 2 trials"
         act = f"{len(replicates.trials)} trials"
-        raise SPYValueError(lgl, 'replicates', act)
+        raise SPYValueError(lgl, "replicates", act)
 
     # 1st average the replicates which
     # gives the single trial jackknife estimate
-    jack_avg = spy.mean(replicates, dim='trials')
+    jack_avg = spy.mean(replicates, dim="trials")
 
     # compute the bias, shapes should match as both
     # quantities come from the same data and
     # got computed by the same CR
     if jack_avg.data.shape != direct_estimate.data.shape:
-        msg = ("Got mismatching shapes for jackknife bias computation:\n"
-               f"jack: {jack_avg.data.shape}, original estimate: {direct_estimate.data.shape}"
-               )
+        msg = (
+            "Got mismatching shapes for jackknife bias computation:\n"
+            f"jack: {jack_avg.data.shape}, original estimate: {direct_estimate.data.shape}"
+        )
         raise SPYError(msg)
 
     nTrials = len(replicates.trials)
     prefac = nTrials - 1
     # to avoid different type real/complex warning..
     prefac = prefac + 0j if np.issubdtype(direct_estimate.data.dtype, complex) else prefac
     bias = prefac * (jack_avg - direct_estimate)
 
     # Variance calculation, it is always real (as opposed to pseudo-variance)
     # compute sequentially into accumulator array
     var = np.zeros(direct_estimate.data.shape, dtype=np.float32)
     for loo in replicates.trials:
         # need abs for complex variance
-        var += (np.abs(jack_avg.trials[0] - loo))**2
+        var += (np.abs(jack_avg.trials[0] - loo)) ** 2
     # normalize
-    var *= (nTrials - 1)
+    var *= nTrials - 1
 
     # create the syncopy data object for the variance
-    variance = direct_estimate.__class__(samplerate=direct_estimate.samplerate,
-                                         dimord=direct_estimate.dimord)
+    variance = direct_estimate.__class__(samplerate=direct_estimate.samplerate, dimord=direct_estimate.dimord)
 
     # bind to syncopy object -> creates the hdf5 dataset
     variance.data = var
     propagate_properties(direct_estimate, variance)
 
     return bias, variance
```

### Comparing `esi_syncopy-2023.5/syncopy/statistics/psth.py` & `esi_syncopy-2023.7/syncopy/statistics/psth.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 import numpy as np
 import logging
 import platform
 from scipy.stats import iqr
 
 
-def psth(trl_dat,
-         trl_start,
-         onset,
-         trl_end,
-         chan_unit_combs=None,
-         tbins=None,
-         output='rate',
-         samplerate=1000):
+def psth(
+    trl_dat,
+    trl_start,
+    onset,
+    trl_end,
+    chan_unit_combs=None,
+    tbins=None,
+    output="rate",
+    samplerate=1000,
+):
 
     """
     Peristimulus time histogram
 
     The single trial input `trl_dat` needs 3 columns:
         - 1st: the sample numbers of the spikes
         - 2nd: channel number of each spike
@@ -61,15 +63,17 @@
 
     # for readability
     samples = trl_dat[:, 0]
     channels = trl_dat[:, 1]
     units = trl_dat[:, 2]
 
     logger = logging.getLogger("syncopy_" + platform.node())
-    logger.debug(f"Computing peristimulus time histogram (PSTH) on data with {samples.size} samples, {channels.size} channels, {units.size} units and samplerate {samplerate}.")
+    logger.debug(
+        f"Computing peristimulus time histogram (PSTH) on data with {samples.size} samples, {channels.size} channels, {units.size} units and samplerate {samplerate}."
+    )
 
     # get relative spike times for all events in trial
     times = _calc_time(samples, trl_start, onset, samplerate)
 
     # Auto-select bin widths as backend fallback
     if tbins is None:
         nBins = Rice_rule(len(times))
@@ -102,27 +106,25 @@
     # now map with respect to unit for all single trial channels (-bins)
     map_unit = {u: np.logical_or.reduce([map_cu(c, u) for c in chan_vec]) for u in unique_units}
 
     # map into histogram time x channel dimensions
     map_unit_hist = {u: [np.any(map_cu(c, u)) for c in chan_vec] for u in unique_units}
 
     # configure output
-    if output in ['rate', 'spikecount']:
+    if output in ["rate", "spikecount"]:
         density = False
-    elif output == 'proportion':
+    elif output == "proportion":
         density = True
 
     for i, iunit in enumerate(unique_units):
-        unit_idx = (units == iunit)
+        unit_idx = units == iunit
 
         if np.sum(unit_idx):
             # over all channels, so this counts different units actually
-            unit_counts = np.histogram2d(times[unit_idx],
-                                         channels[unit_idx],
-                                         bins=bins, density=density)[0]
+            unit_counts = np.histogram2d(times[unit_idx], channels[unit_idx], bins=bins, density=density)[0]
 
             # get indices to inject the results
             # at the right position
             cu_idx = map_unit[iunit]
 
             # de-selects non-existent combinations in histogram
             chan_hist_idx = map_unit_hist[iunit]
@@ -149,21 +151,21 @@
 
     if min_idx != 0:
         counts[:min_idx] = np.nan
     if max_idx != 0:
         counts[max_idx:] = np.nan
 
     # normalize to counts per second
-    if output == 'rate':
+    if output == "rate":
         tbin_width = np.diff(tbins)[0]
         counts *= 1 / tbin_width
 
     # normalize only along time axis for 1d time-histograms
     # `density=True` normalized the full 2d-histogram
-    elif output == 'proportion':
+    elif output == "proportion":
         norm = np.nansum(counts, axis=0)[None, :]
         # deal with potential 0's
         norm[norm == 0] = 1
         counts /= norm
 
     return counts, bins
 
@@ -194,14 +196,15 @@
 
     combs = np.unique(np.concatenate(combs), axis=0)
     return combs
 
 
 # --- Bin selection rules ---
 
+
 def sqrt_rule(nSamples):
 
     """
     Get number of bins via square root of number of samples
     """
 
     return int(np.ceil(np.sqrt(nSamples)))
```

### Comparing `esi_syncopy-2023.5/syncopy/statistics/spike_psth.py` & `esi_syncopy-2023.7/syncopy/statistics/spike_psth.py`

 * *Files 5% similar despite different names*

```diff
@@ -14,37 +14,39 @@
 from syncopy.shared.tools import get_defaults, get_frontend_cfg
 from syncopy.datatype import TimeLockData
 
 from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYInfo
 from syncopy.shared.kwarg_decorators import (
     unwrap_cfg,
     unwrap_select,
-    detect_parallel_client
+    detect_parallel_client,
 )
 from syncopy.shared.input_processors import check_passed_kwargs
 from syncopy.shared.latency import get_analysis_window, create_trial_selection
 
 # Local imports
 from syncopy.statistics.compRoutines import PSTH
 from syncopy.statistics.psth import Rice_rule, sqrt_rule, get_chan_unit_combs
 
-available_binsizes = {'rice': Rice_rule, 'sqrt': sqrt_rule}
-available_outputs = ['rate', 'spikecount', 'proportion']
+available_binsizes = {"rice": Rice_rule, "sqrt": sqrt_rule}
+available_outputs = ["rate", "spikecount", "proportion"]
 
 
 @unwrap_cfg
 @unwrap_select
 @detect_parallel_client
-def spike_psth(data,
-               binsize='rice',
-               output='rate',
-               latency='maxperiod',
-               vartriallen=True,
-               keeptrials=True,
-               **kwargs):
+def spike_psth(
+    data,
+    binsize="rice",
+    output="rate",
+    latency="maxperiod",
+    vartriallen=True,
+    keeptrials=True,
+    **kwargs,
+):
 
     """
     Peristimulus time histogram
 
     Parameters
     ----------
     data : :class:`~syncopy.SpikeData`
@@ -99,22 +101,26 @@
 
     >>> spy.spike_psth(spd, binsize=0.05, latency=[0.1, 0.5], vartriallen=False)
     """
 
     # Make sure our one mandatory input object can be processed
     try:
         data_parser(
-            data, varname="data", dataclass="SpikeData",
-            writable=None, empty=False, dimord=['sample', 'channel', 'unit']
+            data,
+            varname="data",
+            dataclass="SpikeData",
+            writable=None,
+            empty=False,
+            dimord=["sample", "channel", "unit"],
         )
     except Exception as exc:
         raise exc
 
     if not isinstance(vartriallen, bool):
-        raise SPYTypeError(vartriallen, varname='vartriallen', expected='Bool')
+        raise SPYTypeError(vartriallen, varname="vartriallen", expected="Bool")
 
     defaults = get_defaults(spike_psth)
     lcls = locals()
     # check for ineffective additional kwargs
     check_passed_kwargs(lcls, defaults, frontend_name="spike_psth")
     # save frontend call in cfg
     new_cfg = get_frontend_cfg(defaults, lcls, kwargs)
@@ -131,21 +137,21 @@
         trials = data.trials
         trl_starts, trl_ends = data.trialintervals[:, 0], data.trialintervals[:, 1]
 
     # validate output parameter
     if output not in available_outputs:
         lgl = f"one of {available_outputs}"
         act = output
-        raise SPYValueError(lgl, 'output', act)
+        raise SPYValueError(lgl, "output", act)
 
     if isinstance(binsize, str):
         if binsize not in available_binsizes:
             lgl = f"one of {available_binsizes}"
             act = output
-            raise SPYValueError(lgl, 'output', act)
+            raise SPYValueError(lgl, "output", act)
 
     # --- parse and digest `latency` (time window of analysis) ---
 
     window = get_analysis_window(data, latency)
 
     # to restore later
     select_backup = None if data.selection is None else deepcopy(data.selection.select)
@@ -174,78 +180,79 @@
     av_trl_size = np.diff(sinfo).sum() / len(trials)
 
     if binsize in available_binsizes:
         nBins = available_binsizes[binsize](av_trl_size)
         bins = np.linspace(*window, nBins)
     else:
         # make sure we have at least 2 bins
-        scalar_parser(binsize, varname='binsize', lims=[0, np.diff(window).squeeze()])
+        scalar_parser(binsize, varname="binsize", lims=[0, np.diff(window).squeeze()])
         # include rightmost bin edge
         bins = np.arange(window[0], window[1] + binsize, binsize)
         nBins = len(bins)
 
     # it's a sequential loop to get an array of [chan, unit] indices
     combs = get_chan_unit_combs(trials)
 
     # --- populate the log
 
-    log_dict = {'bins': bins,
-                'binsize': binsize,
-                'latency': latency,
-                'output': output,
-                'vartriallen': vartriallen,
-                'numDiscard': numDiscard
-                }
+    log_dict = {
+        "bins": bins,
+        "binsize": binsize,
+        "latency": latency,
+        "output": output,
+        "vartriallen": vartriallen,
+        "numDiscard": numDiscard,
+    }
 
     # --- set up CR ---
 
     # trl_start` and `onset` for distributing positional args to psth_cF
     trl_starts = trl_def[:, 0]
     trl_ends = trl_def[:, 1]
     trigger_onsets = trl_def[:, 2]
-    psth_cR = PSTH(trl_starts,
-                   trigger_onsets,
-                   trl_ends,
-                   chan_unit_combs=combs,
-                   tbins=bins,
-                   output=output,
-                   samplerate=data.samplerate
-                   )
+    psth_cR = PSTH(
+        trl_starts,
+        trigger_onsets,
+        trl_ends,
+        chan_unit_combs=combs,
+        tbins=bins,
+        output=output,
+        samplerate=data.samplerate,
+    )
 
     # only available dimord labels ['time', 'channel'])
     psth_results = TimeLockData()
-    psth_cR.initialize(data, chan_per_worker=None,
-                       out_stackingdim=psth_results._stackingDim,
-                       keeptrials=keeptrials)
-
-    psth_cR.compute(data,
-                    psth_results,
-                    parallel=kwargs.get("parallel"),
-                    log_dict=log_dict)
+    psth_cR.initialize(
+        data,
+        chan_per_worker=None,
+        out_stackingdim=psth_results._stackingDim,
+        keeptrials=keeptrials,
+    )
 
+    psth_cR.compute(data, psth_results, parallel=kwargs.get("parallel"), log_dict=log_dict)
 
     # calculate trial average and variance
-    avg = spy.mean(psth_results, dim='trials', parallel=False)
-    var = spy.var(psth_results, dim='trials', parallel=False)
+    avg = spy.mean(psth_results, dim="trials", parallel=False)
+    var = spy.var(psth_results, dim="trials", parallel=False)
 
     # attach data to TimeLockData
-    psth_results._update_dataset('avg', avg.data)
-    psth_results._update_dataset('var', var.data)
+    psth_results._update_dataset("avg", avg.data)
+    psth_results._update_dataset("var", var.data)
 
     # unregister datasets to detach from objects
     avg._unregister_dataset("data", del_from_file=False)
     var._unregister_dataset("data", del_from_file=False)
 
     # scramble filenames and delete unneeded objects
-    avg.filename, var.filename = '', ''
+    avg.filename, var.filename = "", ""
     del avg, var
 
     # -- propagate old cfg and attach this one --
     psth_results.cfg.update(data.cfg)
-    psth_results.cfg.update({'spike_psth': new_cfg})
+    psth_results.cfg.update({"spike_psth": new_cfg})
 
     # finally revert possible in-place selections
     if select_backup is None:
         data.selection = None
     else:
         data.selectdata(select_backup, inplace=True)
```

### Comparing `esi_syncopy-2023.5/syncopy/statistics/summary_stats.py` & `esi_syncopy-2023.7/syncopy/statistics/summary_stats.py`

 * *Files 3% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # Local imports
 # from .selectdata import _get_selection_size
 from syncopy.shared.parsers import data_parser
 from syncopy.shared.errors import SPYValueError, SPYWarning
 from syncopy.statistics.compRoutines import NumpyStatDim
 from syncopy.shared.kwarg_decorators import unwrap_select, detect_parallel_client
 
-__all__ = ['mean', 'std', 'var', 'median', 'itc']
+__all__ = ["mean", "std", "var", "median", "itc"]
 
 
 @unwrap_select
 @detect_parallel_client
 def mean(spy_data, dim, keeptrials=True, **kwargs):
     """
     Calculates the average along arbitrary dimensions of a Syncopy
@@ -46,19 +46,15 @@
     -------
     res : Syncopy data object
         New object with the desired dimension averaged out
 
     """
 
     # call general backend function with desired operation
-    return _statistics(spy_data,
-                       operation='mean',
-                       dim=dim,
-                       keeptrials=keeptrials,
-                       **kwargs)
+    return _statistics(spy_data, operation="mean", dim=dim, keeptrials=keeptrials, **kwargs)
 
 
 @unwrap_select
 @detect_parallel_client
 def std(spy_data, dim, keeptrials=True, **kwargs):
     """
     Calculates the standard deviation along arbitrary dimensions
@@ -83,19 +79,15 @@
     -------
     res : Syncopy data object
         New object with the desired dimension averaged out
 
     """
 
     # call general backend function with desired operation
-    return _statistics(spy_data,
-                       operation='std',
-                       dim=dim,
-                       keeptrials=keeptrials,
-                       **kwargs)
+    return _statistics(spy_data, operation="std", dim=dim, keeptrials=keeptrials, **kwargs)
 
 
 @unwrap_select
 @detect_parallel_client
 def var(spy_data, dim, keeptrials=True, **kwargs):
     """
     Calculates the variance along arbitrary dimensions of a Syncopy
@@ -120,19 +112,15 @@
     -------
     res : Syncopy data object
         New object with the desired dimension averaged out
 
     """
 
     # call general backend function with desired operation
-    return _statistics(spy_data,
-                       operation='var',
-                       dim=dim,
-                       keeptrials=keeptrials,
-                       **kwargs)
+    return _statistics(spy_data, operation="var", dim=dim, keeptrials=keeptrials, **kwargs)
 
 
 @unwrap_select
 @detect_parallel_client
 def median(spy_data, dim, keeptrials=True, **kwargs):
     """
     Calculates the median along arbitrary dimensions of a
@@ -157,19 +145,16 @@
     -------
     res : Syncopy data object
         New object with the median of the desired dimension
 
     """
 
     # call general backend function with desired operation
-    return _statistics(spy_data,
-                       operation='median',
-                       dim=dim,
-                       keeptrials=keeptrials,
-                       **kwargs)
+    return _statistics(spy_data, operation="median", dim=dim, keeptrials=keeptrials, **kwargs)
+
 
 @unwrap_select
 def itc(spec_data, **kwargs):
     r"""
     Calculates the inter trial coherence for a
     SpectralData `spec_data` object, the input
     spectrum needs to be complex.
@@ -195,30 +180,29 @@
     Returns
     -------
     res  : :class:`~syncopy.SpectralData`
         The frequency dependent order parameters,
         the inter trial coherence
     """
 
-    data_parser(spec_data,
-                varname='spec_data',
-                dataclass='SpectralData',
-                empty=False)
+    data_parser(spec_data, varname="spec_data", dataclass="SpectralData", empty=False)
 
     if spec_data.data.dtype != np.complex64 and spec_data.data.dtype != np.complex128:
         lgl = "complex valued spectra, set `output='fourier` in spy.freqanalysis!"
         act = "real valued spectral data"
-        raise SPYValueError(lgl, 'spec_data', act)
+        raise SPYValueError(lgl, "spec_data", act)
 
     logger = logging.getLogger("syncopy_" + platform.node())
-    logger.debug(f"Computing intertrial coherence on SpectralData instance with shape {spec_data.data.shape}.")
+    logger.debug(
+        f"Computing intertrial coherence on SpectralData instance with shape {spec_data.data.shape}."
+    )
 
     # takes care of remaining checks
-    res = _trial_statistics(spec_data, operation='itc')
-    write_log(spec_data, res, kwargs, op_name='itc')
+    res = _trial_statistics(spec_data, operation="itc")
+    write_log(spec_data, res, kwargs, op_name="itc")
     # attach cfg
     res.cfg.update(spec_data.cfg)
     return res
 
 
 def _statistics(spy_data, operation, dim, keeptrials=True, **kwargs):
 
@@ -252,85 +236,93 @@
     See also
     --------
     NumpyStatDim: Compute class for parallel computation of trial-by-trial statistics
     _trial_statistics: Sequential computation of statistics over trials
     """
 
     # check that we have a non-empty Syncopy data object
-    data_parser(spy_data, varname='spy_data', empty=False)
+    data_parser(spy_data, varname="spy_data", empty=False)
 
-    if dim != 'trials' and dim not in spy_data.dimord:
+    if dim != "trials" and dim not in spy_data.dimord:
         lgl = f"one of {spy_data.dimord} or 'trials'"
         act = dim
-        raise SPYValueError(lgl, 'dim', act)
+        raise SPYValueError(lgl, "dim", act)
 
-    log_dict = {'input': spy_data.filename,
-                'operation': operation,
-                'dim': dim,
-                'keeptrials': keeptrials}
+    log_dict = {
+        "input": spy_data.filename,
+        "operation": operation,
+        "dim": dim,
+        "keeptrials": keeptrials,
+    }
 
     logger = logging.getLogger("syncopy_" + platform.node())
-    logger.debug(f"Computing descriptive statistic {operation} on input from {spy_data.filename} along dimension {dim}, keeptrials={keeptrials}.")
+    logger.debug(
+        f"Computing descriptive statistic {operation} on input from {spy_data.filename} along dimension {dim}, keeptrials={keeptrials}."
+    )
 
     # If no active selection is present, create a "fake" all-to-all selection
     # to harmonize processing down the road (and attach `_cleanup` attribute for later removal)
     if spy_data.selection is None:
         spy_data.selectdata(inplace=True)
         spy_data.selection._cleanup = True
     else:
         # log also possible selections
-        log_dict['selection'] = getattr(spy_data.selection, dim)
+        log_dict["selection"] = getattr(spy_data.selection, dim)
 
     # trial statistics
-    if dim == 'trials':
-        if kwargs.get('parallel'):
+    if dim == "trials":
+        if kwargs.get("parallel"):
             msg = "Trial statistics can be only computed sequentially, ignoring `parallel` keyword"
             SPYWarning(msg)
         out = _trial_statistics(spy_data, operation)
 
         # we have to attach the log here as no CR is involved
         # strip of non-sensical parameter
-        log_dict.pop('keeptrials')
-        write_log(spy_data, out, log_dict, 'trial statistics')
+        log_dict.pop("keeptrials")
+        write_log(spy_data, out, log_dict, "trial statistics")
 
     # any other statistic
     else:
 
-        chan_per_worker = kwargs.get('chan_per_worker')
-        if chan_per_worker is not None and 'channel' in dim:
+        chan_per_worker = kwargs.get("chan_per_worker")
+        if chan_per_worker is not None and "channel" in dim:
             msg = "Parallelization over channels not possible for channel averages"
             SPYWarning(msg)
             chan_per_worker = None
 
         axis = spy_data.dimord.index(dim)
         avCR = NumpyStatDim(operation=operation, axis=axis)
 
         # ---------------------------------
         # Initialize output and call the CR
         # ---------------------------------
 
         # initialize output object of same datatype
         out = spy_data.__class__(dimord=spy_data.dimord)
 
-        avCR.initialize(spy_data, spy_data._stackingDim,
-                        keeptrials=keeptrials, chan_per_worker=chan_per_worker)
+        avCR.initialize(
+            spy_data,
+            spy_data._stackingDim,
+            keeptrials=keeptrials,
+            chan_per_worker=chan_per_worker,
+        )
         avCR.compute(spy_data, out, parallel=kwargs.get("parallel"), log_dict=log_dict)
 
     # revert helper all-to-all selection
-    if hasattr(spy_data.selection, '_cleanup'):
-        spy_data.cfg.pop('selectdata')
+    if hasattr(spy_data.selection, "_cleanup"):
+        spy_data.cfg.pop("selectdata")
         spy_data.selection = None
 
     # re-attach config
     out.cfg.update(spy_data.cfg)
 
     return out
 
 
-def _trial_statistics(in_data, operation='mean'):
+def _trial_statistics(in_data, operation="mean"):
     """
     Calculates simple statistics (mean, std, ...) over trials. No trivial
     parallelization is possible here, hence we fallback to good ol' sequential
     computing. For this to work, the shapes of all trials have to match exactly.
 
     To be still memory safe, the computations stream new data on a trial-by-trial
     basis and then 'manually' accumulate trial-by-trial to the result.
@@ -342,78 +334,76 @@
         in_data.selectdata(inplace=True)
         in_data.selection._cleanup = True
 
     nTrials = len(in_data.selection.trials)
     if nTrials < 1:
         lgl = "at least 1 trial"
         act = f"got {nTrials} trials"
-        raise SPYValueError(lgl, 'in_data', act)
+        raise SPYValueError(lgl, "in_data", act)
 
     # index 1st selected trial
     idx0 = in_data.selection.trial_ids[0]
     # we always have at least one (all-to-all) trial selection
     out_shape = in_data.selection.trials[idx0].shape
 
     # now look at the other ones
     for trl in in_data.selection.trials:
         if trl.shape != out_shape:
             lgl = "all trials to have the same shape"
             act = f"found trials of different shape: {out_shape} and {trl.shape}"
-            raise SPYValueError(lgl, 'in_data', act)
+            raise SPYValueError(lgl, "in_data", act)
 
     # this is the target array, such that we will have
     # the data of 2 trials concurrently in memory
     result = np.zeros(out_shape, dtype=in_data.data.dtype.type)
 
     # --- now we can compute the desired statistic ---
 
-    if operation == 'mean':
+    if operation == "mean":
         result = _trial_average(in_data, result)
-    elif operation == 'var':
+    elif operation == "var":
         result = _trial_var(in_data, result)
-    elif operation == 'std':
+    elif operation == "std":
         result = np.sqrt(_trial_var(in_data, result))
-    elif operation == 'itc':
+    elif operation == "itc":
         # itc is only available for SpectralData
         # ..get's checked in `spy.itc` above
         result = _trial_circ_average(in_data, result)
         # average out tapers (if any)
-        taper_ax = in_data.dimord.index('taper')
+        taper_ax = in_data.dimord.index("taper")
         result = np.mean(result, axis=taper_ax, keepdims=True)
         # now take the abs to get the resultant vector
         result = np.abs(result)
         # silence already digested taper selection
         in_data.selection._taper = None
 
     # there is no apparent clever way to achieve
     # this efficiently over multiple dimensions
-    elif operation == 'median':
+    elif operation == "median":
         raise NotImplementedError("Trial median not supported at the moment")
 
     # --- Consctruct the single-trial(!) Syncopy output object
 
-    out_data = in_data.__class__(data=result,
-                                 dimord=in_data.dimord,
-                                 samplerate=in_data.samplerate)
+    out_data = in_data.__class__(data=result, dimord=in_data.dimord, samplerate=in_data.samplerate)
 
     # only 1 trial left, all trials had to have the same shape
     # so just copy from the 1st
     out_data.trialdefinition = in_data.selection.trialdefinition[0, :][None, :]
 
     # propagate the rest of the properties
     for prop in in_data.selection._dimProps:
         selection = getattr(in_data.selection, prop)
         if selection is not None:
             if np.issubdtype(type(selection), np.number):
                 selection = [selection]
             setattr(out_data, prop, getattr(in_data, prop)[selection])
 
     # revert helper all-to-all selection
-    if hasattr(in_data.selection, '_cleanup'):
-        in_data.cfg.pop('selectdata')
+    if hasattr(in_data.selection, "_cleanup"):
+        in_data.cfg.pop("selectdata")
         in_data.selection = None
 
     return out_data
 
 
 def _trial_average(in_data, out_arr):
     """
@@ -454,15 +444,15 @@
     # first we need the trial average
     average = np.zeros(out_arr.shape, dtype=out_arr.dtype)
     average = _trial_average(in_data, average)
 
     trials = in_data.selection.trials
     for trl in trials:
         # absolute value for complex numbers
-        out_arr += np.abs(trl - average)**2
+        out_arr += np.abs(trl - average) ** 2
 
     # normalize
     out_arr /= len(trials)
 
     return out_arr
 
 
@@ -502,17 +492,18 @@
     take care of the log here
     """
     # Copy log from source object and write header
     out._log = str(data._log) + out._log
     logHead = f"computed {op_name} with settings\n"
     logOpts = ""
     for k, v in log_dict.items():
-        logOpts += "\t{key:s} = {value:s}\n".format(key=k,
-                                                    value=str(v) if len(str(v)) < 80
-                                                    else str(v)[:30] + ", ..., " + str(v)[-30:])
+        logOpts += "\t{key:s} = {value:s}\n".format(
+            key=k,
+            value=str(v) if len(str(v)) < 80 else str(v)[:30] + ", ..., " + str(v)[-30:],
+        )
     out.log = logHead + logOpts
 
 
 def _attach_stat_doc(orig_doc):
     """
     NOT USED ATM - could be useful for other method doc strings
 
@@ -524,16 +515,16 @@
     dimension parameter to be named ``dim``
     """
 
     # the wrapper
     def _attach_doc(func):
         # delete the `spy_data` entries which
         # are not needed (got self in the methods)
-        doc = orig_doc.replace(' ``spy_data``', '')
-        idx1 = doc.find('spy_data')
-        idx2 = doc.find('dim', idx1)
+        doc = orig_doc.replace(" ``spy_data``", "")
+        idx1 = doc.find("spy_data")
+        idx2 = doc.find("dim", idx1)
         doc = doc[:idx1] + doc[idx2:]
 
         func.__doc__ = doc
         return func
 
     return _attach_doc
```

### Comparing `esi_syncopy-2023.5/syncopy/statistics/timelockanalysis.py` & `esi_syncopy-2023.7/syncopy/statistics/timelockanalysis.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,40 +8,39 @@
 import h5py
 from copy import deepcopy
 
 # Syncopy imports
 
 import syncopy as spy
 from syncopy.shared.parsers import data_parser
-from syncopy.shared.kwarg_decorators import (unwrap_cfg, unwrap_select,
-                                             detect_parallel_client)
+from syncopy.shared.kwarg_decorators import (
+    unwrap_cfg,
+    unwrap_select,
+    detect_parallel_client,
+)
 from syncopy.shared.input_processors import (
     check_effective_parameters,
-    check_passed_kwargs
+    check_passed_kwargs,
 )
 from syncopy.shared.tools import get_defaults, get_frontend_cfg
 from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYInfo, SPYWarning
 from syncopy.shared.latency import get_analysis_window, create_trial_selection
 
 # local imports
 from syncopy.statistics.compRoutines import Covariance
 
 __all__ = ["timelockanalysis"]
 
 
 @unwrap_cfg
 @unwrap_select
 @detect_parallel_client
-def timelockanalysis(data,
-                     latency='maxperiod',
-                     covariance=False,
-                     ddof=None,
-                     trials='all',
-                     keeptrials=False,
-                     **kwargs):
+def timelockanalysis(
+    data, latency="maxperiod", covariance=False, ddof=None, trials="all", keeptrials=False, **kwargs
+):
     """
     Average, variance and covariance for :class:`~syncopy.AnalogData` objects across trials.
 
     If input ``data`` is not timelocked already, trial cutting and selections will be
     applied according to the ``latency`` setting.
 
     Parameters
@@ -71,69 +70,69 @@
         Time locked data object, with additional datasets:
         "avg", "var" and "cov" if ``convariance`` was set to ``True``
 
     """
 
     # -- check user input --
 
-    data_parser(data, varname="data", empty=False,
-                dataclass="AnalogData")
+    data_parser(data, varname="data", empty=False, dataclass="AnalogData")
 
     if ddof is not None:
         if not isinstance(ddof, int) or ddof < 0:
             lgl = "positive integer value"
             act = ddof
-            raise SPYValueError(lgl, 'ddof', act)
+            raise SPYValueError(lgl, "ddof", act)
 
     if not isinstance(covariance, bool):
-        raise SPYTypeError(covariance, varname='covariance', expected='bool')
+        raise SPYTypeError(covariance, varname="covariance", expected="bool")
 
     if not isinstance(keeptrials, bool):
-        raise SPYTypeError(covariance, varname='keeptrials', expected='bool')
+        raise SPYTypeError(covariance, varname="keeptrials", expected="bool")
 
     # latency gets checked within selectdata(latency=...)
 
     # -- standard block to check and store provided kwargs/cfg --
 
     defaults = get_defaults(timelockanalysis)
     lcls = locals()
     # check for ineffective additional kwargs
     check_passed_kwargs(lcls, defaults, frontend_name="timelockanalysis")
     # save frontend call in cfg
     new_cfg = get_frontend_cfg(defaults, lcls, kwargs)
 
-    log_dict = {'latency': latency,
-                'covariance': covariance,
-                'ddof': ddof,
-                'trials': trials
-                }
+    log_dict = {
+        "latency": latency,
+        "covariance": covariance,
+        "ddof": ddof,
+        "trials": trials,
+    }
 
     # -- create outtput object --
 
     # start empty, data gets added later
     tld = spy.TimeLockData(samplerate=data.samplerate)
 
     # -- propagate old cfg and attach this one --
     tld.cfg.update(data.cfg)
-    tld.cfg.update({'timelockanalysis': new_cfg})
+    tld.cfg.update({"timelockanalysis": new_cfg})
 
     # to restore later as we apply selection inside here
     select_backup = None if data.selection is None else deepcopy(data.selection.select)
 
     if data.selection is not None:
-        if trials != 'all' and data.selection.select.get('trials') is not None:
+        if trials != "all" and data.selection.select.get("trials") is not None:
             lgl = "either `trials != 'all'` or selection"
             act = "trial keyword and trial selection"
-            raise SPYValueError(lgl, 'trials', act)
+            raise SPYValueError(lgl, "trials", act)
         # evaluate legacy `trials` keyword value as selection
-        elif trials != 'all':
+        elif trials != "all":
             select = data.selection.select
-            select['trials'] = trials
+            select["trials"] = trials
             data.selectdata(select, inplace=True)
-    elif trials != 'all':
+    elif trials != "all":
         # error handling done here
         data.selectdata(trials=trials, inplace=True)
 
     # digest selections
     if data.selection is not None:
         # select trials either via selection of keyword:
         trl_def = data.selection.trialdefinition
@@ -156,67 +155,65 @@
         data.selectdata(data.selection.select, latency=latency, inplace=True)
     else:
         # create new selection
         data.selectdata(latency=latency, inplace=True)
 
     # stream copy cut/selected trials/time window into new dataset
     # by exploiting the in place selection
-    dset = _dataset_from_trials(data,
-                                dset_name='data',
-                                filename=tld._gen_filename())
+    dset = _dataset_from_trials(data, dset_name="data", filename=tld._gen_filename())
 
     # no copy here
     tld.data = dset
     tld.trialdefinition = data.selection.trialdefinition
 
     # now calculate via standard statistics
-    avg = spy.mean(tld, dim='trials', parallel=False)
-    var = spy.var(tld, dim='trials', parallel=False)
+    avg = spy.mean(tld, dim="trials", parallel=False)
+    var = spy.var(tld, dim="trials", parallel=False)
 
     # attach data to TimeLockData
-    tld._update_dataset('avg', avg.data)
-    tld._update_dataset('var', var.data)
+    tld._update_dataset("avg", avg.data)
+    tld._update_dataset("var", var.data)
 
     # explicitly delete unneeded objects but keep the data
     avg._persistent_hdf5, var._persistent_hdf5 = True, True
     del avg, var
 
     # -- set up covariance CR --
 
     if covariance:
-        check_effective_parameters(Covariance, defaults, lcls, besides=['covariance', 'trials', 'latency'])
-        covCR = Covariance(ddof=ddof, statAxis=data.dimord.index('time'))
+        check_effective_parameters(Covariance, defaults, lcls, besides=["covariance", "trials", "latency"])
+        covCR = Covariance(ddof=ddof, statAxis=data.dimord.index("time"))
         # dimord is time x freq x channel x channel
         out = spy.CrossSpectralData(dimord=spy.CrossSpectralData._defaultDimord)
 
         covCR.initialize(
             data,
             out._stackingDim,
             keeptrials=keeptrials,
         )
         # and compute
         covCR.compute(data, out, parallel=kwargs.get("parallel"), log_dict=log_dict)
 
         # attach computed cov as array
-        tld._update_dataset('cov', out.data[:, 0, ...].squeeze())
+        tld._update_dataset("cov", out.data[:, 0, ...].squeeze())
 
     # -- restore initial selection or wipe --
 
     if select_backup:
         # this rewrites the cfg
         data.selectdata(select_backup, inplace=True)
     else:
         data.selection = None
         # erase local selection entry
-        data.cfg.pop('selectdata')
+        data.cfg.pop("selectdata")
 
     return tld
 
 
-def _dataset_from_trials(spy_data, dset_name='new_data', filename=None):
+def _dataset_from_trials(spy_data, dset_name="new_data", filename=None):
     """
     Helper to construct a new dataset from
     a TrialIndexer, respecting selections
 
     This function is only needed if a dataset is to be transferred
     between two different Syncopy data classes, for the
     same data class a standard ``new = old.selectdata(..., inplace=False)``
@@ -242,15 +239,15 @@
     new_shape[stackDim] = stackingDimSize
 
     if filename is None:
         # generates new name with same extension
         filename = spy_data._gen_filename()
 
     # create new hdf5 File and dataset
-    with h5py.File(filename, mode='w') as h5f:
+    with h5py.File(filename, mode="w") as h5f:
         new_ds = h5f.create_dataset(dset_name, shape=new_shape)
 
         # all-to-all indexer
         idx = [slice(None) for _ in range(len(new_shape))]
         # stacking dim chunk size counter
         stacking = 0
         # now stream the trials into the new dataset
@@ -260,8 +257,8 @@
             # define the chunk and increment stacking dim indexer
             idx[stackDim] = slice(stacking, stacking + trl_len)
             stacking += trl_len
             # insert the trial
             new_ds[tuple(idx)] = trl
 
     # open again for reading and return dataset directly
-    return h5py.File(filename, mode='r+')[dset_name]
+    return h5py.File(filename, mode="r+")[dset_name]
```

### Comparing `esi_syncopy-2023.5/syncopy/synthdata/analog.py` & `esi_syncopy-2023.7/syncopy/synthdata/analog.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 # -*- coding: utf-8 -*-
 #
 # Synthetic analog data generators for testing and tutorials
 #
 
 # Builtin/3rd party package imports
 import numpy as np
+
 # syncopy imports
 from .utils import collect_trials
 
 
 _2pi = np.pi * 2
 
 
@@ -31,15 +32,15 @@
 
     Returns
     --------
     wn : :class:`syncopy.AnalogData` or numpy.ndarray
     """
 
     rng = np.random.default_rng(seed)
-    signal = rng.normal(size=(nSamples, nChannels))
+    signal = rng.normal(size=(nSamples, nChannels)).astype("f4")
     return signal
 
 
 @collect_trials
 def linear_trend(y_max, nSamples=1000, nChannels=2):
     """
     A linear trend  on all channels from 0 to `y_max` in `nSamples`.
@@ -54,15 +55,15 @@
     nChannels : int
         Number of channels
 
     Returns
     --------
     trend : :class:`syncopy.AnalogData` or numpy.ndarray
     """
-    trend = np.linspace(0, y_max, nSamples)
+    trend = np.linspace(0, y_max, nSamples, dtype="f4")
     return np.column_stack([trend for _ in range(nChannels)])
 
 
 @collect_trials
 def harmonic(freq, samplerate, nSamples=1000, nChannels=2):
     """
     A harmonic with frequency `freq`.
@@ -82,38 +83,39 @@
     --------
     harm : :class:`syncopy.AnalogData` or numpy.ndarray
 
     """
     # the sampling times vector needed for construction
     tvec = np.arange(nSamples) * 1 / samplerate
     # the  harmonic
-    harm = np.cos(2 * np.pi * freq * tvec)
+    harm = np.cos(2 * np.pi * freq * tvec, dtype="f4")
     return np.column_stack([harm for _ in range(nChannels)])
 
 
 # noisy phase evolution <-> phase diffusion
 @collect_trials
-def phase_diffusion(freq,
-                    eps=.1,
-                    samplerate=1000,
-                    nChannels=2,
-                    nSamples=1000,
-                    rand_ini=False,
-                    return_phase=False,
-                    seed=None):
+def phase_diffusion(
+    freq,
+    eps=0.1,
+    samplerate=1000,
+    nChannels=2,
+    nSamples=1000,
+    rand_ini=False,
+    return_phase=False,
+    seed=None,
+):
 
-    """
+    r"""
     Linear (harmonic) phase evolution plus a Brownian noise term
     inducing phase diffusion around the deterministic phase velocity (angular frequency).
 
     The linear phase increments are given by
 
     .. math::
-
-        \Delta \phi = 2\pi  \frac{freq}{samplerate}.
+        \Delta \phi = 2\pi \frac{freq}{samplerate}
 
     The Brownian increments are scaled with `eps` relative to these
     phase increments, meaning the relative phase diffusion is frequency
     independent.
 
     Parameters
     ----------
@@ -138,27 +140,38 @@
           Set to an `int` to get reproducible results, or `None` for random ones.
 
     Returns
     -------
     phases : :class:`syncopy.AnalogData` or numpy.ndarray
         Synthetic `nSamples` x `nChannels` data array simulating noisy phase
         evolution/diffusion
+
+    Examples
+    --------
+    Weak phase diffusion around the 60Hz harmonic:
+
+    >>> signals = spy.synthdata.phase_diffusion(freq=60, eps=0.01)
+
+    Return the unwrapped phase directly:
+
+    >>> phases = spy.synthdata.phase_diffusion(freq=60, eps=0.01, return_phase=True)
+
     """
 
     # white noise
     wn = white_noise(nSamples=nSamples, nChannels=nChannels, seed=seed, nTrials=None)
 
-    tvec = np.linspace(0, nSamples / samplerate, nSamples)
+    tvec = np.linspace(0, nSamples / samplerate, nSamples, dtype="f4")
     omega0 = 2 * np.pi * freq
     lin_phase = np.tile(omega0 * tvec, (nChannels, 1)).T
 
     # randomize initial phase
     if rand_ini:
         rng = np.random.default_rng(seed)
-        ps0 = 2 * np.pi * rng.uniform(size=nChannels)
+        ps0 = 2 * np.pi * rng.uniform(size=nChannels).astype("f4")
         lin_phase += ps0
 
     # relative Brownian increments
     rel_eps = np.sqrt(omega0 / samplerate * eps)
     brown_incr = rel_eps * wn
 
     # combine harmonic and diffusive dyncamics
@@ -213,15 +226,15 @@
         solution of the network dynamics
     """
 
     # default system layout as in Dhamala 2008:
     # unidirectional (2->1) coupling
     if AdjMat is None:
         AdjMat = np.zeros((2, 2), dtype=np.float32)
-        AdjMat[1, 0] = .25
+        AdjMat[1, 0] = 0.25
     else:
         # cast to our standard type
         AdjMat = AdjMat.astype(np.float32)
 
     nChannels = AdjMat.shape[0]
     alpha1, alpha2 = alphas
     # diagonal 'self-interaction' with lag 1
@@ -264,18 +277,15 @@
     """
 
     # configure AR2 network to arrive at the uncoupled
     # AR1 processes
     alphas = [alpha, 0]
     AdjMat = np.diag(np.zeros(nChannels))
 
-    signal = ar2_network(AdjMat=AdjMat,
-                         nSamples=nSamples,
-                         alphas=alphas,
-                         seed=seed, nTrials=None)
+    signal = ar2_network(AdjMat=AdjMat, nSamples=nSamples, alphas=alphas, seed=seed, nTrials=None)
 
     return signal
 
 
 def ar2_peak_freq(a1, a2, samplerate=1):
     """
     Helper function to tune spectral peak of AR(2) process
```

### Comparing `esi_syncopy-2023.5/syncopy/synthdata/spikes.py` & `esi_syncopy-2023.7/syncopy/synthdata/spikes.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,28 +1,32 @@
 # -*- coding: utf-8 -*-
 #
 # Synthetic spike data generators for testing and tutorials
 #
 
 # Builtin/3rd party package imports
 import numpy as np
+
 # syncopy imports
 from syncopy import SpikeData
-
+from syncopy.shared.kwarg_decorators import unwrap_cfg
 
 # ---- Synthetic SpikeData ----
 
 
-def poisson_noise(nTrials=10,
-                  nSpikes=10000,
-                  nChannels=3,
-                  nUnits=10,
-                  intensity=.1,
-                  samplerate=10000,
-                  seed=None):
+@unwrap_cfg
+def poisson_noise(
+    nTrials=10,
+    nSpikes=10000,
+    nChannels=3,
+    nUnits=10,
+    intensity=0.1,
+    samplerate=10000,
+    seed=None,
+):
 
     """
     Poisson (Shot-)noise generator
 
     The expected trial length in samples is given by:
 
         ``nSpikes`` / (``intensity`` * ``nTrials``)
@@ -90,18 +94,15 @@
         return pvec / pvec.sum()
 
     # total length of all trials combined
     rng = np.random.default_rng(seed)
     T_max = int(nSpikes / intensity)
 
     spike_samples = np.sort(rng.choice(range(T_max), size=nSpikes, replace=False))
-    channels = rng.choice(
-        np.arange(nChannels), p=get_rdm_weights(nChannels),
-        size=nSpikes, replace=True
-    )
+    channels = rng.choice(np.arange(nChannels), p=get_rdm_weights(nChannels), size=nSpikes, replace=True)
 
     uvec = np.arange(nUnits)
     pvec = get_rdm_weights(nUnits)
     units = rng.choice(uvec, p=pvec, size=nSpikes, replace=True)
 
     # originally fixed trial size
     step = T_max // nTrials
@@ -112,15 +113,17 @@
     idx_end = trl_intervals[1:] - 1
 
     # now randomize trial length a bit, max 10% size difference
     idx_end = idx_end - np.r_[rng.integers(step // 10, size=nTrials - 1), 0]
 
     shortest_trial = np.min(idx_end - idx_start)
     idx_offset = -rng.choice(
-        np.arange(0.05 * shortest_trial, 0.2 * shortest_trial, dtype=int), size=nTrials, replace=True
+        np.arange(0.05 * shortest_trial, 0.2 * shortest_trial, dtype=int),
+        size=nTrials,
+        replace=True,
     )
 
     trldef = np.vstack([idx_start, idx_end, idx_offset]).T
     data = np.vstack([spike_samples, channels, units]).T
     sdata = SpikeData(
         data=data,
         trialdefinition=trldef,
```

### Comparing `esi_syncopy-2023.5/syncopy/synthdata/utils.py` & `esi_syncopy-2023.7/syncopy/synthdata/utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 import functools
 
 from syncopy import AnalogData
 from syncopy.shared.parsers import scalar_parser
 from syncopy.shared.kwarg_decorators import (
     unwrap_cfg,
     _append_docstring,
-    _append_signature
+    _append_signature,
 )
 
 
 def collect_trials(trial_func):
     """
     Decorator to wrap around a single trial (nSamples x nChannels shaped np.ndarray)
     synthetic data function. Creates a generator expression to arrive
@@ -51,48 +51,49 @@
         seed_array = None  # One seed per trial.
         # Use the single seed to create one seed per trial.
         if nTrials is not None and seed is not None and seed_per_trial:
             rng = np.random.default_rng(seed)
             seed_array = rng.integers(1_000_000, size=nTrials)
 
         # append samplerate parameter if also needed by the generator
-        if 'samplerate' in signature(trial_func).parameters.keys():
-            tf_kwargs['samplerate'] = samplerate
+        if "samplerate" in signature(trial_func).parameters.keys():
+            tf_kwargs["samplerate"] = samplerate
 
         # bypass: directly return a single trial (may pass on the scalar seed if the function supports it)
         if nTrials is None:
-            if 'seed' in signature(trial_func).parameters.keys():
-                tf_kwargs['seed'] = seed
+            if "seed" in signature(trial_func).parameters.keys():
+                tf_kwargs["seed"] = seed
             return trial_func(**tf_kwargs)
 
         # collect trials
         else:
-            scalar_parser(nTrials, 'nTrials', ntype="int_like", lims=[1, np.inf])
+            scalar_parser(nTrials, "nTrials", ntype="int_like", lims=[1, np.inf])
 
             # create the trial generator
             def mk_trl_generator():
 
                 for trial_idx in range(nTrials):
-                    if 'seed' in signature(trial_func).parameters.keys():
+                    if "seed" in signature(trial_func).parameters.keys():
                         if seed_array is not None:
-                            tf_kwargs['seed'] = seed_array[trial_idx]
+                            tf_kwargs["seed"] = seed_array[trial_idx]
                         else:
-                            tf_kwargs['seed'] = seed
+                            tf_kwargs["seed"] = seed
                     yield trial_func(*args, **tf_kwargs)
 
             trl_generator = mk_trl_generator()
 
             data = AnalogData(trl_generator, samplerate=samplerate)
 
         return data
 
     # Append `nTrials` and `seed` keyword entry to wrapped function's docstring and signature
     nTrialsDocEntry = (
         "    nTrials : int or None\n"
         "        Number of trials for the returned :class:`~syncopy.AnalogData` object.\n"
         "        When set to `None` a single-trial :class:`~numpy.ndarray`\n"
-        "        is returned.")
+        "        is returned."
+    )
 
     wrapper_synth.__doc__ = _append_docstring(trial_func, nTrialsDocEntry)
     wrapper_synth.__signature__ = _append_signature(trial_func, "nTrials", kwdefault=100)
 
     return wrapper_synth
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/README.md` & `esi_syncopy-2023.7/syncopy/tests/README.md`

 * *Files identical despite different names*

### Comparing `esi_syncopy-2023.5/syncopy/tests/backend/run_tests.sh` & `esi_syncopy-2023.7/syncopy/tests/backend/run_tests.sh`

 * *Files identical despite different names*

### Comparing `esi_syncopy-2023.5/syncopy/tests/backend/test_conn.py` & `esi_syncopy-2023.7/syncopy/tests/backend/test_conn.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,19 +4,15 @@
 #
 import numpy as np
 import matplotlib.pyplot as ppl
 
 from syncopy import synthdata
 from syncopy.connectivity import csd
 from syncopy.connectivity import ST_compRoutines as stCR
-from syncopy.connectivity.wilson_sf import (
-    wilson_sf,
-    regularize_csd,
-    max_rel_err
-)
+from syncopy.connectivity.wilson_sf import wilson_sf, regularize_csd, max_rel_err
 from syncopy.connectivity.granger import granger
 
 
 def test_coherence():
 
     """
     Tests the csd normalization to
@@ -36,23 +32,20 @@
     nFreq = nSamples // 2 + 1
     nChannel = len(phase_shifts)
     avCSD = np.zeros((nFreq, nChannel, nChannel), dtype=np.complex64)
 
     for i in range(nTrials):
 
         # 1 phase phase shifted harmonics + white noise + constant, SNR = 1
-        trl_dat = [np.cos(harm_freq * 2 * np. pi * tvec + ps)
-                   for ps in phase_shifts]
+        trl_dat = [np.cos(harm_freq * 2 * np.pi * tvec + ps) for ps in phase_shifts]
         trl_dat = np.array(trl_dat).T
         trl_dat = np.array(trl_dat) + np.random.randn(nSamples, len(phase_shifts))
 
         # process every trial individually
-        CSD, freqs = csd.csd(trl_dat, fs,
-                             taper='hann',
-                             norm=False)  # this is important!
+        CSD, freqs = csd.csd(trl_dat, fs, taper="hann", norm=False)  # this is important!
 
         assert avCSD.shape == CSD.shape
         avCSD += CSD
 
     # this is the trial average
     avCSD /= nTrials
 
@@ -62,20 +55,20 @@
     # output has shape (1, nFreq, nChannels, nChannels)
     assert Cij.shape == avCSD.shape
 
     # coherence between channel 0 and 1
     coh = Cij[:, 0, 1]
 
     fig, ax = ppl.subplots(figsize=(6, 4), num=None)
-    ax.set_xlabel('frequency (Hz)')
-    ax.set_ylabel('coherence')
-    ax.set_ylim((-.02, 1.05))
-    ax.set_title(f'{nTrials} trials averaged coherence,  SNR=1')
+    ax.set_xlabel("frequency (Hz)")
+    ax.set_ylabel("coherence")
+    ax.set_ylim((-0.02, 1.05))
+    ax.set_title(f"{nTrials} trials averaged coherence,  SNR=1")
 
-    ax.plot(freqs, coh, lw=1.5, alpha=0.8, c='cornflowerblue')
+    ax.plot(freqs, coh, lw=1.5, alpha=0.8, c="cornflowerblue")
 
     # we test for the highest peak sitting at
     # the vicinity (± 5Hz) of the harmonic
     peak_val = np.max(coh)
     peak_idx = np.argmax(coh)
     peak_freq = freqs[peak_idx]
 
@@ -84,16 +77,16 @@
     # we test that the peak value
     # is at least 0.9 and max 1
     assert 0.9 < peak_val < 1
 
     # trial averaging should suppress the noise
     # we test that away from the harmonic the coherence is low
     level = 0.4
-    assert np.all(coh[:peak_idx - 2] < level)
-    assert np.all(coh[peak_idx + 2:] < level)
+    assert np.all(coh[: peak_idx - 2] < level)
+    assert np.all(coh[peak_idx + 2 :] < level)
 
 
 def test_csd():
 
     """
     Tests multi-tapered single trial cross spectral
     densities
@@ -102,40 +95,36 @@
     nSamples = 1001
     fs = 1000
     tvec = np.arange(nSamples) / fs
     harm_freq = 40
     phase_shifts = np.array([0, np.pi / 2, np.pi])
 
     # 1 phase phase shifted harmonics + white noise, SNR = 1
-    data = [np.cos(harm_freq * 2 * np. pi * tvec + ps)
-            for ps in phase_shifts]
+    data = [np.cos(harm_freq * 2 * np.pi * tvec + ps) for ps in phase_shifts]
     data = np.array(data).T
     data = np.array(data) + np.random.randn(nSamples, len(phase_shifts))
 
     bw = 8  # Hz
     NW = nSamples * bw / (2 * fs)
-    Kmax = int(2 * NW - 1)   # multiple tapers for single trial coherence
-    CSD, freqs = csd.csd(data, fs,
-                         taper='dpss',
-                         taper_opt={'Kmax': Kmax, 'NW': NW},
-                         norm=True)
+    Kmax = int(2 * NW - 1)  # multiple tapers for single trial coherence
+    CSD, freqs = csd.csd(data, fs, taper="dpss", taper_opt={"Kmax": Kmax, "NW": NW}, norm=True)
 
     # output has shape (nFreq, nChannels, nChannels)
     assert CSD.shape == (len(freqs), data.shape[1], data.shape[1])
 
     # single trial coherence between channel 0 and 1
     coh = np.abs(CSD[:, 0, 1])
 
     fig, ax = ppl.subplots(figsize=(6, 4), num=None)
-    ax.set_xlabel('frequency (Hz)')
-    ax.set_ylabel('coherence')
-    ax.set_ylim((-.02, 1.05))
-    ax.set_title(f'MTM coherence, {Kmax} tapers, SNR=1')
+    ax.set_xlabel("frequency (Hz)")
+    ax.set_ylabel("coherence")
+    ax.set_ylim((-0.02, 1.05))
+    ax.set_title(f"MTM coherence, {Kmax} tapers, SNR=1")
 
-    ax.plot(freqs, coh, lw=1.5, alpha=0.8, c='cornflowerblue')
+    ax.plot(freqs, coh, lw=1.5, alpha=0.8, c="cornflowerblue")
 
     # we test for the highest peak sitting at
     # the vicinity (± 5Hz) of one the harmonic
     peak_val = np.max(coh)
     peak_idx = np.argmax(coh)
     peak_freq = freqs[peak_idx]
     assert harm_freq - 5 < peak_freq < harm_freq + 5
@@ -190,16 +179,15 @@
     nTrials = 150
     CSDav = np.zeros((nSamples // 2 + 1, nChannels, nChannels), dtype=np.complex64)
     for _ in range(nTrials):
 
         sol = synthdata.ar2_network(nSamples=nSamples, seed=None, nTrials=None)
         # --- get the (single trial) CSD ---
 
-        CSD, freqs = csd.csd(sol, fs,
-                             norm=False)
+        CSD, freqs = csd.csd(sol, fs, norm=False)
 
         CSDav += CSD
 
     CSDav /= nTrials
 
     # --- factorize CSD with Wilson's algorithm ---
 
@@ -210,22 +198,20 @@
 
     # reconstitute
     CSDfac = H @ Sigma @ H.conj().transpose(0, 2, 1)
     err = max_rel_err(CSDav, CSDfac)
     assert err < 1e-6
 
     fig, ax = ppl.subplots(figsize=(6, 4))
-    ax.set_xlabel('frequency (Hz)')
-    ax.set_ylabel(r'$|CSD_{ij}(f)|$')
+    ax.set_xlabel("frequency (Hz)")
+    ax.set_ylabel(r"$|CSD_{ij}(f)|$")
     chan = nChannels // 2
     # show (real) auto-spectra
-    ax.plot(freqs, np.abs(CSDav[:, chan, chan]),
-            '-o', label='original CSD', ms=3)
-    ax.plot(freqs, np.abs(CSDfac[:, chan, chan]),
-            '-o', label='factorized CSD', ms=3)
+    ax.plot(freqs, np.abs(CSDav[:, chan, chan]), "-o", label="original CSD", ms=3)
+    ax.plot(freqs, np.abs(CSDfac[:, chan, chan]), "-o", label="factorized CSD", ms=3)
     # ax.set_xlim((350, 450))
     ax.legend()
 
 
 def test_regularization():
 
     """
@@ -278,34 +264,31 @@
         # -- simulate 2 AR(2) processes with 2->1 coupling --
         sol = synthdata.ar2_network(nSamples=nSamples, nTrials=None)
 
         # --- get CSD ---
         bw = 2
         NW = bw * nSamples / (2 * fs)
         Kmax = int(2 * NW - 1)  # optimal number of tapers
-        CSD, freqs = csd.csd(sol, fs,
-                             taper='dpss',
-                             taper_opt={'Kmax': Kmax, 'NW': NW},
-                             demean_taper=True)
+        CSD, freqs = csd.csd(sol, fs, taper="dpss", taper_opt={"Kmax": Kmax, "NW": NW}, demean_taper=True)
 
         CSDav += CSD
 
     CSDav /= nTrials
     # with only 2 channels this CSD is well conditioned
     assert np.linalg.cond(CSDav).max() < 1e2
     H, Sigma, conv, err = wilson_sf(CSDav, direct_inversion=True)
 
     G = granger(CSDav, H, Sigma)
     assert G.shape == CSDav.shape
 
     fig, ax = ppl.subplots(figsize=(6, 4))
-    ax.set_xlabel('frequency (Hz)')
-    ax.set_ylabel(r'Granger causality(f)')
-    ax.plot(freqs, G[:, 0, 1], label=r'Granger $1\rightarrow2$')
-    ax.plot(freqs, G[:, 1, 0], label=r'Granger $2\rightarrow1$')
+    ax.set_xlabel("frequency (Hz)")
+    ax.set_ylabel(r"Granger causality(f)")
+    ax.plot(freqs, G[:, 0, 1], label=r"Granger $1\rightarrow2$")
+    ax.plot(freqs, G[:, 1, 0], label=r"Granger $2\rightarrow1$")
     ax.legend()
 
     # check for directional causality at 40Hz
     freq_idx = np.argmin(freqs < 40)
     assert 39 < freqs[freq_idx] < 41
 
     # check low to no causality for 1->2
@@ -318,10 +301,10 @@
     G2 = granger(CSDav, H, Sigma)
 
     # check low to no causality for 1->2
     assert G2[freq_idx, 0, 1] < 0.1
     # check high causality for 2->1
     assert G2[freq_idx, 1, 0] > 0.7
 
-    ax.plot(freqs, G2[:, 0, 1], label=r'Granger (LS) $1\rightarrow2$')
-    ax.plot(freqs, G2[:, 1, 0], label=r'Granger (LS) $2\rightarrow1$')
+    ax.plot(freqs, G2[:, 0, 1], label=r"Granger (LS) $1\rightarrow2$")
+    ax.plot(freqs, G2[:, 1, 0], label=r"Granger (LS) $2\rightarrow1$")
     ax.legend()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/backend/test_fooofspy.py` & `esi_syncopy-2023.7/syncopy/tests/test_specest_fooof.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,208 +1,247 @@
 # -*- coding: utf-8 -*-
 #
-# syncopy.specest fooof backend tests
-#
-import numpy as np
-import pytest
-
+# Test FOOOF integration from user/frontend perspective.
 
-from syncopy.specest.fooofspy import fooofspy
-from syncopy.tests.backend.test_resampling import trl_av_power
-from syncopy import synthdata as sd
-from fooof.sim.gen import gen_power_spectrum
 
+import pytest
+import numpy as np
+import inspect
 import matplotlib.pyplot as plt
+import dask.distributed as dd
 
+# Local imports
+from syncopy import freqanalysis
+from syncopy.shared.tools import get_defaults
+from syncopy.shared.errors import SPYValueError
+from syncopy.tests.test_metadata import _get_fooof_signal
+import syncopy as spy
 
-def _power_spectrum(freq_range=[3, 40],
-                    freq_res=0.5):
-
-    # use len 2 for fixed, 3 for knee. order is: offset, (knee), exponent.
-    aperiodic_params = [1, 1]
-
-    # the Gaussians: Mean (Center Frequency), height (Power), and standard deviation (Bandwidth).
-    periodic_params = [[10, 0.2, 1.25], [30, 0.15, 2]]
 
-    noise_level = 0.001
-    freqs, powers = gen_power_spectrum(freq_range, aperiodic_params,
-                                       periodic_params, nlv=noise_level, freq_res=freq_res)
-    return freqs, powers
+def _plot_powerspec_linear(freqs, powers, title="Power spectrum"):
+    """Simple, internal plotting function to plot x versus y. Uses linear scale.
 
+    Parameters
+    ----------
+    powers: can be a vector or a dict with keys being labels and values being vectors
+    save: str interpreted as file name if you want to save the figure, None if you do not want to save to disk.
 
-def AR1_plus_harm_spec(nTrials=30, hfreq=30, ratio=0.7):
-
-    """
-    Create AR(1) background + ratio * (harmonic + phase diffusion)
-    and take the mtmfft with 1Hz spectral smoothing
+    Called for plotting side effect.
     """
-    fs = 400
-    nSamples = 1000
-    # single channel and alpha2 = 0 <-> single AR(1)
-    signals = [sd.ar2_network(AdjMat=np.zeros(1),
-                              alphas=[0.8, 0],
-                              nSamples=nSamples) + ratio * sd.phase_diffusion(freq=hfreq,
-                                                                              fs=fs, eps=0.1,
-                                                                              nChannels=1)
-               for i in range(nTrials)]
-
-    power, freqs = trl_av_power(signals, nSamples, fs, tapsmofrq=1)
-
-    return freqs, power
+    plt.ion()
+    plt.figure()
+    if isinstance(powers, dict):
+        for label, power in powers.items():
+            plt.plot(freqs, power, label=label)
+    else:
+        plt.plot(freqs, powers)
+    plt.xlabel("Frequency (Hz)")
+    plt.ylabel("Power (a.u.)")
+    plt.legend()
+    plt.title(title)
+
+
+def _fft(analog_data, select={"channel": 0}, foilim=[1.0, 100]):
+    """Run standard mtmfft with trial averaging on AnalogData instance."""
+    if not isinstance(analog_data, spy.datatype.continuous_data.AnalogData):
+        raise ValueError(
+            "Parameter 'analog_data' must be a syncopy.datatype.continuous_data.AnalogData instance."
+        )
+    cfg = get_defaults(freqanalysis)
+    cfg.method = "mtmfft"
+    cfg.taper = "hann"
+    cfg.select = select
+    cfg.keeptrials = False  # Averages signal over all (selected) trials.
+    cfg.output = "pow"
+    cfg.foilim = foilim
+    return freqanalysis(cfg, analog_data)
 
 
-class TestSpfooof():
+def _show_spec_log(analog_data, title=None):
+    """Plot the power spectrum for an AnalogData object. Uses singlepanelplot, so data are shown on a log scale.
 
-    freqs, powers = _power_spectrum()
-
-    def test_output_fooof_single_channel(self, freqs=freqs, powers=powers):
-        """
-        Tests spfooof with output 'fooof' and a single input spectrum/channel.
-        This will return the full, fooofed spectrum.
-        """
-        spectra, details = fooofspy(powers, freqs, out_type='fooof', fooof_opt={'peak_width_limits': (1.0, 12.0)})
-
-        assert spectra.shape == (freqs.size, 1)
-        assert details['settings_used']['out_type'] == 'fooof'
-        assert all(key in details for key in ("aperiodic_params", "gaussian_params",
-                                              "peak_params", "n_peaks", "r_squared",
-                                              "error", "settings_used"))
-
-        assert details['settings_used']['fooof_opt']['peak_threshold'] == 2.0  # Should be in and at default value.
-
-        # Ensure the results resemble the params used to generate the artificial data
-        # See the _power_spectrum() function above for the origins of these values.
-        assert np.allclose(details['gaussian_params'][0][0], [10, 0.2, 1.25], atol=0.5)  # The first peak
-        assert np.allclose(details['gaussian_params'][0][1], [30, 0.15, 2], atol=2.0)  # The second peak
-
-    def test_output_fooof_several_channels(self, freqs=freqs, powers=powers):
-        """
-        Tests spfooof with output 'fooof' and several input signals/channels.
-        This will return the full, fooofed spectrum.
-        """
-        num_channels = 3
-        # Copy signal to create channels.
-        powers = np.tile(powers, num_channels).reshape(powers.size, num_channels)
-        spectra, details = fooofspy(powers, freqs, out_type='fooof', fooof_opt={'peak_width_limits': (1.0, 12.0)})
-
-        assert spectra.shape == (freqs.size, num_channels)
-        assert details['settings_used']['out_type'] == 'fooof'
-        assert all(key in details for key in ("aperiodic_params",
-                                              "gaussian_params",
-                                              "peak_params",
-                                              "n_peaks",
-                                              "r_squared",
-                                              "error",
-                                              "settings_used"))
-
-        # Should be in and at default value.
-        assert details['settings_used']['fooof_opt']['peak_threshold'] == 2.0
+    Performs mtmfft with `_fft()` to do that. Use `matplotlib.pyplot.ion()` if you dont see the plot.
+    """
+    if not isinstance(analog_data, spy.datatype.continuous_data.AnalogData):
+        raise ValueError(
+            "Parameter 'analog_data' must be a syncopy.datatype.continuous_data.AnalogData instance."
+        )
+    spp(_fft(analog_data), title=title)
+
+
+def spp(dt, title=None):
+    """Single panet plot with a title."""
+    if not isinstance(dt, spy.datatype.base_data.BaseData):
+        raise ValueError("Parameter 'dt' must be a syncopy.datatype instance.")
+    fig, ax = dt.singlepanelplot()
+    if title is not None:
+        ax.set_title(title)
+    return fig, ax
 
-    def test_output_fooof_aperiodic(self, freqs=freqs, powers=powers):
-        """
-        Tests spfooof with output 'fooof_aperiodic' and a single input signal.
-        This will return the aperiodic part of the fit.
-        """
-        spectra, details = fooofspy(powers, freqs, out_type='fooof_aperiodic', fooof_opt={'peak_width_limits': (1.0, 12.0)})
 
-        assert spectra.shape == (freqs.size, 1)
-        assert details['settings_used']['out_type'] == 'fooof_aperiodic'
-        assert all(key in details for key in ("aperiodic_params",
-                                              "gaussian_params",
-                                              "peak_params",
-                                              "n_peaks",
-                                              "r_squared",
-                                              "error",
-                                              "settings_used"))
-        # Should be in and at default value.
-        assert details['settings_used']['fooof_opt']['peak_threshold'] == 2.0
+class TestFooofSpy:
+    """
+    Test the frontend (user API) for running FOOOF. FOOOF is a post-processing of an FFT, and
+    to request the post-processing, the user sets the method to "mtmfft", and the output to
+    one of the available FOOOF output types.
+    """
 
-    def test_output_fooof_peaks(self, freqs=freqs, powers=powers):
-        """
-        Tests spfooof with output 'fooof_peaks' and a single input signal.
-        This will return the Gaussian fit of the periodic part of the spectrum.
-        """
-        spectra, details = fooofspy(powers, freqs, out_type='fooof_peaks', fooof_opt={'peak_width_limits': (1.0, 12.0)})
+    seed = 42
+    tfData = _get_fooof_signal(seed=seed)
 
-        assert spectra.shape == (freqs.size, 1)
-        assert details['settings_used']['out_type'] == 'fooof_peaks'
-        assert all(key in details for key in ("aperiodic_params", "gaussian_params", "peak_params", "n_peaks", "r_squared", "error", "settings_used"))
-        # Should be in and at default value.
-        assert details['settings_used']['fooof_opt']['peak_threshold'] == 2.0
-
-    def test_together(self, freqs=freqs, powers=powers):
-        fooof_opt = {'peak_width_limits': (1.0, 12.0)}
-        spec_fooof, det_fooof = fooofspy(powers, freqs, out_type='fooof', fooof_opt=fooof_opt)
-        spec_fooof_aperiodic, det_fooof_aperiodic = fooofspy(powers, freqs, out_type='fooof_aperiodic', fooof_opt=fooof_opt)
-        spec_fooof_peaks, det_fooof_peaks = fooofspy(powers, freqs, out_type='fooof_peaks', fooof_opt=fooof_opt)
-
-        # Ensure details are correct
-        assert det_fooof['settings_used']['out_type'] == 'fooof'
-        assert det_fooof_aperiodic['settings_used']['out_type'] == 'fooof_aperiodic'
-        assert det_fooof_peaks['settings_used']['out_type'] == 'fooof_peaks'
-
-        # Ensure output shapes are as expected.
-        assert spec_fooof.shape == spec_fooof_aperiodic.shape
-        assert spec_fooof.shape == spec_fooof_peaks.shape
-        assert spec_fooof.shape == (powers.size, 1)
-        assert spec_fooof.shape == (freqs.size, 1)
-
-        fooofed_spectrum = spec_fooof.squeeze()
-        fooof_aperiodic = spec_fooof_aperiodic.squeeze()
-        fooof_peaks = spec_fooof_peaks.squeeze()
-
-        assert np.max(fooof_peaks) < np.max(fooofed_spectrum)
-
-        # Visually compare data and fits.
-        plt.figure()
-        plt.plot(freqs, powers, label="Raw input data")
-        plt.plot(freqs, fooofed_spectrum, label="Fooofed spectrum")
-        plt.plot(freqs, fooof_aperiodic, label="Fooof aperiodic fit")
-        plt.plot(freqs, fooof_peaks, label="Fooof peaks fit")
-        plt.xlabel('Frequency (Hz)')
-        plt.ylabel('Power (Db)')
-        plt.legend()
-        plt.title("Comparison of raw data and fooof results, linear scale.")
-        # plt.show()
+    @staticmethod
+    def get_fooof_cfg():
+        cfg = get_defaults(freqanalysis)
+        cfg.method = "mtmfft"
+        cfg.taper = "hann"
+        cfg.select = {"channel": 0}
+        cfg.keeptrials = False
+        cfg.output = "fooof"
+        cfg.foilim = [1.0, 100.0]
+        return cfg
+
+    def test_output_fooof_fails_with_freq_zero(self):
+        """The fooof package ignores input values of zero frequency, and shortens the output array
+        in that case with a warning. This is not acceptable for us, as the expected output dimension
+        will not off by one. Also it is questionable whether users would want that. We therefore use
+        consider it an error to pass an input frequency axis that contains the zero, and throw an
+        error in the frontend to stop before any expensive computations happen. This test checks for
+        that error.
+        """
+        cfg = TestFooofSpy.get_fooof_cfg()
+        cfg["foilim"] = [0.0, 100.0]  # Include the zero in tfData.
+        with pytest.raises(SPYValueError) as err:
+            _ = freqanalysis(cfg, _get_fooof_signal(seed=self.seed))  # tfData contains zero.
+        assert "a frequency range that does not include zero" in str(err.value)
+
+    def test_output_fooof_works_with_freq_zero_in_data_after_setting_frequency(self):
+        """
+        This tests the intended operation with output type 'fooof': with an input that does not
+        include zero, ensured by using the 'frequency' argument/setting when calling freqanalysis.
+
+        This returns the full, fooofed spectrum.
+        """
+        cfg = TestFooofSpy.get_fooof_cfg()
+        cfg.pop("fooof_opt", None)
+        fooof_opt = {"peak_width_limits": (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
+        spec_dt = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
+
+        # check frequency axis
+        assert spec_dt.freq.size == 100
+        assert spec_dt.freq[0] == 1
+        assert spec_dt.freq[99] == 100.0
+
+        # check the log
+        assert "fooof_method = fooof" in spec_dt._log
+        assert "fooof_aperiodic" not in spec_dt._log
+        assert "fooof_peaks" not in spec_dt._log
+        assert "fooof_opt" in spec_dt._log
+
+        # check the data
+        assert spec_dt.data.ndim == 4
+        assert spec_dt.data.shape == (1, 1, 100, 1)
+        assert not np.isnan(spec_dt.data).any()
+
+        # check that the cfg is correct (required for replay)
+        assert spec_dt.cfg["freqanalysis"]["output"] == "fooof"
+
+        # Plot it.
+        # _plot_powerspec_linear(freqs=spec_dt.freq, powers=spec_dt.data[0, 0, :, 0], title="fooof full model, for ar1 data (linear scale)")
+        # spp(spec_dt, "FOOOF full model")
+        # plt.savefig("spp.png")
+
+    def test_output_fooof_aperiodic(self):
+        """Test fooof with output type 'fooof_aperiodic'. A spectrum containing only the aperiodic part is returned."""
+
+        cfg = TestFooofSpy.get_fooof_cfg()
+        cfg.output = "fooof_aperiodic"
+        cfg.pop("fooof_opt", None)
+        fooof_opt = {"peak_width_limits": (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
+        spec_dt = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
+
+        # log
+        assert "fooof" in spec_dt._log  # from the method
+        assert "fooof_method = fooof_aperiodic" in spec_dt._log
+        assert "fooof_peaks" not in spec_dt._log
+
+        # check the data
+        assert spec_dt.data.ndim == 4
+        assert spec_dt.data.shape == (1, 1, 100, 1)
+        assert not np.isnan(spec_dt.data).any()
+
+    def test_output_fooof_peaks(self):
+        """Test fooof with output type 'fooof_peaks'. A spectrum containing only the peaks (actually, the Gaussians fit to the peaks) is returned."""
+        cfg = TestFooofSpy.get_fooof_cfg()
+        cfg.output = "fooof_peaks"
+        cfg.pop("fooof_opt", None)
+        fooof_opt = {"peak_width_limits": (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
+        spec_dt = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
+        assert spec_dt.data.ndim == 4
+        assert "fooof" in spec_dt._log
+        assert "fooof_method = fooof_peaks" in spec_dt._log
+        assert "fooof_aperiodic" not in spec_dt._log
+
+    def test_different_fooof_outputs_are_consistent(self):
+        """Test fooof with all output types plotted into a single plot and ensure consistent output."""
+        cfg = TestFooofSpy.get_fooof_cfg()
+        cfg["output"] = "pow"
+        cfg["foilim"] = [10, 70]
+        cfg.pop("fooof_opt", None)
+        fooof_opt = {
+            "peak_width_limits": (6.0, 12.0),
+            "min_peak_height": 0.2,
+        }  # Increase lower limit to avoid fooof warning.
+
+        out_fft = freqanalysis(cfg, _get_fooof_signal(seed=self.seed))
+        cfg["output"] = "fooof"
+        out_fooof = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
+        cfg["output"] = "fooof_aperiodic"
+        out_fooof_aperiodic = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
+        cfg["output"] = "fooof_peaks"
+        out_fooof_peaks = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
+
+        assert (out_fooof.freq == out_fooof_aperiodic.freq).all()
+        assert (out_fooof.freq == out_fooof_peaks.freq).all()
+
+        assert out_fooof.data.shape == out_fooof_aperiodic.data.shape
+        assert out_fooof.data.shape == out_fooof_peaks.data.shape
+
+        # biggest peak is at 30Hz
+        f1_ind = out_fooof_peaks.show(channel=0).argmax()
+        assert 27 < out_fooof_peaks.freq[f1_ind] < 33
+
+        plot_data = {
+            "Raw input data": np.ravel(out_fft.data),
+            "Fooofed spectrum": np.ravel(out_fooof.data),
+            "Fooof aperiodic fit": np.ravel(out_fooof_aperiodic.data),
+            "Fooof peaks fit": np.ravel(out_fooof_peaks.data),
+        }
+        # _plot_powerspec_linear(out_fooof.freq, powers=plot_data, title="Outputs from different fooof methods for ar1 data (linear scale)")
+
+    def test_frontend_settings_are_merged_with_defaults_used_in_backend(self):
+        cfg = TestFooofSpy.get_fooof_cfg()
+        cfg.output = "fooof_peaks"
+        cfg.pop("fooof_opt", None)
+        fooof_opt = {"max_n_peaks": 8, "peak_width_limits": (1.0, 12.0)}
+        spec_dt = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
+
+        assert spec_dt.data.ndim == 4
+
+    def test_parallel(self, testcluster):
+
+        plt.ioff()
+        client = dd.Client(testcluster)
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (inspect.ismethod(getattr(self, attr)) and "parallel" not in attr)
+        ]
+
+        for test in all_tests:
+            test_method = getattr(self, test)
+            test_method()
+        client.close()
+        plt.ion()
 
-    def test_the_fooof_opt_settings_are_used(self, freqs=freqs, powers=powers):
-        """
-        Tests spfooof with output 'fooof_peaks' and a single input signal.
-        This will return the Gaussian fit of the periodic part of the spectrum.
-        """
-        fooof_opt = {'peak_threshold': 3.0, 'peak_width_limits': (1.0, 12.0)}
-        spectra, details = fooofspy(powers, freqs, out_type='fooof_peaks', fooof_opt=fooof_opt)
 
-        assert spectra.shape == (freqs.size, 1)
-        assert details['settings_used']['out_type'] == 'fooof_peaks'
-        assert all(key in details for key in ("aperiodic_params", "gaussian_params", "peak_params", "n_peaks", "r_squared", "error", "settings_used"))
-        # Should reflect our custom value.
-        assert details['settings_used']['fooof_opt']['peak_threshold'] == 3.0
-        # No custom value => should be at default.
-        assert details['settings_used']['fooof_opt']['min_peak_height'] == 0.0
-
-    def test_exception_empty_freqs(self):
-        # The input frequencies must not be None.
-        with pytest.raises(ValueError) as err:
-            spectra, details = fooofspy(self.powers, None)
-        assert "input frequencies are required and must not be None" in str(err.value)
-
-    def test_exception_freq_length_does_not_match_spectrum_length(self):
-        # The input frequencies must have the same length as the spectrum.
-        with pytest.raises(ValueError) as err:
-            self.test_output_fooof_single_channel(freqs=np.arange(self.powers.size + 1),
-                                                  powers=self.powers)
-        assert "signal length" in str(err.value)
-        assert "must match the number of frequency labels" in str(err.value)
-
-    def test_exception_on_invalid_output_type(self):
-        # Invalid out_type is rejected.
-        with pytest.raises(ValueError) as err:
-            spectra, details = fooofspy(self.powers, self.freqs, out_type='fooof_invalidout')
-        assert "out_type" in str(err.value)
-
-    def test_exception_on_invalid_fooof_opt_entry(self):
-        # Invalid fooof_opt entry is rejected.
-        with pytest.raises(ValueError) as err:
-            fooof_opt = {'peak_threshold': 2.0, 'invalid_key': 42}
-            spectra, details = fooofspy(self.powers, self.freqs, fooof_opt=fooof_opt)
-        assert "fooof_opt" in str(err.value)
+if __name__ == "__main__":
+    T = TestFooofSpy()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/backend/test_resampling.py` & `esi_syncopy-2023.7/syncopy/tests/backend/test_resampling.py`

 * *Files 1% similar despite different names*

```diff
@@ -67,30 +67,28 @@
     # -- test resampling --
 
     rs_fs = 205
     # make sure we have a non-integer division
     assert orig_fs % rs_fs > 1  # strictly > 0 would be enough..
 
     # -- test SciPy default --
-    rs_dataSP = [resampling.resample(signal, orig_fs, rs_fs, lpfreq=-1)
-                 for signal in data]
+    rs_dataSP = [resampling.resample(signal, orig_fs, rs_fs, lpfreq=-1) for signal in data]
 
     rs_powerSP, rs_freqsSP = trl_av_power(rs_dataSP, nSamples, rs_fs)
 
     # here we have implicit FIR filtering built in,
     # hence there should be again no gain
     # NOTE: There is however a quite slow roll-off
     # relax gain condition to tolerate losses up to 6%
     gain = rs_powerSP.mean() / orig_power.mean()
     assert 0.94 < gain < 1.02
 
     # -- use backend with homegrown default firws --
 
-    rs_data = [resampling.resample(signals, orig_fs, rs_fs, lpfreq=None, order=None)
-               for signals in data]
+    rs_data = [resampling.resample(signals, orig_fs, rs_fs, lpfreq=None, order=None) for signals in data]
     rs_power, rs_freqs = trl_av_power(rs_data, nSamples, rs_fs)
     gain = rs_power.mean() / orig_power.mean()
     # NOTE: this works very well and we can
     # give again harder constraints on the gain (2%)
     assert 0.98 < gain < 1.02
 
     # -- plot all the power spectra --
@@ -110,14 +108,12 @@
 
 
 def trl_av_power(data, nSamples, fs, tapsmofrq=1):
 
     power = []
     for signal in data:
         NW, Kmax = mtmfft._get_dpss_pars(tapsmofrq, nSamples, fs)
-        ftr, freqs = mtmfft.mtmfft(
-            signal, samplerate=fs, taper="dpss", taper_opt={"Kmax": Kmax, "NW": NW}
-        )
+        ftr, freqs = mtmfft.mtmfft(signal, samplerate=fs, taper="dpss", taper_opt={"Kmax": Kmax, "NW": NW})
         power.append(np.real(ftr * ftr.conj()).mean(axis=0))
     # trial averaging
     power = np.mean(power, axis=0)
     return power, freqs
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/backend/test_timefreq.py` & `esi_syncopy-2023.7/syncopy/tests/backend/test_timefreq.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,17 +4,15 @@
 
 from syncopy.specest import mtmfft
 from syncopy.specest import mtmconvol
 from syncopy.specest import superlet, wavelet
 from syncopy.specest import wavelets as spywave
 
 
-def gen_testdata(freqs=[20, 40, 60],
-                 cycles=11, fs=1000,
-                 eps=0):
+def gen_testdata(freqs=[20, 40, 60], cycles=11, fs=1000, eps=0):
 
     """
     Harmonic superposition of multiple
     few-cycle oscillations akin to the
     example of Figure 3 in Moca et al. 2021 NatComm
 
     Each harmonic has a frequency neighbor with +10Hz
@@ -26,15 +24,15 @@
 
         # 10 cycles of f1
         tvec = np.arange(cycles / freq, step=1 / fs)
 
         harmonic = np.cos(2 * np.pi * freq * tvec)
         # frequency neighbor
         f_neighbor = np.cos(2 * np.pi * (freq + 10) * tvec)
-        packet = harmonic +  f_neighbor
+        packet = harmonic + f_neighbor
 
         # 2 cycles time neighbor
         delta_t = np.zeros(int(2 / freq * fs))
 
         # 5 cycles break
         pad = np.zeros(int(5 / freq * fs))
 
@@ -57,15 +55,15 @@
 
 # generate 3 packets at 20, 40 and 60Hz with 12 cycles each
 # Noise variance is given by eps
 signal_freqs = np.array([20, 50, 80])
 
 cycles = 12
 A = 5  # signal amplitude
-signal = A * gen_testdata(freqs=signal_freqs, cycles=cycles, fs=fs, eps=0.)
+signal = A * gen_testdata(freqs=signal_freqs, cycles=cycles, fs=fs, eps=0.0)
 
 # define frequencies of interest for wavelet methods
 foi = np.arange(1, 101, step=1)
 
 # closest spectral indices to validate time-freq results
 freq_idx = []
 for frequency in signal_freqs:
@@ -78,199 +76,215 @@
     window_size = 500
 
     # default - stft pads with 0's to make windows fit
     # we choose N-1 overlap to retrieve a time-freq estimate
     # for each epoch in the signal
 
     # the transforms have shape (nTime, nTaper, nFreq, nChannel)
-    ftr, freqs = mtmconvol.mtmconvol(signal,
-                                     samplerate=fs, taper='hann',
-                                     nperseg=window_size,
-                                     noverlap=window_size - 1)
+    ftr, freqs = mtmconvol.mtmconvol(
+        signal,
+        samplerate=fs,
+        taper="hann",
+        nperseg=window_size,
+        noverlap=window_size - 1,
+    )
 
     # absolute squared for power spectrum and taper averaging
     spec = np.real(ftr * ftr.conj()).mean(axis=1)[:, :, 0]  # 1st Channel
 
-    fig, (ax1, ax2) = ppl.subplots(2, 1,
-                                   sharex=True,
-                                   gridspec_kw={"height_ratios": [1, 3]},
-                                   figsize=(6, 6))
+    fig, (ax1, ax2) = ppl.subplots(2, 1, sharex=True, gridspec_kw={"height_ratios": [1, 3]}, figsize=(6, 6))
 
     ax1.set_title("Short Time Fourier Transform")
-    ax1.plot(np.arange(signal.size) / fs, signal, c='cornflowerblue')
-    ax1.set_ylabel('signal (a.u.)')
+    ax1.plot(np.arange(signal.size) / fs, signal, c="cornflowerblue")
+    ax1.set_ylabel("signal (a.u.)")
 
     ax2.set_xlabel("time (s)")
     ax2.set_ylabel("frequency (Hz)")
 
     df = freqs[1] - freqs[0]
     # shift half a frequency bin
     extent = [0, len(signal) / fs, freqs[0] - df / 2, freqs[-1] - df / 2]
     # test also the plotting
     # scale with amplitude
-    ax2.imshow(spec.T,
-               cmap='magma',
-               aspect='auto',
-               origin='lower',
-               extent=extent,
-               vmin=0,
-               vmax=.5 * A**2)
+    ax2.imshow(
+        spec.T,
+        cmap="magma",
+        aspect="auto",
+        origin="lower",
+        extent=extent,
+        vmin=0,
+        vmax=0.5 * A**2,
+    )
 
     # zoom into foi region
     ax2.set_ylim((foi[0], foi[-1]))
 
     # get the 'mappable' for the colorbar
     im = ax2.images[0]
-    fig.colorbar(im, ax=ax2, orientation='horizontal',
-                 shrink=0.7, pad=0.2, label='amplitude (a.u.)')
+    fig.colorbar(
+        im,
+        ax=ax2,
+        orientation="horizontal",
+        shrink=0.7,
+        pad=0.2,
+        label="amplitude (a.u.)",
+    )
 
     # closest spectral indices to validate time-freq results
     freq_idx = []
     for frequency in signal_freqs:
         freq_idx.append(np.argmax(freqs >= frequency))
 
     # test amplitude normalization
     for idx, frequency in zip(freq_idx, signal_freqs):
 
-        ax2.plot([0, len(signal) / fs],
-                 [frequency, frequency],
-                 '--',
-                 c='0.5')
+        ax2.plot([0, len(signal) / fs], [frequency, frequency], "--", c="0.5")
 
         # integrated power at the respective frquency
-        cycle_num = (spec[:, idx] > .5 * A**2 / np.e**2).sum() / fs * frequency
-        print(f'{cycle_num} cycles for the {frequency} Hz band')
+        cycle_num = (spec[:, idx] > 0.5 * A**2 / np.e**2).sum() / fs * frequency
+        print(f"{cycle_num} cycles for the {frequency} Hz band")
         # we have 2 times the cycles for each frequency (temporal neighbor)
         assert cycle_num > 2 * cycles
         # power should decay fast, so we don't detect more cycles
         assert cycle_num < 3 * cycles
 
     fig.tight_layout()
 
     # -------------------------
     # test multi-taper analysis
     # -------------------------
 
-    taper = 'dpss'
+    taper = "dpss"
     tapsmofrq = 5  # two-sided in Hz
     # set parameters for scipy.signal.windows.dpss
     NW = tapsmofrq * window_size / fs
     # from the minBw setting NW always is at least 1
     Kmax = int(2 * NW - 1)  # optimal number of tapers
 
-    taper_opt = {'Kmax': Kmax, 'NW': NW}
+    taper_opt = {"Kmax": Kmax, "NW": NW}
     print(taper_opt)
     # the transforms have shape (nTime, nTaper, nFreq, nChannel)
-    ftr2, freqs2 = mtmconvol.mtmconvol(signal,
-                                       samplerate=fs, taper=taper, taper_opt=taper_opt,
-                                       nperseg=window_size,
-                                       noverlap=window_size - 1)
+    ftr2, freqs2 = mtmconvol.mtmconvol(
+        signal,
+        samplerate=fs,
+        taper=taper,
+        taper_opt=taper_opt,
+        nperseg=window_size,
+        noverlap=window_size - 1,
+    )
 
     spec2 = np.real((ftr2 * ftr2.conj()).mean(axis=1)[..., 0])
 
-    fig, (ax1, ax2) = ppl.subplots(2, 1,
-                                   sharex=True,
-                                   gridspec_kw={"height_ratios": [1, 3]},
-                                   figsize=(6, 6))
+    fig, (ax1, ax2) = ppl.subplots(2, 1, sharex=True, gridspec_kw={"height_ratios": [1, 3]}, figsize=(6, 6))
 
     ax1.set_title("Multi-Taper STFT")
-    ax1.plot(np.arange(signal.size) / fs, signal, c='cornflowerblue')
-    ax1.set_ylabel('signal (a.u.)')
+    ax1.plot(np.arange(signal.size) / fs, signal, c="cornflowerblue")
+    ax1.set_ylabel("signal (a.u.)")
 
     ax2.set_xlabel("time (s)")
     ax2.set_ylabel("frequency (Hz)")
 
     # test also the plotting
     # scale with amplitude
-    ax2.imshow(spec2.T,
-               cmap='magma',
-               aspect='auto',
-               origin='lower',
-               extent=extent,
-               vmin=0,
-               vmax=.5 * A**2)
+    ax2.imshow(
+        spec2.T,
+        cmap="magma",
+        aspect="auto",
+        origin="lower",
+        extent=extent,
+        vmin=0,
+        vmax=0.5 * A**2,
+    )
 
     # zoom into foi region
     ax2.set_ylim((foi[0], foi[-1]))
 
     # get the 'mappable' for the colorbar
     im = ax2.images[0]
-    fig.colorbar(im, ax=ax2, orientation='horizontal',
-                 shrink=0.7, pad=0.2, label='amplitude (a.u.)')
+    fig.colorbar(
+        im,
+        ax=ax2,
+        orientation="horizontal",
+        shrink=0.7,
+        pad=0.2,
+        label="amplitude (a.u.)",
+    )
 
     fig.tight_layout()
 
     for idx, frequency in zip(freq_idx, signal_freqs):
 
-        ax2.plot([0, len(signal) / fs],
-                 [frequency, frequency],
-                 '--',
-                 c='0.5')
+        ax2.plot([0, len(signal) / fs], [frequency, frequency], "--", c="0.5")
 
     # for multi-taper stft we can't
     # check for the whole time domain
     # due to too much spectral broadening/smearing
     # so we just check that the maximum estimated
     # power within one bin is within 15% bounds of the real power
     nBins = tapsmofrq
-    assert 0.4 * A**2  < spec2.max() * nBins < .65 * A**2 
+    assert 0.4 * A**2 < spec2.max() * nBins < 0.65 * A**2
+
 
 def test_superlet():
 
     scalesSL = superlet.scale_from_period(1 / foi)
 
     # spec shape is nScales x nTime (x nChannels)
-    spec = superlet.superlet(signal,
-                             samplerate=fs,
-                             scales=scalesSL,
-                             order_max=20,
-                             order_min=2,
-                             c_1=1,
-                             adaptive=False)
+    spec = superlet.superlet(
+        signal,
+        samplerate=fs,
+        scales=scalesSL,
+        order_max=20,
+        order_min=2,
+        c_1=1,
+        adaptive=False,
+    )
     # power spectrum
     power_spec = np.real(spec * spec.conj())
 
-    fig, (ax1, ax2) = ppl.subplots(2, 1,
-                                   sharex=True,
-                                   gridspec_kw={"height_ratios": [1, 3]},
-                                   figsize=(6, 6))
+    fig, (ax1, ax2) = ppl.subplots(2, 1, sharex=True, gridspec_kw={"height_ratios": [1, 3]}, figsize=(6, 6))
 
     ax1.set_title("Superlet Transform")
-    ax1.plot(np.arange(signal.size) / fs, signal, c='cornflowerblue')
-    ax1.set_ylabel('signal (a.u.)')
+    ax1.plot(np.arange(signal.size) / fs, signal, c="cornflowerblue")
+    ax1.set_ylabel("signal (a.u.)")
 
     ax2.set_xlabel("time (s)")
     ax2.set_ylabel("frequency (Hz)")
     extent = [0, len(signal) / fs, foi[0], foi[-1]]
     # test also the plotting
     # scale with amplitude
-    ax2.imshow(power_spec,
-               cmap='magma',
-               aspect='auto',
-               extent=extent,
-               origin='lower',
-               vmin=0,
-               vmax=1.2 * A**2)
+    ax2.imshow(
+        power_spec,
+        cmap="magma",
+        aspect="auto",
+        extent=extent,
+        origin="lower",
+        vmin=0,
+        vmax=1.2 * A**2,
+    )
 
     # get the 'mappable'
     im = ax2.images[0]
-    fig.colorbar(im, ax=ax2, orientation='horizontal',
-                 shrink=0.7, pad=0.2, label='amplitude (a.u.)')
+    fig.colorbar(
+        im,
+        ax=ax2,
+        orientation="horizontal",
+        shrink=0.7,
+        pad=0.2,
+        label="amplitude (a.u.)",
+    )
 
     for idx, frequency in zip(freq_idx, signal_freqs):
 
-        ax2.plot([0, len(signal) / fs],
-                 [frequency, frequency],
-                 '--',
-                 c='0.5')
+        ax2.plot([0, len(signal) / fs], [frequency, frequency], "--", c="0.5")
 
         # number of cycles with relevant
         # amplitude at the respective frequency
-        cycle_num = (power_spec[idx, :] > (A / np.e)**2).sum() / fs * frequency
-        print(f'{cycle_num} cycles for the {frequency} band')
+        cycle_num = (power_spec[idx, :] > (A / np.e) ** 2).sum() / fs * frequency
+        print(f"{cycle_num} cycles for the {frequency} band")
         # we have 2 times the cycles for each frequency (temporal neighbor)
         assert cycle_num > 2 * cycles
         # power should decay fast, so we don't detect more cycles
         assert cycle_num < 3 * cycles
 
     fig.tight_layout()
 
@@ -278,59 +292,58 @@
 def test_wavelet():
 
     # get a wavelet function
     wfun = spywave.Morlet(10)
     scales = wfun.scale_from_period(1 / foi)
 
     # spec shape is nScales x nTime (x nChannels)
-    spec = wavelet.wavelet(signal,
-                           samplerate=fs,
-                           scales=scales,
-                           wavelet=wfun)
+    spec = wavelet.wavelet(signal, samplerate=fs, scales=scales, wavelet=wfun)
     # amplitude spectrum
     power_spec = np.real(spec * spec.conj())
 
-    fig, (ax1, ax2) = ppl.subplots(2, 1,
-                                   sharex=True,
-                                   gridspec_kw={"height_ratios": [1, 3]},
-                                   figsize=(6, 6))
+    fig, (ax1, ax2) = ppl.subplots(2, 1, sharex=True, gridspec_kw={"height_ratios": [1, 3]}, figsize=(6, 6))
     ax1.set_title("Wavelet Transform")
-    ax1.plot(np.arange(signal.size) / fs, signal, c='cornflowerblue')
-    ax1.set_ylabel('signal (a.u.)')
+    ax1.plot(np.arange(signal.size) / fs, signal, c="cornflowerblue")
+    ax1.set_ylabel("signal (a.u.)")
 
     ax2.set_xlabel("time (s)")
     ax2.set_ylabel("frequency (Hz)")
     extent = [0, len(signal) / fs, foi[0], foi[-1]]
 
     # test also the plotting
     # scale with amplitude
-    ax2.imshow(power_spec,
-               cmap='magma',
-               aspect='auto',
-               extent=extent,
-               origin='lower',
-               vmin=0,
-               vmax=1.2 * A**2)
+    ax2.imshow(
+        power_spec,
+        cmap="magma",
+        aspect="auto",
+        extent=extent,
+        origin="lower",
+        vmin=0,
+        vmax=1.2 * A**2,
+    )
 
     # get the 'mappable'
     im = ax2.images[0]
-    fig.colorbar(im, ax=ax2, orientation='horizontal',
-                 shrink=0.7, pad=0.2, label='amplitude (a.u.)')
+    fig.colorbar(
+        im,
+        ax=ax2,
+        orientation="horizontal",
+        shrink=0.7,
+        pad=0.2,
+        label="amplitude (a.u.)",
+    )
 
     for idx, frequency in zip(freq_idx, signal_freqs):
 
-        ax2.plot([0, len(signal) / fs],
-                 [frequency, frequency],
-                 '--',
-                 c='0.5')
+        ax2.plot([0, len(signal) / fs], [frequency, frequency], "--", c="0.5")
 
         # number of cycles with relevant
         # amplitude at the respective frequency
-        cycle_num = (power_spec[idx, :] > (A / np.e)**2).sum() / fs * frequency
-        print(f'{cycle_num} cycles for the {frequency} band')
+        cycle_num = (power_spec[idx, :] > (A / np.e) ** 2).sum() / fs * frequency
+        print(f"{cycle_num} cycles for the {frequency} band")
         # we have at least 2 times the cycles for each frequency (temporal neighbor)
         assert cycle_num > 2 * cycles
         # power should decay fast, so we don't detect more cycles
         assert cycle_num < 3 * cycles
 
     fig.tight_layout()
 
@@ -356,52 +369,52 @@
     # with 1000Hz sampling frequency and 1000 samples this gives
     # exactly 1Hz frequency resolution ranging from 0 - 500Hz:
     assert freqs[f1] == f1
     assert freqs[f2] == f2
 
     # average over potential tapers (only 1 here)
     spec = np.real(ftr * ftr.conj()).mean(axis=0)
-    powers = spec[:, 0]   # only 1 channel
+    powers = spec[:, 0]  # only 1 channel
     # our FFT normalisation recovers the integrated squared signal amplitudes
     # as frequency bin width is 1Hz, one bin 'integral' is enough
     assert np.allclose([0.5 * A1**2, 0.5 * A2**2], powers[[f1, f2]])
 
     fig, ax = ppl.subplots()
     ax.set_title(f"Power spectrum {A1} x 40Hz + {A2} x 100Hz")
     ax.plot(freqs[:150], powers[:150], label="No taper", lw=2)
-    ax.set_xlabel('frequency (Hz)')
-    ax.set_ylabel('power-density')
+    ax.set_xlabel("frequency (Hz)")
+    ax.set_ylabel("power-density")
 
     # -------------------------
     # test multi-taper analysis
     # -------------------------
     tapsmofrq = 10  # Hz
     # set parameters for scipy.signal.windows.dpss
     NW = tapsmofrq * nSamples / (2 * fs)
     # from the minBw setting NW always is at least 1
     Kmax = int(2 * NW - 1)  # optimal number of tapers
 
-    taper_opt = {'Kmax': Kmax, 'NW': NW}
+    taper_opt = {"Kmax": Kmax, "NW": NW}
     ftr, freqs = mtmfft.mtmfft(signal, fs, taper="dpss", taper_opt=taper_opt)
     # average over tapers
     dpss_spec = np.real(ftr * ftr.conj()).mean(axis=0)
-    dpss_powers = dpss_spec[:, 0]   # only 1 channel
+    dpss_powers = dpss_spec[:, 0]  # only 1 channel
     # check for integrated power (and taper normalisation)
     # summing up all dpss powers should give total power of the
     # test signal which is (A1**2 + A2**2) / 2
     assert np.allclose(np.sum(dpss_powers) * 2, A1**2 + A2**2, atol=1e-2)
 
     ax.plot(freqs[:150], dpss_powers[:150], label="Slepian", lw=2)
     ax.legend()
 
     # -----------------
     # test kaiser taper (is boxcar for beta -> inf)
     # -----------------
 
-    taper_opt = {'beta': 3}
+    taper_opt = {"beta": 3}
     ftr, freqs = mtmfft.mtmfft(signal, fs, taper="kaiser", taper_opt=taper_opt)
     # average over tapers (only 1 here)
     kaiser_spec = np.real(ftr * ftr.conj()).mean(axis=0)
     kaiser_powers = kaiser_spec[:, 0]  # only 1 channel
     # check for amplitudes (and taper normalisation)
     # normalization less exact for arbitraty windows
     assert np.allclose(np.sum(kaiser_powers) * 2, A1**2 + A2**2, atol=1.5)
@@ -411,26 +424,26 @@
     # -------------------------------
     # test all other window functions (which don't need a parameter)
     # -------------------------------
 
     for win in windows.__all__:
         taper_opt = {}
         # that guy isn't symmetric
-        if win == 'exponential':
+        if win == "exponential":
             continue
         # that guy is deprecated
-        if win == 'hanning':
+        if win == "hanning":
             continue
         try:
             ftr, freqs = mtmfft.mtmfft(signal, fs, taper=win, taper_opt=taper_opt)
             # average over tapers (only 1 here)
             spec = np.real(ftr * ftr.conj()).mean(axis=0)
             powers = spec[:, 0]  # only 1 channel
             print(np.sum(powers), win)
-            if win != 'tukey':
+            if win != "tukey":
                 assert np.allclose(np.sum(powers) * 2, A1**2 + A2**2, atol=4)
             # not sure why tukey and triang are so off..
             else:
                 assert np.allclose(np.sum(powers) * 2, A1**2 + A2**2, atol=8)
 
         except TypeError:
             # we didn't provide default parameters..
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/conftest.py` & `esi_syncopy-2023.7/syncopy/tests/conftest.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,58 +15,71 @@
 # If acme is available, either launch a SLURM cluster on a cluster node or
 # create a `LocalCluster` object if tests are run on a single machine. If
 # acme is not available, launch a custom SLURM cluster or again just a local
 # cluster as fallback
 cluster = None
 if __acme__:
     from acme.dask_helpers import esi_cluster_setup
+
     if sys.platform != "win32":
         import resource
+
         if max(resource.getrlimit(resource.RLIMIT_NOFILE)) < 1024:
-            msg = "Not enough open file descriptors allowed. Consider increasing " +\
-                "the limit using, e.g., `ulimit -Sn 1024`"
+            msg = (
+                "Not enough open file descriptors allowed. Consider increasing "
+                + "the limit using, e.g., `ulimit -Sn 1024`"
+            )
             raise ValueError(msg)
     if is_slurm_node():
-        cluster = esi_cluster_setup(partition="8GB", n_jobs=4,
-                                    timeout=360, interactive=False,
-                                    start_client=False)
+        cluster = esi_cluster_setup(
+            partition="8GB",
+            n_jobs=4,
+            timeout=360,
+            interactive=False,
+            start_client=False,
+        )
     else:
         cluster = dd.LocalCluster(n_workers=4)
 else:
     # manually start slurm cluster
     if is_slurm_node():
         n_jobs = 3
         reqMem = 32
-        ESIQueue = 'S'
+        ESIQueue = "S"
         slurm_wdir = "/cs/slurm/syncopy/"
 
-        cluster = dj.SLURMCluster(cores=1, memory=f'{reqMem} GB', processes=1,
-                                  local_directory=slurm_wdir,
-                                  queue=f'{reqMem}GB{ESIQueue}',
-                                  python=sys.executable)
+        cluster = dj.SLURMCluster(
+            cores=1,
+            memory=f"{reqMem} GB",
+            processes=1,
+            local_directory=slurm_wdir,
+            queue=f"{reqMem}GB{ESIQueue}",
+            python=sys.executable,
+        )
         cluster.scale(n_jobs)
     else:
         cluster = dd.LocalCluster(n_workers=4)
 
 # Set up a pytest fixture `testcluster` that uses the constructed cluster object
 @pytest.fixture
 def testcluster():
     return cluster
 
+
 # Re-order tests to first run stuff in test_packagesetup.py, then everything else
 def pytest_collection_modifyitems(items):
 
     # Collect tests to be run in this session and registered setup-related tests
     allTests = [testFunc.name if hasattr(testFunc, "name") else "" for testFunc in items]
-    setupTests = [name for name in dir(setupTestModule)
-                  if not name.startswith("__") and not name.startswith("@")]
+    setupTests = [
+        name for name in dir(setupTestModule) if not name.startswith("__") and not name.startswith("@")
+    ]
 
     # If queried tests contain setup-tests, prioritize them
     newOrder = []
     for testFirst in setupTests:
         if testFirst in allTests:
             newOrder.append(allTests.index(testFirst))
-    newOrder += [allTests.index(testFunc) for testFunc in allTests
-                 if testFunc not in setupTests]
+    newOrder += [allTests.index(testFunc) for testFunc in allTests if testFunc not in setupTests]
 
     # Save potentially re-ordered test sequence
     items[:] = [items[idx] for idx in newOrder]
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/helpers.py` & `esi_syncopy-2023.7/syncopy/tests/helpers.py`

 * *Files 13% similar despite different names*

```diff
@@ -5,49 +5,51 @@
 # The `run_` function signatures take a callable,
 # the `method_call`, as 1st argument
 #
 
 # 3rd party imports
 import itertools
 import numpy as np
+import os
 import matplotlib.pyplot as plt
 from syncopy.shared.errors import SPYValueError, SPYTypeError
+from os.path import expanduser
 
 # fix random generators
 test_seed = 42
 
 
 def run_padding_test(method_call, pad_length):
     """
     The callable should test a solution and support
     a single keyword argument `pad`
     """
 
-    pad_options = [pad_length, 'nextpow2', 'maxperlen']
+    pad_options = [pad_length, "nextpow2", "maxperlen"]
     for pad in pad_options:
         method_call(pad=pad)
 
     # test invalid pads
     try:
-        method_call(pad=-0.1) # trials should be longer than 0.1 seconds
+        method_call(pad=-0.1)  # trials should be longer than 0.1 seconds
     except SPYValueError as err:
-        assert 'pad' in str(err)
-        assert 'expected value to be greater' in str(err)
+        assert "pad" in str(err)
+        assert "expected value to be greater" in str(err)
 
     try:
-        method_call(pad='IamNoPad')
+        method_call(pad="IamNoPad")
     except SPYValueError as err:
-        assert 'Invalid value of `pad`' in str(err)
-        assert 'nextpow2' in str(err)
+        assert "Invalid value of `pad`" in str(err)
+        assert "nextpow2" in str(err)
 
     try:
         method_call(pad=np.array([1000]))
     except SPYValueError as err:
-        assert 'Invalid value of `pad`' in str(err)
-        assert 'nextpow2' in str(err)
+        assert "Invalid value of `pad`" in str(err)
+        assert "nextpow2" in str(err)
 
 
 def run_polyremoval_test(method_call):
     """
     The callable should test a solution and support
     a single keyword argument `polyremoval`
     """
@@ -56,26 +58,26 @@
     for poly in poly_options:
         method_call(polyremoval=poly)
 
     # test invalid polyremoval options
     try:
         method_call(polyremoval=2)
     except SPYValueError as err:
-        assert 'polyremoval' in str(err)
-        assert 'expected value to be greater' in str(err)
+        assert "polyremoval" in str(err)
+        assert "expected value to be greater" in str(err)
 
     try:
-        method_call(polyremoval='IamNoPad')
+        method_call(polyremoval="IamNoPad")
     except SPYTypeError as err:
-        assert 'Wrong type of `polyremoval`' in str(err)
+        assert "Wrong type of `polyremoval`" in str(err)
 
     try:
         method_call(polyremoval=np.array([1000]))
     except SPYTypeError as err:
-        assert 'Wrong type of `polyremoval`' in str(err)
+        assert "Wrong type of `polyremoval`" in str(err)
 
 
 def mk_selection_dicts(nTrials, nChannels, toi_min, toi_max, min_len=0.25):
 
     """
     Takes 5 numbers, the last three descibing a `latency` time-interval
     and creates cartesian product like `select` keyword
@@ -95,50 +97,66 @@
     assert (toi_max - toi_min) > 0.25
 
     # create 1 random trial and channel selections
     trials, channels = [], []
     for _ in range(1):
 
         sizeTr = np.random.randint(10, nTrials + 1)
-        trials.append(list(np.random.choice(
-            nTrials, size=sizeTr
-        )
-        ))
+        trials.append(list(np.random.choice(nTrials, size=sizeTr)))
 
         sizeCh = np.random.randint(2, nChannels + 1)
-        channels.append(['channel' + str(i + 1)
-                         for i in
-                         np.random.choice(
-                             nChannels, size=sizeCh, replace=False)])
+        channels.append(
+            ["channel" + str(i + 1) for i in np.random.choice(nChannels, size=sizeCh, replace=False)]
+        )
 
     # 1 random toilim
     toilims = []
     while len(toilims) < 1:
 
         toil = np.sort(np.random.rand(2)) * (toi_max - toi_min) + toi_min
         # at least min_len (250ms)
         if np.diff(toil) < min_len:
             continue
         else:
             toilims.append(toil)
 
     # combinatorics of all selection options
     # order matters to assign the selection dict keys!
-    toilim_combinations = itertools.product(trials,
-                                            channels,
-                                            toilims)
+    toilim_combinations = itertools.product(trials, channels, toilims)
 
     selections = []
     for comb in toilim_combinations:
 
         sel_dct = {}
-        sel_dct['trials'] = comb[0]
-        sel_dct['channel'] = comb[1]
-        sel_dct['latency'] = comb[2]
+        sel_dct["trials"] = comb[0]
+        sel_dct["channel"] = comb[1]
+        sel_dct["latency"] = comb[2]
         selections.append(sel_dct)
 
     return selections
 
+
 def teardown():
     """Cleanup to run at the end of a set of tests, typically at the end of a Test class."""
     # Close matplotlib plot windows:
-    plt.close('all')
+    try:
+        plt.close("all")
+    except:
+        pass
+
+
+def get_file_from_anywhere(possible_locations):
+    """
+    Helper function to get a file from a list of possible locations.
+    Useful to run tests on different systems. If you do not have access
+    to the ESI cluster, this allows you to still run tests on your local
+    machine, all you need to do is copy the required files to your local
+    machine and add the path to the list of possible locations.
+
+    Parameters
+    ----------
+    possible_locations : list of strings, will be expanded with ```os.path.expanduser``` so it is fine to give somehthing like '~/file.txt'.
+    """
+    for loc in possible_locations:
+        if os.path.isfile(expanduser(loc)):
+            return loc
+    return None
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/local_spy.py` & `esi_syncopy-2023.7/syncopy/tests/local_spy.py`

 * *Files 5% similar despite different names*

```diff
@@ -21,39 +21,45 @@
 
     nSamples = 1000
     fs = 500
 
     trls = []
     AdjMat = np.zeros((2, 2))
     # coupling from 0 to 1
-    AdjMat[0, 1] = .15
-    alphas = [.55, -.8]
-    adata = synthdata.ar2_network(nTrials, samplerate=fs,
-                                  AdjMat=AdjMat,
-                                  nSamples=nSamples,
-                                  alphas=alphas)
-    adata += synthdata.ar2_network(nTrials, AdjMat=np.zeros((2, 2)),
-                                   samplerate=fs,
-                                   nSamples=nSamples,
-                                   alphas=[0.9, 0])
+    AdjMat[0, 1] = 0.15
+    alphas = [0.55, -0.8]
+    adata = synthdata.ar2_network(nTrials, samplerate=fs, AdjMat=AdjMat, nSamples=nSamples, alphas=alphas)
+    adata += synthdata.ar2_network(
+        nTrials,
+        AdjMat=np.zeros((2, 2)),
+        samplerate=fs,
+        nSamples=nSamples,
+        alphas=[0.9, 0],
+    )
 
     spec = spy.freqanalysis(adata, tapsmofrq=2, keeptrials=False)
     foi = np.linspace(40, 160, 25)
-    coh = spy.connectivityanalysis(adata, method='coh', tapsmofrq=5)
+    coh = spy.connectivityanalysis(adata, method="coh", tapsmofrq=5)
 
     # show new plotting
     # adata.singlepanelplot(trials=12, toilim=[0, 0.35])
 
     # mtmfft spectrum
     # spec.singlepanelplot()
     # coh.singlepanelplot(channel_i=0, channel_j=1)
 
-    specf2 = spy.freqanalysis(adata, tapsmofrq=2, keeptrials=False, foi=foi,
-                              output="fooof_peaks", fooof_opt={'max_n_peaks': 2})
+    specf2 = spy.freqanalysis(
+        adata,
+        tapsmofrq=2,
+        keeptrials=False,
+        foi=foi,
+        output="fooof_peaks",
+        fooof_opt={"max_n_peaks": 2},
+    )
 
     # print("Start: Testing parallel computation of mtmfft")
     # spec4 = spy.freqanalysis(adata, tapsmofrq=2, keeptrials=True, foi=foi, parallel=True, output="pow")
     # print("End: Testing parallel computation of mtmfft")
 
-    #spec.singlepanelplot()
-    #specf.singlepanelplot()
-    #specf2.singlepanelplot()S
+    # spec.singlepanelplot()
+    # specf.singlepanelplot()
+    # specf2.singlepanelplot()S
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/misc.py` & `esi_syncopy-2023.7/syncopy/tests/misc.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 import numpy as np
 import dask.distributed as dd
 
 # Local imports
 from syncopy.datatype import AnalogData
 from syncopy.shared.filetypes import _data_classname_to_extension, FILE_EXT
 from syncopy import __plt__, __acme__
+
 if __plt__:
     import matplotlib.pyplot as plt
     from matplotlib.backends.backend_agg import FigureCanvasAgg
 if __acme__:
     import dask.distributed as dd
 
 
@@ -31,17 +32,21 @@
     """
 
     # If we're not running on Windows abort
     if sys.platform != "win32":
         return False
 
     # Use the windows management instrumentation command-line to extract machine manufacturer
-    out, err = subprocess.Popen("wmic computersystem get manufacturer",
-                                stdout=subprocess.PIPE, stderr=subprocess.PIPE,
-                                text=True, shell=True).communicate()
+    out, err = subprocess.Popen(
+        "wmic computersystem get manufacturer",
+        stdout=subprocess.PIPE,
+        stderr=subprocess.PIPE,
+        text=True,
+        shell=True,
+    ).communicate()
 
     # If the vendor name contains any "virtual"-flavor, we're probably running
     # in a VM - if the above command triggered an error, abort
     if len(err) == 0:
         vendor = out.split()[1].lower()
         vmlist = ["vmware", "virtual", "virtualbox", "vbox", "qemu"]
         return any([virtual in vendor for virtual in vmlist])
@@ -52,25 +57,36 @@
 def is_slurm_node():
     """
     Returns `True` if code is running on a SLURM-managed cluster node, `False`
     otherwise
     """
 
     # Simply test if the srun command is available
-    out, err = subprocess.Popen("srun --version",
-                                stdout=subprocess.PIPE, stderr=subprocess.PIPE,
-                                text=True, shell=True).communicate()
+    out, err = subprocess.Popen(
+        "srun --version",
+        stdout=subprocess.PIPE,
+        stderr=subprocess.PIPE,
+        text=True,
+        shell=True,
+    ).communicate()
     if len(out) > 0:
         return True
     else:
         return False
 
 
-def generate_artificial_data(nTrials=2, nChannels=2, equidistant=True, seed=42,
-                             overlapping=False, inmemory=True, dimord="default"):
+def generate_artificial_data(
+    nTrials=2,
+    nChannels=2,
+    equidistant=True,
+    seed=42,
+    overlapping=False,
+    inmemory=True,
+    dimord="default",
+):
     """
     Create :class:`~syncopy.AnalogData` object with synthetic harmonic signal(s)
 
     Parameters
     ----------
     nTrials : int
         Number of trials to populate synthetic data object with
@@ -179,61 +195,65 @@
     rng = np.random.default_rng(seed)
 
     # Either construct the full data array in memory using tiling or create
     # an HDF5 container in `__storage__` and fill it trial-by-trial
     # NOTE: use `swapaxes` here to ensure two objects created w/same seed really
     # are affected w/identical additive noise patterns, no matter their respective
     # `dimord`.
-    out = AnalogData(samplerate=1/dt, dimord=dimord)
+    out = AnalogData(samplerate=1 / dt, dimord=dimord)
     if inmemory:
         idx[timeAxis] = nTrials
         sig = np.tile(sig, idx)
         shp = [slice(None), slice(None)]
         for iTrial in range(nTrials):
-            shp[timeAxis] = slice(iTrial*t.size, (iTrial + 1)*t.size)
+            shp[timeAxis] = slice(iTrial * t.size, (iTrial + 1) * t.size)
             noise = rng.standard_normal((t.size, nChannels)).astype(sig.dtype) * 0.5
             sig[tuple(shp)] += np.swapaxes(noise, timeAxis, 0)
         out.data = sig
     else:
         with h5py.File(out.filename, "w") as h5f:
             shp = list(sig.shape)
             shp[timeAxis] *= nTrials
             dset = h5f.create_dataset("data", shape=tuple(shp), dtype=sig.dtype)
             shp = [slice(None), slice(None)]
             for iTrial in range(nTrials):
-                shp[timeAxis] = slice(iTrial*t.size, (iTrial + 1)*t.size)
+                shp[timeAxis] = slice(iTrial * t.size, (iTrial + 1) * t.size)
                 noise = rng.standard_normal((t.size, nChannels)).astype(sig.dtype) * 0.5
                 dset[tuple(shp)] = sig + np.swapaxes(noise, timeAxis, 0)
                 dset.flush()
         out.data = h5py.File(out.filename, "r+")["data"]
 
     # Define by-trial offsets to generate (non-)equidistant/(non-)overlapping trials
-    trialdefinition = np.zeros((nTrials, 3), dtype='int')
+    trialdefinition = np.zeros((nTrials, 3), dtype="int")
     if equidistant:
         equiOffset = 0
         if overlapping:
             equiOffset = 100
         offsets = np.full((nTrials,), equiOffset, dtype=sig.dtype)
     else:
-        offsets = rng.integers(low=int(0.1*t.size), high=int(0.2*t.size), size=(nTrials,))
+        offsets = rng.integers(low=int(0.1 * t.size), high=int(0.2 * t.size), size=(nTrials,))
 
     # Using generated offsets, construct trialdef array and make sure initial
     # and end-samples are within data bounds (only relevant if overlapping
     # trials are built)
-    shift = (-1)**(not overlapping)
+    shift = (-1) ** (not overlapping)
     for iTrial in range(nTrials):
-        trialdefinition[iTrial, :] = np.array([iTrial*t.size - shift*offsets[iTrial],
-                                               (iTrial + 1)*t.size + shift*offsets[iTrial],
-                                               -1000])
+        trialdefinition[iTrial, :] = np.array(
+            [
+                iTrial * t.size - shift * offsets[iTrial],
+                (iTrial + 1) * t.size + shift * offsets[iTrial],
+                -1000,
+            ]
+        )
     if equidistant:
         trialdefinition[0, :2] += equiOffset
         trialdefinition[-1, :2] -= equiOffset
     else:
         trialdefinition[0, 0] = 0
-        trialdefinition[-1, 1] = nTrials*t.size
+        trialdefinition[-1, 1] = nTrials * t.size
     out.definetrial(trialdefinition)
 
     return out
 
 
 def construct_spy_filename(basepath, obj):
     basename = os.path.split(basepath)[1]
@@ -297,12 +317,14 @@
     if isinstance(testcluster, dd.LocalCluster):
         # client.restart()
         client = dd.get_client()
         client.close()
         time.sleep(1.0)
         client = dd.Client(testcluster)
         waiting = 0
-        while len([w["memory_limit"] for w in testcluster.scheduler_info["workers"].values()]) == 0 \
-            and waiting < timeout:
-                time.sleep(1.0)
-                waiting += 1
+        while (
+            len([w["memory_limit"] for w in testcluster.scheduler_info["workers"].values()]) == 0
+            and waiting < timeout
+        ):
+            time.sleep(1.0)
+            waiting += 1
     return
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/run_tests.cmd` & `esi_syncopy-2023.7/syncopy/tests/run_tests.cmd`

 * *Files identical despite different names*

### Comparing `esi_syncopy-2023.5/syncopy/tests/run_tests.sh` & `esi_syncopy-2023.7/syncopy/tests/run_tests.sh`

 * *Files identical despite different names*

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_attach_dataset.py` & `esi_syncopy-2023.7/syncopy/tests/test_attach_dataset.py`

 * *Files 1% similar despite different names*

```diff
@@ -54,29 +54,27 @@
 
         spkd._register_dataset("dset_mean", extra_data2)
         assert hasattr(spkd, "_dset_mean")
         assert isinstance(spkd._dset_mean, h5py.Dataset)
         assert isinstance(spkd._dset_mean.file, h5py.File)
         assert np.array_equal(spkd._dset_mean[()], extra_data2)
 
-
-
-
     def test_destruction(self):
         """
         Test destructor: there should be no exceptions/errors on destruction.
         """
+
         def some_local_func():
             spkd = get_spike_data()
             extra_data = np.zeros((3, 3), dtype=np.float64)
             spkd._register_dataset("dset_mean", extra_data)
             # Let spkd get out of scope to call destructor.
 
         some_local_func()
-        assert not 'spkd' in locals()
+        assert not "spkd" in locals()
 
     def test_comparison_with_and_without_extra_dset(self):
         """
         Test comparison operator: if one instance has an extra dataset, they should not be equal.
         """
         spkd1 = get_spike_data()
         spkd2 = spkd1.copy()
@@ -130,15 +128,17 @@
         assert spkd1 != spkd2
 
         # Note that copies, with *identical* extra seq data attached to them after copying,
         # are also not equal. This is due to the implementation of h5py.Dataset equality in h5py, which
         # is based on the `id` of the dataset. See https://github.com/h5py/h5py/blob/master/h5py/_hl/base.py#L348
         # We show this here, and one should keep it in mind:
         spkd3._register_dataset("dset_mean", extra_data1)
-        assert spkd3 != spkd1  # Even though they are copies, with identical `np.ndarrays` attached as `h5py.Datasets`!
+        assert (
+            spkd3 != spkd1
+        )  # Even though they are copies, with identical `np.ndarrays` attached as `h5py.Datasets`!
 
     def test_detach(self):
         """
         Test that we can attach and detach an extra sequential dataset to Syncopy SpikeData Object.
         """
         spkd = get_spike_data()
 
@@ -195,42 +195,43 @@
             # Let it get out of scope to call destructor.
             del adt
             del adt2
 
         extra_data1 = np.zeros((3, 3), dtype=np.float64)
         extra_data2 = np.zeros((3, 3), dtype=np.float64) + 2
         some_local_func(extra_data1, extra_data2)
-        assert 'adt' not in locals()
+        assert "adt" not in locals()
 
         # repeat with hdf5 datasets
         tfile1 = tempfile.NamedTemporaryFile(suffix=".h5", delete=False)
         tfile1.close()
         tfile2 = tempfile.NamedTemporaryFile(suffix=".h5", delete=False)
         tfile2.close()
-        with h5py.File(tfile1.name, 'w') as file1:
+        with h5py.File(tfile1.name, "w") as file1:
             extra_ds1 = file1.create_dataset("d1", extra_data1.shape)
             extra_ds1[()] = extra_data1
 
-            with h5py.File(tfile2.name, 'w') as file2:
+            with h5py.File(tfile2.name, "w") as file2:
                 extra_ds2 = file2.create_dataset("d2", extra_data2.shape)
                 extra_ds2[()] = extra_data2
 
                 some_local_func(extra_ds1, extra_ds2)
-                assert 'adt' not in locals()
+                assert "adt" not in locals()
 
         tfile1.close()
         os.unlink(tfile1.name)
         tfile2.close()
         os.unlink(tfile2.name)
 
     def test_attach_None_to_analog_data(self):
         """
         Test that we can run attach, update and detach an extra sequential
         dataset with None data to Syncopy AnalogData Object.
         """
+
         def some_local_func():
             adt = _get_fooof_signal()
             assert isinstance(adt, spy.AnalogData)
 
             # Copying
             adt2 = adt.copy()
 
@@ -251,15 +252,15 @@
             adt._unregister_dataset("dset_mean", del_from_file=True)
             assert not hasattr(adt, "_dset_mean")
             assert not "dset_mean" in h5py.File(adt.filename, "r").keys()
             assert "data" in h5py.File(adt.filename, "r").keys()
             # Let it get out of scope to call destructor.
 
         some_local_func()
-        assert not 'adt' in locals()
+        assert not "adt" in locals()
 
     def test_run_psth_with_attached_dset(self):
         """
         Test that we can run a cF on a Syncopy Data Object without any
         side effects, i.e., the cF should just run and leave the extra dataset alone.
 
         We do NOT expect the cF to interact with the extra dataset in any way! This also
@@ -268,17 +269,15 @@
         in 'process_metadata'.
         """
         spkd = get_spike_data()
 
         extra_data = np.zeros((3, 3), dtype=np.float64)
         spkd._register_dataset("dset_mean", extra_data)
 
-        counts = spy.spike_psth(spkd,
-                                self.cfg,
-                                keeptrials=True)
+        counts = spy.spike_psth(spkd, self.cfg, keeptrials=True)
 
         # Make sure we did not interfere with the PSTH computation.
         assert np.allclose(np.diff(counts.time[0]), self.cfg.binsize)
 
         # Make sure the extra data set is there.
         assert hasattr(spkd, "_dset_mean")
 
@@ -298,27 +297,26 @@
 
         extra_data_diff_shape = np.zeros((3, 3, 5), dtype=np.float64)
         with pytest.raises(SPYValueError, match="dataset with shape"):
             spkd._register_dataset("dset_mean", extra_data_diff_shape)
 
         # Show that we can delete the old one, and attach a new one with different shape.
         spkd._unregister_dataset("dset_mean")
-        spkd._register_dataset("dset_mean", extra_data_diff_type) # Fine this time, it's new.
+        spkd._register_dataset("dset_mean", extra_data_diff_type)  # Fine this time, it's new.
 
     def test_save_load_unregister(self):
         """Test that saving and loading with attached seq datasets works.
-           Also tests that the attached datasets gets deleted from the
-           backing HDF5 file when calling `_unregister_dataset()`.
+        Also tests that the attached datasets gets deleted from the
+        backing HDF5 file when calling `_unregister_dataset()`.
         """
 
         spkd = get_spike_data()
         extra_data = np.zeros((3, 3), dtype=np.float64)
         spkd._register_dataset("dset_mean", extra_data)
 
-
         tfile0 = tempfile.NamedTemporaryFile(suffix=".spike", delete=True)
         tfile0.close()
         # Test save and load.
         tmp_spy_filename = tfile0.name
         spy.save(spkd, filename=tmp_spy_filename)
         spkd2 = spy.load(filename=tmp_spy_filename)
         assert isinstance(spkd2._dset_mean, h5py.Dataset)
@@ -333,15 +331,15 @@
 
         # repeat with hdf5 datasets
         tfile1 = tempfile.NamedTemporaryFile(suffix=".spike", delete=False)
         tfile1.close()
         tfile2 = tempfile.NamedTemporaryFile(suffix=".spike", delete=True)
         tfile2.close()
 
-        file1 = h5py.File(tfile1.name, 'w')
+        file1 = h5py.File(tfile1.name, "w")
         extra_dset = file1.create_dataset("d1", extra_data.shape)
         extra_dset[()] = extra_data
 
         spkd._register_dataset("dset_mean", extra_dset)
 
         # Test save and load.
         tmp_spy_filename = tfile2.name
@@ -352,9 +350,9 @@
 
         # Test delete/unregister.
         spkd2._unregister_dataset("dset_mean")
         tfile2.close()
         assert "dset_mean" not in h5py.File(tmp_spy_filename, mode="r").keys()
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestAttachDataset()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_basedata.py` & `esi_syncopy-2023.7/syncopy/tests/test_basedata.py`

 * *Files 3% similar despite different names*

```diff
@@ -18,64 +18,83 @@
 from syncopy.tests.misc import is_win_vm, is_slurm_node
 
 # Construct decorators for skipping certain tests
 skip_in_vm = pytest.mark.skipif(is_win_vm(), reason="running in Win VM")
 skip_in_slurm = pytest.mark.skipif(is_slurm_node(), reason="running on cluster node")
 
 # Collect all supported binary arithmetic operators
-arithmetics = [lambda x, y: x + y,
-               lambda x, y: x - y,
-               lambda x, y: x * y,
-               lambda x, y: x / y,
-               lambda x, y: x ** y]
+arithmetics = [
+    lambda x, y: x + y,
+    lambda x, y: x - y,
+    lambda x, y: x * y,
+    lambda x, y: x / y,
+    lambda x, y: x**y,
+]
 
 
 # Test BaseData methods that work identically for all regular classes
-class TestBaseData():
+class TestBaseData:
 
     # Allocate test-datasets for AnalogData, SpectralData, SpikeData and EventData objects
     nChannels = 10
     nSamples = 30
     nTrials = 5
     nFreqs = 15
     nSpikes = 50
     data = {}
     trl = {}
     samplerate = 1.0
 
     # Generate 2D array simulating an AnalogData array
     data["AnalogData"] = np.arange(1, nChannels * nSamples + 1).reshape(nSamples, nChannels)
-    trl["AnalogData"] = np.vstack([np.arange(0, nSamples, 5),
-                                   np.arange(5, nSamples + 5, 5),
-                                   np.ones((int(nSamples / 5), )),
-                                   np.ones((int(nSamples / 5), )) * np.pi]).T
+    trl["AnalogData"] = np.vstack(
+        [
+            np.arange(0, nSamples, 5),
+            np.arange(5, nSamples + 5, 5),
+            np.ones((int(nSamples / 5),)),
+            np.ones((int(nSamples / 5),)) * np.pi,
+        ]
+    ).T
 
     # Generate a 4D array simulating a SpectralData array (`nTrials` stands in for tapers)
-    data["SpectralData"] = np.arange(1, nChannels * nSamples * nTrials * nFreqs + 1).reshape(nSamples, nTrials, nFreqs, nChannels)
+    data["SpectralData"] = np.arange(1, nChannels * nSamples * nTrials * nFreqs + 1).reshape(
+        nSamples, nTrials, nFreqs, nChannels
+    )
     trl["SpectralData"] = trl["AnalogData"]
 
     # Generate a 4D array simulating a CorssSpectralData array
-    data["CrossSpectralData"] = np.arange(1, nChannels * nChannels * nSamples * nFreqs + 1).reshape(nSamples, nFreqs, nChannels, nChannels)
+    data["CrossSpectralData"] = np.arange(1, nChannels * nChannels * nSamples * nFreqs + 1).reshape(
+        nSamples, nFreqs, nChannels, nChannels
+    )
     trl["CrossSpectralData"] = trl["AnalogData"]
 
     # Use a fixed random number generator seed to simulate a 2D SpikeData array
     seed = np.random.RandomState(13)
-    data["SpikeData"] = np.vstack([seed.choice(nSamples, size=nSpikes),
-                                   seed.choice(nChannels, size=nSpikes),
-                                   seed.choice(int(nChannels / 2), size=nSpikes)]).T.astype(int)
+    data["SpikeData"] = np.vstack(
+        [
+            seed.choice(nSamples, size=nSpikes),
+            seed.choice(nChannels, size=nSpikes),
+            seed.choice(int(nChannels / 2), size=nSpikes),
+        ]
+    ).T.astype(int)
     trl["SpikeData"] = trl["AnalogData"]
 
     # Use a simple binary trigger pattern to simulate EventData
-    data["EventData"] = np.vstack([np.arange(0, nSamples, 5),
-                                   np.zeros((int(nSamples / 5), ))]).T.astype(int)
+    data["EventData"] = np.vstack([np.arange(0, nSamples, 5), np.zeros((int(nSamples / 5),))]).T.astype(int)
     data["EventData"][1::2, 1] = 1
     trl["EventData"] = trl["AnalogData"]
 
     # Define data classes to be used in tests below
-    classes = ["AnalogData", "SpectralData", "CrossSpectralData", "SpikeData", "EventData"]
+    classes = [
+        "AnalogData",
+        "SpectralData",
+        "CrossSpectralData",
+        "SpikeData",
+        "EventData",
+    ]
 
     # Allocation to `data` property is tested with all members of `classes`
     def test_data_alloc(self):
         with tempfile.TemporaryDirectory() as tdir:
             hname = os.path.join(tdir, "dummy.h5")
 
             for dclass in self.classes:
@@ -115,25 +134,26 @@
                 with pytest.raises(SPYValueError):
                     getattr(spd, dclass)(data=dset)
 
                 # ensure synthetic data allocation via list of arrays works
                 dummy = getattr(spd, dclass)(data=[self.data[dclass], self.data[dclass]])
                 assert len(dummy.trials) == 2
 
-                dummy = getattr(spd, dclass)(data=[self.data[dclass], self.data[dclass]],
-                                             samplerate=10.0)
+                dummy = getattr(spd, dclass)(data=[self.data[dclass], self.data[dclass]], samplerate=10.0)
                 assert len(dummy.trials) == 2
                 assert dummy.samplerate == 10
 
                 if any(["ContinuousData" in str(base) for base in self.__class__.__mro__]):
                     nChan = self.data[dclass].shape[dummy.dimord.index("channel")]
-                    dummy = getattr(spd, dclass)(data=[self.data[dclass], self.data[dclass]],
-                                                 channel=['label'] * nChan)
+                    dummy = getattr(spd, dclass)(
+                        data=[self.data[dclass], self.data[dclass]],
+                        channel=["label"] * nChan,
+                    )
                     assert len(dummy.trials) == 2
-                    assert np.array_equal(dummy.channel, np.array(['label'] * nChan))
+                    assert np.array_equal(dummy.channel, np.array(["label"] * nChan))
 
                 # the most egregious input errors are caught by `array_parser`; only
                 # test list-routine-specific stuff: complex/real mismatch
                 with pytest.raises(SPYValueError) as spyval:
                     getattr(spd, dclass)(data=[self.data[dclass], np.complex64(self.data[dclass])])
                     assert "same numeric type (real/complex)" in str(spyval.value)
 
@@ -143,25 +163,24 @@
 
             time.sleep(0.01)
             del dummy
 
     # Assignment of trialdefinition array is tested with all members of `classes`
     def test_trialdef(self):
         for dclass in self.classes:
-            dummy = getattr(spd, dclass)(self.data[dclass],
-                                         samplerate=self.samplerate)
+            dummy = getattr(spd, dclass)(self.data[dclass], samplerate=self.samplerate)
             dummy.trialdefinition = self.trl[dclass]
             assert np.array_equal(dummy.sampleinfo, self.trl[dclass][:, :2])
             assert np.array_equal(dummy._t0, self.trl[dclass][:, 2])
             assert np.array_equal(dummy.trialinfo.flatten(), self.trl[dclass][:, 3])
 
     def test_trials_property(self):
 
         # 3 trials, trial index = data values
-        data = AnalogData([i * np.ones((2,2)) for i in range(3)], samplerate=1)
+        data = AnalogData([i * np.ones((2, 2)) for i in range(3)], samplerate=1)
 
         # single index access
         assert np.all(data.trials[0] == 0)
         assert np.all(data.trials[1] == 1)
         assert np.all(data.trials[2] == 2)
 
         # iterator
@@ -173,26 +192,26 @@
         data.selectdata(trials=[0, 2], inplace=True)
         all_selected_trials = [trl for trl in data.selection.trials]
         assert data.selection.trial_ids == [0, 2]
         assert len(all_selected_trials) == 2
         assert all([np.all(data.selection.trials[i] == i) for i in data.selection.trial_ids])
 
         # check that non-existing trials get catched
-        with pytest.raises(SPYValueError, match='existing trials'):
+        with pytest.raises(SPYValueError, match="existing trials"):
             data.trials[999]
         # selections have absolute trial indices!
-        with pytest.raises(SPYValueError, match='existing trials'):
+        with pytest.raises(SPYValueError, match="existing trials"):
             data.selection.trials[1]
 
         # check that invalid trial indexing gets catched
-        with pytest.raises(SPYTypeError, match='trial index'):
+        with pytest.raises(SPYTypeError, match="trial index"):
             data.trials[range(4)]
-        with pytest.raises(SPYTypeError, match='trial index'):
+        with pytest.raises(SPYTypeError, match="trial index"):
             data.trials[2:3]
-        with pytest.raises(SPYTypeError, match='trial index'):
+        with pytest.raises(SPYTypeError, match="trial index"):
             data.trials[np.arange(3)]
 
     # Test ``_gen_filename`` with `AnalogData` only - method is independent from concrete data object
     def test_filename(self):
         # ensure we're salting sufficiently to create at least `numf`
         # distinct pseudo-random filenames in `__storage__`
         numf = 1000
@@ -211,20 +230,18 @@
 
                 hname = os.path.join(tdir, "dummy.h5")
                 h5f = h5py.File(hname, mode="w")
                 h5f.create_dataset("data", data=self.data[dclass])
                 h5f.close()
 
                 # hash-matching of shallow-copied HDF5 dataset
-                dummy = getattr(spd, dclass)(data=h5py.File(hname, 'r')["data"],
-                                             samplerate=self.samplerate)
+                dummy = getattr(spd, dclass)(data=h5py.File(hname, "r")["data"], samplerate=self.samplerate)
 
                 # attach some aux. info
-                dummy.info = {'sth': 4, 'important': [1, 2],
-                              'to-remember': {'v1': 2}}
+                dummy.info = {"sth": 4, "important": [1, 2], "to-remember": {"v1": 2}}
 
                 # test integrity of deep-copy
                 dummy.trialdefinition = self.trl[dclass]
                 dummy2 = dummy.copy()
                 assert dummy2.filename != dummy.filename
                 assert np.array_equal(dummy.sampleinfo, dummy2.sampleinfo)
                 assert np.array_equal(dummy._t0, dummy2._t0)
@@ -245,34 +262,61 @@
 
         # Define list of classes arithmetic ops should and should not work with
         continuousClasses = ["AnalogData", "SpectralData", "CrossSpectralData"]
         discreteClasses = ["SpikeData", "EventData"]
 
         # Illegal classes for arithmetics
         for dclass in discreteClasses:
-            dummy = getattr(spd, dclass)(self.data[dclass],
-                                         trialdefinition=self.trl[dclass],
-                                         samplerate=self.samplerate)
+            dummy = getattr(spd, dclass)(
+                self.data[dclass],
+                trialdefinition=self.trl[dclass],
+                samplerate=self.samplerate,
+            )
             for operation in arithmetics:
                 with pytest.raises(SPYTypeError) as spytyp:
                     operation(dummy, 2)
-                    assert "Wrong type of base: expected `AnalogData`, `SpectralData` or `CrossSpectralData`" in str(spytyp.value)
+                    assert (
+                        "Wrong type of base: expected `AnalogData`, `SpectralData` or `CrossSpectralData`"
+                        in str(spytyp.value)
+                    )
 
         # Now, test basic error handling for allowed classes
         for dclass in continuousClasses:
-            dummy = getattr(spd, dclass)(self.data[dclass],
-                                         samplerate=self.samplerate)
+            dummy = getattr(spd, dclass)(self.data[dclass], samplerate=self.samplerate)
             dummy.trialdefinition = self.trl[dclass]
             otherClass = list(set(self.classes).difference([dclass]))[0]
-            other = getattr(spd, otherClass)(self.data[otherClass],
-                                             samplerate=self.samplerate)
+            other = getattr(spd, otherClass)(self.data[otherClass], samplerate=self.samplerate)
             other.trialdefinition = self.trl[dclass]
             complexArr = np.complex64(dummy.trials[0])
             complexNum = 3 + 4j
 
+            # -- test that NumPy broadcasting rules are respected --
+
+            trl_shape = dummy.trials[0].shape
+            # 1st dimension is different from last in `self.data`
+            assert trl_shape[0] != trl_shape[-1]
+
+            vec_ok = np.zeros(trl_shape[-1])
+            res = dummy * vec_ok
+            # everything zeroed
+            assert np.allclose(res.data[()], 0)
+
+            with pytest.raises(SPYValueError, match="Invalid value of `operand`"):
+                vec_not_ok = np.zeros(trl_shape[0])
+                _ = dummy * vec_not_ok
+
+            # array multiplication works as usual trial-by-trial
+            arr = np.zeros(np.prod(trl_shape)).reshape(trl_shape)
+            res = dummy * arr
+
+            # everything zeroed
+            assert np.allclose(res.data[()], 0)
+
+            # -- test exceptions  --
+
             # Start w/the one operator that does not handle zeros well...
             with pytest.raises(SPYValueError) as spyval:
                 _ = dummy / 0
                 assert "expected non-zero scalar for division" in str(spyval.value)
 
             # Go through all supported operators and try to sabotage them
             for operation in arithmetics:
@@ -298,29 +342,27 @@
                     err = "expected Syncopy {} object found {}"
                     assert err.format(dclass, otherClass) in str(spytyp.value)
 
         # Next, validate proper functionality of `==` operator for Syncopy objects
         for dclass in self.classes:
 
             # Start simple compare obj to itself, to empty object and compare two empties
-            dummy = getattr(spd, dclass)(self.data[dclass],
-                                         samplerate=self.samplerate)
+            dummy = getattr(spd, dclass)(self.data[dclass], samplerate=self.samplerate)
             dummy.trialdefinition = self.trl[dclass]
             assert dummy == dummy
             assert dummy != getattr(spd, dclass)()
             assert getattr(spd, dclass)() == getattr(spd, dclass)()
 
             # Basic type mismatch
             assert dummy != complexArr
             assert dummy != complexNum
 
             # Two differing Syncopy object classes
             otherClass = list(set(self.classes).difference([dclass]))[0]
-            other = getattr(spd, otherClass)(self.data[otherClass],
-                                             samplerate=self.samplerate)
+            other = getattr(spd, otherClass)(self.data[otherClass], samplerate=self.samplerate)
             other.trialdefinition = self.trl[otherClass]
             assert dummy != other
 
             dummy3 = dummy.copy()
             assert dummy3 == dummy
 
             # Ensure differing samplerate evaluates to `False`
@@ -347,54 +389,57 @@
             # Different trials
             dummy3 = dummy.selectdata(trials=list(range(len(dummy.trials) - 1)))
             assert dummy3 != dummy
 
             # Different trial offsets
             trl = self.trl[dclass]
             trl[:, 1] -= 1
-            dummy3 = getattr(spd, dclass)(self.data[dclass],
-                                          samplerate=self.samplerate)
+            dummy3 = getattr(spd, dclass)(self.data[dclass], samplerate=self.samplerate)
             dummy3.trialdefinition = trl
             assert dummy3 != dummy
 
             # Different trial annotations
             trl = self.trl[dclass]
             trl[:, -1] = np.sqrt(2)
-            dummy3 = getattr(spd, dclass)(self.data[dclass],
-                                          samplerate=self.samplerate)
+            dummy3 = getattr(spd, dclass)(self.data[dclass], samplerate=self.samplerate)
             dummy3.trialdefinition = trl
             assert dummy3 != dummy
 
             # Difference in actual numerical data
             dummy3 = dummy.copy()
             for dsetName in dummy3._hdfFileDatasetProperties:
                 if dsetName == "data":
                     getattr(dummy3, dsetName)[0, 0] = -99
             assert dummy3.data != dummy.data
 
             del dummy, dummy3, other
 
         # Same objects but different dimords: `ContinuousData`` children
         for dclass in continuousClasses:
-            dummy = getattr(spd, dclass)(self.data[dclass],
-                                         samplerate=self.samplerate)
+            dummy = getattr(spd, dclass)(self.data[dclass], samplerate=self.samplerate)
             dummy.trialdefinition = self.trl[dclass]
-            ymmud = getattr(spd, dclass)(self.data[dclass].T,
-                                         dimord=dummy.dimord[::-1],
-                                         samplerate=self.samplerate)
+            ymmud = getattr(spd, dclass)(
+                self.data[dclass].T,
+                dimord=dummy.dimord[::-1],
+                samplerate=self.samplerate,
+            )
             ymmud.trialdefinition = self.trl[dclass]
             assert dummy != ymmud
 
         # Same objects but different dimords: `DiscreteData` children
         for dclass in discreteClasses:
-            dummy = getattr(spd, dclass)(self.data[dclass],
-                                         trialdefinition=self.trl[dclass],
-                                         samplerate=self.samplerate)
-            ymmud = getattr(spd, dclass)(self.data[dclass],
-                                         dimord=dummy.dimord[::-1],
-                                         trialdefinition=self.trl[dclass],
-                                         samplerate=self.samplerate)
+            dummy = getattr(spd, dclass)(
+                self.data[dclass],
+                trialdefinition=self.trl[dclass],
+                samplerate=self.samplerate,
+            )
+            ymmud = getattr(spd, dclass)(
+                self.data[dclass],
+                dimord=dummy.dimord[::-1],
+                trialdefinition=self.trl[dclass],
+                samplerate=self.samplerate,
+            )
             assert dummy != ymmud
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestBaseData()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_cfg.py` & `esi_syncopy-2023.7/syncopy/tests/test_cfg.py`

 * *Files 11% similar despite different names*

```diff
@@ -13,39 +13,39 @@
 # Local imports
 import syncopy as spy
 
 from syncopy import synthdata
 from syncopy.shared.tools import StructDict
 
 
-availableFrontend_cfgs = {'freqanalysis': {'method': 'mtmconvol', 't_ftimwin': 0.1, 'foi': np.arange(1,60)},
-                          'preprocessing': {'freq': 10, 'filter_class': 'firws', 'filter_type': 'hp'},
-                          'resampledata': {'resamplefs': 125, 'lpfreq': 60},
-                          'connectivityanalysis': {'method': 'coh', 'tapsmofrq': 5},
-                          'selectdata': {'trials': np.array([1, 7, 3]), 'channel': [np.int64(2), 0]}
-                          }
+availableFrontend_cfgs = {
+    "freqanalysis": {"method": "mtmconvol", "t_ftimwin": 0.1, "foi": np.arange(1, 60)},
+    "preprocessing": {"freq": 10, "filter_class": "firws", "filter_type": "hp"},
+    "resampledata": {"resamplefs": 125, "lpfreq": 60},
+    "connectivityanalysis": {"method": "coh", "tapsmofrq": 5},
+    "selectdata": {"trials": np.array([1, 7, 3]), "channel": [np.int64(2), 0]},
+}
 
 
 class TestCfg:
 
     nSamples = 100
     nChannels = 3
     nTrials = 10
     fs = 200
     fNy = fs / 2
 
     # -- use flat white noise as test data --
 
-    adata = synthdata.white_noise(nTrials=nTrials,
-                                  nSamples=nSamples,
-                                  nChannels=nChannels,
-                                  samplerate=fs)
+    adata = synthdata.white_noise(
+        nTrials=nTrials, nSamples=nSamples, nChannels=nChannels, samplerate=fs, seed=42
+    )
 
     # for toi tests, -1s offset
-    time_span = [-.9, -.6]
+    time_span = [-0.9, -0.6]
     flow, fhigh = 0.3 * fNy, 0.4 * fNy
 
     def test_single_frontends(self):
 
         for frontend in availableFrontend_cfgs.keys():
 
             # unwrap cfg into keywords
@@ -54,15 +54,15 @@
             res2 = getattr(spy, frontend)(self.adata, res.cfg)
 
             # same results
             assert np.allclose(res.data[:], res2.data[:])
             assert res.cfg == res2.cfg
 
             # check that it's not just the defaults (mtmfft)
-            if frontend == 'freqanalysis':
+            if frontend == "freqanalysis":
                 res3 = getattr(spy, frontend)(self.adata)
                 assert res.data.shape != res3.data.shape
                 assert res.cfg != res3.cfg
 
     def test_io(self):
 
         for frontend in availableFrontend_cfgs.keys():
@@ -86,77 +86,76 @@
                 assert np.allclose(res.data[:], res2.data[:])
                 assert res.cfg == res2.cfg
 
                 del res, res2
 
     def test_selection(self):
 
-        select = {'latency': self.time_span, 'trials': [1, 2, 3], 'channel': [2, 0]}
+        select = {"latency": self.time_span, "trials": [1, 2, 3], "channel": [2, 0]}
         for frontend in availableFrontend_cfgs.keys():
             # select kw for selectdata makes no direct sense
-            if frontend == 'selectdata':
+            if frontend == "selectdata":
                 continue
-            res = getattr(spy, frontend)(self.adata,
-                                         cfg=availableFrontend_cfgs[frontend],
-                                         select=select)
+            res = getattr(spy, frontend)(self.adata, cfg=availableFrontend_cfgs[frontend], select=select)
 
             # now replay with cfg from preceding frontend call
             res2 = getattr(spy, frontend)(self.adata, res.cfg)
 
             # same results
-            assert 'select' in res.cfg[frontend]
-            assert 'select' in res2.cfg[frontend]
+            assert "select" in res.cfg[frontend]
+            assert "select" in res2.cfg[frontend]
             assert np.allclose(res.data[:], res2.data[:])
             assert res.cfg == res2.cfg
 
     def test_chaining_frontends(self):
 
         # only preprocessing makes sense to chain atm
-        res_pp = spy.preprocessing(self.adata, cfg=availableFrontend_cfgs['preprocessing'])
+        res_pp = spy.preprocessing(self.adata, cfg=availableFrontend_cfgs["preprocessing"])
 
         for frontend in availableFrontend_cfgs.keys():
-            res = getattr(spy, frontend)(res_pp,
-                                         cfg=availableFrontend_cfgs[frontend])
+            res = getattr(spy, frontend)(res_pp, cfg=availableFrontend_cfgs[frontend])
 
             # now replay with cfg from preceding frontend calls
             # note we can use the final results `res.cfg` for both calls!
             res_pp2 = spy.preprocessing(self.adata, res.cfg)
             res2 = getattr(spy, frontend)(res_pp2, res.cfg)
 
             # same results
             assert np.allclose(res.data[:], res2.data[:])
             assert res.cfg == res2.cfg
 
     def test_chaining_frontends_with_fooof_types(self):
 
         # only preprocessing makes sense to chain atm
-        res_pp = spy.preprocessing(self.adata, cfg=availableFrontend_cfgs['preprocessing'])
+        res_pp = spy.preprocessing(self.adata, cfg=availableFrontend_cfgs["preprocessing"])
 
-        frontend = 'freqanalysis'
-        frontend_cfg = {'method': 'mtmfft', 'output': 'fooof', 'foilim': [0.5, 100.]}
+        frontend = "freqanalysis"
+        frontend_cfg = {"method": "mtmfft", "output": "fooof", "foilim": [0.5, 100.0]}
 
-        res = getattr(spy, frontend)(res_pp,
-                                        cfg=frontend_cfg)
+        res = getattr(spy, frontend)(res_pp, cfg=frontend_cfg)
 
         # now replay with cfg from preceding frontend calls
         # note we can use the final results `res.cfg` for both calls!
         res_pp2 = spy.preprocessing(self.adata, res.cfg)
         res2 = getattr(spy, frontend)(res_pp2, res.cfg)
 
         # same results
         assert np.allclose(res.data[:], res2.data[:])
         assert res.cfg == res2.cfg
 
     def test_parallel(self, testcluster):
 
         client = dd.Client(testcluster)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (inspect.ismethod(getattr(self, attr)) and "parallel" not in attr)
+        ]
 
         for test_name in all_tests:
             test_method = getattr(self, test_name)
             test_method()
         client.close()
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestCfg()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_computationalroutine.py` & `esi_syncopy-2023.7/syncopy/tests/test_computationalroutine.py`

 * *Files 13% similar despite different names*

```diff
@@ -52,31 +52,37 @@
         out.trialdefinition = trl
         out.samplerate = data.samplerate
         out.channel = np.array(data.channel[chanSec])
 
 
 @unwrap_cfg
 @unwrap_select
-def filter_manager(data, b=None, a=None,
-                   out=None, select=None, chan_per_worker=None, keeptrials=True,
-                   parallel=False, parallel_store=None, log_dict=None):
+def filter_manager(
+    data,
+    b=None,
+    a=None,
+    out=None,
+    select=None,
+    chan_per_worker=None,
+    keeptrials=True,
+    parallel=False,
+    parallel_store=None,
+    log_dict=None,
+):
     newOut = False
     if out is None:
         newOut = True
         out = AnalogData(dimord=AnalogData._defaultDimord)
     myfilter = LowPassFilter(b, a=a)
     myfilter.initialize(data, out._stackingDim, chan_per_worker=chan_per_worker, keeptrials=keeptrials)
-    myfilter.compute(data, out,
-                     parallel=parallel,
-                     parallel_store=parallel_store,
-                     log_dict=log_dict)
+    myfilter.compute(data, out, parallel=parallel, parallel_store=parallel_store, log_dict=log_dict)
     return out if newOut else None
 
 
-class TestComputationalRoutine():
+class TestComputationalRoutine:
 
     # Construct linear combination of low- and high-frequency sine waves
     # and use an IIR filter to reconstruct the low-frequency component
     nChannels = 32
     nTrials = 8
     fData = 2
     fNoise = 64
@@ -96,39 +102,49 @@
 
     # Construct artificial equidistant trial-definition array
     trl = np.zeros((nTrials, 3), dtype="int")
     for ntrial in range(nTrials):
         trl[ntrial, :] = np.array([ntrial * fs, (ntrial + 1) * fs, -500])
 
     # Create reference AnalogData objects with equidistant trial spacing
-    sigdata = AnalogData(data=sig, samplerate=fs, trialdefinition=trl,
-                         dimord=["time", "channel"])
-    origdata = AnalogData(data=orig, samplerate=fs, trialdefinition=trl,
-                          dimord=["time", "channel"])
+    sigdata = AnalogData(data=sig, samplerate=fs, trialdefinition=trl, dimord=["time", "channel"])
+    origdata = AnalogData(data=orig, samplerate=fs, trialdefinition=trl, dimord=["time", "channel"])
 
     # Set by-worker channel-count for channel-parallelization
     chanPerWrkr = 7
 
     # Data selections to be tested w/`sigdata`
-    sigdataSelections = [None,
-                         {"trials": [3, 1, 0],
-                          "channel": ["channel" + str(i) for i in range(12, 28)][::-1]},
-                         {"trials": [0, 1, 2],
-                          "channel": range(0, int(nChannels / 2)),
-                          "latency": [-0.25, 0.25]}]
+    sigdataSelections = [
+        None,
+        {
+            "trials": [3, 1, 0],
+            "channel": ["channel" + str(i) for i in range(12, 28)][::-1],
+        },
+        {
+            "trials": [0, 1, 2],
+            "channel": range(0, int(nChannels / 2)),
+            "latency": [-0.25, 0.25],
+        },
+    ]
 
     # Data selections to be tested w/`artdata` generated below (use fixed but arbitrary
     seed = np.random.RandomState(13)
-    artdataSelections = [None,
-                         {"trials": [3, 1, 0],
-                          "channel": ["channel" + str(i) for i in range(12, 28)][::-1],
-                          "latency": None},
-                         {"trials": [0, 1, 2],
-                          "channel": range(0, int(nChannels / 2)),
-                          "latency": [-0.5, 0.6]}]
+    artdataSelections = [
+        None,
+        {
+            "trials": [3, 1, 0],
+            "channel": ["channel" + str(i) for i in range(12, 28)][::-1],
+            "latency": None,
+        },
+        {
+            "trials": [0, 1, 2],
+            "channel": range(0, int(nChannels / 2)),
+            "latency": [-0.5, 0.6],
+        },
+    ]
 
     # Error tolerances and respective quality metrics (depend on data selection!)
     tols = [1e-6, 1e-6, 1e-2]
     metrix = [np.max, np.max, np.mean]
 
     def test_sequential_equidistant(self):
         for sk, select in enumerate(self.sigdataSelections):
@@ -158,15 +174,15 @@
             assert np.array_equal(out.channel, out_sel.channel)
             assert np.array_equal(out.time, out_sel.time)
 
             out = filter_manager(self.sigdata, self.b, self.a, select=select, keeptrials=False)
 
             # check correct signal filtering (especially wrt data-selection)
             if select is None:
-                reference = self.orig[:self.t.size, :]
+                reference = self.orig[: self.t.size, :]
             else:
                 ref = np.zeros(out.trials[0].shape)
                 for tk, trlno in enumerate(sel.trial_ids):
                     ref += self.origdata.trials[trlno][sel.time[trlno], sel.channel]
                     # check for correct time selection (accounting for trial-averaging)
                     assert np.array_equal(out.time[0], self.sigdata.time[0][sel.time[0]])
                 reference = ref / len(sel.trial_ids)
@@ -181,19 +197,21 @@
             out_sel = filter_manager(selected, self.b, self.a, keeptrials=False)
             assert np.array_equal(out.data, out_sel.data)
             assert np.array_equal(out.channel, out_sel.channel)
             assert np.array_equal(out.time, out_sel.time)
 
     def test_sequential_nonequidistant(self):
         for overlapping in [False, True]:
-            nonequidata = generate_artificial_data(nTrials=self.nTrials,
-                                                   nChannels=self.nChannels,
-                                                   equidistant=False,
-                                                   overlapping=overlapping,
-                                                   inmemory=False)
+            nonequidata = generate_artificial_data(
+                nTrials=self.nTrials,
+                nChannels=self.nChannels,
+                equidistant=False,
+                overlapping=overlapping,
+                inmemory=False,
+            )
 
             for select in self.artdataSelections:
                 sel = Selector(nonequidata, select)
                 out = filter_manager(nonequidata, self.b, self.a, select=select)
 
                 # compare expected w/actual shape of computed data
                 reference = 0
@@ -217,45 +235,51 @@
                 assert np.array_equal(out.channel, out_sel.channel)
                 for tk in range(len(out.trials)):
                     assert np.array_equal(out.time[tk], out_sel.time[tk])
 
     def test_sequential_saveload(self):
         for sk, select in enumerate(self.sigdataSelections):
             sel = Selector(self.sigdata, select)
-            out = filter_manager(self.sigdata, self.b, self.a, select=select,
-                                 log_dict={"a": "this is a", "b": "this is b"})
+            out = filter_manager(
+                self.sigdata,
+                self.b,
+                self.a,
+                select=select,
+                log_dict={"a": "this is a", "b": "this is b"},
+            )
 
             assert len(out.trials) == len(sel.trial_ids)
             # ensure our `log_dict` specification was respected
             assert "lowpass" in out._log
             assert "a = this is a" in out._log
             assert "b = this is b" in out._log
 
             # ensure pre-selection is equivalent to in-place selection
             if select is None:
                 selected = self.sigdata.selectdata()
             else:
                 selected = self.sigdata.selectdata(**select)
-            out_sel = filter_manager(selected, self.b, self.a,
-                                     log_dict={"a": "this is a", "b": "this is b"})
+            out_sel = filter_manager(selected, self.b, self.a, log_dict={"a": "this is a", "b": "this is b"})
             assert len(out.trials) == len(out_sel.trials)
             assert "lowpass" in out._log
             assert "a = this is a" in out._log
             assert "b = this is b" in out._log
 
             # save and re-load result, ensure nothing funky happens
             with tempfile.TemporaryDirectory() as tdir:
 
                 fname = os.path.join(tdir, "dummy")
                 print(f"test_computationalroutine: saving to {fname}")
                 out.save(fname)
                 print(f"test_computationalroutine: loading...")
                 dummy = load(fname)
                 print(f"test_computationalroutine: loading done")
-                assert out.filename == dummy.filename, f"save: expected out.filename '{out.filename}' == dummy.filename '{dummy.filename}'."
+                assert (
+                    out.filename == dummy.filename
+                ), f"save: expected out.filename '{out.filename}' == dummy.filename '{dummy.filename}'."
                 if select is None:
                     reference = self.orig
                 else:
                     ref = []
                     for tk, trlno in enumerate(sel.trial_ids):
                         ref.append(self.origdata.trials[trlno][sel.time[trlno], sel.channel])
                         assert np.array_equal(dummy.time[tk], self.sigdata.time[trlno][sel.time[trlno]])
@@ -279,17 +303,23 @@
         for parallel_store in [True, False]:
             for chan_per_worker in [None, self.chanPerWrkr]:
                 for sk, select in enumerate(self.sigdataSelections):
                     # FIXME: remove as soon as channel-parallelization works w/channel selectors
                     if chan_per_worker is not None:
                         select = None
                     sel = Selector(self.sigdata, select)
-                    out = filter_manager(self.sigdata, self.b, self.a, select=select,
-                                         chan_per_worker=chan_per_worker, parallel=True,
-                                         parallel_store=parallel_store)
+                    out = filter_manager(
+                        self.sigdata,
+                        self.b,
+                        self.a,
+                        select=select,
+                        chan_per_worker=chan_per_worker,
+                        parallel=True,
+                        parallel_store=parallel_store,
+                    )
 
                     assert out.data.is_virtual == parallel_store
 
                     # check correct signal filtering (especially wrt data-selection)
                     if select is None:
                         reference = self.orig
                     else:
@@ -304,137 +334,182 @@
 
                     # ensure correct no. HDF5 files were generated for virtual data-set
                     if parallel_store:
                         nfiles = len(glob(os.path.join(os.path.splitext(out.filename)[0], "*.h5")))
                         if chan_per_worker is None:
                             assert nfiles == len(sel.trial_ids)
                         else:
-                            assert nfiles == len(sel.trial_ids) * (int(out.channel.size /
-                                                                    chan_per_worker) +
-                                                                int(out.channel.size % chan_per_worker > 0))
+                            assert nfiles == len(sel.trial_ids) * (
+                                int(out.channel.size / chan_per_worker)
+                                + int(out.channel.size % chan_per_worker > 0)
+                            )
 
                     # ensure pre-selection is equivalent to in-place selection
                     if select is None:
                         selected = self.sigdata.selectdata()
                     else:
                         selected = self.sigdata.selectdata(**select)
-                    out_sel = filter_manager(selected, self.b, self.a,
-                                             chan_per_worker=chan_per_worker, parallel=True,
-                                             parallel_store=parallel_store)
+                    out_sel = filter_manager(
+                        selected,
+                        self.b,
+                        self.a,
+                        chan_per_worker=chan_per_worker,
+                        parallel=True,
+                        parallel_store=parallel_store,
+                    )
                     assert np.allclose(out.data, out_sel.data)
                     assert np.array_equal(out.channel, out_sel.channel)
                     assert np.array_equal(out.time, out_sel.time)
 
-                    out = filter_manager(self.sigdata, self.b, self.a, select=select,
-                                         parallel=True, parallel_store=parallel_store,
-                                         keeptrials=False)
+                    out = filter_manager(
+                        self.sigdata,
+                        self.b,
+                        self.a,
+                        select=select,
+                        parallel=True,
+                        parallel_store=parallel_store,
+                        keeptrials=False,
+                    )
 
                     # check correct signal filtering (especially wrt data-selection)
                     if select is None:
-                        reference = self.orig[:self.t.size, :]
+                        reference = self.orig[: self.t.size, :]
                     else:
                         ref = np.zeros(out.trials[0].shape)
                         for trlno, trlno in enumerate(sel.trial_ids):
                             ref += self.origdata.trials[trlno][sel.time[trlno], sel.channel]
                             # check for correct time selection (accounting for trial-averaging)
                             assert np.array_equal(out.time[0], self.sigdata.time[0][sel.time[0]])
                         reference = ref / len(sel.trial_ids)
                     assert self.metrix[sk](np.abs(out.data - reference)) < self.tols[sk]
                     assert np.array_equal(out.channel, self.sigdata.channel[sel.channel])
                     assert out.data.is_virtual == False
 
                     # ensure pre-selection is equivalent to in-place selection
-                    out_sel = filter_manager(selected, self.b, self.a,
-                                             parallel=True, parallel_store=parallel_store,
-                                             keeptrials=False)
+                    out_sel = filter_manager(
+                        selected,
+                        self.b,
+                        self.a,
+                        parallel=True,
+                        parallel_store=parallel_store,
+                        keeptrials=False,
+                    )
                     assert np.allclose(out.data, out_sel.data)
                     assert np.array_equal(out.channel, out_sel.channel)
                     assert np.array_equal(out.time, out_sel.time)
 
         client.close()
 
     def test_parallel_nonequidistant(self, testcluster):
         client = dd.Client(testcluster)
         for overlapping in [False, True]:
-            nonequidata = generate_artificial_data(nTrials=self.nTrials,
-                                                    nChannels=self.nChannels,
-                                                    equidistant=False,
-                                                    overlapping=overlapping,
-                                                    inmemory=False)
+            nonequidata = generate_artificial_data(
+                nTrials=self.nTrials,
+                nChannels=self.nChannels,
+                equidistant=False,
+                overlapping=overlapping,
+                inmemory=False,
+            )
 
             for parallel_store in [True, False]:
                 for chan_per_worker in [None, self.chanPerWrkr]:
                     for select in self.artdataSelections:
                         # FIXME: remove as soon as channel-parallelization works w/channel selectors
                         if chan_per_worker is not None:
                             select = None
                         sel = Selector(nonequidata, select)
-                        out = filter_manager(nonequidata, self.b, self.a, select=select,
-                                             chan_per_worker=chan_per_worker, parallel=True,
-                                             parallel_store=parallel_store)
+                        out = filter_manager(
+                            nonequidata,
+                            self.b,
+                            self.a,
+                            select=select,
+                            chan_per_worker=chan_per_worker,
+                            parallel=True,
+                            parallel_store=parallel_store,
+                        )
 
                         # compare expected w/actual shape of computed data
                         reference = 0
                         for tk, trlno in enumerate(sel.trial_ids):
                             reference += nonequidata.trials[trlno][sel.time[trlno]].shape[0]
                             # check for correct time selection
                             # FIXME: remove `if` below as soon as `time` prop for lists is fixed
                             if not isinstance(sel.time[0], list):
-                                assert np.array_equal(out.time[tk], nonequidata.time[trlno][sel.time[trlno]])
+                                assert np.array_equal(
+                                    out.time[tk],
+                                    nonequidata.time[trlno][sel.time[trlno]],
+                                )
                         assert out.data.shape[0] == reference
                         assert np.array_equal(out.channel, nonequidata.channel[sel.channel])
                         assert out.data.is_virtual == parallel_store
 
                         if parallel_store:
                             nfiles = len(glob(os.path.join(os.path.splitext(out.filename)[0], "*.h5")))
                             if chan_per_worker is None:
                                 assert nfiles == len(sel.trial_ids)
                             else:
-                                assert nfiles == len(sel.trial_ids) * (int(out.channel.size /
-                                                                        chan_per_worker) +
-                                                                    int(out.channel.size % chan_per_worker > 0))
+                                assert nfiles == len(sel.trial_ids) * (
+                                    int(out.channel.size / chan_per_worker)
+                                    + int(out.channel.size % chan_per_worker > 0)
+                                )
 
                         # ensure pre-selection is equivalent to in-place selection
                         if select is None:
                             selected = nonequidata.selectdata()
                         else:
                             selected = nonequidata.selectdata(**select)
-                        out_sel = filter_manager(selected, self.b, self.a,
-                                                 chan_per_worker=chan_per_worker, parallel=True,
-                                                 parallel_store=parallel_store)
+                        out_sel = filter_manager(
+                            selected,
+                            self.b,
+                            self.a,
+                            chan_per_worker=chan_per_worker,
+                            parallel=True,
+                            parallel_store=parallel_store,
+                        )
                         assert np.allclose(out.data, out_sel.data)
                         assert np.array_equal(out.channel, out_sel.channel)
                         for tk in range(len(out.trials)):
                             assert np.array_equal(out.time[tk], out_sel.time[tk])
 
         client.close()
 
     def test_parallel_saveload(self, testcluster):
         client = dd.Client(testcluster)
         for parallel_store in [True, False]:
             for sk, select in enumerate(self.sigdataSelections):
                 sel = Selector(self.sigdata, select)
-                out = filter_manager(self.sigdata, self.b, self.a, select=select,
-                                     log_dict={"a": "this is a", "b": "this is b"},
-                                     parallel=True, parallel_store=parallel_store)
+                out = filter_manager(
+                    self.sigdata,
+                    self.b,
+                    self.a,
+                    select=select,
+                    log_dict={"a": "this is a", "b": "this is b"},
+                    parallel=True,
+                    parallel_store=parallel_store,
+                )
 
                 assert len(out.trials) == len(sel.trial_ids)
                 # ensure our `log_dict` specification was respected
                 assert "lowpass" in out._log
                 assert "a = this is a" in out._log
                 assert "b = this is b" in out._log
 
                 # ensure pre-selection is equivalent to in-place selection
                 if select is None:
                     selected = self.sigdata.selectdata()
                 else:
                     selected = self.sigdata.selectdata(**select)
-                out_sel = filter_manager(selected, self.b, self.a,
-                                         log_dict={"a": "this is a", "b": "this is b"},
-                                         parallel=True, parallel_store=parallel_store)
+                out_sel = filter_manager(
+                    selected,
+                    self.b,
+                    self.a,
+                    log_dict={"a": "this is a", "b": "this is b"},
+                    parallel=True,
+                    parallel_store=parallel_store,
+                )
                 # only keyword args (`a` in this case here) are stored in `cfg`
                 assert len(out.trials) == len(sel.trial_ids)
                 # ensure our `log_dict` specification was respected
                 assert "lowpass" in out._log
                 assert "a = this is a" in out._log
                 assert "b = this is b" in out._log
 
@@ -447,15 +522,18 @@
                     assert not out.data.is_virtual
                     if select is None:
                         reference = self.orig
                     else:
                         ref = []
                         for tk, trlno in enumerate(sel.trial_ids):
                             ref.append(self.origdata.trials[trlno][sel.time[trlno], sel.channel])
-                            assert np.array_equal(dummy.time[tk], self.sigdata.time[trlno][sel.time[trlno]])
+                            assert np.array_equal(
+                                dummy.time[tk],
+                                self.sigdata.time[trlno][sel.time[trlno]],
+                            )
                         reference = np.vstack(ref)
                     assert self.metrix[sk](np.abs(dummy.data - reference)) < self.tols[sk]
                     assert np.array_equal(dummy.channel, self.sigdata.channel[sel.channel])
                     # del dummy, out
 
                     # ensure out_sel is written/read correctly
                     fname2 = os.path.join(tdir, "dummy2")
@@ -465,10 +543,10 @@
                     assert np.array_equal(dummy.channel, dummy2.channel)
                     assert np.array_equal(dummy.time, dummy2.time)
                     assert not dummy2.data.is_virtual
                     del dummy, dummy2, out, out_sel
 
         client.close()
 
+
 if __name__ == "__main__":
     T1 = TestComputationalRoutine()
-
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_connectivity.py` & `esi_syncopy-2023.7/syncopy/tests/test_connectivity.py`

 * *Files 9% similar despite different names*

```diff
@@ -35,56 +35,53 @@
     and in a multi-taper setting, the tapers can't be averaged before.
     """
 
     # mockup data
     ad = AnalogData([np.ones((5, 10)) for _ in range(2)], samplerate=200)
 
     def test_spectral_output(self):
-        for wrong_output in ['pow', 'abs', 'imag', 'real']:
+        for wrong_output in ["pow", "abs", "imag", "real"]:
             spec = spy.freqanalysis(self.ad, output=wrong_output)
 
             with pytest.raises(SPYValueError) as err:
-                cafunc(spec, method='granger')
+                cafunc(spec, method="granger")
             assert "expected complex valued" in str(err.value)
 
             with pytest.raises(SPYValueError) as err:
-                cafunc(spec, method='coh')
+                cafunc(spec, method="coh")
             assert "expected complex valued" in str(err.value)
 
     def test_spectral_multitaper(self):
 
         # default with needed output='fourier' does not work already in freqanalysis
         # -> taper averaging with keeptapers=False not meaningful with fourier output
         with pytest.raises(SPYValueError) as err:
-            spec = spy.freqanalysis(self.ad, tapsmofrq=0.1, output='fourier')
+            spec = spy.freqanalysis(self.ad, tapsmofrq=0.1, output="fourier")
         assert "expected 'pow'|False" in str(err.value)
 
         # single trial /  trial averaging makes no sense
-        spec = spy.freqanalysis(self.ad, tapsmofrq=0.1, keeptrials=False,
-                                output='fourier', keeptapers=True)
+        spec = spy.freqanalysis(self.ad, tapsmofrq=0.1, keeptrials=False, output="fourier", keeptapers=True)
         with pytest.raises(SPYValueError) as err:
-            cafunc(spec, method='coh')
+            cafunc(spec, method="coh")
         assert "expected multi-trial input data" in str(err.value)
 
     def test_spectral_corr(self):
 
         # method='corr' does not work with SpectralData
-        spec = spy.freqanalysis(self.ad, tapsmofrq=0.1,
-                                output='fourier', keeptapers=True)
+        spec = spy.freqanalysis(self.ad, tapsmofrq=0.1, output="fourier", keeptapers=True)
         with pytest.raises(SPYValueError) as err:
-            cafunc(spec, method='corr')
+            cafunc(spec, method="corr")
         assert "expected AnalogData" in str(err.value)
 
     def test_tf_input(self):
-        """ No time-resolved Granger implemented yet """
-        spec = spy.freqanalysis(self.ad, method='mtmconvol',
-                                t_ftimwin=0.01, output='fourier')
+        """No time-resolved Granger implemented yet"""
+        spec = spy.freqanalysis(self.ad, method="mtmconvol", t_ftimwin=0.01, output="fourier")
 
         with pytest.raises(NotImplementedError) as err:
-            cafunc(spec, method='granger')
+            cafunc(spec, method="granger")
         assert "Granger causality from tf-spectra" in str(err.value)
 
 
 class TestGranger:
 
     nTrials = 200
     nChannels = 4
@@ -103,166 +100,166 @@
 
     # channel indices of coupling
     # a number other than 0 at AdjMat(i,j)
     # means coupling from i->j
     cpl_idx = np.where(AdjMat)
     nocpl_idx = np.where(AdjMat == 0)
 
-    data = synthdata.ar2_network(AdjMat=AdjMat,
-                                 nSamples=nSamples,
-                                 samplerate=fs,
-                                 nTrials=nTrials,
-                                 seed=42)
+    data = synthdata.ar2_network(AdjMat=AdjMat, nSamples=nSamples, samplerate=fs, nTrials=nTrials, seed=42)
 
-    time_span = [-1, nSamples / fs - 1]   # -1s offset
+    time_span = [-1, nSamples / fs - 1]  # -1s offset
 
     cfg = spy.StructDict()
     cfg.tapsmofrq = 1
     cfg.foi = None
-    spec = spy.freqanalysis(data, cfg, output='fourier', keeptapers=True, demean_taper=True)
+    spec = spy.freqanalysis(data, cfg, output="fourier", keeptapers=True, demean_taper=True)
 
     def test_spec_input_frontend(self):
         assert isinstance(self.spec, SpectralData)
         cfg = self.cfg.copy()
         cfg.pop("tapsmofrq", None)
-        res = spy.connectivityanalysis(self.spec, method='granger', cfg=cfg)
+        res = spy.connectivityanalysis(self.spec, method="granger", cfg=cfg)
         assert isinstance(res, spy.CrossSpectralData)
 
     def test_gr_solution(self, **kwargs):
 
         # re-run spectral analysis
         if len(kwargs) != 0:
-            spec = spy.freqanalysis(self.data, self.cfg, output='fourier',
-                                    keeptapers=True, demean_taper=True, **kwargs)
+            spec = spy.freqanalysis(
+                self.data,
+                self.cfg,
+                output="fourier",
+                keeptapers=True,
+                demean_taper=True,
+                **kwargs,
+            )
         else:
             spec = self.spec
 
         # sanity check
         assert isinstance(self.data, AnalogData)
         assert isinstance(spec, SpectralData)
 
         # from SpectralData
-        Gcaus_spec = cafunc(spec, method='granger', **kwargs)
+        Gcaus_spec = cafunc(spec, method="granger", **kwargs)
 
         # from AnalogData directly, needs cfg for spectral analyis
-        Gcaus_ad = cafunc(self.data, method='granger',
-                          cfg=self.cfg, **kwargs)
+        Gcaus_ad = cafunc(self.data, method="granger", cfg=self.cfg, **kwargs)
 
         # same results on all channels and freqs within 2%
         assert np.allclose(Gcaus_ad.trials[0], Gcaus_spec.trials[0], atol=2e-2)
 
         for Gcaus in [Gcaus_spec, Gcaus_ad]:
             # check all channel combinations with coupling
             for i, j in zip(*self.cpl_idx):
                 peak = Gcaus.data[0, :, i, j].max()
                 peak_frq = Gcaus.freq[Gcaus.data[0, :, i, j].argmax()]
                 cval = self.AdjMat[i, j]
 
                 dbg_str = f"{peak:.2f}\t{self.AdjMat[i,j]:.2f}\t {peak_frq:.2f}\t"
-                print(dbg_str, f'\t {i}', f' {j}')
+                print(dbg_str, f"\t {i}", f" {j}")
 
                 # test for directional coupling
                 # at the right frequency range
                 assert peak >= cval
                 assert 35 < peak_frq < 45
 
                 # only plot with defaults
                 if len(kwargs) == 0:
                     Gcaus.singlepanelplot(channel_i=i, channel_j=j)
                     # plot_Granger(Gcaus, i, j)
                     # ppl.legend()
 
             # check .info for default test
             if len(kwargs) == 0:
-                assert Gcaus.info['converged']
-                assert Gcaus.info['max rel. err'] < 1e-5
-                assert Gcaus.info['reg. factor'] == 0
-                assert Gcaus.info['initial cond. num'] > 10
+                assert Gcaus.info["converged"]
+                assert Gcaus.info["max rel. err"] < 1e-5
+                assert Gcaus.info["reg. factor"] == 0
+                assert Gcaus.info["initial cond. num"] > 10
 
             # Test that 'metadata_keys' in the Granger ComputationalRoutine is up-to-date. All listed
             #  keys should exist...
             for k in spy.connectivity.AV_compRoutines.GrangerCausality.metadata_keys:
                 assert k in Gcaus.info
             # ... and no unmentioned extra keys should be in there.
             assert len(Gcaus.info) == len(spy.connectivity.AV_compRoutines.GrangerCausality.metadata_keys)
 
     def test_gr_selections(self):
 
         # trial, channel and toi selections
-        selections = helpers.mk_selection_dicts(self.nTrials,
-                                                self.nChannels,
-                                                *self.time_span)
+        selections = helpers.mk_selection_dicts(self.nTrials, self.nChannels, *self.time_span)
 
         for sel_dct in selections:
             print(sel_dct)
-            Gcaus_ad = cafunc(self.data, self.cfg,
-                              method='granger', select=sel_dct)
+            Gcaus_ad = cafunc(self.data, self.cfg, method="granger", select=sel_dct)
 
             # selections act on spectral analysis, remove latency
             # sel_dct.pop('latency')
-            spec = spy.freqanalysis(self.data, self.cfg, output='fourier',
-                                    keeptapers=True, select=sel_dct, demean_taper=True)
+            spec = spy.freqanalysis(
+                self.data,
+                self.cfg,
+                output="fourier",
+                keeptapers=True,
+                select=sel_dct,
+                demean_taper=True,
+            )
 
-            Gcaus_spec = cafunc(spec, method='granger')
+            Gcaus_spec = cafunc(spec, method="granger")
 
             # check here just for finiteness and positivity
             assert np.all(np.isfinite(Gcaus_ad.data))
             assert np.all(Gcaus_ad.data[0, ...] >= -1e-10)
 
             # same results
             assert np.allclose(Gcaus_ad.trials[0], Gcaus_spec.trials[0], atol=1e-2)
 
         # test one final selection into a result
         # obtained via orignal SpectralData input
-        selections[0].pop('latency')
-        result_ad = cafunc(self.data, self.cfg, method='granger', select=selections[0])
-        result_spec = cafunc(self.spec, method='granger', select=selections[0])
-        assert np.allclose(result_ad.trials[0], result_spec.trials[0], atol=2e-2)
+        selections[0].pop("latency")
+        result_ad = cafunc(self.data, self.cfg, method="granger", select=selections[0])
+        result_spec = cafunc(self.spec, method="granger", select=selections[0])
+        assert np.allclose(result_ad.trials[0], result_spec.trials[0], atol=2e-1)
 
     def test_gr_foi(self):
 
         try:
-            cafunc(self.data,
-                   method='granger',
-                   foi=np.arange(0, 70)
-                   )
+            cafunc(self.data, method="granger", foi=np.arange(0, 70))
         except SPYValueError as err:
-            assert 'no foi specification' in str(err)
+            assert "no foi specification" in str(err)
 
         try:
-            cafunc(self.data,
-                   method='granger',
-                   foilim=[0, 70]
-                   )
+            cafunc(self.data, method="granger", foilim=[0, 70])
         except SPYValueError as err:
-            assert 'no foi specification' in str(err)
+            assert "no foi specification" in str(err)
 
     def test_gr_cfg(self):
 
         call = lambda cfg: cafunc(self.data, cfg)
-        run_cfg_test(call, method='granger',
-                     cfg=get_defaults(cafunc))
+        run_cfg_test(call, method="granger", cfg=get_defaults(cafunc))
 
     @skip_low_mem
     def test_gr_parallel(self, testcluster):
 
         ppl.ioff()
         client = dd.Client(testcluster)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (inspect.ismethod(getattr(self, attr)) and "parallel" not in attr)
+        ]
 
         for test in all_tests:
             test_method = getattr(self, test)
             test_method()
         client.close()
         ppl.ion()
 
     def test_gr_padding(self):
 
-        pad_length = 6   # seconds
+        pad_length = 6  # seconds
         call = lambda pad: self.test_gr_solution(pad=pad)
         helpers.run_padding_test(call, pad_length)
 
     def test_gr_polyremoval(self):
 
         call = lambda polyremoval: self.test_gr_solution(polyremoval=polyremoval)
         helpers.run_polyremoval_test(call)
@@ -275,41 +272,48 @@
     nTrials = 100
     fs = 1000
 
     # -- two harmonics with individual phase diffusion --
 
     f1, f2 = 20, 40
     # a lot of phase diffusion (1% per step) in the 20Hz band
-    s1 = synthdata.phase_diffusion(nTrials=nTrials, freq=f1,
-                                   eps=.03,
-                                   nChannels=nChannels,
-                                   nSamples=nSamples,
-                                   seed=helpers.test_seed)
+    s1 = synthdata.phase_diffusion(
+        nTrials=nTrials,
+        freq=f1,
+        eps=0.03,
+        nChannels=nChannels,
+        nSamples=nSamples,
+        seed=helpers.test_seed,
+    )
 
     # little diffusion in the 40Hz band
-    s2 = synthdata.phase_diffusion(nTrials=nTrials, freq=f2,
-                                   eps=.001,
-                                   nChannels=nChannels,
-                                   nSamples=nSamples,
-                                   seed=helpers.test_seed)
-
-    wn = synthdata.white_noise(nTrials=nTrials, nChannels=nChannels, nSamples=nSamples,
-                               seed=helpers.test_seed)
+    s2 = synthdata.phase_diffusion(
+        nTrials=nTrials,
+        freq=f2,
+        eps=0.001,
+        nChannels=nChannels,
+        nSamples=nSamples,
+        seed=helpers.test_seed,
+    )
+
+    wn = synthdata.white_noise(
+        nTrials=nTrials, nChannels=nChannels, nSamples=nSamples, seed=helpers.test_seed
+    )
 
     # superposition
     data = s1 + s2 + wn
     data.samplerate = fs
-    time_span = [-1, nSamples / fs - 1]   # -1s offset
+    time_span = [-1, nSamples / fs - 1]  # -1s offset
 
     # spectral analysis
     cfg = spy.StructDict()
     cfg.tapsmofrq = 1.5
     cfg.foilim = [5, 60]
 
-    spec = spy.freqanalysis(data, cfg, output='fourier', keeptapers=True)
+    spec = spy.freqanalysis(data, cfg, output="fourier", keeptapers=True)
 
     def test_timedep_coh(self):
         """
         Time dependent coherence of phase diffusing signals.
         Starting from a common phase, they'll decorrelate over time.
         """
 
@@ -317,93 +321,93 @@
         # will go down noticably over the observation time
         test_data = self.data
         # check number of samples
         nSamples = self.nSamples
         # assert test_data.time[0].size == nSamples
 
         # get time-frequency spec for the non-stationary signal
-        spec_tf = spy.freqanalysis(test_data, method='mtmconvol',
-                                   t_ftimwin=0.3, foilim=[5, 100],
-                                   output='fourier')
+        spec_tf = spy.freqanalysis(
+            test_data,
+            method="mtmconvol",
+            t_ftimwin=0.3,
+            foilim=[5, 100],
+            output="fourier",
+        )
 
         # check that we have still the same time axis
         assert np.all(spec_tf.time[0] == test_data.time[0])
 
         # abs averaged spectra for plotting
-        spec_tf_abs = spy.SpectralData(data=np.abs(spec_tf.data),
-                                       samplerate=self.fs,
-                                       trialdefinition=spec_tf.trialdefinition)
+        spec_tf_abs = spy.SpectralData(
+            data=np.abs(spec_tf.data),
+            samplerate=self.fs,
+            trialdefinition=spec_tf.trialdefinition,
+        )
         spec_tf_abs.freq = spec_tf.freq
         spec_tf_abs.multipanelplot(trials=0)
 
         # rough check that the power in the 20Hz band is lower than in the 40Hz
         # band due to more phase diffusion
-        spec_tf_avabs = spy.mean(spec_tf_abs, dim='trials')
+        spec_tf_avabs = spy.mean(spec_tf_abs, dim="trials")
         profile_20 = spec_tf_avabs.show(frequency=self.f1)[:, 1]
         profile_40 = spec_tf_avabs.show(frequency=self.f2)[:, 1]
 
         assert profile_40.mean() > profile_20.mean()
         assert profile_40.max() > profile_20.max()
 
         # compute time dependent coherence
-        coh = cafunc(data=spec_tf, method='coh')
+        coh = cafunc(data=spec_tf, method="coh")
 
         # check that we have still the same time axis
         assert np.all(coh.time[0] == test_data.time[0])
 
         # not exactly beautiful but it makes the point
         coh.singlepanelplot(channel_i=0, channel_j=1, frequency=[7, 60])
 
         # plot the coherence over time just along three different frequency bands
         ppl.figure()
         cprofile20 = coh.show(frequency=self.f1, channel_i=0, channel_j=1)
-        ppl.plot(cprofile20, label='20Hz')
+        ppl.plot(cprofile20, label="20Hz")
 
         # coherence goes down more slowly
         cprofile40 = coh.show(frequency=self.f2, channel_i=0, channel_j=1)
-        ppl.plot(cprofile40, label='40Hz')
+        ppl.plot(cprofile40, label="40Hz")
 
         # here is nothing
         cprofile10 = coh.show(frequency=10, channel_i=0, channel_j=1)
-        ppl.plot(cprofile10, label='10Hz')
-        ppl.xlabel('samples')
-        ppl.ylabel('coherence')
+        ppl.plot(cprofile10, label="10Hz")
+        ppl.xlabel("samples")
+        ppl.ylabel("coherence")
         ppl.legend()
 
         # check that the 20 Hz band has high coherence only in the beginning
         assert cprofile20.max() > 0.9
         # later coherence goes down
-        assert cprofile20[int(0.9 * nSamples):].max() < 0.4
+        assert cprofile20[int(0.9 * nSamples) :].max() < 0.4
         # side band never has high coherence except for the very beginning
         assert cprofile10.max() < 0.5
 
     def test_coh_solution(self, **kwargs):
 
         # re-run spectral analysis
         if len(kwargs) != 0:
-            spec = spy.freqanalysis(self.data, self.cfg, output='fourier',
-                                    keeptapers=True, **kwargs)
+            spec = spy.freqanalysis(self.data, self.cfg, output="fourier", keeptapers=True, **kwargs)
         else:
             spec = self.spec
         # sanity check
         assert isinstance(self.data, AnalogData)
         assert isinstance(spec, SpectralData)
 
-        res_spec = cafunc(data=spec,
-                          method='coh',
-                          **kwargs)
+        res_spec = cafunc(data=spec, method="coh", **kwargs)
 
         # needs same cfg for spectral analysis
-        res_ad = cafunc(data=self.data,
-                        method='coh',
-                        cfg=self.cfg,
-                        **kwargs)
+        res_ad = cafunc(data=self.data, method="coh", cfg=self.cfg, **kwargs)
 
         # same results on all channels and freqs
-        assert np.allclose(res_spec.trials[0], res_ad.trials[0])
+        assert np.allclose(res_spec.trials[0], res_ad.trials[0], atol=1e-3)
 
         for res in [res_spec, res_ad]:
             # coherence at the harmonic frequencies
             idx_f1 = np.argmin(res.freq < self.f1)
             peak_f1 = res.data[0, idx_f1, 0, 1]
             idx_f2 = np.argmin(res.freq < self.f2)
             peak_f2 = res.data[0, idx_f2, 0, 1]
@@ -421,225 +425,235 @@
             assert np.all(res.data[0, null_idx, 0, 1] < 0.2)
 
             if len(kwargs) == 0:
                 res.singlepanelplot(channel_i=0, channel_j=1)
 
     def test_coh_selections(self):
 
-        selections = helpers.mk_selection_dicts(self.nTrials,
-                                                self.nChannels,
-                                                *self.time_span)
+        selections = helpers.mk_selection_dicts(self.nTrials, self.nChannels, *self.time_span)
 
         for sel_dct in selections:
-            result = cafunc(self.data, self.cfg, method='coh', select=sel_dct)
+            result = cafunc(self.data, self.cfg, method="coh", select=sel_dct)
 
             # re-run spectral analysis with selection
-            spec = spy.freqanalysis(self.data, self.cfg, output='fourier',
-                                    keeptapers=True, select=sel_dct)
+            spec = spy.freqanalysis(self.data, self.cfg, output="fourier", keeptapers=True, select=sel_dct)
 
-            result_spec = cafunc(spec, method='coh')
+            result_spec = cafunc(spec, method="coh")
 
             # check here just for finiteness and positivity
             assert np.all(np.isfinite(result.data))
             assert np.all(result.data[0, ...] >= -1e-10)
 
             # same results
             assert np.allclose(result.trials[0], result_spec.trials[0], atol=1e-3)
 
         # test one final selection into a result
         # obtained via orignal SpectralData input
-        selections[0].pop('latency')
-        result_ad = cafunc(self.data, self.cfg, method='coh', select=selections[0])
-        result_spec = cafunc(self.spec, method='coh', select=selections[0])
+        selections[0].pop("latency")
+        result_ad = cafunc(self.data, self.cfg, method="coh", select=selections[0])
+        result_spec = cafunc(self.spec, method="coh", select=selections[0])
         assert np.allclose(result_ad.trials[0], result_spec.trials[0], atol=1e-3)
 
     def test_coh_foi(self):
 
         # 2 frequencies
         foilim = [[2, 60], [7.65, 45.1234], None]
         for foil in foilim:
-            result = cafunc(self.data, method='coh', foilim=foil)
+            result = cafunc(self.data, method="coh", foilim=foil)
             # check here just for finiteness and positivity
             assert np.all(np.isfinite(result.data))
             assert np.all(result.data[0, ...] >= -1e-10)
 
         # make sure out-of-range foilim  are detected
-        with pytest.raises(SPYValueError, match='foilim'):
-            result = cafunc(self.data, method='coh', foilim=[-1, 70])
+        with pytest.raises(SPYValueError, match="foilim"):
+            result = cafunc(self.data, method="coh", foilim=[-1, 70])
 
         # make sure invalid foilim are detected
-        with pytest.raises(SPYValueError, match='foilim'):
-            result = cafunc(self.data, method='coh', foilim=[None, None])
+        with pytest.raises(SPYValueError, match="foilim"):
+            result = cafunc(self.data, method="coh", foilim=[None, None])
 
-        with pytest.raises(SPYValueError, match='foilim'):
-            result = cafunc(self.data, method='coh', foilim='abc')
+        with pytest.raises(SPYValueError, match="foilim"):
+            result = cafunc(self.data, method="coh", foilim="abc")
 
     def test_coh_cfg(self):
 
         call = lambda cfg: cafunc(self.data, cfg)
-        run_cfg_test(call, method='coh',
-                     cfg=get_defaults(cafunc))
+        run_cfg_test(call, method="coh", cfg=get_defaults(cafunc))
 
     @skip_low_mem
     def test_coh_parallel(self, testcluster):
         check_parallel(self, testcluster)
 
     def test_coh_padding(self):
 
-        pad_length = 2   # seconds
+        pad_length = 2  # seconds
         call = lambda pad: self.test_coh_solution(pad=pad)
         helpers.run_padding_test(call, pad_length)
 
     def test_coh_polyremoval(self):
 
         call = lambda polyremoval: self.test_coh_solution(polyremoval=polyremoval)
         helpers.run_polyremoval_test(call)
 
     def test_coh_outputs(self):
 
         for output in connectivity_outputs:
-            coh = cafunc(self.data,
-                         method='coh',
-                         output=output)
+            coh = cafunc(self.data, method="coh", output=output)
 
-            if output in ['complex', 'fourier']:
+            if output in ["complex", "fourier"]:
                 # we have imaginary parts
                 assert not np.all(np.imag(coh.trials[0]) == 0)
-            elif output == 'angle':
+            elif output == "angle":
                 # all values in [-pi, pi]
                 assert np.all((coh.trials[0] < np.pi) | (coh.trials[0] > -np.pi))
             else:
                 # strictly real outputs
                 assert np.all(np.imag(coh.trials[0]) == 0)
 
 
 class TestCSD:
     nSamples = 1400
     nChannels = 4
     nTrials = 100
     fs = 1000
-    Method = 'csd'
+    Method = "csd"
 
     # -- two harmonics with individual phase diffusion --
 
     f1, f2 = 20, 40
     # a lot of phase diffusion (1% per step) in the 20Hz band
-    s1 = synthdata.phase_diffusion(nTrials=nTrials, freq=f1,
-                                   eps=.01,
-                                   nChannels=nChannels,
-                                   nSamples=nSamples,
-                                   seed=42)
+    s1 = synthdata.phase_diffusion(
+        nTrials=nTrials,
+        freq=f1,
+        eps=0.01,
+        nChannels=nChannels,
+        nSamples=nSamples,
+        seed=42,
+    )
 
     # little diffusion in the 40Hz band
-    s2 = synthdata.phase_diffusion(nTrials=nTrials, freq=f2,
-                                   eps=.001,
-                                   nChannels=nChannels,
-                                   nSamples=nSamples,
-                                   seed=42)
+    s2 = synthdata.phase_diffusion(
+        nTrials=nTrials,
+        freq=f2,
+        eps=0.001,
+        nChannels=nChannels,
+        nSamples=nSamples,
+        seed=42,
+    )
 
     wn = synthdata.white_noise(nTrials=nTrials, nChannels=nChannels, nSamples=nSamples)
 
     # superposition
     data = s1 + s2 + wn
     data.samplerate = fs
-    time_span = [-1, nSamples / fs - 1]   # -1s offset
+    time_span = [-1, nSamples / fs - 1]  # -1s offset
 
     # spectral analysis
     cfg = spy.StructDict()
     cfg.tapsmofrq = 1.5
     cfg.foilim = [5, 60]
 
-    spec = spy.freqanalysis(data, cfg, output='fourier', keeptapers=True)
+    spec = spy.freqanalysis(data, cfg, output="fourier", keeptapers=True)
 
     def test_data_output_type(self):
-        cross_spec = spy.connectivityanalysis(self.spec, method='csd')
+        cross_spec = spy.connectivityanalysis(self.spec, method="csd")
         assert np.all(self.spec.freq == cross_spec.freq)
-        assert cross_spec.data.dtype.name == 'complex64'
+        assert cross_spec.data.dtype.name == "complex64"
         assert cross_spec.data.shape != self.spec.data.shape
 
     @skip_low_mem
     def test_csd_parallel(self, testcluster):
         check_parallel(self, testcluster)
 
     def test_csd_input(self):
         assert isinstance(self.spec, SpectralData)
 
     def test_csd_cfg_replay(self):
         cross_spec = spy.connectivityanalysis(self.spec, method=self.Method)
         assert len(cross_spec.cfg) == 2
-        assert np.all([True for cfg in zip(self.spec.cfg['freqanalysis'], cross_spec.cfg['freqanalysis']) if cfg[0] == cfg[1]])
-        assert cross_spec.cfg['connectivityanalysis'].method == self.Method
+        assert np.all(
+            [
+                True
+                for cfg in zip(self.spec.cfg["freqanalysis"], cross_spec.cfg["freqanalysis"])
+                if cfg[0] == cfg[1]
+            ]
+        )
+        assert cross_spec.cfg["connectivityanalysis"].method == self.Method
 
-        first_cfg = cross_spec.cfg['connectivityanalysis']
+        first_cfg = cross_spec.cfg["connectivityanalysis"]
         first_res = spy.connectivityanalysis(self.spec, cfg=first_cfg)
         replay_res = spy.connectivityanalysis(self.spec, cfg=first_res.cfg)
 
         assert np.allclose(first_res.data[:], replay_res.data[:])
         assert first_res.cfg == replay_res.cfg
 
 
 class TestCorrelation:
 
     nChannels = 5
     nTrials = 50
     fs = 1000
-    nSamples = 2001   # 2s long signals
+    nSamples = 2001  # 2s long signals
 
     # -- a single harmonic with phase shifts between channels
 
-    f1 = 10   # period is 0.1s
+    f1 = 10  # period is 0.1s
     trls = []
     for _ in range(nTrials):
 
         # no phase diffusion
-        p1 = synthdata.phase_diffusion(freq=f1,
-                                       eps=0,
-                                       nChannels=nChannels,
-                                       nSamples=nSamples,
-                                       seed=42,
-                                       return_phase=True,
-                                       nTrials=None)
+        p1 = synthdata.phase_diffusion(
+            freq=f1,
+            eps=0,
+            nChannels=nChannels,
+            nSamples=nSamples,
+            seed=42,
+            return_phase=True,
+            nTrials=None,
+        )
         # same frequency but more diffusion
-        p2 = synthdata.phase_diffusion(freq=f1,
-                                       eps=0.1,
-                                       nChannels=1,
-                                       nSamples=nSamples,
-                                       seed=42,
-                                       return_phase=True,
-                                       nTrials=None)
+        p2 = synthdata.phase_diffusion(
+            freq=f1,
+            eps=0.1,
+            nChannels=1,
+            nSamples=nSamples,
+            seed=42,
+            return_phase=True,
+            nTrials=None,
+        )
 
         # set 2nd channel to higher phase diffusion
         p1[:, 1] = p2[:, 0]
         # add a pi/2 phase shift for the even channels
         p1[:, 2::2] += np.pi / 2
 
         trls.append(np.cos(p1))
 
     data = AnalogData(trls, samplerate=fs)
     time_span = [-1, nSamples / fs - 1]  # -1s offset
 
     def test_corr_solution(self, **kwargs):
 
         # `keeptrials=False` is the default here!
-        corr = cafunc(data=self.data, method='corr', **kwargs)
+        corr = cafunc(data=self.data, method="corr", **kwargs)
 
         # test 0-lag autocorr is 1 for all channels
-        assert np.all(corr.data[0, 0].diagonal() > .99)
+        assert np.all(corr.data[0, 0].diagonal() > 0.99)
 
         # test that at exactly the period-lag
         # correlations remain high w/o phase diffusion
         period_idx = int(1 / self.f1 * self.fs)
         # 100 samples is one period
         assert np.allclose(100, period_idx)
         auto_00 = corr.data[:, 0, 0, 0]
-        assert np.all(auto_00[::period_idx] > .99)
+        assert np.all(auto_00[::period_idx] > 0.99)
 
         # test for auto-corr minima at half the period
-        assert auto_00[period_idx // 2] < -.99
-        assert auto_00[period_idx // 2 + period_idx] < -.99
+        assert auto_00[period_idx // 2] < -0.99
+        assert auto_00[period_idx // 2 + period_idx] < -0.99
 
         # test signal with phase diffusion (2nd channel) has
         # decaying correlations (diffusion may lead to later
         # increases of auto-correlation again, hence we check
         # only the first 5 periods)
         auto_11 = corr.data[:, 0, 1, 1]
         assert np.all(np.diff(auto_11[::period_idx])[:5] < 0)
@@ -648,83 +662,79 @@
         # crosscorr maximum to 1/4 of the period
         cross_02 = corr.data[:, 0, 0, 2]
         lag_idx = int(1 / self.f1 * self.fs * 0.25)
         # 25 samples is 1/4th period
         assert np.allclose(25, lag_idx)
         assert cross_02[lag_idx] > 0.99
         # same for a period multiple
-        assert cross_02[lag_idx + period_idx] > .99
+        assert cross_02[lag_idx + period_idx] > 0.99
         # plus half the period a minimum occurs
-        assert cross_02[lag_idx + period_idx // 2] < -.99
+        assert cross_02[lag_idx + period_idx // 2] < -0.99
 
         # test for (anti-)symmetry
         cross_20 = corr.data[:, 0, 2, 0]
         assert cross_20[-lag_idx] > 0.99
         assert cross_20[-lag_idx - period_idx] > 0.99
 
         # only plot for simple solution test
         if len(kwargs) == 0:
 
             # test that keeptrials=False yields (almost) same results
             # as post-hoc trial averaging
-            corr_st = cafunc(data=self.data, method='corr', keeptrials=True)
-            corr_st_trl_avg = spy.mean(corr_st, dim='trials')
+            corr_st = cafunc(data=self.data, method="corr", keeptrials=True)
+            corr_st_trl_avg = spy.mean(corr_st, dim="trials")
             # keeptrials=False normalizes with global signal variances
             # hence there can be actually small differences
             assert np.allclose(corr_st_trl_avg.data[()], corr.data[()], atol=1e-2)
 
-            plot_corr(corr, 0, 0, label='corr 0-0')
-            plot_corr(corr, 1, 1, label='corr 1-1')
-            plot_corr(corr, 0, 2, label='corr 0-2')
-            ppl.xlim((-.01, 0.5))
+            plot_corr(corr, 0, 0, label="corr 0-0")
+            plot_corr(corr, 1, 1, label="corr 1-1")
+            plot_corr(corr, 0, 2, label="corr 0-2")
+            ppl.xlim((-0.01, 0.5))
             ppl.ylim((-1.1, 1.3))
             ppl.legend(ncol=3)
 
     def test_corr_padding(self):
 
-        self.test_corr_solution(pad='maxperlen')
+        self.test_corr_solution(pad="maxperlen")
         # no padding is allowed for
         # this method
         try:
             self.test_corr_solution(pad=1000)
         except SPYValueError as err:
-            assert 'pad' in str(err)
-            assert 'no padding needed/allowed' in str(err)
+            assert "pad" in str(err)
+            assert "no padding needed/allowed" in str(err)
 
         try:
-            self.test_corr_solution(pad='nextpow2')
+            self.test_corr_solution(pad="nextpow2")
         except SPYValueError as err:
-            assert 'pad' in str(err)
-            assert 'no padding needed/allowed' in str(err)
+            assert "pad" in str(err)
+            assert "no padding needed/allowed" in str(err)
 
         try:
-            self.test_corr_solution(pad='IamNoPad')
+            self.test_corr_solution(pad="IamNoPad")
         except SPYValueError as err:
-            assert 'Invalid value of `pad`' in str(err)
-            assert 'no padding needed/allowed' in str(err)
+            assert "Invalid value of `pad`" in str(err)
+            assert "no padding needed/allowed" in str(err)
 
     def test_corr_selections(self):
 
-        selections = helpers.mk_selection_dicts(self.nTrials,
-                                                self.nChannels,
-                                                *self.time_span)
+        selections = helpers.mk_selection_dicts(self.nTrials, self.nChannels, *self.time_span)
 
         for sel_dct in selections:
 
-            result = cafunc(self.data, method='corr', select=sel_dct)
+            result = cafunc(self.data, method="corr", select=sel_dct)
 
             # check here just for finiteness and positivity
             assert np.all(np.isfinite(result.data))
 
     def test_corr_cfg(self):
 
         call = lambda cfg: cafunc(self.data, cfg)
-        run_cfg_test(call, method='corr',
-                     positivity=False,
-                     cfg=get_defaults(cafunc))
+        run_cfg_test(call, method="corr", positivity=False, cfg=get_defaults(cafunc))
 
     @skip_low_mem
     def test_corr_parallel(self, testcluster):
         check_parallel(self, testcluster)
 
     def test_corr_polyremoval(self):
 
@@ -739,80 +749,97 @@
     nTrials = 20
     fs = 1000
 
     # -- one harmonic with individual phase diffusion --
 
     f1 = 20
     # phase diffusion (1% per step) in the 20Hz band
-    s1 = synthdata.phase_diffusion(nTrials=nTrials, freq=f1,
-                                   eps=.01,
-                                   nChannels=nChannels,
-                                   nSamples=nSamples,
-                                   seed=helpers.test_seed)
-    wn = synthdata.white_noise(nTrials=nTrials, nChannels=nChannels, nSamples=nSamples,
-                               seed=helpers.test_seed)
+    s1 = synthdata.phase_diffusion(
+        nTrials=nTrials,
+        freq=f1,
+        eps=0.01,
+        nChannels=nChannels,
+        nSamples=nSamples,
+        seed=helpers.test_seed,
+    )
+    wn = synthdata.white_noise(
+        nTrials=nTrials, nChannels=nChannels, nSamples=nSamples, seed=helpers.test_seed
+    )
 
     # superposition
     data = s1 + wn
     data.samplerate = fs
-    time_span = [-1, nSamples / fs - 1]   # -1s offset
+    time_span = [-1, nSamples / fs - 1]  # -1s offset
 
     # spectral analysis
     cfg = spy.StructDict()
-    cfg.tapsmofrq = 1.5
+    cfg.tapsmofrq = 2.
     cfg.foilim = [5, 60]
 
-    spec = spy.freqanalysis(data, cfg, output='fourier', keeptapers=True)
+    spec = spy.freqanalysis(data, cfg, output="fourier", keeptapers=True)
 
     def test_timedep_ppc(self):
         """
         Time dependent PPC of phase diffusing signals.
         Starting from a common phase, they'll decorrelate over time.
         """
 
         # 20Hz band has strong diffusion so coherence
         # will go down noticably over the observation time
         test_data = self.data
         # check number of samples
         assert test_data.time[0].size == self.nSamples
 
         # get time-frequency spec for the non-stationary signal
-        spec_tf = spy.freqanalysis(test_data, method='mtmconvol',
-                                   t_ftimwin=0.3, foilim=[5, 100],
-                                   output='fourier')
+        spec_tf = spy.freqanalysis(
+            test_data,
+            method="mtmconvol",
+            t_ftimwin=0.3,
+            foilim=[5, 100],
+            output="fourier",
+        )
 
         # compute time dependent coherence
-        ppc = cafunc(data=spec_tf, method='ppc')
+        ppc = cafunc(data=spec_tf, method="ppc")
 
         # check that we have still the same time axis
         assert np.all(ppc.time[0] == test_data.time[0])
 
         # not exactly beautiful but it makes the point
         ppc.singlepanelplot(channel_i=0, channel_j=1, frequency=[7, 60])
 
         # for visual comparison to the coherence which has more bias
-        coh = cafunc(data=spec_tf, method='coh')
+        coh = cafunc(data=spec_tf, method="coh")
 
         # plot the coherence over time just along two different frequency bands
         ppl.figure()
         ppc_profile20 = ppc.show(frequency=self.f1, channel_i=0, channel_j=1)
-        ppl.plot(ppc_profile20, label='20Hz')
-        ppl.plot(coh.show(frequency=20, channel_i=0, channel_j=1), ls='--',
-                 label='20Hz coherence', c='k', alpha=0.4)
+        ppl.plot(ppc_profile20, label="20Hz")
+        ppl.plot(
+            coh.show(frequency=20, channel_i=0, channel_j=1),
+            ls="--",
+            label="20Hz coherence",
+            c="k",
+            alpha=0.4,
+        )
 
         # here is nothing
         ppc_profile50 = ppc.show(frequency=50, channel_i=0, channel_j=1)
-        ppl.plot(ppc_profile50, label='50Hz')
-
-        ppl.plot(coh.show(frequency=50, channel_i=0, channel_j=1),
-                 label='50Hz coherence', c='k', alpha=0.4)
+        ppl.plot(ppc_profile50, label="50Hz")
 
-        ppl.title(f'PPC(t), nTrials={self.nTrials}')
-        ppl.xlabel('samples')
-        ppl.ylabel('PPC')
+        ppl.plot(
+            coh.show(frequency=50, channel_i=0, channel_j=1),
+            label="50Hz coherence",
+            c="k",
+            alpha=0.4,
+        )
+
+        ppl.title(f"PPC(t), nTrials={self.nTrials}")
+        ppl.xlabel("samples")
+        ppl.ylabel("PPC")
         ppl.legend()
 
         # check that the 20 Hz band has high PPC only in the beginning
         assert ppc_profile20.max() > 0.9
         assert ppc_profile20.argmax() < self.nSamples / 10
         # side band never has high PPC
         # note that we can go lower as compared to the coherence
@@ -823,35 +850,29 @@
         # for the classic coherence
         assert np.all(coh.show(frequency=50, channel_i=0, channel_j=1) > ppc_profile50)
 
     def test_ppc_solution(self, **kwargs):
 
         # re-run spectral analysis
         if len(kwargs) != 0:
-            spec = spy.freqanalysis(self.data, self.cfg, output='fourier',
-                                    keeptapers=True, **kwargs)
+            spec = spy.freqanalysis(self.data, self.cfg, output="fourier", keeptapers=True, **kwargs)
         else:
             spec = self.spec
         # sanity check
         assert isinstance(self.data, AnalogData)
         assert isinstance(spec, SpectralData)
 
-        res_spec = cafunc(data=spec,
-                          method='ppc',
-                          **kwargs)
+        res_spec = cafunc(data=spec, method="ppc", **kwargs)
 
         # needs same cfg for spectral analysis
-        res_ad = cafunc(data=self.data,
-                        method='ppc',
-                        cfg=self.cfg,
-                        **kwargs)
+        res_ad = cafunc(data=self.data, method="ppc", cfg=self.cfg, **kwargs)
 
         # same results on all channels and freqs
         # irrespective of AnalogData or SpectralData input
-        assert np.allclose(res_spec.trials[0], res_ad.trials[0])
+        assert np.allclose(res_spec.trials[0], res_ad.trials[0], atol=1e-2)
 
         for res in [res_spec, res_ad]:
             # coherence at the harmonic frequencies
             idx_f1 = np.argmin(res.freq < self.f1)
             peak_f1 = res.data[0, idx_f1, 0, 1]
 
             assert peak_f1 > 0.25
@@ -865,129 +886,128 @@
             assert np.all(res.data[0, null_idx, 0, 1] < 0.2)
 
             if len(kwargs) == 0:
                 res.singlepanelplot(channel_i=0, channel_j=1)
 
     def test_ppc_selections(self):
 
-        sel_dct = helpers.mk_selection_dicts(self.nTrials,
-                                             self.nChannels,
-                                             *self.time_span)[0]
+        sel_dct = helpers.mk_selection_dicts(self.nTrials, self.nChannels, *self.time_span)[0]
 
-        result = cafunc(self.data, self.cfg, method='coh', select=sel_dct)
+        result = cafunc(self.data, self.cfg, method="coh", select=sel_dct)
 
         # re-run spectral analysis with selection
-        spec = spy.freqanalysis(self.data, self.cfg, output='fourier',
-                                keeptapers=True, select=sel_dct)
+        spec = spy.freqanalysis(self.data, self.cfg, output="fourier", keeptapers=True, select=sel_dct)
 
-        result_spec = cafunc(spec, method='coh')
+        result_spec = cafunc(spec, method="coh")
 
         # check here just for finiteness and positivity
         assert np.all(np.isfinite(result.data))
         assert np.all(result.data[0, ...] >= -1e-10)
 
         # same results
         assert np.allclose(result.trials[0], result_spec.trials[0], atol=1e-3)
 
         # test one final selection into a result
         # obtained via orignal SpectralData input
-        sel_dct.pop('latency')
-        result_ad = cafunc(self.data, self.cfg, method='ppc', select=sel_dct)
-        result_spec = cafunc(self.spec, method='ppc', select=sel_dct)
+        sel_dct.pop("latency")
+        result_ad = cafunc(self.data, self.cfg, method="ppc", select=sel_dct)
+        result_spec = cafunc(self.spec, method="ppc", select=sel_dct)
         assert np.allclose(result_ad.trials[0], result_spec.trials[0], atol=1e-3)
 
     def test_ppc_foi(self):
 
         # make sure out-of-range foilim  are detected
-        with pytest.raises(SPYValueError, match='foilim'):
-            _ = cafunc(self.data, method='ppc', foilim=[-1, 70])
+        with pytest.raises(SPYValueError, match="foilim"):
+            _ = cafunc(self.data, method="ppc", foilim=[-1, 70])
 
         # make sure invalid foilim are detected
-        with pytest.raises(SPYValueError, match='foilim'):
-            _ = cafunc(self.data, method='ppc', foilim=[None, None])
+        with pytest.raises(SPYValueError, match="foilim"):
+            _ = cafunc(self.data, method="ppc", foilim=[None, None])
 
-        with pytest.raises(SPYValueError, match='foilim'):
-            _ = cafunc(self.data, method='ppc', foilim='abc')
+        with pytest.raises(SPYValueError, match="foilim"):
+            _ = cafunc(self.data, method="ppc", foilim="abc")
 
     @skip_low_mem
     def test_ppc_parallel(self, testcluster):
         check_parallel(self, testcluster)
 
     def test_ppc_padding(self):
 
-        pad_length = 2   # seconds
+        pad_length = 2  # seconds
         call = lambda pad: self.test_ppc_solution(pad=pad)
         helpers.run_padding_test(call, pad_length)
 
     def test_ppc_polyremoval(self):
 
         call = lambda polyremoval: self.test_ppc_solution(polyremoval=polyremoval)
         helpers.run_polyremoval_test(call)
 
 
 def check_parallel(TestClass, testcluster):
     ppl.ioff()
     client = dd.Client(testcluster)
-    all_tests = [attr for attr in TestClass.__dir__()
-                 if (inspect.ismethod(getattr(TestClass, attr)) and 'parallel' not in attr)]
+    all_tests = [
+        attr
+        for attr in TestClass.__dir__()
+        if (inspect.ismethod(getattr(TestClass, attr)) and "parallel" not in attr)
+    ]
     for test in all_tests:
         test_method = getattr(TestClass, test)
         test_method()
     client.close()
     ppl.ion()
 
 
 def run_cfg_test(method_call, method, cfg, positivity=True):
 
     cfg.method = method
-    if method != 'granger':
+    if method != "granger":
         cfg.frequency = [0, 70]
     # test general tapers with
     # additional parameters
-    cfg.taper = 'kaiser'
-    cfg.taper_opt = {'beta': 2}
+    cfg.taper = "kaiser"
+    cfg.taper_opt = {"beta": 2}
 
-    cfg.output = 'abs'
+    cfg.output = "abs"
 
     result = method_call(cfg)
 
     # check here just for finiteness and positivity
     assert np.all(np.isfinite(result.data))
     if positivity:
         assert np.all(result.data[0, ...] >= -1e-10)
 
 
 def plot_Granger(G, i, j):
 
     ax = ppl.gca()
-    ax.set_xlabel('frequency (Hz)')
-    ax.set_ylabel(r'Granger causality(f)')
-    ax.plot(G.freq, G.data[0, :, i, j], label=f'Granger {i}-{j}',
-            alpha=0.7, lw=1.3)
-    ax.set_ylim((-.1, 1.3))
+    ax.set_xlabel("frequency (Hz)")
+    ax.set_ylabel(r"Granger causality(f)")
+    ax.plot(G.freq, G.data[0, :, i, j], label=f"Granger {i}-{j}", alpha=0.7, lw=1.3)
+    ax.set_ylim((-0.1, 1.3))
 
 
-def plot_coh(res, i, j, label=''):
+def plot_coh(res, i, j, label=""):
 
     ax = ppl.gca()
-    ax.set_xlabel('frequency (Hz)')
-    ax.set_ylabel('coherence $|CSD|^2$')
+    ax.set_xlabel("frequency (Hz)")
+    ax.set_ylabel("coherence $|CSD|^2$")
     ax.plot(res.freq, res.data[0, :, i, j], label=label)
     ax.legend()
 
 
-def plot_corr(res, i, j, label=''):
+def plot_corr(res, i, j, label=""):
 
     ax = ppl.gca()
-    ax.set_xlabel('lag (s)')
-    ax.set_ylabel('Correlation')
+    ax.set_xlabel("lag (s)")
+    ax.set_ylabel("Correlation")
     ax.plot(res.time[0], res.data[:, 0, i, j], label=label)
     ax.legend()
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestGranger()
     T2 = TestCoherence()
     T3 = TestCorrelation()
     T4 = TestSpectralInput()
     T5 = TestCSD()
     T6 = TestPPC()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_continuousdata.py` & `esi_syncopy-2023.7/syncopy/tests/test_continuousdata.py`

 * *Files 9% similar despite different names*

```diff
@@ -16,53 +16,69 @@
 # Local imports
 import syncopy as spy
 from syncopy.datatype import AnalogData, SpectralData, CrossSpectralData, TimeLockData
 from syncopy.io import save, load
 from syncopy.datatype.methods.selectdata import selectdata
 from syncopy.shared.errors import SPYValueError, SPYTypeError
 from syncopy.shared.tools import StructDict
-from syncopy.tests.misc import flush_local_cluster, generate_artificial_data, construct_spy_filename
+from syncopy.tests.misc import (
+    flush_local_cluster,
+    generate_artificial_data,
+    construct_spy_filename,
+)
 from syncopy.tests import helpers
 
 
 # Construct decorators for skipping certain tests
 skip_legacy = pytest.mark.skipif(True, reason="code not used atm")
 
 # Collect all supported binary arithmetic operators
-arithmetics = [lambda x, y: x + y,
-               lambda x, y: x - y,
-               lambda x, y: x * y,
-               lambda x, y: x / y,
-               lambda x, y: x ** y]
+arithmetics = [
+    lambda x, y: x + y,
+    lambda x, y: x - y,
+    lambda x, y: x * y,
+    lambda x, y: x / y,
+    lambda x, y: x**y,
+]
 
 # Module-wide set of testing selections
 trialSelections = [
     "all",  # enforce below selections in all trials of `dummy`
-    [3, 1, 2]  # minimally unordered
+    [3, 1, 2],  # minimally unordered
 ]
 chanSelections = [
-    ["channel03", "channel01", "channel01", "channel02"],  # string selection w/repetition + unordered
-    [4, 2, 2, 5, 5],   # repetition + unordered
+    [
+        "channel03",
+        "channel01",
+        "channel01",
+        "channel02",
+    ],  # string selection w/repetition + unordered
+    [4, 2, 2, 5, 5],  # repetition + unordered
     range(5, 8),  # narrow range
     "channel02",  # str selection
-    1  # scalar selection
+    1,  # scalar selection
 ]
 latencySelections = [
-    'all',
-    'minperiod',
+    "all",
+    "minperiod",
     [0.5, 1.5],  # regular range - 'maxperiod'
-    [1., 1.5],
+    [1.0, 1.5],
 ]
 frequencySelections = [
     [2, 11],  # regular range
     [1, 2.0],  # minimal range (just two-time points)
     # [1.0, np.inf]  # unbounded from above, dropped support
 ]
 taperSelections = [
-    ["TestTaper_03", "TestTaper_01", "TestTaper_01", "TestTaper_02"],  # string selection w/repetition + unordered
+    [
+        "TestTaper_03",
+        "TestTaper_01",
+        "TestTaper_01",
+        "TestTaper_02",
+    ],  # string selection w/repetition + unordered
     "TestTaper_03",  # singe str
     0,  # scalar selection
     [0, 1, 1, 2, 3],  # preserve repetition, don't convert to slice
     range(2, 5),  # narrow range
 ]
 timeSelections = list(zip(["latency"] * len(latencySelections), latencySelections))
 freqSelections = list(zip(["frequency"] * len(frequencySelections), frequencySelections))
@@ -151,41 +167,43 @@
     try:
         result = operation(dummy, dummy2)
         cleanSelection = True
     except SPYValueError:
         cleanSelection = False
     if cleanSelection:
         for tk, trl in enumerate(result.trials):
-            assert np.array_equal(trl, operation(selected.trials[tk],
-                                                 selected.trials[tk]))
+            assert np.array_equal(trl, operation(selected.trials[tk], selected.trials[tk]))
         selected = ymmud.selectdata(**kwdict)
         ymmud.selectdata(inplace=True, **kwdict)
         ymmud2.selectdata(inplace=True, **kwdict)
         result = operation(ymmud, ymmud2)
         for tk, trl in enumerate(result.trials):
-            assert np.array_equal(trl, operation(selected.trials[tk],
-                                                 selected.trials[tk]))
+            assert np.array_equal(trl, operation(selected.trials[tk], selected.trials[tk]))
 
     # Very important: clear manually set selections for next iteration
     dummy.selection = None
     dummy2.selection = None
     ymmud.selection = None
     ymmud2.selection = None
 
 
-class TestAnalogData():
+class TestAnalogData:
 
     # Allocate test-dataset
     nc = 10
     ns = 30
     data = np.arange(1, nc * ns + 1, dtype="float").reshape(ns, nc)
-    trl = np.vstack([np.arange(0, ns, 5),
-                     np.arange(5, ns + 5, 5),
-                     np.ones((int(ns / 5), )),
-                     np.ones((int(ns / 5), )) * np.pi]).T
+    trl = np.vstack(
+        [
+            np.arange(0, ns, 5),
+            np.arange(5, ns + 5, 5),
+            np.ones((int(ns / 5),)),
+            np.ones((int(ns / 5),)) * np.pi,
+        ]
+    ).T
     samplerate = 2.0
 
     def test_constructor(self):
 
         # -- test empty --
         dummy = AnalogData()
         assert len(dummy.cfg) == 0
@@ -239,15 +257,15 @@
         # however dims which are not the stacking dim still have to match
         with pytest.raises(SPYValueError, match="mismatching shapes"):
             gen1 = (np.ones((2, i + 1)) for i in range(nTrials))
             _ = AnalogData(data=gen1)
 
         # if we change the dimord/stacking dim, this is fine
         gen1 = (np.ones((2, i + 1)) for i in range(nTrials))
-        dummy3 = AnalogData(data=gen1, dimord=['channel', 'time'])
+        dummy3 = AnalogData(data=gen1, dimord=["channel", "time"])
         assert len(dummy3.trials) == nTrials
 
         # -- test with list of syncopy objects --
 
         concat = AnalogData([dummy2, dummy])
         assert len(concat.trials) == len(dummy.trials) + len(dummy2.trials)
         # check trial sizes kept consistent
@@ -263,55 +281,61 @@
         # samplerate is missing
         with pytest.raises(SPYValueError, match="missing attribute"):
             _ = AnalogData([dummy, dummy4])
 
         dummy4.samplerate = 1
         # channel labels are not the same
         with pytest.raises(SPYValueError, match="different attribute"):
-            dummy4.channel = ['c1', 'c2']
+            dummy4.channel = ["c1", "c2"]
             _ = AnalogData([dummy, dummy4])
 
         # mismatching shape
         dummy5 = AnalogData([np.ones((2, 3))], samplerate=1)
         with pytest.raises(SPYValueError, match="mismatching shapes"):
             _ = AnalogData([dummy, dummy5])
 
         # wrong stacking dim
         with pytest.raises(SPYValueError, match="different stacking"):
             _ = AnalogData([dummy, dummy3])
 
         # test channel property propagation
-        dummy.channel = ['c1', 'c2']
-        dummy2.channel = ['c1', 'c2']
+        dummy.channel = ["c1", "c2"]
+        dummy2.channel = ["c1", "c2"]
 
         concat2 = AnalogData([dummy, dummy2])
         assert np.all(concat2.channel == dummy.channel)
 
     def test_trialretrieval(self):
         # test ``_get_trial`` with NumPy array: regular order
         dummy = AnalogData(data=self.data, trialdefinition=self.trl)
         for trlno, start in enumerate(range(0, self.ns, 5)):
-            trl_ref = self.data[start:start + 5, :]
+            trl_ref = self.data[start : start + 5, :]
             assert np.array_equal(dummy._get_trial(trlno), trl_ref)
 
         # test ``_get_trial`` with NumPy array: swapped dimensions
-        dummy = AnalogData(self.data.T, trialdefinition=self.trl,
-                           dimord=["channel", "time"])
+        dummy = AnalogData(self.data.T, trialdefinition=self.trl, dimord=["channel", "time"])
         for trlno, start in enumerate(range(0, self.ns, 5)):
-            trl_ref = self.data.T[:, start:start + 5]
+            trl_ref = self.data.T[:, start : start + 5]
             assert np.array_equal(dummy._get_trial(trlno), trl_ref)
 
         del dummy
 
     def test_saveload(self):
         with tempfile.TemporaryDirectory() as tdir:
             fname = os.path.join(tdir, "dummy")
 
             # basic but most important: ensure object integrity is preserved
-            checkAttr = ["channel", "data", "dimord", "sampleinfo", "samplerate", "trialinfo"]
+            checkAttr = [
+                "channel",
+                "data",
+                "dimord",
+                "sampleinfo",
+                "samplerate",
+                "trialinfo",
+            ]
             dummy = AnalogData(data=self.data, samplerate=1000)
             dummy.save(fname)
             filename = construct_spy_filename(fname, dummy)
             # NOTE: We removed support for loading data via the constructor
             # dummy2 = AnalogData(filename)
             # for attr in checkAttr:
             #     assert np.array_equal(getattr(dummy, attr), getattr(dummy2, attr))
@@ -335,16 +359,15 @@
             assert np.array_equal(dummy.sampleinfo, dummy2.sampleinfo)
             assert np.array_equal(dummy._t0, dummy2._t0)
             assert np.array_equal(dummy.trialinfo, dummy2.trialinfo)
 
             del dummy, dummy2  # avoid PermissionError in Windows
 
             # swap dimensions and ensure `dimord` is preserved
-            dummy = AnalogData(data=self.data,
-                               dimord=["channel", "time"], samplerate=1000)
+            dummy = AnalogData(data=self.data, dimord=["channel", "time"], samplerate=1000)
             dummy.save(fname + "_dimswap")
             filename = construct_spy_filename(fname + "_dimswap", dummy)
             dummy2 = load(filename)
             assert dummy2.dimord == dummy.dimord
             assert dummy2.channel.size == self.ns  # swapped
             assert dummy2.data.shape == dummy.data.shape
 
@@ -353,28 +376,28 @@
             del dummy, dummy2
             time.sleep(0.1)
 
     # test arithmetic operations
     def test_ang_arithmetic(self):
 
         # Create testing objects and corresponding arrays to perform arithmetics with
-        dummy = AnalogData(data=self.data,
-                           trialdefinition=self.trl,
-                           samplerate=self.samplerate)
-        ymmud = AnalogData(data=self.data.T,
-                           trialdefinition=self.trl,
-                           samplerate=self.samplerate,
-                           dimord=AnalogData._defaultDimord[::-1])
-        dummy2 = AnalogData(data=self.data,
-                            trialdefinition=self.trl,
-                            samplerate=self.samplerate)
-        ymmud2 = AnalogData(data=self.data.T,
-                            trialdefinition=self.trl,
-                            samplerate=self.samplerate,
-                            dimord=AnalogData._defaultDimord[::-1])
+        dummy = AnalogData(data=self.data, trialdefinition=self.trl, samplerate=self.samplerate)
+        ymmud = AnalogData(
+            data=self.data.T,
+            trialdefinition=self.trl,
+            samplerate=self.samplerate,
+            dimord=AnalogData._defaultDimord[::-1],
+        )
+        dummy2 = AnalogData(data=self.data, trialdefinition=self.trl, samplerate=self.samplerate)
+        ymmud2 = AnalogData(
+            data=self.data.T,
+            trialdefinition=self.trl,
+            samplerate=self.samplerate,
+            dimord=AnalogData._defaultDimord[::-1],
+        )
 
         # Perform basic arithmetic with +, -, *, / and ** (pow)
         for operation in arithmetics:
 
             # First, ensure `dimord` is respected
             with pytest.raises(SPYValueError) as spyval:
                 operation(dummy, ymmud)
@@ -385,42 +408,44 @@
             kwdict = {}
             kwdict["trials"] = trialSelections[1]
             kwdict["channel"] = chanSelections[3]
             kwdict[timeSelections[2][0]] = timeSelections[2][1]
             _selection_op_tests(dummy, ymmud, dummy2, ymmud2, kwdict, operation)
 
         # Finally, perform a representative chained operation to ensure chaining works
-        result = (dummy + dummy2) / dummy ** 3
+        result = (dummy + dummy2) / dummy**3
         for tk, trl in enumerate(result.trials):
-            assert np.array_equal(trl,
-                                  (dummy.trials[tk] + dummy2.trials[tk]) / dummy.trials[tk] ** 3)
+            assert np.array_equal(trl, (dummy.trials[tk] + dummy2.trials[tk]) / dummy.trials[tk] ** 3)
 
     def test_parallel(self, testcluster):
         # repeat selected test w/parallel processing engine
         client = dd.Client(testcluster)
-        slow_tests = ["test_dataselection",
-                      "test_ang_arithmetic"]
+        slow_tests = ["test_ang_arithmetic"]
         for test in slow_tests:
             getattr(self, test)()
             flush_local_cluster(testcluster)
         client.close()
 
 
-class TestSpectralData():
+class TestSpectralData:
 
     # Allocate test-dataset
     nc = 10
     ns = 30
     nt = 5
     nf = 15
     data = np.arange(1, nc * ns * nt * nf + 1, dtype="float").reshape(ns, nt, nf, nc)
-    trl = np.vstack([np.arange(0, ns, 5),
-                     np.arange(5, ns + 5, 5),
-                     np.ones((int(ns / 5), )),
-                     np.ones((int(ns / 5), )) * np.pi]).T
+    trl = np.vstack(
+        [
+            np.arange(0, ns, 5),
+            np.arange(5, ns + 5, 5),
+            np.ones((int(ns / 5),)),
+            np.ones((int(ns / 5),)) * np.pi,
+        ]
+    ).T
     data2 = np.moveaxis(data, 0, -1)
     samplerate = 2.0
 
     def test_sd_empty(self):
         dummy = SpectralData()
         assert len(dummy.cfg) == 0
         for attr in ["channel", "data", "freq", "sampleinfo", "taper", "trialinfo"]:
@@ -463,33 +488,44 @@
         # check trial sizes
         assert len(concat.trials) == 2 * nTrials
 
     def test_sd_trialretrieval(self):
         # test ``_get_trial`` with NumPy array: regular order
         dummy = SpectralData(self.data, trialdefinition=self.trl)
         for trlno, start in enumerate(range(0, self.ns, 5)):
-            trl_ref = self.data[start:start + 5, ...]
+            trl_ref = self.data[start : start + 5, ...]
             assert np.array_equal(dummy._get_trial(trlno), trl_ref)
 
         # test ``_get_trial`` with NumPy array: swapped dimensions
-        dummy = SpectralData(self.data2, trialdefinition=self.trl,
-                             dimord=["taper", "channel", "freq", "time"])
+        dummy = SpectralData(
+            self.data2,
+            trialdefinition=self.trl,
+            dimord=["taper", "channel", "freq", "time"],
+        )
         for trlno, start in enumerate(range(0, self.ns, 5)):
-            trl_ref = self.data2[..., start:start + 5]
+            trl_ref = self.data2[..., start : start + 5]
             assert np.array_equal(dummy._get_trial(trlno), trl_ref)
 
         del dummy
 
     def test_sd_saveload(self):
         with tempfile.TemporaryDirectory() as tdir:
             fname = os.path.join(tdir, "dummy")
 
             # basic but most important: ensure object integrity is preserved
-            checkAttr = ["channel", "data", "dimord", "freq", "sampleinfo",
-                         "samplerate", "taper", "trialinfo"]
+            checkAttr = [
+                "channel",
+                "data",
+                "dimord",
+                "freq",
+                "sampleinfo",
+                "samplerate",
+                "taper",
+                "trialinfo",
+            ]
             dummy = SpectralData(self.data, samplerate=1000)
             dummy.save(fname)
             filename = construct_spy_filename(fname, dummy)
             # dummy2 = SpectralData(filename)
             # for attr in checkAttr:
             #     assert np.array_equal(getattr(dummy, attr), getattr(dummy2, attr))
             dummy3 = load(fname)
@@ -509,16 +545,15 @@
 
             # test getters
             assert np.array_equal(dummy.sampleinfo, dummy2.sampleinfo)
             assert np.array_equal(dummy._t0, dummy2._t0)
             assert np.array_equal(dummy.trialinfo, dummy2.trialinfo)
 
             # swap dimensions and ensure `dimord` is preserved
-            dummy = SpectralData(self.data, dimord=["time", "channel", "taper", "freq"],
-                                 samplerate=1000)
+            dummy = SpectralData(self.data, dimord=["time", "channel", "taper", "freq"], samplerate=1000)
             dummy.save(fname + "_dimswap")
             filename = construct_spy_filename(fname + "_dimswap", dummy)
             dummy2 = load(filename)
             assert dummy2.dimord == dummy.dimord
             assert dummy2.channel.size == self.nt  # swapped
             assert dummy2.taper.size == self.nf  # swapped
             assert dummy2.data.shape == dummy.data.shape
@@ -526,94 +561,113 @@
             # Delete all open references to file objects b4 closing tmp dir
             del dummy, dummy2
 
     # test arithmetic operations
     def test_sd_arithmetic(self):
 
         # Create testing objects and corresponding arrays to perform arithmetics with
-        dummy = SpectralData(data=self.data,
-                             trialdefinition=self.trl,
-                             samplerate=self.samplerate,
-                             taper=["TestTaper_0{}".format(k) for k in range(1, self.nt + 1)])
-        dummyC = SpectralData(data=np.complex64(self.data),
-                              trialdefinition=self.trl,
-                              samplerate=self.samplerate,
-                              taper=["TestTaper_0{}".format(k) for k in range(1, self.nt + 1)])
-        ymmud = SpectralData(data=np.transpose(self.data, [3, 2, 1, 0]),
-                             trialdefinition=self.trl,
-                             samplerate=self.samplerate,
-                             taper=["TestTaper_0{}".format(k) for k in range(1, self.nt + 1)],
-                             dimord=SpectralData._defaultDimord[::-1])
-        dummy2 = SpectralData(data=self.data,
-                              trialdefinition=self.trl,
-                              samplerate=self.samplerate,
-                              taper=["TestTaper_0{}".format(k) for k in range(1, self.nt + 1)])
-        ymmud2 = SpectralData(data=np.transpose(self.data, [3, 2, 1, 0]),
-                              trialdefinition=self.trl,
-                              samplerate=self.samplerate,
-                              taper=["TestTaper_0{}".format(k) for k in range(1, self.nt + 1)],
-                              dimord=SpectralData._defaultDimord[::-1])
+        dummy = SpectralData(
+            data=self.data,
+            trialdefinition=self.trl,
+            samplerate=self.samplerate,
+            taper=["TestTaper_0{}".format(k) for k in range(1, self.nt + 1)],
+        )
+        dummyC = SpectralData(
+            data=np.complex64(self.data),
+            trialdefinition=self.trl,
+            samplerate=self.samplerate,
+            taper=["TestTaper_0{}".format(k) for k in range(1, self.nt + 1)],
+        )
+        ymmud = SpectralData(
+            data=np.transpose(self.data, [3, 2, 1, 0]),
+            trialdefinition=self.trl,
+            samplerate=self.samplerate,
+            taper=["TestTaper_0{}".format(k) for k in range(1, self.nt + 1)],
+            dimord=SpectralData._defaultDimord[::-1],
+        )
+        dummy2 = SpectralData(
+            data=self.data,
+            trialdefinition=self.trl,
+            samplerate=self.samplerate,
+            taper=["TestTaper_0{}".format(k) for k in range(1, self.nt + 1)],
+        )
+        ymmud2 = SpectralData(
+            data=np.transpose(self.data, [3, 2, 1, 0]),
+            trialdefinition=self.trl,
+            samplerate=self.samplerate,
+            taper=["TestTaper_0{}".format(k) for k in range(1, self.nt + 1)],
+            dimord=SpectralData._defaultDimord[::-1],
+        )
 
         # Perform basic arithmetic with +, -, *, / and ** (pow)
         for operation in arithmetics:
 
             # First, ensure `dimord` is respected
             with pytest.raises(SPYValueError) as spyval:
                 operation(dummy, ymmud)
                 assert "expected Syncopy 'time' x 'channel' data object" in str(spyval.value)
 
             _base_op_tests(dummy, ymmud, dummy2, ymmud2, dummyC, operation)
 
-
             kwdict = {}
             kwdict["trials"] = trialSelections[1]
             kwdict["channel"] = chanSelections[3]
             kwdict[timeSelections[2][0]] = timeSelections[2][1]
             kwdict[freqSelections[1][0]] = freqSelections[1][1]
             kwdict["taper"] = taperSelections[2]
             _selection_op_tests(dummy, ymmud, dummy2, ymmud2, kwdict, operation)
 
         # Finally, perform a representative chained operation to ensure chaining works
-        result = (dummy + dummy2) / dummy ** 3
+        result = (dummy + dummy2) / dummy**3
         for tk, trl in enumerate(result.trials):
-            assert np.array_equal(trl,
-                                  (dummy.trials[tk] + dummy2.trials[tk]) / dummy.trials[tk] ** 3)
+            assert np.array_equal(trl, (dummy.trials[tk] + dummy2.trials[tk]) / dummy.trials[tk] ** 3)
 
     def test_sd_parallel(self, testcluster):
         # repeat selected test w/parallel processing engine
         client = dd.Client(testcluster)
         par_tests = ["test_sd_arithmetic", "test_sd_concat"]
         for test in par_tests:
             getattr(self, test)()
             flush_local_cluster(testcluster)
         client.close()
 
 
-class TestCrossSpectralData():
+class TestCrossSpectralData:
 
     # Allocate test-dataset
     nci = 10
     ncj = 12
     nl = 3
     nt = 6
     ns = nt * nl
     nf = 15
     data = np.arange(1, nci * ncj * ns * nf + 1, dtype="float").reshape(ns, nf, nci, ncj)
-    trl = np.vstack([np.arange(0, ns, nl),
-                     np.arange(nl, ns + nl, nl),
-                     np.ones((int(ns / nl), )),
-                     np.ones((int(ns / nl), )) * np.pi]).T
+    trl = np.vstack(
+        [
+            np.arange(0, ns, nl),
+            np.arange(nl, ns + nl, nl),
+            np.ones((int(ns / nl),)),
+            np.ones((int(ns / nl),)) * np.pi,
+        ]
+    ).T
     data2 = np.moveaxis(data, 0, -1)
     samplerate = 2.0
 
     def test_csd_empty(self):
         dummy = CrossSpectralData()
         assert len(dummy.cfg) == 0
         assert dummy.dimord is None
-        for attr in ["channel_i", "channel_j", "data", "freq", "sampleinfo", "trialinfo"]:
+        for attr in [
+            "channel_i",
+            "channel_j",
+            "data",
+            "freq",
+            "sampleinfo",
+            "trialinfo",
+        ]:
             assert getattr(dummy, attr) is None
         with pytest.raises(SPYTypeError):
             CrossSpectralData({})
 
     def test_csd_nparray(self):
         dummy = CrossSpectralData(self.data)
         assert dummy.dimord == CrossSpectralData._defaultDimord
@@ -655,33 +709,41 @@
             concat2 = CrossSpectralData([dummy, dummy3])
 
     def test_csd_trialretrieval(self):
         # test ``_get_trial`` with NumPy array: regular order
         dummy = CrossSpectralData(self.data)
         dummy.trialdefinition = self.trl
         for trlno, start in enumerate(range(0, self.ns, self.nl)):
-            trl_ref = self.data[start:start + self.nl, ...]
+            trl_ref = self.data[start : start + self.nl, ...]
             assert np.array_equal(dummy.trials[trlno], trl_ref)
 
         # test ``_get_trial`` with NumPy array: swapped dimensions
         dummy = CrossSpectralData(self.data2, dimord=["freq", "channel_i", "channel_j", "time"])
         dummy.trialdefinition = self.trl
         for trlno, start in enumerate(range(0, self.ns, self.nl)):
-            trl_ref = self.data2[..., start:start + self.nl]
+            trl_ref = self.data2[..., start : start + self.nl]
             assert np.array_equal(dummy.trials[trlno], trl_ref)
 
         del dummy
 
     def test_csd_saveload(self):
         with tempfile.TemporaryDirectory() as tdir:
             fname = os.path.join(tdir, "dummy")
 
             # basic but most important: ensure object integrity is preserved
-            checkAttr = ["channel_i", "channel_i", "data", "dimord", "freq", "sampleinfo",
-                         "samplerate", "trialinfo"]
+            checkAttr = [
+                "channel_i",
+                "channel_i",
+                "data",
+                "dimord",
+                "freq",
+                "sampleinfo",
+                "samplerate",
+                "trialinfo",
+            ]
             dummy = CrossSpectralData(self.data, samplerate=1000)
             dummy.save(fname)
             filename = construct_spy_filename(fname, dummy)
             dummy3 = load(fname)
             for attr in checkAttr:
                 assert np.array_equal(getattr(dummy3, attr), getattr(dummy, attr))
             save(dummy3, container=os.path.join(tdir, "ymmud"))
@@ -699,16 +761,19 @@
 
             # test getters
             assert np.array_equal(dummy.sampleinfo, dummy2.sampleinfo)
             assert np.array_equal(dummy._t0, dummy2._t0)
             assert np.array_equal(dummy.trialinfo, dummy2.trialinfo)
 
             # swap dimensions and ensure `dimord` is preserved
-            dummy = CrossSpectralData(self.data, dimord=["freq", "channel_j", "channel_i", "time"],
-                                      samplerate=1000)
+            dummy = CrossSpectralData(
+                self.data,
+                dimord=["freq", "channel_j", "channel_i", "time"],
+                samplerate=1000,
+            )
             dummy.save(fname + "_dimswap")
             filename = construct_spy_filename(fname + "_dimswap", dummy)
             dummy2 = load(filename)
             assert dummy2.dimord == dummy.dimord
             assert dummy2.channel_i.size == self.nci  # swapped
             assert dummy2.channel_j.size == self.nf  # swapped
             assert dummy2.freq.size == self.ns  # swapped
@@ -717,56 +782,58 @@
             # Delete all open references to file objects b4 closing tmp dir
             del dummy, dummy2
 
     # test arithmetic operations
     def test_csd_arithmetic(self):
 
         # Create testing objects and corresponding arrays to perform arithmetics with
-        dummy = CrossSpectralData(data=self.data,
-                                  samplerate=self.samplerate)
+        dummy = CrossSpectralData(data=self.data, samplerate=self.samplerate)
         dummy.trialdefinition = self.trl
-        dummyC = CrossSpectralData(data=np.complex64(self.data),
-                                   samplerate=self.samplerate)
+        dummyC = CrossSpectralData(data=np.complex64(self.data), samplerate=self.samplerate)
         dummyC.trialdefinition = self.trl
-        ymmud = CrossSpectralData(data=np.transpose(self.data, [3, 2, 1, 0]),
-                                  samplerate=self.samplerate,
-                                  dimord=CrossSpectralData._defaultDimord[::-1])
+        ymmud = CrossSpectralData(
+            data=np.transpose(self.data, [3, 2, 1, 0]),
+            samplerate=self.samplerate,
+            dimord=CrossSpectralData._defaultDimord[::-1],
+        )
         ymmud.trialdefinition = self.trl
-        dummy2 = CrossSpectralData(data=self.data,
-                                   samplerate=self.samplerate)
+        dummy2 = CrossSpectralData(data=self.data, samplerate=self.samplerate)
         dummy2.trialdefinition = self.trl
-        ymmud2 = CrossSpectralData(data=np.transpose(self.data, [3, 2, 1, 0]),
-                                   samplerate=self.samplerate,
-                                   dimord=CrossSpectralData._defaultDimord[::-1])
+        ymmud2 = CrossSpectralData(
+            data=np.transpose(self.data, [3, 2, 1, 0]),
+            samplerate=self.samplerate,
+            dimord=CrossSpectralData._defaultDimord[::-1],
+        )
         ymmud2.trialdefinition = self.trl
 
         # Perform basic arithmetic with +, -, *, / and ** (pow)
         for operation in arithmetics:
 
             # First, ensure `dimord` is respected
             with pytest.raises(SPYValueError) as spyval:
                 operation(dummy, ymmud)
-                assert "expected Syncopy 'time' x 'freq' x 'channel_i' x 'channel_j' data object" in str(spyval.value)
+                assert "expected Syncopy 'time' x 'freq' x 'channel_i' x 'channel_j' data object" in str(
+                    spyval.value
+                )
 
             _base_op_tests(dummy, ymmud, dummy2, ymmud2, dummyC, operation)
 
             # Go through full selection stack - WARNING: this takes > 1 hour
             kwdict = {}
             kwdict["trials"] = trialSelections[1]
             kwdict["channel_i"] = chanSelections[3]
             kwdict["channel_j"] = chanSelections[4]
             kwdict[timeSelections[2][0]] = timeSelections[2][1]
             kwdict[freqSelections[1][0]] = freqSelections[1][1]
             _selection_op_tests(dummy, ymmud, dummy2, ymmud2, kwdict, operation)
 
         # Finally, perform a representative chained operation to ensure chaining works
-        result = (dummy + dummy2) / dummy ** 3
+        result = (dummy + dummy2) / dummy**3
         for tk, trl in enumerate(result.trials):
-            assert np.array_equal(trl,
-                                  (dummy.trials[tk] + dummy2.trials[tk]) / dummy.trials[tk] ** 3)
+            assert np.array_equal(trl, (dummy.trials[tk] + dummy2.trials[tk]) / dummy.trials[tk] ** 3)
 
     def test_csd_parallel(self, testcluster):
         # repeat selected test w/parallel processing engine
         client = dd.Client(testcluster)
         par_tests = ["test_csd_arithmetic", "test_csd_concat"]
         for test in par_tests:
             getattr(self, test)
@@ -777,17 +844,17 @@
 class TestTimeLockData:
     """Tests for the `TimeLockData` data type, which is derived from `ContinuousData`."""
 
     def test_create(self):
         """Test instantiation, and that expected properties/datasets specific to this data type exist."""
         tld = TimeLockData()
 
-        assert hasattr(tld, '_avg')
-        assert hasattr(tld, '_var')
-        assert hasattr(tld, '_cov')
+        assert hasattr(tld, "_avg")
+        assert hasattr(tld, "_var")
+        assert hasattr(tld, "_cov")
         assert tld.avg is None
         assert tld.var is None
         assert tld.cov is None
 
     def test_modify_properties(self):
         """Test modification of the extra datasets avg, var, cov."""
         tld = TimeLockData()
@@ -795,27 +862,27 @@
         avg_data = np.zeros((3, 3), dtype=np.float64)
         tld._update_dataset("avg", avg_data)
         assert isinstance(tld.avg, h5py.Dataset)
         assert np.array_equal(avg_data, tld.avg)
 
         # Try to overwrite data via setter, which should not work.
         avg_data2 = np.zeros((4, 4, 4), dtype=np.float32)
-        with pytest.raises(AttributeError, match="can't set attribute"):
+        with pytest.raises(AttributeError, match="set"):
             tld.avg = avg_data2
 
         # But we can do it with _update_dataset:
         tld._update_dataset("avg", avg_data2)
         assert np.array_equal(avg_data2, tld.avg)
 
         # ... or of course, directly using '_avg':
         tld2 = TimeLockData()
         avg_data3 = np.zeros((2, 2), dtype=np.float32)
         tld2._avg = avg_data3
         assert np.array_equal(avg_data3, tld2.avg)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
 
     T1 = TestAnalogData()
     T2 = TestSpectralData()
     T3 = TestTimeLockData()
     T4 = TestCrossSpectralData()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_datatype_util.py` & `esi_syncopy-2023.7/syncopy/tests/test_datatype_util.py`

 * *Files 16% similar despite different names*

```diff
@@ -7,16 +7,15 @@
 import os
 import tempfile
 
 # Local imports
 from syncopy.datatype.util import get_dir_size
 
 
-class TestDirSize():
-
+class TestDirSize:
     def test_dirsize(self):
         with tempfile.TemporaryDirectory() as tdir:
             fname = "tmpfile"
             for file_idx in range(20):
                 tf = os.path.join(tdir, fname + str(file_idx))
                 with open(tf, "w") as f:
                     f.write(f"This is a dummy file {file_idx}.")
@@ -25,13 +24,10 @@
             assert dir_size_byte > 200
             assert dir_size_byte < 2000
             assert dir_size_byte == 470
             dir_size_gb, num_files = get_dir_size(tdir, out="GB")
             assert dir_size_gb < 1e-6
 
 
-
-
-if __name__ == '__main__':
+if __name__ == "__main__":
 
     T1 = TestDirSize()
-
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_decorators.py` & `esi_syncopy-2023.7/syncopy/tests/test_decorators.py`

 * *Files 2% similar despite different names*

```diff
@@ -28,15 +28,15 @@
         if groupbychan in data.channel:
             group = data.filename
     else:
         group = data.filename
     return group
 
 
-class TestSpyCalls():
+class TestSpyCalls:
 
     nChan = 13
     nObjs = nChan
 
     # Generate `nChan` objects whose channel-labeling scheme obeys:
     # ob1.channel =  ["A", "B", "C", ..., "M"]
     # ob2.channel =  [     "B", "C", ..., "M", "N"]
@@ -88,18 +88,21 @@
         # both keywords
         fname = group_objects(cfg=cfg, data=self.data)
         assert fname == self.data.filename
 
     def test_invalidcallstyles(self):
 
         # expected error messages
-        errmsg1 = "expected Syncopy data object provided either via " +\
-                 "`cfg`/keyword or positional arguments, not both"
-        errmsg2 = "expected Syncopy data object provided either via `cfg` " +\
-            "or as keyword argument, not both"
+        errmsg1 = (
+            "expected Syncopy data object provided either via "
+            + "`cfg`/keyword or positional arguments, not both"
+        )
+        errmsg2 = (
+            "expected Syncopy data object provided either via `cfg` " + "or as keyword argument, not both"
+        )
         errmsg3 = "expected either 'data' or 'dataset' in `cfg`/keywords, not both"
 
         # ensure things break reliably for 'data' as well as 'dataset'
         for key in ["data", "dataset"]:
 
             # data + cfg w/data
             cfg = StructDict()
@@ -131,45 +134,44 @@
             with pytest.raises(SPYValueError) as exc:
                 group_objects(cfg, dataset=self.data)
             assert errmsg2 in str(exc.value)
 
         # cfg (no data) but double-whammied
         cfg = StructDict()
         cfg.groupbychan = None
-        with pytest.raises(SPYValueError)as exc:
+        with pytest.raises(SPYValueError) as exc:
             group_objects(self.data, cfg, cfg=cfg)
         assert "expected `cfg` either as positional or keyword argument, not both" in str(exc.value)
 
         # keyword set via cfg and kwarg
         with pytest.raises(SPYValueError) as exc:
             group_objects(self.data, cfg, groupbychan="invalid")
         assert "set in both `cfg` and via explicit keyword" in str(exc.value)
 
         # both data and dataset in cfg/keywords
         cfg = StructDict()
         cfg.data = self.data
         cfg.dataset = self.data
-        with pytest.raises(SPYValueError)as exc:
+        with pytest.raises(SPYValueError) as exc:
             group_objects(cfg)
         assert errmsg3 in str(exc.value)
-        with pytest.raises(SPYValueError)as exc:
+        with pytest.raises(SPYValueError) as exc:
             group_objects(data=self.data, dataset=self.data)
         assert errmsg3 in str(exc.value)
 
         # data/dataset do not contain Syncopy object
-        with pytest.raises(SPYError)as exc:
+        with pytest.raises(SPYError) as exc:
             group_objects(data="invalid")
         assert "`data` must be Syncopy data object!" in str(exc.value)
 
         # cfg is not dict/StructDict
-        with pytest.raises(SPYTypeError)as exc:
+        with pytest.raises(SPYTypeError) as exc:
             group_objects(cfg="invalid")
         assert "Wrong type of `cfg`: expected dictionary-like" in str(exc.value)
 
-
     def test_varargin(self):
         """
         This was originally meant to test multiple Syncopy objects
         as 'data' input at once, this functionality was deprecated and got
         removed. What remains are the `groupbychan` cfg and select tests.
         """
 
@@ -215,30 +217,32 @@
             cfg = StructDict()
             cfg.dataset = self.dataObjs[0]
             cfg.groupbychan = letter
             fnameList = group_objects(cfg)
             assert self.dataObjs[0].filename == fnameList
 
             # data positional + select keyword
-            fnameList = [group_objects(data, select={"channel": [letter]}) for data in self.dataObjs[:letterIdx + 1]]
+            fnameList = [
+                group_objects(data, select={"channel": [letter]}) for data in self.dataObjs[: letterIdx + 1]
+            ]
             fnameList = [el for el in fnameList if el is not None]
             assert groupList == fnameList
 
             # data positional + cfg w/select
             cfg = StructDict()
             cfg.select = {"channel": [letter]}
-            fnameList = [group_objects(data, cfg) for data in self.dataObjs[:letterIdx + 1]]
+            fnameList = [group_objects(data, cfg) for data in self.dataObjs[: letterIdx + 1]]
             fnameList = [el for el in fnameList if el is not None]
 
             assert groupList == fnameList
 
         # invalid selection
         with pytest.raises(SPYValueError) as exc:
             group_objects(self.dataObjs[0], select={"channel": ["Z"]})
         assert "expected list/array of channel existing names or indices" in str(exc.value)
 
         # data does not only contain Syncopy objects
         cfg = StructDict()
         cfg.data = "invalid"
-        with pytest.raises(SPYError)as exc:
+        with pytest.raises(SPYError) as exc:
             group_objects(cfg)
         assert "`data` must be Syncopy data object!" in str(exc.value)
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_discretedata.py` & `esi_syncopy-2023.7/syncopy/tests/test_discretedata.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,88 +17,106 @@
 from syncopy.datatype import AnalogData, SpikeData, EventData
 from syncopy.io import save, load
 from syncopy.shared.errors import SPYValueError, SPYTypeError
 from syncopy.tests.misc import construct_spy_filename
 from syncopy.tests.test_selectdata import getSpikeData
 
 
-class TestSpikeData():
+class TestSpikeData:
 
     # Allocate test-dataset
     nc = 10
     ns = 30
     nd = 50
     seed = np.random.RandomState(13)
-    data = np.vstack([seed.choice(ns, size=nd),
-                      seed.choice(nc, size=nd),
-                      seed.choice(int(nc / 2), size=nd)]).T
-    data = data[data[:,0].argsort()]
+    data = np.vstack(
+        [
+            seed.choice(ns, size=nd),
+            seed.choice(nc, size=nd),
+            seed.choice(int(nc / 2), size=nd),
+        ]
+    ).T
+    data = data[data[:, 0].argsort()]
     data2 = data.copy()
     data2[:, -1] = data[:, 0]
     data2[:, 0] = data[:, -1]
-    trl = np.vstack([np.arange(0, ns, 5),
-                     np.arange(5, ns + 5, 5),
-                     np.ones((int(ns / 5), )),
-                     np.ones((int(ns / 5), )) * np.pi]).T
+    trl = np.vstack(
+        [
+            np.arange(0, ns, 5),
+            np.arange(5, ns + 5, 5),
+            np.ones((int(ns / 5),)),
+            np.ones((int(ns / 5),)) * np.pi,
+        ]
+    ).T
     num_smp = np.unique(data[:, 0]).size
     num_chn = data[:, 1].max() + 1
     num_unt = data[:, 2].max() + 1
 
     def test_init(self):
 
         # data and no labels triggers default labels
-        dummy = SpikeData(data=4  * np.ones((2, 3), dtype=int))
+        dummy = SpikeData(data=4 * np.ones((2, 3), dtype=int))
         # labels are 0-based
-        assert dummy.channel == 'channel05'
-        assert dummy.unit == 'unit05'
+        assert dummy.channel == "channel5"
+        assert dummy.unit == "unit5"
 
         # data and fitting labels is fine
-        assert isinstance(SpikeData(data=np.ones((2, 3), dtype=int), channel=['only_channel']),
-                          SpikeData)
+        assert isinstance(
+            SpikeData(data=np.ones((2, 3), dtype=int), channel=["only_channel"]),
+            SpikeData,
+        )
 
         # --- invalid inits ---
 
         # non-integer types
-        with pytest.raises(SPYTypeError, match='expected integer like'):
-            _ = SpikeData(data=np.ones((2, 3)), unit=['unit1', 'unit2'])
+        with pytest.raises(SPYTypeError, match="expected integer like"):
+            _ = SpikeData(data=np.ones((2, 3)), unit=["unit1", "unit2"])
 
-        with pytest.raises(SPYTypeError, match='expected integer like'):
+        with pytest.raises(SPYTypeError, match="expected integer like"):
             data = np.array([np.nan, 2, np.nan])[:, np.newaxis]
-            _ = SpikeData(data=data, unit=['unit1', 'unit2'])
+            _ = SpikeData(data=data, unit=["unit1", "unit2"])
 
         # data and too many labels
-        with pytest.raises(SPYValueError, match='expected exactly 1 unit'):
-            _ = SpikeData(data=np.ones((2, 3), dtype=int), unit=['unit1', 'unit2'])
+        with pytest.raises(SPYValueError, match="expected exactly 1 unit"):
+            _ = SpikeData(data=np.ones((2, 3), dtype=int), unit=["unit1", "unit2"])
 
         # no data but labels
-        with pytest.raises(SPYValueError, match='cannot assign `channel` without data'):
-            _ = SpikeData(channel=['a', 'b', 'c'])
+        with pytest.raises(SPYValueError, match="cannot assign `channel` without data"):
+            _ = SpikeData(channel=["a", "b", "c"])
 
     def test_register_dset(self):
         sdata = SpikeData(self.data, samplerate=10)
         assert not sdata._is_empty()
-        sdata._register_dataset("blah", np.zeros((3,3), dtype=float))
-
+        sdata._register_dataset("blah", np.zeros((3, 3), dtype=float))
 
     def test_empty(self):
         dummy = SpikeData()
         assert len(dummy.cfg) == 0
         assert dummy.dimord is None
-        for attr in ["channel", "data", "sampleinfo", "samplerate",
-                     "trialid", "trialinfo", "unit"]:
+        for attr in [
+            "channel",
+            "data",
+            "sampleinfo",
+            "samplerate",
+            "trialid",
+            "trialinfo",
+            "unit",
+        ]:
             assert getattr(dummy, attr) is None
         with pytest.raises(SPYTypeError):
             SpikeData({})
 
     def test_issue_257_fixed_no_error_for_empty_data(self):
         """This tests that empty datasets are not allowed"""
-        with pytest.raises(SPYValueError, match='non empty'):
-            data = SpikeData(np.column_stack(([],[],[])).astype(int),
-                             dimord=['sample', 'channel', 'unit'],
-                             samplerate=30000)
+        with pytest.raises(SPYValueError, match="non empty"):
+            data = SpikeData(
+                np.column_stack(([], [], [])).astype(int),
+                dimord=["sample", "channel", "unit"],
+                samplerate=30000,
+            )
 
     def test_nparray(self):
         dummy = SpikeData(self.data)
         assert dummy.dimord == ["sample", "channel", "unit"]
         assert dummy.channel.size == self.num_chn
         # NOTE: SpikeData.sample is currently empty
         # assert dummy.sample.size == self.num_smp
@@ -112,36 +130,45 @@
             SpikeData(np.ones((3,)))
 
     def test_trialretrieval(self):
         # test ``_get_trial`` with NumPy array: regular order
         dummy = SpikeData(self.data, trialdefinition=self.trl)
         smp = self.data[:, 0]
         for trlno, start in enumerate(range(0, self.ns, 5)):
-            idx = np.intersect1d(np.where(smp >= start)[0],
-                                 np.where(smp < start + 5)[0])
+            idx = np.intersect1d(np.where(smp >= start)[0], np.where(smp < start + 5)[0])
             trl_ref = self.data[idx, ...]
             assert np.array_equal(dummy._get_trial(trlno), trl_ref)
 
         # test ``_get_trial`` with NumPy array: swapped dimensions
-        dummy = SpikeData(self.data2, trialdefinition=self.trl,
-                          dimord=["unit", "channel", "sample"])
+        dummy = SpikeData(self.data2, trialdefinition=self.trl, dimord=["unit", "channel", "sample"])
         smp = self.data2[:, -1]
         for trlno, start in enumerate(range(0, self.ns, 5)):
-            idx = np.intersect1d(np.where(smp >= start)[0],
-                                 np.where(smp < start + 5)[0])
+            idx = np.intersect1d(np.where(smp >= start)[0], np.where(smp < start + 5)[0])
             trl_ref = self.data2[idx, ...]
             assert np.array_equal(dummy._get_trial(trlno), trl_ref)
 
+    def test_str_rep_with_trials(self):
+        """Test string representation of SpikeData with trialdefinition. Ensure that the bug with the string representation is fixed."""
+        dummy = SpikeData(self.data, trialdefinition=self.trl)
+        assert "samplerate" in str(dummy)
+
     def test_saveload(self):
         with tempfile.TemporaryDirectory() as tdir:
             fname = os.path.join(tdir, "dummy")
 
             # basic but most important: ensure object integrity is preserved
-            checkAttr = ["channel", "data", "dimord", "sampleinfo",
-                         "samplerate", "trialinfo", "unit"]
+            checkAttr = [
+                "channel",
+                "data",
+                "dimord",
+                "sampleinfo",
+                "samplerate",
+                "trialinfo",
+                "unit",
+            ]
             dummy = SpikeData(self.data, samplerate=10)
             dummy.save(fname)
             filename = construct_spy_filename(fname, dummy)
             # dummy2 = SpikeData(filename)
             # for attr in checkAttr:
             #     assert np.array_equal(getattr(dummy, attr), getattr(dummy2, attr))
             dummy3 = load(fname)
@@ -182,30 +209,33 @@
             assert dummy2.data.shape == dummy.data.shape
 
             # Delete all open references to file objects b4 closing tmp dir
             del dummy, dummy2
             time.sleep(0.1)
 
 
-class TestEventData():
+class TestEventData:
 
     # Allocate test-datasets
     nc = 10
     ns = 30
-    data = np.vstack([np.arange(0, ns, 5),
-                      np.zeros((int(ns / 5), ))]).T.astype(int)
+    data = np.vstack([np.arange(0, ns, 5), np.zeros((int(ns / 5),))]).T.astype(int)
     data[1::2, 1] = 1
     data2 = data.copy()
     data2[:, -1] = data[:, 0]
     data2[:, 0] = data[:, -1]
     data3 = np.hstack([data2, data2])
-    trl = np.vstack([np.arange(0, ns, 5),
-                     np.arange(5, ns + 5, 5),
-                     np.ones((int(ns / 5), )),
-                     np.ones((int(ns / 5), )) * np.pi]).T
+    trl = np.vstack(
+        [
+            np.arange(0, ns, 5),
+            np.arange(5, ns + 5, 5),
+            np.ones((int(ns / 5),)),
+            np.ones((int(ns / 5),)) * np.pi,
+        ]
+    ).T
     num_smp = np.unique(data[:, 0]).size
     num_evt = np.unique(data[:, 1]).size
     customDimord = ["sample", "eventid", "custom1", "custom2"]
 
     adata = np.arange(1, nc * ns + 1).reshape(ns, nc)
 
     def test_ed_empty(self):
@@ -230,45 +260,45 @@
         # wrong shape for data-type
         with pytest.raises(SPYValueError):
             EventData(np.ones((3,)))
 
     def test_register_dset(self):
         edata = EventData(self.data, samplerate=10)
         assert not edata._is_empty()
-        edata._register_dataset("blah", np.zeros((3,3), dtype=float))
+        edata._register_dataset("blah", np.zeros((3, 3), dtype=float))
+
+    def test_str_rep_with_trials(self):
+        """Test string representation of EventData with trialdefinition. Ensure that the bug with the string representation is fixed."""
+        dummy = EventData(self.data, trialdefinition=self.trl)
+        assert "samplerate" in str(dummy)  # The real test is that 'str(dummy)' does not raise an error.
 
     def test_ed_trialretrieval(self):
         # test ``_get_trial`` with NumPy array: regular order
         dummy = EventData(self.data, trialdefinition=self.trl)
         smp = self.data[:, 0]
         for trlno, start in enumerate(range(0, self.ns, 5)):
-            idx = np.intersect1d(np.where(smp >= start)[0],
-                                 np.where(smp < start + 5)[0])
+            idx = np.intersect1d(np.where(smp >= start)[0], np.where(smp < start + 5)[0])
             trl_ref = self.data[idx, ...]
             assert np.array_equal(dummy._get_trial(trlno), trl_ref)
 
         # test `_get_trial` with NumPy array: swapped dimensions
-        dummy = EventData(self.data2, trialdefinition=self.trl,
-                          dimord=["eventid", "sample"])
+        dummy = EventData(self.data2, trialdefinition=self.trl, dimord=["eventid", "sample"])
         smp = self.data2[:, -1]
         for trlno, start in enumerate(range(0, self.ns, 5)):
-            idx = np.intersect1d(np.where(smp >= start)[0],
-                                 np.where(smp < start + 5)[0])
+            idx = np.intersect1d(np.where(smp >= start)[0], np.where(smp < start + 5)[0])
             trl_ref = self.data2[idx, ...]
             assert np.array_equal(dummy._get_trial(trlno), trl_ref)
 
         # test `_get_trial` with NumPy array: customized columns names
         nuDimord = ["eventid", "sample", "custom1", "custom2"]
-        dummy = EventData(self.data3, trialdefinition=self.trl,
-                          dimord=nuDimord)
+        dummy = EventData(self.data3, trialdefinition=self.trl, dimord=nuDimord)
         assert dummy.dimord == nuDimord
         smp = self.data3[:, -1]
         for trlno, start in enumerate(range(0, self.ns, 5)):
-            idx = np.intersect1d(np.where(smp >= start)[0],
-                                 np.where(smp < start + 5)[0])
+            idx = np.intersect1d(np.where(smp >= start)[0], np.where(smp < start + 5)[0])
             trl_ref = self.data3[idx, ...]
             assert np.array_equal(dummy._get_trial(trlno), trl_ref)
 
     def test_ed_saveload(self):
         with tempfile.TemporaryDirectory() as tdir:
             fname = os.path.join(tdir, "dummy")
 
@@ -313,15 +343,19 @@
             dummy2 = load(filename)
             assert dummy2.dimord == dummy.dimord
             assert dummy2.eventid.size == self.num_smp  # swapped
             assert dummy2.data.shape == dummy.data.shape
             del dummy, dummy2
 
             # save dataset w/custom column names and ensure `dimord` is preserved
-            dummy = EventData(np.hstack([self.data, self.data]), dimord=self.customDimord, samplerate=10)
+            dummy = EventData(
+                np.hstack([self.data, self.data]),
+                dimord=self.customDimord,
+                samplerate=10,
+            )
             dummy.save(fname + "_customDimord")
             filename = construct_spy_filename(fname + "_customDimord", dummy)
             dummy2 = load(filename)
             assert dummy2.dimord == dummy.dimord
             assert dummy2.eventid.size == self.num_evt
             assert dummy2.data.shape == dummy.data.shape
 
@@ -333,29 +367,32 @@
 
         # Create sampleinfo w/ EventData vs. AnalogData samplerate
         sr_e = 2
         sr_a = 1
         pre = 2
         post = 1
         msk = self.data[:, 1] == 1
-        sinfo = np.vstack([self.data[msk, 0] / sr_e - pre,
-                           self.data[msk, 0] / sr_e + post]).T
+        sinfo = np.vstack([self.data[msk, 0] / sr_e - pre, self.data[msk, 0] / sr_e + post]).T
         sinfo_e = np.round(sinfo * sr_e).astype(int)
         sinfo_a = np.round(sinfo * sr_a).astype(int)
 
         # Compute sampleinfo w/pre, post and trigger
         evt_dummy = EventData(self.data, samplerate=sr_e)
         evt_dummy.definetrial(pre=pre, post=post, trigger=1)
         assert np.array_equal(evt_dummy.sampleinfo, sinfo_e)
 
         # Compute sampleinfo w/ start/stop combination
         evt_dummy = EventData(self.data, samplerate=sr_e)
         evt_dummy.definetrial(start=0, stop=1)
-        sinfo2 = np.vstack([self.data[np.where(self.data[:, 1] == 0)[0], 0],
-                            self.data[np.where(self.data[:, 1] == 1)[0], 0]]).T
+        sinfo2 = np.vstack(
+            [
+                self.data[np.where(self.data[:, 1] == 0)[0], 0],
+                self.data[np.where(self.data[:, 1] == 1)[0], 0],
+            ]
+        ).T
         assert np.array_equal(sinfo2, evt_dummy.sampleinfo)
 
         # Same w/ more complicated data array
         samples = np.arange(0, int(self.ns / 3), 3)[1:]
         dappend = np.vstack([samples, np.full(samples.shape, 2)]).T
         data3 = np.vstack([self.data, dappend])
         data3 = np.hstack([data3, data3])
@@ -370,26 +407,26 @@
         stops = [1, 2, 0]
         sinfo3 = np.empty((3, 2))
         dsamps = list(data3[:, 0])
         dcodes = list(data3[:, 1])
         for sk, (start, stop) in enumerate(zip(starts, stops)):
             idx = dcodes.index(start)
             start = dsamps[idx]
-            dcodes = dcodes[idx + 1:]
-            dsamps = dsamps[idx + 1:]
+            dcodes = dcodes[idx + 1 :]
+            dsamps = dsamps[idx + 1 :]
             idx = dcodes.index(stop)
             stop = dsamps[idx]
-            dcodes = dcodes[idx + 1:]
-            dsamps = dsamps[idx + 1:]
+            dcodes = dcodes[idx + 1 :]
+            dsamps = dsamps[idx + 1 :]
             sinfo3[sk, :] = [start, stop]
         evt_dummy = EventData(data3, dimord=self.customDimord, samplerate=sr_e)
         evt_dummy.definetrial(start=[2, 2, 1], stop=[1, 2, 0])
         assert np.array_equal(evt_dummy.sampleinfo, sinfo3)
 
-        # Attach computed sampleinfo to AnalogData (data and data3 must yield identical resutls)
+        # Attach computed sampleinfo to AnalogData (data and data3 must yield identical results)
         evt_dummy = EventData(data=self.data, samplerate=sr_e)
         evt_dummy.definetrial(pre=pre, post=post, trigger=1)
         ang_dummy = AnalogData(self.adata, samplerate=sr_a)
         ang_dummy.definetrial(evt_dummy)
         assert np.array_equal(ang_dummy.sampleinfo, sinfo_a)
         evt_dummy = EventData(data=data3, dimord=self.customDimord, samplerate=sr_e)
         evt_dummy.definetrial(pre=pre, post=post, trigger=1)
@@ -403,35 +440,35 @@
         assert np.array_equal(ang_dummy.sampleinfo, sinfo_a)
         evt_dummy = EventData(data=data3, dimord=self.customDimord, samplerate=sr_e)
         ang_dummy = AnalogData(self.adata, samplerate=sr_a)
         ang_dummy.definetrial(evt_dummy, pre=pre, post=post, trigger=1)
         assert np.array_equal(ang_dummy.sampleinfo, sinfo_a)
 
         # Extend data and provoke an exception due to out of bounds error
-        smp = np.vstack([np.arange(self.ns, int(2.5 * self.ns), 5),
-                         np.zeros((int((1.5 * self.ns) / 5),))]).T.astype(int)
+        smp = np.vstack(
+            [
+                np.arange(self.ns, int(2.5 * self.ns), 5),
+                np.zeros((int((1.5 * self.ns) / 5),)),
+            ]
+        ).T.astype(int)
         smp[1::2, 1] = 1
         smp = np.hstack([smp, smp])
         data4 = np.vstack([data3, smp])
         evt_dummy = EventData(data=data4, dimord=self.customDimord, samplerate=sr_e)
         evt_dummy.definetrial(pre=pre, post=post, trigger=1)
-        # with pytest.raises(SPYValueError):
-        # ang_dummy.definetrial(evt_dummy)
 
         # Trimming edges produces zero-length trial
         with pytest.raises(SPYValueError):
             ang_dummy.definetrial(evt_dummy, clip_edges=True)
 
         # We need `clip_edges` to make trial-definition work
         data4 = data4[:-2, :]
         data4[-2, 0] = data4[-1, 0]
         evt_dummy = EventData(data=data4, dimord=self.customDimord, samplerate=sr_e)
         evt_dummy.definetrial(pre=pre, post=post, trigger=1)
-        # with pytest.raises(SPYValueError):
-        # ang_dummy.definetrial(evt_dummy)
         ang_dummy.definetrial(evt_dummy, clip_edges=True)
         assert ang_dummy.sampleinfo[-1, 1] == self.ns
 
         # Check both pre/start and/or post/stop being None
         evt_dummy = EventData(data=self.data, samplerate=sr_e)
         with pytest.raises(SPYValueError):
             evt_dummy.definetrial(trigger=1, post=post)
@@ -459,20 +496,23 @@
         evt_dummy = EventData(samplerate=sr_e)
         with pytest.raises(SPYValueError):
             evt_dummy.definetrial(pre=pre, post=post, trigger=1)
         ang_dummy = AnalogData(self.adata, samplerate=sr_a)
         with pytest.raises(SPYValueError):
             ang_dummy.definetrial(evt_dummy, pre=pre, post=post, trigger=1)
 
-class TestWaveform():
 
+class TestWaveform:
     def test_waveform_invalid_set(self):
         """Sets invalid waveform for data: dimension mismatch"""
         spiked = SpikeData(data=np.ones((2, 3), dtype=int), samplerate=10)
-        assert spiked.data.shape == (2, 3,)
+        assert spiked.data.shape == (
+            2,
+            3,
+        )
         with pytest.raises(SPYValueError, match="wrong size waveform"):
             spiked.waveform = np.ones((3, 3), dtype=int)
 
     def test_waveform_invalid_set_emptydata(self):
         """Tries to set waveform without any data."""
         spiked = SpikeData()
         with pytest.raises(SPYValueError, match="Please assign data first"):
@@ -485,67 +525,83 @@
             spiked.waveform = np.ones((3), dtype=int)
 
     def test_waveform_valid_set(self):
         """Sets waveform in a correct way."""
         spiked = SpikeData(data=np.ones((2, 3), dtype=int), samplerate=10)
 
         assert not spiked._is_empty()
-        assert spiked.data.shape == (2, 3,)
+        assert spiked.data.shape == (
+            2,
+            3,
+        )
         assert type(spiked.data) == h5py.Dataset
         assert spiked._get_backing_hdf5_file_handle() is not None
         spiked.waveform = np.ones((2, 3), dtype=int)
         assert "waveform" in spiked._hdfFileDatasetProperties
-        assert spiked.waveform.shape == (2, 3,)
+        assert spiked.waveform.shape == (
+            2,
+            3,
+        )
 
     def test_waveform_valid_set_with_None(self):
         """Sets waveform to None, which is valid."""
         spiked = SpikeData(data=np.ones((2, 3), dtype=int), samplerate=10)
         assert not spiked._is_empty()
-        assert spiked.data.shape == (2, 3,)
+        assert spiked.data.shape == (
+            2,
+            3,
+        )
         spiked.waveform = np.ones((2, 3), dtype=int)
-        assert spiked.waveform.shape == (2, 3,)
+        assert spiked.waveform.shape == (
+            2,
+            3,
+        )
         spiked.waveform = None
         assert spiked.waveform is None
         # try to set again
         spiked.waveform = np.ones((2, 3), dtype=int)
-        assert spiked.waveform.shape == (2, 3,)
-
+        assert spiked.waveform.shape == (
+            2,
+            3,
+        )
 
     def test_waveform_selection_trial(self):
         numSpikes, waveform_dimsize = 20, 50
-        spiked = getSpikeData(nSpikes = numSpikes)
+        spiked = getSpikeData(nSpikes=numSpikes)
         assert sum([s.shape[0] for s in spiked.trials]) == numSpikes
         assert spiked.waveform is None
         spiked.waveform = np.ones((numSpikes, 3, waveform_dimsize), dtype=int)
         for spikeidx in range(numSpikes):
             spiked.waveform[spikeidx, :, :] = np.ones((3, waveform_dimsize), dtype=int) * spikeidx
 
         trial0_nspikes = spiked.trials[0].shape[0]
         trial2_nspikes = spiked.trials[2].shape[0]
 
         # Select 2 trials and verify that the number of spikes is correct.
-        selection = { 'trials': [0, 2] }
+        selection = {"trials": [0, 2]}
         res = spiked.selectdata(selection)
         assert len(res.trials) == 2
         assert res.trials[0].shape[0] == trial0_nspikes
         assert res.trials[1].shape[0] == trial2_nspikes
 
         # Verify that the waveform selection is also correct.
         assert res.waveform is not None
         assert res.waveform.shape[0] == trial0_nspikes + trial2_nspikes  # Verify selection on waveform
 
         # Verify on data level.
         expected_data_indices = np.where((spiked.trialid == 0) | (spiked.trialid == 2))[0]
         for spike_idx in range(res.waveform.shape[0]):
-            assert np.all(res.waveform[spike_idx, :, :] == spiked.waveform[expected_data_indices][spike_idx, :, :])
+            assert np.all(
+                res.waveform[spike_idx, :, :] == spiked.waveform[expected_data_indices][spike_idx, :, :]
+            )
 
     def test_save_load_with_waveform(self):
         """Test saving file with waveform data."""
         numSpikes, waveform_dimsize = 20, 50
-        spiked = getSpikeData(nSpikes = numSpikes)
+        spiked = getSpikeData(nSpikes=numSpikes)
         spiked.waveform = np.ones((numSpikes, 3, waveform_dimsize), dtype=int)
 
         tfile1 = tempfile.NamedTemporaryFile(suffix=".spike", delete=True)
         tfile1.close()
         tmp_spy_filename = tfile1.name
         save(spiked, filename=tmp_spy_filename)
         assert "waveform" in h5py.File(tmp_spy_filename, mode="r").keys()
@@ -561,27 +617,27 @@
         # Test that we can set waveform again after deleting it.
         spkd2.waveform = np.ones((numSpikes, 3, waveform_dimsize), dtype=int)
 
         tfile1.close()
 
     def test_psth_with_waveform(self):
         """Test that the waveform does not break frontend functions, like PSTH.
-           The waveform should just be ignored, and the resulting TimeLockData
-           will of course NOT have a waveform.
+        The waveform should just be ignored, and the resulting TimeLockData
+        will of course NOT have a waveform.
         """
         numSpikes, waveform_dimsize = 20, 50
-        spiked = getSpikeData(nSpikes = numSpikes)
+        spiked = getSpikeData(nSpikes=numSpikes)
         spiked.waveform = np.ones((numSpikes, 3, waveform_dimsize), dtype=int)
 
         cfg = spy.StructDict()
         cfg.binsize = 0.1
-        cfg.latency = 'maxperiod'  # frontend default
+        cfg.latency = "maxperiod"  # frontend default
         res = spy.spike_psth(spiked, cfg)
         assert type(res) == spy.TimeLockData
         assert not hasattr(res, "waveform")
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
 
     T1 = TestSpikeData()
     T2 = TestEventData()
-    T3 = TestWaveform()
+    T3 = TestWaveform()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_info.py` & `esi_syncopy-2023.7/syncopy/tests/test_info.py`

 * *Files 18% similar despite different names*

```diff
@@ -13,20 +13,24 @@
 from syncopy.shared.tools import SerializableDict
 from syncopy.shared.errors import SPYTypeError
 
 
 class TestInfo:
 
     # serializable dict
-    ok_dict = {'sth': 4, 'important': [1, 2],
-               'to': {'v1': 2}, 'remember': 'need more coffe'}
+    ok_dict = {
+        "sth": 4,
+        "important": [1, 2],
+        "to": {"v1": 2},
+        "remember": "need more coffe",
+    }
     # non-serializable dict
-    ns_dict = {'sth': 4, 'not_serializable': {'v1': range(2)}}
+    ns_dict = {"sth": 4, "not_serializable": {"v1": range(2)}}
     # dict with non-serializable keys
-    ns_dict2 = {range(2) : 'small_range', range(1000) : 'large_range'}
+    ns_dict2 = {range(2): "small_range", range(1000): "large_range"}
 
     # test setter
     def test_property(self):
 
         # as .info is a basedata property,
         # testing for one derived class should suffice
         adata = spy.AnalogData([np.ones((3, 1))], samplerate=1)
@@ -46,33 +50,33 @@
         # clear with empty dict
         adata.info = {}
         assert len(adata.info) == 0
         assert len(self.ok_dict) != 0
 
         # test we're catching non-serializable dictionary entries
         with pytest.raises(SPYTypeError, match="expected serializable data type"):
-            adata.info['new-var'] = np.arange(3)
+            adata.info["new-var"] = np.arange(3)
         with pytest.raises(SPYTypeError, match="expected serializable data type"):
             adata.info = self.ns_dict
 
         # test that we also catch non-serializable keys
         with pytest.raises(SPYTypeError, match="expected serializable data type"):
             adata.info = self.ns_dict2
 
         # this interestingly still does NOT work (numbers are np.float64):
         with pytest.raises(SPYTypeError, match="expected serializable data type"):
-            adata.info['new-var'] = list(np.arange(3))
+            adata.info["new-var"] = list(np.arange(3))
 
         # even this.. numbers are still np.int64
         with pytest.raises(SPYTypeError, match="expected serializable data type"):
-            adata.info['new-var'] = list(np.arange(3, dtype=int))
+            adata.info["new-var"] = list(np.arange(3, dtype=int))
 
         # this then works, hope is that users don't abuse it
-        adata.info['new-var'] = list(np.arange(3, dtype=float))
-        assert np.allclose(adata.info['new-var'], np.arange(3))
+        adata.info["new-var"] = list(np.arange(3, dtype=float))
+        assert np.allclose(adata.info["new-var"], np.arange(3))
 
     # test aux. info dict saving and loading
     def test_io(self):
         with tempfile.TemporaryDirectory() as tdir:
 
             fname = os.path.join(tdir, "dummy")
             dummy = spy.AnalogData([np.ones((3, 1))], samplerate=1)
@@ -82,9 +86,9 @@
             spy.save(dummy, fname)
             del dummy
 
             dummy2 = spy.load(fname)
             assert dummy2.info == self.ok_dict
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestInfo()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_logging.py` & `esi_syncopy-2023.7/syncopy/tests/test_logging.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,30 +9,31 @@
 # Local imports
 import syncopy as spy
 from syncopy.shared.log import get_logger, get_parallel_logger
 from syncopy.shared.errors import SPYLog
 
 
 class TestLogging:
-
     def test_seq_logfile_exists(self):
         logfile = os.path.join(spy.__logdir__, "syncopy.log")
         assert os.path.isfile(logfile)
 
     def test_par_logfile_exists(self):
         par_logfile = os.path.join(spy.__logdir__, f"syncopy_{platform.node()}.log")
         assert os.path.isfile(par_logfile)
 
     def test_default_log_level_is_important(self):
         # Ensure the log level is at default (that user did not change SPYLOGLEVEL on test system)
         assert os.getenv("SPYLOGLEVEL", "IMPORTANT") == "IMPORTANT"
 
         logfile = os.path.join(spy.__logdir__, "syncopy.log")
         assert os.path.isfile(logfile)
-        num_lines_initial = sum(1 for line in open(logfile)) # The log file gets appended, so it will most likely *not* be empty.
+        num_lines_initial = sum(
+            1 for line in open(logfile)
+        )  # The log file gets appended, so it will most likely *not* be empty.
 
         # Log something with log level info and DEBUG, which should not affect the logfile.
         logger = get_logger()
         logger.info("I am adding an INFO level log entry.")
         SPYLog("I am adding a DEBUG level log entry.", loglevel="DEBUG")
 
         num_lines_after_info_debug = sum(1 for line in open(logfile))
@@ -48,15 +49,17 @@
     def test_default_parellel_log_level_is_important(self):
         # Ensure the log level is at default (that user did not change SPYLOGLEVEL on test system)
         assert os.getenv("SPYLOGLEVEL", "IMPORTANT") == "IMPORTANT"
         assert os.getenv("SPYPARLOGLEVEL", "IMPORTANT") == "IMPORTANT"
 
         par_logfile = os.path.join(spy.__logdir__, f"syncopy_{platform.node()}.log")
         assert os.path.isfile(par_logfile)
-        num_lines_initial = sum(1 for line in open(par_logfile)) # The log file gets appended, so it will most likely *not* be empty.
+        num_lines_initial = sum(
+            1 for line in open(par_logfile)
+        )  # The log file gets appended, so it will most likely *not* be empty.
 
         # Log something with log level info and DEBUG, which should not affect the logfile.
         par_logger = get_parallel_logger()
         par_logger.info("I am adding an INFO level log entry.")
         par_logger.debug("I am adding a DEBUG level log entry.")
 
         num_lines_after_info_debug = sum(1 for line in open(par_logfile))
@@ -72,15 +75,17 @@
 
     def test_log_function_is_in_root_namespace_with_seq(self):
         """Tests sequential logger via spy.log function."""
         assert os.getenv("SPYLOGLEVEL", "IMPORTANT") == "IMPORTANT"
 
         logfile = os.path.join(spy.__logdir__, "syncopy.log")
         assert os.path.isfile(logfile)
-        num_lines_initial = sum(1 for line in open(logfile)) # The log file gets appended, so it will most likely *not* be empty.
+        num_lines_initial = sum(
+            1 for line in open(logfile)
+        )  # The log file gets appended, so it will most likely *not* be empty.
 
         # Log something with log level info and DEBUG, which should not affect the logfile.
         spy.log("I am adding an INFO level log entry.", level="INFO")
 
         num_lines_after_info_debug = sum(1 for line in open(logfile))
         assert num_lines_initial == num_lines_after_info_debug
 
@@ -93,29 +98,23 @@
 
     def test_log_function_is_in_root_namespace_with_par(self):
         """Tests parallel logger via spy.log function."""
         assert os.getenv("SPYPARLOGLEVEL", "IMPORTANT") == "IMPORTANT"
 
         par_logfile = os.path.join(spy.__logdir__, f"syncopy_{platform.node()}.log")
         assert os.path.isfile(par_logfile)
-        num_lines_initial = sum(1 for line in open(par_logfile)) # The log file gets appended, so it will most likely *not* be empty.
+        num_lines_initial = sum(
+            1 for line in open(par_logfile)
+        )  # The log file gets appended, so it will most likely *not* be empty.
 
         # Log something with log level info and DEBUG, which should not affect the logfile.
         spy.log("I am adding an INFO level log entry.", level="INFO", par=True)
 
         num_lines_after_info_debug = sum(1 for line in open(par_logfile))
         assert num_lines_initial == num_lines_after_info_debug
 
         # Now log something with log level WARNING
         spy.log("I am adding a IMPORTANT level log entry.", level="IMPORTANT", par=True)
         spy.log("This is the last warning.", level="IMPORTANT", par=True)
 
         num_lines_after_warning = sum(1 for line in open(par_logfile))
         assert num_lines_after_warning > num_lines_after_info_debug
-
-
-
-
-
-
-
-
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_metadata.py` & `esi_syncopy-2023.7/syncopy/tests/test_metadata.py`

 * *Files 7% similar despite different names*

```diff
@@ -14,79 +14,116 @@
 import dask.distributed as dd
 
 # Local imports
 from syncopy import freqanalysis
 from syncopy.datatype.methods.copy import copy
 from syncopy.shared.tools import get_defaults
 from syncopy.synthdata import ar2_network, phase_diffusion
-from syncopy.shared.metadata import encode_unique_md_label, decode_unique_md_label, parse_cF_returns, _parse_backend_metadata, _merge_md_list, metadata_from_hdf5_file, metadata_nest, metadata_unnest
+from syncopy.shared.metadata import (
+    encode_unique_md_label,
+    decode_unique_md_label,
+    parse_cF_returns,
+    _parse_backend_metadata,
+    _merge_md_list,
+    metadata_from_hdf5_file,
+    metadata_nest,
+    metadata_unnest,
+)
 from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYWarning
 import syncopy as spy
 
 
-def _get_fooof_signal(nTrials=100, nChannels = 1, nSamples = 1000, seed=None):
+def _get_fooof_signal(nTrials=100, nChannels=1, nSamples=1000, seed=None):
     """
     Produce suitable test signal for fooof, with peaks at 30 and 50 Hz.
 
     Note: One must perform trial averaging during the FFT to get realistic
     data out of it (and reduce noise). Then work with the averaged data.
 
     Returns AnalogData instance.
     """
     samplerate = 1000
-    ar1_part = ar2_network(AdjMat=np.zeros(nChannels), nSamples=nSamples, alphas=[0.9, 0], nTrials=nTrials, seed=seed)
-    pd1 = phase_diffusion(freq=30., eps=.1, samplerate=samplerate, nChannels=nChannels, nSamples=nSamples, nTrials=nTrials, seed=seed)
-    pd2 = phase_diffusion(freq=50., eps=.1, samplerate=samplerate, nChannels=nChannels, nSamples=nSamples, nTrials=nTrials, seed=seed)
-    signal = ar1_part + .8 * pd1 + 0.6 * pd2
+    ar1_part = ar2_network(
+        AdjMat=np.zeros(nChannels),
+        nSamples=nSamples,
+        alphas=[0.9, 0],
+        nTrials=nTrials,
+        seed=seed,
+    )
+    pd1 = phase_diffusion(
+        freq=30.0,
+        eps=0.1,
+        samplerate=samplerate,
+        nChannels=nChannels,
+        nSamples=nSamples,
+        nTrials=nTrials,
+        seed=seed,
+    )
+    pd2 = phase_diffusion(
+        freq=50.0,
+        eps=0.1,
+        samplerate=samplerate,
+        nChannels=nChannels,
+        nSamples=nSamples,
+        nTrials=nTrials,
+        seed=seed,
+    )
+    signal = ar1_part + 0.8 * pd1 + 0.6 * pd2
     return signal
 
 
-class TestMetadataHelpers():
+class TestMetadataHelpers:
     def test_encode_unique_md_label(self):
         assert encode_unique_md_label("label", "1", "2") == "label__1_2"
         assert encode_unique_md_label("label", 1, 2) == "label__1_2"
         assert encode_unique_md_label("label", 1) == "label__1_0"
         assert encode_unique_md_label("label", "1") == "label__1_0"
 
     def test_decode_unique_md_label(self):
         assert ("label", "1", "2") == decode_unique_md_label("label__1_2")
 
     def test_merge_md_list(self):
         assert _merge_md_list(None) is None
-        md1 = {'1c': np.zeros(3), '1d': np.zeros(3)}
-        md2 = {'2c': np.zeros(5), '2d': np.zeros(5)}
+        md1 = {"1c": np.zeros(3), "1d": np.zeros(3)}
+        md2 = {"2c": np.zeros(5), "2d": np.zeros(5)}
         md3 = {}
         mdl = [md1, md2, md3]
         merged = _merge_md_list(mdl)
         assert len(merged) == 4
-        for k in ['1c', '1d', '2c', '2d']:
+        for k in ["1c", "1d", "2c", "2d"]:
             assert k in merged
 
     def test_metadata_from_hdf5_file(self):
         # Test for correct error on hdf5 file without 'data' dataset.
         fd, h5py_filename = tempfile.mkstemp()
         with h5py.File(h5py_filename, "w") as f:
-            f.create_dataset("mydataset", (100,), dtype='i')
+            f.create_dataset("mydataset", (100,), dtype="i")
 
         with pytest.raises(SPYValueError, match="dataset in hd5f file"):
             _ = metadata_from_hdf5_file(h5py_filename)
 
         os.close(fd)
         os.remove(h5py_filename)
 
     def test_get_res_details(self):
         # Test error on invalid input: a dict instead of tuple
         with pytest.raises(SPYValueError) as err:
             _, b = parse_cF_returns({"a": 2})
-        assert "user-supplied compute function must return a single ndarray or a tuple with length exactly 2" in str(err.value)
+        assert (
+            "user-supplied compute function must return a single ndarray or a tuple with length exactly 2"
+            in str(err.value)
+        )
 
         # Test with tuple of incorrect length 3
         with pytest.raises(SPYValueError) as err:
             _, b = parse_cF_returns((1, 2, 3))
-        assert "user-supplied compute function must return a single ndarray or a tuple with length exactly 2" in str(err.value)
+        assert (
+            "user-supplied compute function must return a single ndarray or a tuple with length exactly 2"
+            in str(err.value)
+        )
 
         # Test with tuple, 2nd arg is None
         _, b = parse_cF_returns((np.zeros(3)))
         assert b is None
 
         # Test with ndarray only
         _, b = parse_cF_returns(np.zeros(3))
@@ -95,27 +132,35 @@
         # Test with tuple of correct length, but 2nd value is not dict
         with pytest.raises(SPYValueError) as err:
             a, b = parse_cF_returns((np.zeros(3), np.zeros(4)))
         assert "the second return value of user-supplied compute functions must be a dict" in str(err.value)
 
         # Test with tuple of correct length, 2nd value is dict, but values are not ndarray
         with pytest.raises(SPYValueError) as err:
-            a, b = parse_cF_returns((np.zeros(3), {'a': dict()}))
-        assert "the second return value of user-supplied compute functions must be a dict containing np.ndarrays" in str(err.value)
+            a, b = parse_cF_returns((np.zeros(3), {"a": dict()}))
+        assert (
+            "the second return value of user-supplied compute functions must be a dict containing np.ndarrays"
+            in str(err.value)
+        )
 
         # Test with numpy array with datatype np.object, which is not valid.
-        invalid_val = np.array([np.zeros((5,3)), np.zeros((8,3))], dtype = object)  # The dtype is required to silence numpy deprecation warnings, the dtype will be object even without it.
+        invalid_val = np.array(
+            [np.zeros((5, 3)), np.zeros((8, 3))], dtype=object
+        )  # The dtype is required to silence numpy deprecation warnings, the dtype will be object even without it.
         assert invalid_val.dtype == object
         with pytest.raises(SPYValueError) as err:
-            a, b = parse_cF_returns((np.zeros(3), {'a': invalid_val}))
-        assert "the second return value of user-supplied compute functions must be a dict containing np.ndarrays with datatype other than 'np.object'" in str(err.value)
+            a, b = parse_cF_returns((np.zeros(3), {"a": invalid_val}))
+        assert (
+            "the second return value of user-supplied compute functions must be a dict containing np.ndarrays with datatype other than 'np.object'"
+            in str(err.value)
+        )
 
         # Test with tuple of correct length, 2nd value is dict, and values in ndarray are string (but not object). This is fine.
-        a, b = parse_cF_returns((np.zeros(3), {'a': np.array(['apples', 'foobar', 'cowboy'])}))
-        assert 'a' in b
+        a, b = parse_cF_returns((np.zeros(3), {"a": np.array(["apples", "foobar", "cowboy"])}))
+        assert "a" in b
 
     def test_parse_backend_metadata(self):
         assert _parse_backend_metadata(None) == dict()
 
         # Test for error if input is not dict.
         with pytest.raises(SPYTypeError) as err:
             attrs = _parse_backend_metadata(np.zeros(3))
@@ -128,90 +173,104 @@
 
         # Test that empty input leads to empty output
         attrs = _parse_backend_metadata(dict())
         assert isinstance(attrs, dict)
         assert not attrs
 
         # Test that string-only keys are treated as attributes (not dsets)
-        attrs = _parse_backend_metadata({'attr1': np.zeros(3)})
-        assert 'attr1' in attrs and len(attrs) == 1
+        attrs = _parse_backend_metadata({"attr1": np.zeros(3)})
+        assert "attr1" in attrs and len(attrs) == 1
 
         # Test that error is raised if implicit 'attr 'values are not ndarray
         with pytest.raises(SPYTypeError, match="value in metadata"):
-            attrs = _parse_backend_metadata({'attr1': dict()})
+            attrs = _parse_backend_metadata({"attr1": dict()})
 
         # Test that warning is raised for large data
-        _parse_backend_metadata({'attr1': np.arange(100000)})
+        _parse_backend_metadata({"attr1": np.arange(100000)})
 
     def test_metadata_nest(self):
         # Test with valid input
-        md = { 'ap__0_0': 1, 'ap__0_1': 2, 'pp__0_0': 3, 'pp__0_1': 4}
+        md = {"ap__0_0": 1, "ap__0_1": 2, "pp__0_0": 3, "pp__0_1": 4}
         md_nested = metadata_nest(md)
-        expected = { 'ap' : { 'ap__0_0': 1, 'ap__0_1': 2}, 'pp': {'pp__0_0': 3, 'pp__0_1': 4}}
+        expected = {
+            "ap": {"ap__0_0": 1, "ap__0_1": 2},
+            "pp": {"pp__0_0": 3, "pp__0_1": 4},
+        }
         assert md_nested == expected
 
         # Test exc: key with name identical to unique_label_part of another key already in dict.
         # This leads to an error because the key with name identical to unique_label_part cannot conform
         # to the format expected by the function that splits it into label, trial_idx, chunk_idx.
-        md_dupl = { 'ap__0_0': 1, 'ap__0_1': 2, 'pp__0_0': 3, 'pp__0_1': 4, 'pp' : 3}
-        with pytest.raises(SPYValueError, match="input string in format `<label>__<trial_idx>_<chunk_idx>'"):
+        md_dupl = {"ap__0_0": 1, "ap__0_1": 2, "pp__0_0": 3, "pp__0_1": 4, "pp": 3}
+        with pytest.raises(
+            SPYValueError,
+            match="input string in format `<label>__<trial_idx>_<chunk_idx>'",
+        ):
             _ = metadata_nest(md_dupl)
 
     def test_metadata_unnest(self):
         # Test with valid input
-        md_nested = { 'ap' : { 'ap__0_0': 1, 'ap__0_1': 2}, 'pp': {'pp__0_0': 3, 'pp__0_1': 4}}
+        md_nested = {
+            "ap": {"ap__0_0": 1, "ap__0_1": 2},
+            "pp": {"pp__0_0": 3, "pp__0_1": 4},
+        }
         md_unnested = metadata_unnest(md_nested)
-        expected = { 'ap__0_0': 1, 'ap__0_1': 2, 'pp__0_0': 3, 'pp__0_1': 4}
+        expected = {"ap__0_0": 1, "ap__0_1": 2, "pp__0_0": 3, "pp__0_1": 4}
         assert md_unnested == expected
 
         # Test exc: with duplicate key in several nested dicts.
-        md_nested_dupl_key = { 'ap' : { 'ap__0_0': 1, 'ap__0_1': 2}, 'pp': {'ap__0_0': 3, 'pp__0_1': 4}}
+        md_nested_dupl_key = {
+            "ap": {"ap__0_0": 1, "ap__0_1": 2},
+            "pp": {"ap__0_0": 3, "pp__0_1": 4},
+        }
         with pytest.raises(SPYValueError, match="Duplicate key"):
             _ = metadata_unnest(md_nested_dupl_key)
 
         # Test exc: input not a nested dict
-        md_not_nested = { 'ap' : { 'ap__0_0': 1, 'ap__0_1': 2}, 'pp': 1 }
+        md_not_nested = {"ap": {"ap__0_0": 1, "ap__0_1": 2}, "pp": 1}
         with pytest.raises(SPYValueError, match="is not a dict"):
             _ = metadata_unnest(md_not_nested)
 
 
-class TestMetadataUsingFooof():
+class TestMetadataUsingFooof:
     """
     Test 2nd cF function return value, with FOOOF as example compute method.
     """
 
     tfData = _get_fooof_signal()
-    expected_metadata_keys = spy.specest.compRoutines.FooofSpy.metadata_keys  # ("aperiodic_params", "gaussian_params", "peak_params", "n_peaks", "r_squared", "error",)
+    expected_metadata_keys = (
+        spy.specest.compRoutines.FooofSpy.metadata_keys
+    )  # ("aperiodic_params", "gaussian_params", "peak_params", "n_peaks", "r_squared", "error",)
     num_expected_metadata_keys = len(expected_metadata_keys)
 
     @staticmethod
     def get_fooof_cfg():
         cfg = get_defaults(freqanalysis)
         cfg.method = "mtmfft"
         cfg.taper = "hann"
         cfg.select = {"channel": 0}
         cfg.keeptrials = False
         cfg.output = "fooof"
-        cfg.foilim = [1., 100.]
+        cfg.foilim = [1.0, 100.0]
         return cfg
 
     def test_sequential(self):
         """
         Test metadata propagation with fooof in sequential compute mode.
         """
         cfg = TestMetadataUsingFooof.get_fooof_cfg()
         cfg.parallel = False
-        cfg.pop('fooof_opt', None)
-        fooof_opt = {'peak_width_limits': (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
+        cfg.pop("fooof_opt", None)
+        fooof_opt = {"peak_width_limits": (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
         spec_dt = freqanalysis(cfg, self.tfData, fooof_opt=fooof_opt)
 
         # These are known from the input data and cfg.
         data_size = 100  # Number of samples (per trial) seen by fooof. The full signal returned by _get_fooof_signal() is
-                         # larger, but the cfg.frequency setting (in get_fooof_cfg()) limits to 100 samples.
-        num_trials_fooof = 1 # Because of keeptrials = False in cfg.
+        # larger, but the cfg.frequency setting (in get_fooof_cfg()) limits to 100 samples.
+        num_trials_fooof = 1  # Because of keeptrials = False in cfg.
 
         assert spec_dt.data.shape == (num_trials_fooof, 1, data_size, 1)
 
         # check metadata from 2nd cF return value, added to the hdf5 dataset as attribute.
         keys_unique = [kv + "__0_0" for kv in self.expected_metadata_keys]
 
         # Now for the metadata. This got attached to the syncopy data instance as the 'metadata' attribute. It is a hdf5 group.
@@ -223,41 +282,40 @@
         for kv in keys_unique:
             assert kv in spec_dt_metadata_unnested.keys()
             # Note that the cF-specific unpacking may convert ndarray values into something else. In case of fooof, we convert
             # some ndarrays (all the peak_params__n_m and gaussian_params__n_m) to list, so we accept both types here.
             assert isinstance(spec_dt_metadata_unnested.get(kv), (list, np.ndarray))
 
         # check that the cfg is correct (required for replay)
-        assert spec_dt.cfg['freqanalysis']['output'] == 'fooof'
+        assert spec_dt.cfg["freqanalysis"]["output"] == "fooof"
 
         # Test the metadata_keys entry of the CR:
         for k in spy.specest.compRoutines.FooofSpy.metadata_keys:
             assert k in spec_dt.metadata
         assert len(spec_dt.metadata) == len(spy.specest.compRoutines.FooofSpy.metadata_keys)
 
-
     def test_par_compute_with_sequential_storage(self):
         """
         Test metadata propagation in with parallel compute and sequential storage.
         With trial averaging (`keeptrials=false` in cfg), sequential storage is used.
         """
         cfg = TestMetadataUsingFooof.get_fooof_cfg()
-        cfg.pop('fooof_opt', None)
+        cfg.pop("fooof_opt", None)
         cfg.parallel = True
-        fooof_opt = {'peak_width_limits': (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
+        fooof_opt = {"peak_width_limits": (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
         spec_dt = freqanalysis(cfg, self.tfData, fooof_opt=fooof_opt)
 
         # These are known from the input data and cfg.
         data_size = 100
-        num_trials_fooof = 1 # Because of keeptrials = False in cfg.
+        num_trials_fooof = 1  # Because of keeptrials = False in cfg.
 
         # check frequency axis
         assert spec_dt.freq.size == data_size
         assert spec_dt.freq[0] == 1
-        assert spec_dt.freq[99] == 100.
+        assert spec_dt.freq[99] == 100.0
 
         assert spec_dt.data.shape == (num_trials_fooof, 1, data_size, 1)
         assert not np.isnan(spec_dt.data).any()
 
         # check metadata from 2nd cF return value, added to the hdf5 dataset as attribute.
         keys_unique = [kv + "__0_0" for kv in self.expected_metadata_keys]
 
@@ -269,42 +327,49 @@
         assert isinstance(spec_dt.metadata, dict)  # Make sure it is a standard dict, not a hdf5 group.
         num_metadata_attrs = len(spec_dt.metadata.keys())
         assert num_metadata_attrs == expected_num_metadata_keys
         for kv in keys_unique:
             assert kv in spec_dt_metadata_unnested.keys()
             # Note that the cF-specific unpacking may convert ndarray values into something else. In case of fooof, we convert
             # some ndarrays (all the peak_params__n_m and gaussian_params__n_m) to list, so we accept both types here.
-            assert isinstance(spec_dt_metadata_unnested.get(kv), list) or isinstance(spec_dt_metadata_unnested.get(kv), np.ndarray)
+            assert isinstance(spec_dt_metadata_unnested.get(kv), list) or isinstance(
+                spec_dt_metadata_unnested.get(kv), np.ndarray
+            )
 
         # check that the cfg is correct (required for replay)
-        assert spec_dt.cfg['freqanalysis']['output'] == 'fooof'
+        assert spec_dt.cfg["freqanalysis"]["output"] == "fooof"
 
     def test_par_compute_with_par_storage(self):
         """
         Test metadata propagation in with parallel compute and parallel storage.
         Without trial averaging (`keeptrials=True` in cfg), parallel storage is used.
         """
         cfg = TestMetadataUsingFooof.get_fooof_cfg()
-        cfg.pop('fooof_opt', None)
+        cfg.pop("fooof_opt", None)
         cfg.parallel = True  # Enable parallel computation
         cfg.keeptrials = True  # Enable parallel storage (is turned off when trial averaging is happening)
-        fooof_opt = {'peak_width_limits': (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
+        fooof_opt = {"peak_width_limits": (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
         spec_dt = freqanalysis(cfg, self.tfData, fooof_opt=fooof_opt)
 
         # These are known from the input data and cfg.
         num_trials_fooof = 100
         data_size = 100
 
         # check frequency axis
         assert spec_dt.freq.size == data_size
         assert spec_dt.freq[0] == 1
-        assert spec_dt.freq[99] == 100.
+        assert spec_dt.freq[99] == 100.0
 
         # check the data
-        assert spec_dt.data.shape == (num_trials_fooof, 1, data_size, 1) # Differs from other tests due to `keeptrials=True`.
+        assert spec_dt.data.shape == (
+            num_trials_fooof,
+            1,
+            data_size,
+            1,
+        )  # Differs from other tests due to `keeptrials=True`.
         assert not np.isnan(spec_dt.data).any()
 
         # check metadata from 2nd cF return value, added to the hdf5 dataset 'data' as attributes.
         keys_unique = [kv + "__0_0" for kv in self.expected_metadata_keys]
 
         # Now for the metadata. This got attached to the syncopy data instance as the 'metadata' attribute. It is a hdf5 group.
         assert spec_dt.metadata is not None
@@ -315,62 +380,67 @@
         spec_dt_metadata_unnested = metadata_unnest(spec_dt.metadata)
         assert num_metadata_attrs == expected_num_metadata_keys
         assert len(spec_dt_metadata_unnested.keys()) == expected_num_metadata_keys * num_trials_fooof
         for kv in keys_unique:
             assert (kv) in spec_dt_metadata_unnested.keys()
             # Note that the cF-specific unpacking may convert ndarray values into something else. In case of fooof, we convert
             # some ndarrays (all the peak_params__n_m and gaussian_params__n_m) to list, so we accept both types here.
-            assert isinstance(spec_dt_metadata_unnested.get(kv), list) or isinstance(spec_dt_metadata_unnested.get(kv), np.ndarray)
+            assert isinstance(spec_dt_metadata_unnested.get(kv), list) or isinstance(
+                spec_dt_metadata_unnested.get(kv), np.ndarray
+            )
 
         # check that the cfg is correct (required for replay)
-        assert spec_dt.cfg['freqanalysis']['output'] == 'fooof'
+        assert spec_dt.cfg["freqanalysis"]["output"] == "fooof"
 
     def test_par_with_selections(self):
         """
         Test metadata propagation in with parallel compute and parallel storage,
         and trial selections.
 
         In the case of trial selections, the computation of absolute trial indices
         from relative ones uses the trivial branch for fooof, bacause the selection
         is handled (and consumed) by the mtmfft running before fooof. See the test
         in the `TestMetadataUsingMtmfft` class below for the other branch.
         """
         cfg = TestMetadataUsingFooof.get_fooof_cfg()
-        cfg.pop('fooof_opt', None)
+        cfg.pop("fooof_opt", None)
         cfg.parallel = True  # Enable parallel computation
         cfg.keeptrials = True  # Enable parallel storage (is turned off when trial averaging is happening)
-        fooof_opt = {'peak_width_limits': (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
+        fooof_opt = {"peak_width_limits": (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
         data = self.tfData.copy()
 
         selected_trials = [3, 5, 7]
-        cfg.select = {'trials': selected_trials }
+        cfg.select = {"trials": selected_trials}
 
         spec_dt = freqanalysis(cfg, data, fooof_opt=fooof_opt)
 
         # These are known from the input data and cfg.
         num_trials_fooof_selected = len(selected_trials)
         data_size = 100
 
         # check frequency axis
         assert spec_dt.freq.size == data_size
         assert spec_dt.freq[0] == 1
-        assert spec_dt.freq[99] == 100.
+        assert spec_dt.freq[99] == 100.0
 
         assert spec_dt.data.shape == (num_trials_fooof_selected, 1, data_size, 1)
 
         # check metadata from 2nd cF return value, added to the hdf5 dataset 'data' as attributes.
         keys_unique = [kv + "__0_0" for kv in self.expected_metadata_keys]
 
         # Now for the metadata. This got attached to the syncopy data instance as the 'metadata' attribute. It is a hdf5 group.
         assert spec_dt.metadata is not None
         assert isinstance(spec_dt.metadata, dict)  # Make sure it is a standard dict, not a hdf5 group.
         num_metadata_attrs = len(spec_dt.metadata.keys())
         assert num_metadata_attrs == self.num_expected_metadata_keys
         spec_dt_metadata_unnested = metadata_unnest(spec_dt.metadata)
-        assert len(spec_dt_metadata_unnested.keys()) == self.num_expected_metadata_keys * num_trials_fooof_selected
+        assert (
+            len(spec_dt_metadata_unnested.keys())
+            == self.num_expected_metadata_keys * num_trials_fooof_selected
+        )
         for kv in keys_unique:
             assert (kv) in spec_dt_metadata_unnested.keys()
             assert isinstance(spec_dt_metadata_unnested.get(kv), (list, np.ndarray))
 
     def test_channel_par(self):
         """
         Test metadata propagation in with channel parallelization. This implies
@@ -379,40 +449,45 @@
         For syncopy to use channel parallelization, we must make sure that:
         - we use parallel mode (`cfg.parallel` = `True`)
         - 'chan_per_worker' is set to a positive integer (it is a kwarg of `freqanalysis`)
         - `keeptrials=True`, otherwise it makes no sense
         - We also do not select specific channels in `cfg.select`, as that is not supported with channel parallelisation.
         """
         cfg = TestMetadataUsingFooof.get_fooof_cfg()
-        cfg.pop('fooof_opt', None)
+        cfg.pop("fooof_opt", None)
         cfg.parallel = True  # Enable parallel computation
         cfg.keeptrials = True  # Enable parallel storage (is turned off when trial averaging is happening)
         cfg.select = None
-        fooof_opt = {'peak_width_limits': (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
+        fooof_opt = {"peak_width_limits": (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
         num_trials_fooof = 100
         num_channels = 5
         chan_per_worker = 2
-        multi_chan_data = _get_fooof_signal(nTrials=num_trials_fooof, nChannels = num_channels)
+        multi_chan_data = _get_fooof_signal(nTrials=num_trials_fooof, nChannels=num_channels)
         spec_dt = freqanalysis(cfg, multi_chan_data, fooof_opt=fooof_opt, chan_per_worker=chan_per_worker)
 
         # How many more calls we expect due to channel parallelization.
-        used_parallel = 'used_parallel = True' in spec_dt._log
+        used_parallel = "used_parallel = True" in spec_dt._log
         if used_parallel:
             calls_per_trial = int(math.ceil(num_channels / chan_per_worker))
         else:
             calls_per_trial = 1
         data_size = 100
 
         # check frequency axis
         assert spec_dt.freq.size == data_size
         assert spec_dt.freq[0] == 1
-        assert spec_dt.freq[99] == 100.
+        assert spec_dt.freq[99] == 100.0
 
         # check the data
-        assert spec_dt.data.shape == (num_trials_fooof, 1, data_size, num_channels) # Differs from other tests due to `keeptrials=True` and channels.
+        assert spec_dt.data.shape == (
+            num_trials_fooof,
+            1,
+            data_size,
+            num_channels,
+        )  # Differs from other tests due to `keeptrials=True` and channels.
         assert not np.isnan(spec_dt.data).any()
 
         # check metadata from 2nd cF return value, added to the hdf5 dataset 'data' as attributes.
         keys_unique = list()
         for fde in self.expected_metadata_keys:
             for trial_idx in range(num_trials_fooof):
                 for call_idx in range(calls_per_trial):
@@ -420,29 +495,34 @@
 
         assert spec_dt.metadata is not None
         assert isinstance(spec_dt.metadata, dict)  # Make sure it is a standard dict, not a hdf5 group.
         num_metadata_attrs = len(spec_dt.metadata.keys())
 
         spec_dt_metadata_unnested = metadata_unnest(spec_dt.metadata)
         assert num_metadata_attrs == self.num_expected_metadata_keys
-        assert len(spec_dt_metadata_unnested.keys()) == self.num_expected_metadata_keys * num_trials_fooof * calls_per_trial
+        assert (
+            len(spec_dt_metadata_unnested.keys())
+            == self.num_expected_metadata_keys * num_trials_fooof * calls_per_trial
+        )
         for kv in keys_unique:
             assert kv in spec_dt_metadata_unnested.keys()
             assert isinstance(spec_dt_metadata_unnested.get(kv), (list, np.ndarray))
 
         # check that the cfg is correct (required for replay)
-        assert spec_dt.cfg['freqanalysis']['output'] == 'fooof'
+        assert spec_dt.cfg["freqanalysis"]["output"] == "fooof"
 
     def test_metadata_parallel(self, testcluster):
 
         plt.ioff()
         client = dd.Client(testcluster)
-        all_tests = ["test_par_compute_with_sequential_storage",
-                     "test_par_compute_with_par_storage",
-                     "test_channel_par"]
+        all_tests = [
+            "test_par_compute_with_sequential_storage",
+            "test_par_compute_with_par_storage",
+            "test_channel_par",
+        ]
 
         for test_name in all_tests:
             test_method = getattr(self, test_name)
             test_method()
         client.close()
         plt.ion()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_packagesetup.py` & `esi_syncopy-2023.7/syncopy/tests/test_packagesetup.py`

 * *Files 11% similar despite different names*

```diff
@@ -15,15 +15,16 @@
 from glob import glob
 
 # Local imports
 import syncopy
 
 # Decorator to decide whether or not to run dask-related tests
 skip_in_ghactions = pytest.mark.skipif(
-    "GITHUB_ACTIONS" in os.environ.keys(), reason="Do not execute by GitHub Actions")
+    "GITHUB_ACTIONS" in os.environ.keys(), reason="Do not execute by GitHub Actions"
+)
 
 
 # check if folder creation in `__storage__` works as expected
 def test_storage_access():
     dirNames = [syncopy.__storage__, "first", "second", "third", "fourth"]
     folderCascade = os.path.join(*dirNames)
     os.makedirs(folderCascade)
@@ -49,56 +50,61 @@
     # in custom $SPYTMPDIR; force-kill the process after a few seconds preventing
     # Syncopy from cleaning up its temp storage folder
 
     # this function assumes to be in the root directory of the repository
     # if that is not the case we have to move there
 
     cdir = os.getcwd()
-    while 'syncopy' in cdir:
+    while "syncopy" in cdir:
         head, tail = os.path.split(cdir)
-        if not 'syncopy' in head:
+        if not "syncopy" in head:
             root_dir = os.path.join(head, tail)
             os.chdir(root_dir)
             break
         cdir = head
     # check that we are not entirely somewhere else
     else:
         assert False
 
     tmpDir = tempfile.mkdtemp()
 
     os.environ["SPYTMPDIR"] = tmpDir
-    commandStr = \
-        "import os; " +\
-        "import time; " +\
-        "import numpy as np; " +\
-        "import syncopy as spy; " +\
-        "dummy = spy.AnalogData(data=np.ones((10,10)), samplerate=1); " +\
-        "time.sleep(100)"
+    commandStr = (
+        "import os; "
+        + "import time; "
+        + "import numpy as np; "
+        + "import syncopy as spy; "
+        + "dummy = spy.AnalogData(data=np.ones((10,10)), samplerate=1); "
+        + "time.sleep(100)"
+    )
     process = subprocess.Popen([sys.executable, "-c", commandStr])
     time.sleep(12)
     process.kill()
 
     # get inventory of external Syncopy instance's temp storage
     num_garbage_before = len(glob(os.path.join(tmpDir, "*.analog")))
     assert num_garbage_before >= 0
 
     # launch 2nd external instance with same $SPYTMPDIR, create 2nd `AnalogData`
     # object, run `cleanup` and keep instance alive in background (for max. 100s)
-    commandStr = \
-        "import time; " +\
-        "import syncopy as spy; " +\
-        "import numpy as np; " +\
-        "dummy = spy.AnalogData(data=np.ones((10,10)), samplerate=1); " +\
-        "time.sleep(5)" +\
-        "spy.cleanup(older_than=0, interactive=False, only_current_session=True); " +\
-        "time.sleep(100)"
-    process2 = subprocess.Popen([sys.executable, "-c", commandStr],
-                                stdout=subprocess.PIPE, stderr=subprocess.PIPE,
-                                text=True)
+    commandStr = (
+        "import time; "
+        + "import syncopy as spy; "
+        + "import numpy as np; "
+        + "dummy = spy.AnalogData(data=np.ones((10,10)), samplerate=1); "
+        + "time.sleep(5)"
+        + "spy.cleanup(older_than=0, interactive=False, only_current_session=True); "
+        + "time.sleep(100)"
+    )
+    process2 = subprocess.Popen(
+        [sys.executable, "-c", commandStr],
+        stdout=subprocess.PIPE,
+        stderr=subprocess.PIPE,
+        text=True,
+    )
     time.sleep(12)
 
     num_garbage_after = len(glob(os.path.join(tmpDir, "*.analog")))
 
     # ensure `cleanup` call removed first instance's garbage but 2nd `AnalogData`
     # belonging to 2nd instance launched above is unharmed
     assert num_garbage_after == num_garbage_before
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_parsers.py` & `esi_syncopy-2023.7/syncopy/tests/test_parsers.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,101 +7,101 @@
 import os
 import platform
 import tempfile
 import pytest
 import numpy as np
 
 # Local imports
-from syncopy.shared.parsers import (io_parser, scalar_parser, array_parser,
-                                    filename_parser, data_parser)
+from syncopy.shared.parsers import (
+    io_parser,
+    scalar_parser,
+    array_parser,
+    filename_parser,
+    data_parser,
+)
 from syncopy.shared.tools import get_defaults
 from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYIOError
 from syncopy import AnalogData, SpectralData
 
 
-class TestIoParser():
+class TestIoParser:
     existingFolder = tempfile.gettempdir()
     nonExistingFolder = os.path.join("unlikely", "folder", "to", "exist")
 
     def test_none(self):
         with pytest.raises(SPYTypeError):
             io_parser(None)
 
     def test_exists(self):
-        io_parser(self.existingFolder, varname="existingFolder",
-                  isfile=False, exists=True)
+        io_parser(self.existingFolder, varname="existingFolder", isfile=False, exists=True)
         with pytest.raises(SPYIOError):
-            io_parser(self.existingFolder, varname="existingFolder",
-                      isfile=False, exists=False)
+            io_parser(
+                self.existingFolder,
+                varname="existingFolder",
+                isfile=False,
+                exists=False,
+            )
 
-        io_parser(self.nonExistingFolder, varname="nonExistingFolder",
-                  exists=False)
+        io_parser(self.nonExistingFolder, varname="nonExistingFolder", exists=False)
 
         with pytest.raises(SPYIOError):
-            io_parser(self.nonExistingFolder, varname="nonExistingFolder",
-                      exists=True)
+            io_parser(self.nonExistingFolder, varname="nonExistingFolder", exists=True)
 
     def test_isfile(self):
         with tempfile.NamedTemporaryFile() as f:
             io_parser(f.name, isfile=True, exists=True)
             with pytest.raises(SPYValueError):
                 io_parser(f.name, isfile=False, exists=True)
 
     def test_ext(self):
-        with tempfile.NamedTemporaryFile(suffix='a7f3.lfp') as f:
-            io_parser(f.name, ext=['lfp', 'mua'], exists=True)
-            io_parser(f.name, ext='lfp', exists=True)
+        with tempfile.NamedTemporaryFile(suffix="a7f3.lfp") as f:
+            io_parser(f.name, ext=["lfp", "mua"], exists=True)
+            io_parser(f.name, ext="lfp", exists=True)
             with pytest.raises(SPYValueError):
-                io_parser(f.name, ext='mua', exists=True)
+                io_parser(f.name, ext="mua", exists=True)
 
 
-class TestScalarParser():
+class TestScalarParser:
     def test_none(self):
         with pytest.raises(SPYTypeError):
-            scalar_parser(None, varname="value",
-                          ntype="int_like", lims=[10, 1000])
+            scalar_parser(None, varname="value", ntype="int_like", lims=[10, 1000])
 
     def test_within_limits(self):
         value = 440
-        scalar_parser(value, varname="value",
-                      ntype="int_like", lims=[10, 1000])
+        scalar_parser(value, varname="value", ntype="int_like", lims=[10, 1000])
 
-        freq = 2        # outside bounds
+        freq = 2  # outside bounds
         with pytest.raises(SPYValueError):
-            scalar_parser(freq, varname="freq",
-                          ntype="int_like", lims=[10, 1000])
+            scalar_parser(freq, varname="freq", ntype="int_like", lims=[10, 1000])
 
     def test_integer_like(self):
         freq = 440.0
-        scalar_parser(freq, varname="freq",
-                      ntype="int_like", lims=[10, 1000])
+        scalar_parser(freq, varname="freq", ntype="int_like", lims=[10, 1000])
 
         # not integer-like
         freq = 440.5
         with pytest.raises(SPYValueError):
-            scalar_parser(freq, varname="freq",
-                          ntype="int_like", lims=[10, 1000])
+            scalar_parser(freq, varname="freq", ntype="int_like", lims=[10, 1000])
 
     def test_string(self):
-        freq = '440'
+        freq = "440"
         with pytest.raises(SPYTypeError):
-            scalar_parser(freq, varname="freq",
-                          ntype="int_like", lims=[10, 1000])
+            scalar_parser(freq, varname="freq", ntype="int_like", lims=[10, 1000])
 
     def test_complex_valid(self):
         value = complex(2, -1)
         scalar_parser(value, lims=[-3, 5])  # valid
 
     def test_complex_invalid(self):
         value = complex(2, -1)
         with pytest.raises(SPYValueError):
             scalar_parser(value, lims=[-3, 1])
 
 
-class TestArrayParser():
+class TestArrayParser:
 
     time = np.linspace(0, 10, 100)
 
     def test_none(self):
         with pytest.raises(SPYTypeError):
             array_parser(None, varname="time")
 
@@ -166,22 +166,20 @@
 
     def test_ntype(self):
         # string
         with pytest.raises(SPYTypeError):
             array_parser(str(self.time), varname="time", ntype="numeric")
         # float32 instead of expected float64
         with pytest.raises(SPYValueError):
-            array_parser(np.float32(self.time), varname="time",
-                         ntype='float64')
+            array_parser(np.float32(self.time), varname="time", ntype="float64")
         # invalid mixed-type arrays
         with pytest.raises(SPYTypeError):
-            array_parser([3, 's'], varname="testarr", ntype='str')
+            array_parser([3, "s"], varname="testarr", ntype="str")
         with pytest.raises(SPYTypeError):
-            array_parser([3, 's'], varname="testarr", ntype='int')
-
+            array_parser([3, "s"], varname="testarr", ntype="int")
 
     def test_character_list(self):
         channels = np.array(["channel1", "channel2", "channel3"])
         array_parser(channels, varname="channels", dims=1)
         array_parser(channels, varname="channels", dims=(3,))
         array_parser(channels, varname="channels", dims=(None,))
         with pytest.raises(SPYValueError):
@@ -201,127 +199,128 @@
             errmsg = "'unsorted array'; expected array with elements in ascending order"
             assert errmsg in str(spyval.value)
         with pytest.raises(SPYValueError) as spyval:
             array_parser(ladder[::-1], issorted=True)
             errmsg = "'unsorted array'; expected array with elements in ascending order"
             assert errmsg in str(spyval.value)
         with pytest.raises(SPYValueError) as spyval:
-            array_parser([1+3j, 3, 4], issorted=True)
+            array_parser([1 + 3j, 3, 4], issorted=True)
             errmsg = "'array containing complex elements'; expected real-valued array"
             assert errmsg in str(spyval.value)
         with pytest.raises(SPYValueError) as spyval:
             array_parser(ladder, issorted=False)
             errmsg = "'array with elements in ascending order'; expected unsorted array"
             assert errmsg in str(spyval.value)
         with pytest.raises(SPYValueError) as spyval:
-            array_parser(['a', 'b', 'c'], issorted=True)
+            array_parser(["a", "b", "c"], issorted=True)
             errmsg = "expected dtype = numeric"
             assert errmsg in str(spyval.value)
         with pytest.raises(SPYValueError) as spyval:
             array_parser(np.ones(0), issorted=True)
             errmsg = "'array containing (fewer than) one element"
             assert errmsg in str(spyval.value)
 
 
-class TestFilenameParser():
+class TestFilenameParser:
     referenceResult = {
         "filename": "sessionName_testTag.analog",
         "container": "container.spy",
         "folder": "/tmp/container.spy",
         "tag": "testTag",
         "basename": "sessionName",
-        "extension": ".analog"
-        }
+        "extension": ".analog",
+    }
 
     def test_none(self):
         assert all([value is None for value in filename_parser(None).values()])
 
     def test_fname_only(self):
         fname = "sessionName_testTag.analog"
         assert filename_parser(fname) == {
-            "filename" : fname,
+            "filename": fname,
             "container": None,
             "folder": os.getcwd(),
             "tag": None,
             "basename": "sessionName_testTag",
-            "extension": ".analog"
+            "extension": ".analog",
         }
 
     def test_invalid_ext(self):
         # wrong extension
         with pytest.raises(SPYValueError):
             filename_parser("test.wrongExtension")
 
         # no extension
         with pytest.raises(SPYValueError):
             filename_parser("test")
 
     def test_with_info_ext(self):
         fname = "sessionName_testTag.analog.info"
         assert filename_parser(fname) == {
-            "filename" : fname.replace(".info", ""),
+            "filename": fname.replace(".info", ""),
             "container": None,
             "folder": os.getcwd(),
             "tag": None,
             "basename": "sessionName_testTag",
-            "extension": ".analog"
+            "extension": ".analog",
         }
 
     def test_valid_spy_container(self):
         fname = "sessionName.spy/sessionName_testTag.analog"
         assert filename_parser(fname, is_in_valid_container=True) == {
-            "filename" : "sessionName_testTag.analog",
+            "filename": "sessionName_testTag.analog",
             "container": "sessionName.spy",
             "folder": os.path.join(os.getcwd(), "sessionName.spy"),
             "tag": "testTag",
             "basename": "sessionName",
-            "extension": ".analog"
+            "extension": ".analog",
         }
+
     def test_invalid_spy_container(self):
         fname = "sessionName/sessionName_testTag.analog"
-        with  pytest.raises(SPYValueError):
+        with pytest.raises(SPYValueError):
             filename_parser(fname, is_in_valid_container=True)
 
         fname = "wrongContainer.spy/sessionName_testTag.analog"
-        with  pytest.raises(SPYValueError):
+        with pytest.raises(SPYValueError):
             filename_parser(fname, is_in_valid_container=True)
 
     def test_with_full_path(self):
         fname = os.path.normpath("/tmp/sessionName.spy/sessionName_testTag.analog")
         folder = "{}/tmp".format("C:" if platform.system() == "Windows" else "")
         assert filename_parser(fname, is_in_valid_container=True) == {
-            "filename" : "sessionName_testTag.analog",
+            "filename": "sessionName_testTag.analog",
             "container": "sessionName.spy",
             "folder": os.path.join(os.path.normpath(folder), "sessionName.spy"),
             "tag": "testTag",
             "basename": "sessionName",
-            "extension": ".analog"
-            }
+            "extension": ".analog",
+        }
 
     def test_folder_only(self):
         assert filename_parser("container.spy") == {
-            'filename': None,
-            'container': 'container.spy',
-            'folder': os.getcwd(),
-            'tag': None,
-            'basename': 'container',
-            'extension': '.spy'
-            }
+            "filename": None,
+            "container": "container.spy",
+            "folder": os.getcwd(),
+            "tag": None,
+            "basename": "container",
+            "extension": ".spy",
+        }
         folder = "{}/tmp".format("C:" if platform.system() == "Windows" else "")
         assert filename_parser("/tmp/container.spy") == {
-            'filename': None,
-            'container': 'container.spy',
-            'folder': os.path.normpath(folder),
-            'tag': None,
-            'basename': 'container',
-            'extension': '.spy'
-            }
+            "filename": None,
+            "container": "container.spy",
+            "folder": os.path.normpath(folder),
+            "tag": None,
+            "basename": "container",
+            "extension": ".spy",
+        }
 
 
-class TestDataParser():
+class TestDataParser:
     data = AnalogData()
 
     def test_none(self):
         with pytest.raises(SPYTypeError):
             data_parser(None)
 
     def test_dataclass(self):
@@ -347,13 +346,13 @@
 
     def test_dimord(self):
         with pytest.raises(SPYValueError):
             data_parser(self.data, dimord=["freq", "chan"])
 
 
 def func(input, keyword=None):
-    """ Test function for get_defaults test """
+    """Test function for get_defaults test"""
     pass
 
 
 def test_get_defaults():
     assert get_defaults(func) == {"keyword": None}
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_plotting.py` & `esi_syncopy-2023.7/syncopy/tests/test_plotting.py`

 * *Files 24% similar despite different names*

```diff
@@ -9,32 +9,32 @@
 import numpy as np
 import matplotlib.pyplot as ppl
 
 # Local imports
 import syncopy as spy
 from syncopy import synthdata
 import syncopy.tests.helpers as helpers
-from syncopy.shared.errors import SPYValueError
+from syncopy.shared.errors import SPYValueError, SPYError
 
 
-class TestAnalogPlotting():
+class TestAnalogPlotting:
 
     nTrials = 10
     nChannels = 9
     nSamples = 300
-    adata = synthdata.ar2_network(nTrials=nTrials,
-                                  AdjMat=np.zeros(nChannels),
-                                  nSamples=nSamples,
-                                  seed=helpers.test_seed)
-
-    adata += 0.3 * synthdata.linear_trend(nTrials=nTrials,
-                                          y_max=nSamples / 20,
-                                          nSamples=nSamples,
-                                          nChannels=nChannels)
-
+    adata = synthdata.ar2_network(
+        nTrials=nTrials,
+        AdjMat=np.zeros(nChannels),
+        nSamples=nSamples,
+        seed=helpers.test_seed,
+    )
+
+    adata += 0.3 * synthdata.linear_trend(
+        nTrials=nTrials, y_max=nSamples / 20, nSamples=nSamples, nChannels=nChannels
+    )
 
     # add an offset
     adata = adata + 5
 
     # all trials are equal
     toi_min, toi_max = adata.time[0][0], adata.time[0][-1]
 
@@ -45,59 +45,56 @@
 
         # check if we run the default test
         def_test = not len(kwargs)
 
         if def_test:
             # interactive plotting
             ppl.ion()
-            def_kwargs = {'trials': 1,
-                          'latency': [self.toi_min, 1.2 * self.toi_max]}
+            def_kwargs = {"trials": 1, "latency": [self.toi_min, 1.2 * self.toi_max]}
 
             fig1, ax1 = self.adata.singlepanelplot(**def_kwargs)
             fig2, ax2 = self.adata.singlepanelplot(**def_kwargs, shifted=False)
             fig3, axs = self.adata.multipanelplot(**def_kwargs)
 
             # check axes/figure references work
-            ax1.set_title('Shifted signals')
+            ax1.set_title("Shifted signals")
             fig1.tight_layout()
-            ax2.set_title('Overlayed signals')
+            ax2.set_title("Overlayed signals")
             fig2.tight_layout()
             fig3.suptitle("Multipanel plot")
             fig3.tight_layout()
         else:
             ppl.ioff()
             self.adata.singlepanelplot(**kwargs)
             self.adata.singlepanelplot(**kwargs, shifted=False)
             self.adata.multipanelplot(**kwargs)
-            ppl.close('all')
+            ppl.close("all")
 
     def test_ad_selections(self):
 
         # trial, channel and toi selections
-        selections = helpers.mk_selection_dicts(self.nTrials,
-                                                self.nChannels - 1,
-                                                toi_min=self.toi_min,
-                                                toi_max=self.toi_max)
+        selections = helpers.mk_selection_dicts(
+            self.nTrials, self.nChannels - 1, toi_min=self.toi_min, toi_max=self.toi_max
+        )
 
         # test all combinations
         for sel_dict in selections:
             # only single trial plotting
             # is supported until averaging is availbale
             # take random 1st trial
-            sel_dict['trials'] = sel_dict['trials'][0]
+            sel_dict["trials"] = sel_dict["trials"][0]
             # we have to sort the channels (hdf5 access)
-            sel_dict['channel'] = sorted(sel_dict['channel'])
+            sel_dict["channel"] = sorted(sel_dict["channel"])
             self.test_ad_plotting(**sel_dict)
 
     def test_ad_exceptions(self):
 
         # empty arrays get returned for empty time selection
         with pytest.raises(SPYValueError) as err:
-            self.test_ad_plotting(trials=0,
-                                  latency=[self.toi_max + 1, self.toi_max + 2])
+            self.test_ad_plotting(trials=0, latency=[self.toi_max + 1, self.toi_max + 2])
             assert "zero size" in str(err)
 
         # invalid channel selection
         with pytest.raises(SPYValueError) as err:
             self.test_ad_plotting(trials=0, channel=self.nChannels + 1)
             assert "channel existing names" in str(err)
 
@@ -108,74 +105,74 @@
 
     def test_ad_dimord(self):
         # create new mockup data
         rng = np.random.default_rng(helpers.test_seed)
         nSamples = 100
         nChannels = 4
         # single trial with ('channel', 'time') dimord
-        ad = spy.AnalogData([rng.standard_normal((nChannels, nSamples))], dimord=['channel', 'time'], samplerate=200)
+        ad = spy.AnalogData(
+            [rng.standard_normal((nChannels, nSamples))],
+            dimord=["channel", "time"],
+            samplerate=200,
+        )
 
         for chan in ad.channel:
             fig, ax = ad.singlepanelplot(channel=chan)
         # check that the right axis is the time axis
         xleft, xright = ax.get_xlim()
         assert xright - xleft >= nSamples / ad.samplerate
 
         # test multipanelplot
         fig, axs = ad.multipanelplot()
         # check that we have indeed nChannels axes
         assert axs.size == nChannels
 
-        xleft, xright = axs[0,0].get_xlim()
+        xleft, xright = axs[0, 0].get_xlim()
         # check that the right axis is the time axis
         xleft, xright = ax.get_xlim()
         assert xright - xleft >= nSamples / ad.samplerate
 
 
-class TestSpectralPlotting():
+class TestSpectralPlotting:
 
     nTrials = 10
     nChannels = 4
-    nSamples = 300
+    nSamples = 500
     AdjMat = np.zeros((nChannels, nChannels))
-    adata = synthdata.ar2_network(nTrials=nTrials,
-                                  AdjMat=AdjMat,
-                                  nSamples=nSamples)
+    adata = synthdata.ar2_network(nTrials=nTrials, AdjMat=AdjMat, nSamples=nSamples)
 
     # add AR(1) 'background'
-    adata = adata + 1.2 * synthdata.ar2_network(nTrials=nTrials,
-                                                AdjMat=AdjMat,
-                                                nSamples=nSamples,
-                                                alphas=[0.8, 0])
+    adata = adata + 1.2 * synthdata.ar2_network(
+        nTrials=nTrials, AdjMat=AdjMat, nSamples=nSamples, alphas=[0.8, 0]
+    )
 
     # some interesting range
     frequency = [1, 400]
 
     # all trials are equal
     toi_min, toi_max = adata.time[0][0], adata.time[0][-1]
 
-    spec_fft = spy.freqanalysis(adata, tapsmofrq=1)
-    spec_fft_imag = spy.freqanalysis(adata, output='imag')
-    spec_fft_mtm = spy.freqanalysis(adata, tapsmofrq=1, keeptapers=True)
-    spec_fft_complex = spy.freqanalysis(adata, output='fourier')
+    spec_fft = spy.freqanalysis(adata, tapsmofrq=2)
+    spec_fft_imag = spy.freqanalysis(adata, output="imag")
+    spec_fft_mtm = spy.freqanalysis(adata, tapsmofrq=3, keeptapers=True)
+    spec_fft_complex = spy.freqanalysis(adata, output="fourier")
 
-    spec_wlet = spy.freqanalysis(adata, method='wavelet',
-                                 foi=np.arange(0, 400, step=4))
+    spec_wlet = spy.freqanalysis(adata, method="wavelet", foi=np.arange(0, 400, step=4))
 
     def test_spectral_plotting(self, **kwargs):
 
         # no interactive plotting
         ppl.ioff()
 
         # check if we run the default test
         def_test = not len(kwargs)
 
         if def_test:
             ppl.ion()
-            kwargs = {'trials': self.nTrials - 1, 'frequency': [5, 300]}
+            kwargs = {"trials": self.nTrials - 1, "frequency": [5, 300]}
             # to visually compare
             self.adata.singlepanelplot(trials=self.nTrials - 1, channel=0)
 
             # this simulates the interactive plotting
             fig1, ax1 = self.spec_fft.singlepanelplot(**kwargs)
             fig2, axs = self.spec_fft.multipanelplot(**kwargs)
 
@@ -193,54 +190,52 @@
             assert res is None and res2 is None
             res = self.spec_fft_complex.multipanelplot(**kwargs)
             assert res is None
 
             fig3, ax2 = self.spec_wlet.singlepanelplot(channel=0, **kwargs)
             fig4, axs = self.spec_wlet.multipanelplot(**kwargs)
 
-            ax1.set_title('AR(1) + AR(2)')
-            fig2.suptitle('AR(1) + AR(2)')
+            ax1.set_title("AR(1) + AR(2)")
+            fig2.suptitle("AR(1) + AR(2)")
         else:
             self.spec_wlet.multipanelplot(**kwargs)
             # latency makes no sense for line plots
-            kwargs.pop('latency')
+            kwargs.pop("latency")
             self.spec_fft.singlepanelplot(**kwargs)
             self.spec_fft.multipanelplot(**kwargs)
 
             # take the 1st random channel for 2d spectra
-            if 'channel' in kwargs:
-                chan = kwargs.pop('channel')[0]
+            if "channel" in kwargs:
+                chan = kwargs.pop("channel")[0]
                 self.spec_wlet.singlepanelplot(channel=chan, **kwargs)
-            ppl.close('all')
+            ppl.close("all")
 
     def test_spectral_selections(self):
 
         # trial, channel and toi selections
-        selections = helpers.mk_selection_dicts(self.nTrials,
-                                                self.nChannels - 1,
-                                                toi_min=self.toi_min,
-                                                toi_max=self.toi_max)
+        selections = helpers.mk_selection_dicts(
+            self.nTrials, self.nChannels - 1, toi_min=self.toi_min, toi_max=self.toi_max
+        )
 
         # test all combinations
         for sel_dict in selections:
 
             # only single trial plotting
-            # is supported until averaging is availbale
+            # is supported, use spy.mean() to average beforehand if needed
             # take random 1st trial
-            sel_dict['trials'] = sel_dict['trials'][0]
+            sel_dict["trials"] = sel_dict["trials"][0]
             # we have to sort the channels (hdf5 access)
-            sel_dict['channel'] = sorted(sel_dict['channel'])
+            sel_dict["channel"] = sorted(sel_dict["channel"])
             self.test_spectral_plotting(**sel_dict)
 
     def test_spectral_exceptions(self):
 
         # empty arrays get returned for empty time selection
         with pytest.raises(SPYValueError) as err:
-            self.test_spectral_plotting(trials=0,
-                                        latency=[self.toi_max + 1, self.toi_max + 2])
+            self.test_spectral_plotting(trials=0, latency=[self.toi_max + 1, self.toi_max + 2])
             assert "zero size" in str(err)
 
         # invalid channel selection
         with pytest.raises(SPYValueError) as err:
             self.test_spectral_plotting(trials=0, channel=self.nChannels + 1)
             assert "channel existing names" in str(err)
 
@@ -251,44 +246,43 @@
 
         # invalid foi selection
         with pytest.raises(SPYValueError) as err:
             self.test_spectral_plotting(trials=0, frequency=[-1, 0])
             assert "frequency" in str(err)
 
 
-class TestCrossSpectralPlotting():
+class TestCrossSpectralPlotting:
 
     nTrials = 40
     nChannels = 4
     nSamples = 400
 
     AdjMat = np.zeros((nChannels, nChannels))
-    AdjMat[2, 3] = 0.2   # coupling
-    adata = synthdata.ar2_network(nTrials=nTrials,
-                                  AdjMat=AdjMat,
-                                  nSamples=nSamples)
+    AdjMat[2, 3] = 0.2  # coupling
+    adata = synthdata.ar2_network(nTrials=nTrials, AdjMat=AdjMat, nSamples=nSamples)
 
     # add 'background'
-    adata = adata + .6 * synthdata.ar2_network(nTrials=nTrials,
-                                               AdjMat=np.zeros((nChannels,
-                                                                nChannels)),
-                                               nSamples=nSamples,
-                                               alphas=[0.8, 0])
+    adata = adata + 0.6 * synthdata.ar2_network(
+        nTrials=nTrials,
+        AdjMat=np.zeros((nChannels, nChannels)),
+        nSamples=nSamples,
+        alphas=[0.8, 0],
+    )
 
     # some interesting range
     frequency = [1, 400]
 
     # all trials are equal
     toi_min, toi_max = adata.time[0][0], adata.time[0][-1]
 
-    coh = spy.connectivityanalysis(adata, method='coh', tapsmofrq=1)
-    coh_imag = spy.connectivityanalysis(adata, method='coh', tapsmofrq=1, output='imag')
+    coh = spy.connectivityanalysis(adata, method="coh", tapsmofrq=1)
+    coh_imag = spy.connectivityanalysis(adata, method="coh", tapsmofrq=1, output="imag")
 
-    corr = spy.connectivityanalysis(adata, method='corr')
-    granger = spy.connectivityanalysis(adata, method='granger', tapsmofrq=1)
+    corr = spy.connectivityanalysis(adata, method="corr")
+    granger = spy.connectivityanalysis(adata, method="granger", tapsmofrq=1)
 
     def test_cs_plotting(self, **kwargs):
 
         # no interactive plotting
         ppl.ioff()
 
         # check if we run the default test
@@ -299,77 +293,74 @@
 
             self.coh.singlepanelplot(channel_i=0, channel_j=1, frequency=[50, 320])
             self.coh.singlepanelplot(channel_i=1, channel_j=2, frequency=[50, 320])
             self.coh.singlepanelplot(channel_i=2, channel_j=3, frequency=[50, 320])
 
             self.coh_imag.singlepanelplot(channel_i=1, channel_j=2, frequency=[50, 320])
 
-            self.corr.singlepanelplot(channel_i=0, channel_j=1, latency=[0, .1])
-            self.corr.singlepanelplot(channel_i=1, channel_j=0, latency=[0, .1])
-            self.corr.singlepanelplot(channel_i=2, channel_j=3, latency=[0, .1])
+            self.corr.singlepanelplot(channel_i=0, channel_j=1, latency=[0, 0.1])
+            self.corr.singlepanelplot(channel_i=1, channel_j=0, latency=[0, 0.1])
+            self.corr.singlepanelplot(channel_i=2, channel_j=3, latency=[0, 0.1])
 
             self.granger.singlepanelplot(channel_i=0, channel_j=1, frequency=[50, 320])
             self.granger.singlepanelplot(channel_i=3, channel_j=2, frequency=[50, 320])
             self.granger.singlepanelplot(channel_i=2, channel_j=3, frequency=[50, 320])
 
-        elif 'latency' in kwargs:
+        elif "latency" in kwargs:
 
             self.corr.singlepanelplot(**kwargs)
             self.corr.singlepanelplot(**kwargs)
             self.corr.singlepanelplot(**kwargs)
-            ppl.close('all')
+            ppl.close("all")
 
         else:
 
             self.coh.singlepanelplot(**kwargs)
             self.coh.singlepanelplot(**kwargs)
             self.coh.singlepanelplot(**kwargs)
 
             self.granger.singlepanelplot(**kwargs)
             self.granger.singlepanelplot(**kwargs)
             self.granger.singlepanelplot(**kwargs)
 
     def test_cs_selections(self):
 
         # channel combinations
-        chans = itertools.product(self.coh.channel_i[:self.nTrials - 1],
-                                  self.coh.channel_j[1:])
+        chans = itertools.product(self.coh.channel_i[: self.nTrials - 1], self.coh.channel_j[1:])
 
         # out of range toi selections are no longer allowed..
-        latency = ([0, .1], 'all')
+        latency = ([0, 0.1], "all")
         toilim_comb = itertools.product(chans, latency)
 
         # out of range foi selections are NOT allowed..
-        frequency = ([10., 82.31], 'all')
+        frequency = ([10.0, 82.31], "all")
         foilim_comb = itertools.product(chans, frequency)
 
         for comb in toilim_comb:
             sel_dct = {}
             c1, c2 = comb[0]
-            sel_dct['channel_i'] = c1
-            sel_dct['channel_j'] = c2
-            sel_dct['latency'] = comb[1]
+            sel_dct["channel_i"] = c1
+            sel_dct["channel_j"] = c2
+            sel_dct["latency"] = comb[1]
             self.test_cs_plotting(**sel_dct)
 
         for comb in foilim_comb:
             sel_dct = {}
             c1, c2 = comb[0]
-            sel_dct['channel_i'] = c1
-            sel_dct['channel_j'] = c2
-            sel_dct['frequency'] = comb[1]
+            sel_dct["channel_i"] = c1
+            sel_dct["channel_j"] = c2
+            sel_dct["frequency"] = comb[1]
             self.test_cs_plotting(**sel_dct)
 
     def test_cs_exceptions(self):
 
-        chan_sel = {'channel_i': 0, 'channel_j': 2}
+        chan_sel = {"channel_i": 0, "channel_j": 2}
         # empty arrays get returned for empty time selection
         with pytest.raises(SPYValueError) as err:
-            self.test_cs_plotting(trials=0,
-                                  latency=[self.toi_max + 1, self.toi_max + 2],
-                                  **chan_sel)
+            self.test_cs_plotting(trials=0, latency=[self.toi_max + 1, self.toi_max + 2], **chan_sel)
             assert "zero size" in str(err)
 
         # invalid channel selections
         with pytest.raises(SPYValueError) as err:
             self.test_cs_plotting(trials=0, channel_i=self.nChannels + 1, channel_j=0)
             assert "channel existing names" in str(err)
 
@@ -380,11 +371,104 @@
 
         # invalid foi selection
         with pytest.raises(SPYValueError) as err:
             self.test_cs_plotting(trials=0, frequency=[-1, -0.2], **chan_sel)
             assert "frequency" in str(err)
 
 
-if __name__ == '__main__':
+class TestSpikeDataPlotting:
+
+    cfg = spy.StructDict()
+    cfg.nTrials = 100
+    cfg.nChannels = 80
+    cfg.nSpikes = 100_000
+    cfg.intensity = 0.2
+    cfg.nUnits = 40
+    cfg.samplerate = 10_000
+
+    spd = synthdata.poisson_noise(cfg, seed=42)
+
+    def test_trial_plotting(self):
+
+        # singleplot is for single trial or single unit
+        fig1, ax1 = self.spd.singlepanelplot(unit=11)
+        assert ax1.get_ylabel() == "trials"
+
+        # time/latency selection
+        fig, ax = self.spd.multipanelplot(latency=[0.2, 0.3], unit=np.arange(20, 26))
+        assert ax.size == 6
+
+    def test_unit_plotting(self):
+
+        # singleplot is for single trial or single unit
+        fig1, ax1 = self.spd.singlepanelplot(trials=11, on_yaxis="unit")
+        assert ax1.get_ylabel() == "unit"
+
+        # unit labels for < 20 units selected
+        unit_sel = np.arange(self.cfg.nUnits, step=2)
+        fig2, ax2 = self.spd.singlepanelplot(trials=11, unit=unit_sel, on_yaxis="unit")
+        assert len(ax2.get_yticklabels()) == len(unit_sel)
+        assert ax2.get_ylabel() == ""
+
+        # can plot max. 25 trials
+        fig4, axs1 = self.spd.multipanelplot(trials=np.arange(30, step=2), on_yaxis="unit")
+
+        fig5, axs2 = self.spd.multipanelplot(
+            channel=np.arange(6, 13),
+            unit=[0, 4, 9],
+            trials=np.arange(30, step=2),
+            on_yaxis="unit",
+        )
+
+        # -- test unit selection as this is special --
+        _, axs = self.spd.multipanelplot(
+            trials=[11, 34, self.cfg.nTrials - 1],
+            unit=[0, self.cfg.nUnits - 1],
+            on_yaxis="unit",
+        )
+
+        labels = [lbl._text for lbl in axs[0][0].get_yticklabels()]
+        assert labels == [self.spd.unit[0], self.spd.unit[-1]]
+
+        _, axs = self.spd.multipanelplot(
+            trials=[11, 34, self.cfg.nTrials - 1],
+            unit=["unit04", "unit12", "unit40"],
+            on_yaxis="unit",
+        )
+        assert len(axs[0][0].get_yticklabels()) == 3
+
+    def test_channel_plotting(self):
+
+        # channel labels for < 20 channels selected
+        chan_sel = [0, 3, 11, self.cfg.nChannels - 1]
+        fig3, ax3 = self.spd.singlepanelplot(trials=11, on_yaxis="channel", channel=chan_sel)
+        assert len(ax3.get_yticklabels()) == len(chan_sel)
+        assert ax3.get_ylabel() == ""
+
+    def test_exceptions(self):
+        """
+        Not much to test, mismatching show kwargs get catched by `selectdata` anyways
+        """
+
+        with pytest.raises(SPYValueError, match="expected either 'trials', 'unit' or 'channel'"):
+            self.spd.singlepanelplot(trials=10, on_yaxis="chunit")
+
+        with pytest.raises(SPYValueError, match="expected either 'trials', 'unit' or 'channel'"):
+            self.spd.multipanelplot(trials=[10, 12], on_yaxis="chunit")
+
+        # only 1 trial can be selected with unit/channel on y-axis
+        with pytest.raises(SPYError, match="Please select a single trial"):
+            self.spd.singlepanelplot(trials=[0, 3], on_yaxis="unit")
+
+        # we can plot max. 25 trials/units
+        with pytest.raises(SPYError, match="Please select maximum 25 trials"):
+            self.spd.multipanelplot(on_yaxis="unit")
+
+        with pytest.raises(SPYError, match="Please select maximum 25 units"):
+            self.spd.multipanelplot()
+
+
+if __name__ == "__main__":
     T1 = TestAnalogPlotting()
     T2 = TestSpectralPlotting()
     T3 = TestCrossSpectralPlotting()
+    T5 = TestSpikeDataPlotting()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_preproc.py` & `esi_syncopy-2023.7/syncopy/tests/test_preproc.py`

 * *Files 4% similar despite different names*

```diff
@@ -45,18 +45,17 @@
     trls = []
     for _ in range(nTrials):
         trl = np.random.randn(nSamples, nChannels)
         trls.append(trl)
 
     data = AnalogData(trls, samplerate=fs)
     # for toi tests, -1s offset
-    time_span = [-.8, 4.2]
+    time_span = [-0.8, 4.2]
     flow, fhigh = 0.3 * fNy, 0.4 * fNy
-    freq_kw = {'lp': fhigh, 'hp': flow,
-               'bp': [flow, fhigh], 'bs': [flow, fhigh]}
+    freq_kw = {"lp": fhigh, "hp": flow, "bp": [flow, fhigh], "bs": [flow, fhigh]}
 
     # the unfiltered data
     spec = freqanalysis(data, tapsmofrq=1, keeptrials=False)
 
     def test_but_filter(self, **kwargs):
 
         """
@@ -65,90 +64,89 @@
         Minimum order is 4 to safely pass..
         """
         # check if we run the default test
         def_test = not len(kwargs)
 
         # write default parameters dict
         if def_test:
-            kwargs = {'direction': 'twopass',
-                      'order': 4}
+            kwargs = {"direction": "twopass", "order": 4}
 
         # total power in arbitrary units (for now)
         pow_tot = self.spec.show(channel=0).sum()
         nFreq = self.spec.freq.size
 
         if def_test:
             fig, ax = mk_spec_ax()
 
         for ftype in preproc.availableFilterTypes:
-            filtered = ppfunc(self.data,
-                              filter_class='but',
-                              filter_type=ftype,
-                              freq=self.freq_kw[ftype],
-                              **kwargs)
+            filtered = ppfunc(
+                self.data,
+                filter_class="but",
+                filter_type=ftype,
+                freq=self.freq_kw[ftype],
+                **kwargs,
+            )
 
             # check in frequency space
             spec_f = freqanalysis(filtered, tapsmofrq=1, keeptrials=False)
 
             # get relevant frequency ranges
             # for integrated powers
-            if ftype == 'lp':
+            if ftype == "lp":
                 foilim = [0, self.freq_kw[ftype]]
-            elif ftype == 'hp':
+            elif ftype == "hp":
                 # toilim selections can screw up the
                 # frequency axis of freqanalysis/np.fft.rfftfreq :/
                 foilim = [self.freq_kw[ftype], spec_f.freq[-1]]
             else:
                 foilim = self.freq_kw[ftype]
 
             # remaining power after filtering
             pow_fil = spec_f.show(channel=0, frequency=foilim).sum()
             _, idx = best_match(spec_f.freq, foilim, span=True)
             # ratio of pass-band to total freqency band
             ratio = len(idx) / nFreq
 
             # at least 80% of the ideal filter power
             # should be still around
-            if ftype in ('lp', 'hp'):
+            if ftype in ("lp", "hp"):
                 assert 0.8 * ratio < pow_fil / pow_tot
             # here we have two roll-offs, one at each side
-            elif ftype == 'bp':
+            elif ftype == "bp":
                 assert 0.7 * ratio < pow_fil / pow_tot
             # as well as here
-            elif ftype == 'bs':
+            elif ftype == "bs":
                 assert 0.7 * ratio < (pow_tot - pow_fil) / pow_tot
             if def_test:
                 plot_spec(ax, spec_f, label=ftype)
 
         # plotting
         if def_test:
-            plot_spec(ax, self.spec, c='0.3', label='unfiltered')
-            annotate_foilims(ax, *self.freq_kw['bp'])
+            plot_spec(ax, self.spec, c="0.3", label="unfiltered")
+            annotate_foilims(ax, *self.freq_kw["bp"])
             ax.set_title(f"Twopass Butterworth, order = {kwargs['order']}")
 
     def test_but_kwargs(self):
 
         """
         Test order and direction parameter
         """
 
         for direction in preproc.availableDirections:
-            kwargs = {'direction': direction,
-                      'order': 4}
+            kwargs = {"direction": direction, "order": 4}
             # only for firws
-            if 'minphase' in direction:
+            if "minphase" in direction:
                 with pytest.raises(SPYValueError) as err:
                     self.test_but_filter(**kwargs)
                 assert "expected 'onepass'" in str(err.value)
             else:
                 self.test_but_filter(**kwargs)
 
         for order in [-2, 10, 5.6]:
-            kwargs = {'direction': 'twopass',
-                      'order': order}
+            kwargs = {"direction": "twopass", "order": order}
 
             if order < 1 and isinstance(order, int):
                 with pytest.raises(SPYValueError) as err:
                     self.test_but_filter(**kwargs)
                 assert "value to be greater" in str(err)
             elif not isinstance(order, int):
                 with pytest.raises(SPYValueError) as err:
@@ -156,119 +154,124 @@
                 assert "int_like" in str(err)
             # valid order
             else:
                 self.test_but_filter(**kwargs)
 
     def test_but_selections(self):
 
-        sel_dicts = helpers.mk_selection_dicts(nTrials=20,
-                                               nChannels=2,
-                                               toi_min=self.time_span[0],
-                                               toi_max=self.time_span[1],
-                                               min_len=3.5)
+        sel_dicts = helpers.mk_selection_dicts(
+            nTrials=20,
+            nChannels=2,
+            toi_min=self.time_span[0],
+            toi_max=self.time_span[1],
+            min_len=3.5,
+        )
         for sd in sel_dicts:
             self.test_but_filter(select=sd)
 
     def test_but_polyremoval(self):
 
         helpers.run_polyremoval_test(self.test_but_filter)
 
     def test_but_cfg(self):
 
         cfg = get_defaults(ppfunc)
 
-        cfg.filter_class = 'but'
+        cfg.filter_class = "but"
         cfg.order = 6
-        cfg.direction = 'twopass'
+        cfg.direction = "twopass"
         cfg.freq = 30
-        cfg.filter_type = 'hp'
+        cfg.filter_type = "hp"
 
         result = ppfunc(self.data, cfg)
 
         # check here just for finiteness
         assert np.all(np.isfinite(result.data))
 
     def test_but_parallel(self, testcluster):
 
         ppl.ioff()
         client = dd.Client(testcluster)
         print(client)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (inspect.ismethod(getattr(self, attr)) and "parallel" not in attr)
+        ]
 
         for test_name in all_tests:
             test_method = getattr(self, test_name)
             # don't test parallel selections
-            if 'selection' in test_name:
+            if "selection" in test_name:
                 continue
-            if 'but_filter' in test_name:
+            if "but_filter" in test_name:
                 # test parallelisation along channels
                 test_method(chan_per_worker=2)
             else:
                 test_method()
         client.close()
         ppl.ion()
 
     def test_but_hilbert_rect(self):
 
-        call = lambda **kwargs: ppfunc(self.data,
-                                       freq=20,
-                                       filter_class='but',
-                                       filter_type='lp',
-                                       order=5,
-                                       direction='onepass',
-                                       **kwargs)
+        call = lambda **kwargs: ppfunc(
+            self.data,
+            freq=20,
+            filter_class="but",
+            filter_type="lp",
+            order=5,
+            direction="onepass",
+            **kwargs,
+        )
 
         # test rectification
         filtered = call(rectify=False)
         assert not np.all(filtered.trials[0] > 0)
         rectified = call(rectify=True)
         assert np.all(rectified.trials[0] > 0)
 
         # test simultaneous call to hilbert and rectification
         with pytest.raises(SPYValueError) as err:
-            call(rectify=True, hilbert='abs')
+            call(rectify=True, hilbert="abs")
         assert "either rectifi" in str(err)
         assert "or Hilbert" in str(err)
 
         # test hilbert outputs
         for output in preproc.hilbert_outputs:
             htrafo = call(hilbert=output)
-            if output == 'complex':
+            if output == "complex":
                 assert np.all(np.imag(htrafo.trials[0]) != 0)
             else:
                 assert np.all(np.imag(htrafo.trials[0]) == 0)
 
         # test wrong hilbert parameter
         with pytest.raises(SPYValueError) as err:
-            call(hilbert='absnot')
+            call(hilbert="absnot")
         assert "one of {'" in str(err)
 
     def test_but_NaN(self):
 
         nSamples = 20
         nTrials = 5
         nChannels = 3
 
         # create test data with NaNs in 2 trials
         arr = [(i + 1) * np.ones((nSamples, nChannels)) for i in range(nTrials)]
         # add NaNs in 2nd and last trial
         arr[1][5, 1] = np.nan
         arr[-1][10:15, 2] = np.nan
         adata = AnalogData(data=arr, samplerate=50)
-        res = ppfunc(adata,
-                     freq=20,
-                     filter_class='but')
+        res = ppfunc(adata, freq=20, filter_class="but")
 
         # IIR filters can't work around NaNs
         assert np.sum(np.isnan(res.trials[0])) == 0
         assert np.sum(np.isnan(res.trials[1])) == nSamples
         assert np.sum(np.isnan(res.trials[4])) == nSamples
         # check that metadata got propagated
-        assert res.info['nan_trials'] == [1, 4]
+        assert res.info["nan_trials"] == [1, 4]
 
 
 class TestFIRWS:
 
     nSamples = 1000
     nChannels = 4
     nTrials = 50
@@ -280,18 +283,17 @@
     trls = []
     for _ in range(nTrials):
         trl = np.random.randn(nSamples, nChannels)
         trls.append(trl)
 
     data = AnalogData(trls, samplerate=fs)
     # for toi tests, -1s offset
-    time_span = [-.8, 4.2]
+    time_span = [-0.8, 4.2]
     flow, fhigh = 0.3 * fNy, 0.4 * fNy
-    freq_kw = {'lp': fhigh, 'hp': flow,
-               'bp': [flow, fhigh], 'bs': [flow, fhigh]}
+    freq_kw = {"lp": fhigh, "hp": flow, "bp": [flow, fhigh], "bs": [flow, fhigh]}
 
     # the unfiltered data
     spec = freqanalysis(data, tapsmofrq=1, keeptrials=False)
 
     def test_firws_filter(self, **kwargs):
 
         """
@@ -301,82 +303,81 @@
         200 is safe to pass!
         """
         # check if we run the default test
         def_test = not len(kwargs)
 
         # write default parameters dict
         if def_test:
-            kwargs = {'direction': 'twopass',
-                      'order': 200}
+            kwargs = {"direction": "twopass", "order": 200}
 
         # total power in arbitrary units (for now)
         pow_tot = self.spec.show(channel=0).sum()
         nFreq = self.spec.freq.size
 
         if def_test:
             fig, ax = mk_spec_ax()
 
         for ftype in preproc.availableFilterTypes:
-            filtered = ppfunc(self.data,
-                              filter_class='firws',
-                              filter_type=ftype,
-                              freq=self.freq_kw[ftype],
-                              **kwargs)
+            filtered = ppfunc(
+                self.data,
+                filter_class="firws",
+                filter_type=ftype,
+                freq=self.freq_kw[ftype],
+                **kwargs,
+            )
             # check in frequency space
             spec_f = freqanalysis(filtered, tapsmofrq=1, keeptrials=False)
 
             # get relevant frequency ranges
             # for integrated powers
-            if ftype == 'lp':
+            if ftype == "lp":
                 foilim = [0, self.freq_kw[ftype]]
-            elif ftype == 'hp':
+            elif ftype == "hp":
                 # toilim selections can screw up the
                 # frequency axis of freqanalysis/np.fft.rfftfreq :/
                 foilim = [self.freq_kw[ftype], spec_f.freq[-1]]
             else:
                 foilim = self.freq_kw[ftype]
 
             # remaining power after filtering
             pow_fil = spec_f.show(channel=0, frequency=foilim).sum()
             _, idx = best_match(spec_f.freq, foilim, span=True)
             # ratio of pass-band to total freqency band
             ratio = len(idx) / nFreq
 
             # at least 80% of the ideal filter power
             # should be still around
-            if ftype in ('lp', 'hp'):
+            if ftype in ("lp", "hp"):
                 assert 0.8 * ratio < pow_fil / pow_tot
             # here we have two roll-offs, one at each side
-            elif ftype == 'bp':
+            elif ftype == "bp":
                 assert 0.7 * ratio < pow_fil / pow_tot
             # as well as here
-            elif ftype == 'bs':
+            elif ftype == "bs":
                 assert 0.7 * ratio < (pow_tot - pow_fil) / pow_tot
             if def_test:
                 plot_spec(ax, spec_f, label=ftype)
 
         # plotting
         if def_test:
-            plot_spec(ax, self.spec, c='0.3', label='unfiltered')
-            annotate_foilims(ax, *self.freq_kw['bp'])
+            plot_spec(ax, self.spec, c="0.3", label="unfiltered")
+            annotate_foilims(ax, *self.freq_kw["bp"])
             ax.set_title(f"Twopass FIRWS, order = {kwargs['order']}")
 
     def test_firws_kwargs(self):
 
         """
         Test order and direction parameter
         """
 
         for direction in preproc.availableDirections:
-            kwargs = {'direction': direction,
-                      'order': 200}
+            kwargs = {"direction": direction, "order": 200}
             self.test_firws_filter(**kwargs)
         for order in [-2, 220, 5.6]:
-            kwargs = {'direction': 'twopass',
-                      'order': order}
+            kwargs = {"direction": "twopass", "order": order}
 
             if order < 1 and isinstance(order, int):
                 with pytest.raises(SPYValueError) as err:
                     self.test_firws_filter(**kwargs)
                 assert "value to be greater" in str(err)
 
             elif not isinstance(order, int):
@@ -386,94 +387,101 @@
 
             # valid order
             else:
                 self.test_firws_filter(**kwargs)
 
     def test_firws_selections(self):
 
-        sel_dicts = helpers.mk_selection_dicts(nTrials=20,
-                                               nChannels=2,
-                                               toi_min=self.time_span[0],
-                                               toi_max=self.time_span[1],
-                                               min_len=3.5)
+        sel_dicts = helpers.mk_selection_dicts(
+            nTrials=20,
+            nChannels=2,
+            toi_min=self.time_span[0],
+            toi_max=self.time_span[1],
+            min_len=3.5,
+        )
         for sd in sel_dicts:
             self.test_firws_filter(select=sd, order=200)
 
     def test_firws_polyremoval(self):
 
         helpers.run_polyremoval_test(self.test_firws_filter)
 
     def test_firws_cfg(self):
 
         cfg = get_defaults(ppfunc)
 
-        cfg.filter_class = 'firws'
+        cfg.filter_class = "firws"
         cfg.order = 200
-        cfg.direction = 'twopass'
+        cfg.direction = "twopass"
         cfg.freq = 30
-        cfg.filter_type = 'hp'
+        cfg.filter_type = "hp"
 
         result = ppfunc(self.data, cfg)
 
         # check here just for finiteness
         assert np.all(np.isfinite(result.data))
 
     def test_firws_parallel(self, testcluster):
 
         ppl.ioff()
         client = dd.Client(testcluster)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (inspect.ismethod(getattr(self, attr)) and "parallel" not in attr)
+        ]
 
         for test_name in all_tests:
             test_method = getattr(self, test_name)
             # don't test parallel selections
-            if 'selection' in test_name:
+            if "selection" in test_name:
                 continue
-            if 'firws_filter' in test_name:
+            if "firws_filter" in test_name:
                 # test parallelisation along channels
                 test_method(chan_per_worker=2)
             else:
                 test_method()
         client.close()
         ppl.ion()
 
     def test_firws_hilbert_rect(self):
 
-        call = lambda **kwargs: ppfunc(self.data,
-                                       freq=20,
-                                       filter_class='firws',
-                                       filter_type='lp',
-                                       order=200,
-                                       direction='onepass',
-                                       **kwargs)
+        call = lambda **kwargs: ppfunc(
+            self.data,
+            freq=20,
+            filter_class="firws",
+            filter_type="lp",
+            order=200,
+            direction="onepass",
+            **kwargs,
+        )
 
         # test rectification
         filtered = call(rectify=False)
         assert not np.all(filtered.trials[0] > 0)
         rectified = call(rectify=True)
         assert np.all(rectified.trials[0] > 0)
 
         # test simultaneous call to hilbert and rectification
         with pytest.raises(SPYValueError) as err:
-            call(rectify=True, hilbert='abs')
+            call(rectify=True, hilbert="abs")
         assert "either rectifi" in str(err)
         assert "or Hilbert" in str(err)
 
         # test hilbert outputs
         for output in preproc.hilbert_outputs:
             htrafo = call(hilbert=output)
-            if output == 'complex':
+            if output == "complex":
                 assert np.all(np.imag(htrafo.trials[0]) != 0)
             else:
                 assert np.all(np.imag(htrafo.trials[0]) == 0)
 
         # test wrong hilbert parameter
         with pytest.raises(SPYValueError) as err:
-            call(hilbert='absnot')
+            call(hilbert="absnot")
         assert "one of {'" in str(err)
 
     def test_firws_NaN(self):
 
         nSamples = 20
         nTrials = 5
         nChannels = 3
@@ -481,19 +489,15 @@
 
         # create test data with NaNs in 2 trials
         arr = [(i + 1) * np.ones((nSamples, nChannels)) for i in range(nTrials)]
         # add NaNs in 2nd and last trial
         arr[1][5, 1] = np.nan
         arr[-1][10:15, 2] = np.nan  # "NaN island"
         adata = AnalogData(data=arr, samplerate=50)
-        res = ppfunc(adata,
-                     freq=20,
-                     filter_class='firws',
-                     order=order,
-                     direction='onepass')
+        res = ppfunc(adata, freq=20, filter_class="firws", order=order, direction="onepass")
 
         # no NaNs in 1st trial
         assert np.sum(np.isnan(res.trials[0])) == 0
         # we "want" only NaNs in the result, where the filter
         # support covers/touches the NaN sample(s) in the input
         # for an isolated NaN this happens exactly for order (filter length) samples
         assert np.sum(np.isnan(res.trials[1])) == 1 + order
@@ -501,67 +505,70 @@
         # grows in total also by the filter order
         number_NaN = np.sum(np.isnan(adata.trials[4]))
         assert np.sum(np.isnan(res.trials[4])) == number_NaN + order
         # final sanity check
         assert np.sum(np.isnan(res.trials[4])) < nSamples
 
         # finally check that the metadata got propagated
-        assert res.info['nan_trials'] == [1, 4]
+        assert res.info["nan_trials"] == [1, 4]
 
 
 class TestDetrending:
 
-    """ Test standalone detrending """
+    """Test standalone detrending"""
 
     nTrials = 2
     nSamples = 5000
     AData = sd.linear_trend(nTrials=2, nSamples=nSamples, y_max=10)
-    AData += sd.white_noise(nTrials=2, nSamples=nSamples) + 5  # add constant
+    AData += sd.white_noise(nTrials=2, nSamples=nSamples, seed=42) + 5  # add constant
 
     def test_demeaning(self):
 
         res = ppfunc(self.AData, filter_class=None, polyremoval=0)
 
         orig_c0 = self.AData.show(trials=1, channel=0)
         res_c0 = res.show(trials=1, channel=0)
         # just to make sure we have an offset
         assert np.mean(self.AData.show(trials=1, channel=0)) > 1
-        assert np.allclose(np.mean(res.show(trials=1, channel=0)), 0, atol=1e-5)
+        assert np.allclose(np.mean(res.show(trials=1, channel=0)), 0, atol=2e-5)
 
         # check that the linear trend is still around
         assert np.allclose(orig_c0.max() - orig_c0.min(), res_c0.max() - res_c0.min())
 
     def test_detrending(self):
 
         res = ppfunc(self.AData, filter_class=None, polyremoval=1)
 
         orig_c0 = self.AData.show(trials=1, channel=0)
         res_c0 = res.show(trials=1, channel=0)
 
         # detrended also means demeaned
-        assert np.allclose(np.mean(res.show(trials=1, channel=0)), 0, atol=1e-5)
+        assert np.allclose(np.mean(res.show(trials=1, channel=0)), 0, atol=1e-4)
 
         # check that the linear trend is gone
         assert (orig_c0.max() - orig_c0.min()) > 1.5 * (res_c0.max() - res_c0.min())
 
     def test_exceptions(self):
-        with pytest.raises(SPYValueError, match='neither filtering, detrending or zscore'):
+        with pytest.raises(SPYValueError, match="neither filtering, detrending or zscore"):
             ppfunc(self.AData, filter_class=None, polyremoval=None)
 
-        with pytest.raises(SPYValueError, match='expected value to be greater'):
+        with pytest.raises(SPYValueError, match="expected value to be greater"):
             ppfunc(self.AData, filter_class=None, polyremoval=-1)
 
-        with pytest.raises(SPYValueError, match='expected value to be greater'):
+        with pytest.raises(SPYValueError, match="expected value to be greater"):
             ppfunc(self.AData, filter_class=None, polyremoval=2)
 
     def test_detr_parallel(self, testcluster):
 
         client = dd.Client(testcluster)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (inspect.ismethod(getattr(self, attr)) and "parallel" not in attr)
+        ]
 
         for test_name in all_tests:
             test_method = getattr(self, test_name)
             test_method()
         client.close()
 
     def test_detr_NaN(self):
@@ -574,41 +581,37 @@
         arr = [(i + 1) * np.ones((nSamples, nChannels)) for i in range(nTrials)]
         # add NaNs in 2nd and last trial
         arr[1][5, 1] = np.nan
         arr[-1][10:15, 2] = np.nan
         adata = AnalogData(data=arr, samplerate=50)
 
         # -- demeaning --
-        res = ppfunc(adata,
-                     filter_class=None,
-                     polyremoval=0)
+        res = ppfunc(adata, filter_class=None, polyremoval=0)
 
         # detrending can't work around NaNs
         assert np.sum(np.isnan(res.trials[0])) == 0
         assert np.sum(np.isnan(res.trials[1])) == nSamples
         assert np.sum(np.isnan(res.trials[4])) == nSamples
         # check that metadata got propagated
-        assert res.info['nan_trials'] == [1, 4]
+        assert res.info["nan_trials"] == [1, 4]
 
         # -- linear detrending --
-        res = ppfunc(adata,
-                     filter_class=None,
-                     polyremoval=1)
+        res = ppfunc(adata, filter_class=None, polyremoval=1)
 
         # detrending can't work around NaNs
         assert np.sum(np.isnan(res.trials[0])) == 0
         assert np.sum(np.isnan(res.trials[1])) == nSamples
         assert np.sum(np.isnan(res.trials[4])) == nSamples
         # check that metadata got propagated
-        assert res.info['nan_trials'] == [1, 4]
+        assert res.info["nan_trials"] == [1, 4]
 
 
 class TestStandardize:
 
-    """ Test standalone and general zscore """
+    """Test standalone and general zscore"""
 
     nTrials = 2
     nSamples = 5000
     AData = 100 * sd.white_noise(nTrials=nTrials, nSamples=nSamples) + 5  # add constant
 
     def test_standardize(self):
 
@@ -627,72 +630,75 @@
         assert np.std(orig_c0) > 95
 
         # got normalized to std
         assert np.allclose(np.std(res_c0), 1.0)
 
     def test_firws_standardize(self):
 
-        res1 = ppfunc(self.AData, filter_class='firws', freq=100, zscore=False)
-        res2 = ppfunc(self.AData, filter_class='firws', freq=100, zscore=True)
+        res1 = ppfunc(self.AData, filter_class="firws", freq=100, zscore=False)
+        res2 = ppfunc(self.AData, filter_class="firws", freq=100, zscore=True)
 
         # only low pass filtering does not remove the mean
         assert np.mean(res1.show(channel=1)) > 1
         # with z-score it is gone
         assert np.allclose(np.mean(res2.show(channel=1)), 0, atol=1e-3)
 
     def test_but_standardize(self):
 
-        res1 = ppfunc(self.AData, filter_class='but', freq=100, zscore=False)
-        res2 = ppfunc(self.AData, filter_class='but', freq=100, zscore=True)
+        res1 = ppfunc(self.AData, filter_class="but", freq=100, zscore=False)
+        res2 = ppfunc(self.AData, filter_class="but", freq=100, zscore=True)
 
         # only low pass filtering does not remove the mean
         assert np.mean(res1.show(channel=1)) > 1
         # with z-score it is gone
         assert np.allclose(np.mean(res2.show(channel=1)), 0, atol=1e-3)
 
     def test_exceptions(self):
 
-        with pytest.raises(SPYValueError, match='neither filtering, detrending or zscore'):
+        with pytest.raises(SPYValueError, match="neither filtering, detrending or zscore"):
             ppfunc(self.AData, filter_class=None, polyremoval=None, zscore=False)
 
-        with pytest.raises(SPYValueError, match='expected either `True` or `False`'):
+        with pytest.raises(SPYValueError, match="expected either `True` or `False`"):
             ppfunc(self.AData, filter_class=None, zscore=2)
 
     def test_zscore_parallel(self, testcluster):
 
         client = dd.Client(testcluster)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (inspect.ismethod(getattr(self, attr)) and "parallel" not in attr)
+        ]
 
         for test_name in all_tests:
             test_method = getattr(self, test_name)
             test_method()
         client.close()
 
 
 def mk_spec_ax():
 
     fig, ax = ppl.subplots()
-    ax.set_xlabel('frequency (Hz)')
-    ax.set_ylabel('power (dB)')
+    ax.set_xlabel("frequency (Hz)")
+    ax.set_ylabel("power (dB)")
     return fig, ax
 
 
 def plot_spec(ax, spec, **pkwargs):
 
     ax.plot(spec.freq, spec.show(channel=1), alpha=0.8, **pkwargs)
     ax.legend()
 
 
 def annotate_foilims(ax, flow, fhigh):
 
     ylim = ax.get_ylim()
-    ax.plot([flow, flow], [0, 1], 'k--')
-    ax.plot([fhigh, fhigh], [0, 1], 'k--')
+    ax.plot([flow, flow], [0, 1], "k--")
+    ax.plot([fhigh, fhigh], [0, 1], "k--")
     ax.set_ylim(ylim)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestButterworth()
     T2 = TestFIRWS()
     T3 = TestDetrending()
     T4 = TestStandardize()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_redefinetrial.py` & `esi_syncopy-2023.7/syncopy/tests/test_redefinetrial.py`

 * *Files 11% similar despite different names*

```diff
@@ -17,16 +17,15 @@
 
 class TestRedefinetrial:
 
     nTrials = 10
     samplerate = 10
 
     # equally sized trials, use CrossSpectralData with time axis
-    reg_data = CrossSpectralData(data=[np.ones((10, 7, 2, 2)) for _ in range(nTrials)],
-                                 samplerate=samplerate)
+    reg_data = CrossSpectralData(data=[np.ones((10, 7, 2, 2)) for _ in range(nTrials)], samplerate=samplerate)
 
     irreg_data = reg_data.copy()
     trldef = irreg_data.trialdefinition
     # short 3rd trial
     trldef[2] = [23, 26, -10]
     # longer trial 6 (with overlap)
     trldef[5] = [48, 65, -10]
@@ -37,30 +36,30 @@
     assert not np.all(np.diff(irreg_data.trialintervals) == 0.9)
     # # one short trial
     assert np.sum(np.diff(irreg_data.trialintervals) < 0.9) == 1
     # # one long trial
     assert np.sum(np.diff(irreg_data.trialintervals) > 0.9) == 1
 
     def test_user_input(self):
-        """ check non compatible arguments are catched """
+        """check non compatible arguments are catched"""
 
         # offset and trialdef
-        with pytest.raises(SPYError, match='Incompatible input arguments'):
+        with pytest.raises(SPYError, match="Incompatible input arguments"):
             redefinetrial(self.reg_data, offset=-2, trl=3)
         # relative begin sample and trialdef
-        with pytest.raises(SPYError, match='Incompatible input arguments'):
+        with pytest.raises(SPYError, match="Incompatible input arguments"):
             redefinetrial(self.reg_data, begsample=2000, trl=3)
         # toilim and trialdef
-        with pytest.raises(SPYError, match='Incompatible input arguments'):
+        with pytest.raises(SPYError, match="Incompatible input arguments"):
             redefinetrial(self.reg_data, trl=3, toilim=[1, 2])
         # minlength and toilim
-        with pytest.raises(SPYError, match='Incompatible input arguments'):
+        with pytest.raises(SPYError, match="Incompatible input arguments"):
             redefinetrial(self.reg_data, minlength=2, toilim=[1, 2])
         # relative begin sample and minlength
-        with pytest.raises(SPYError, match='Incompatible input arguments'):
+        with pytest.raises(SPYError, match="Incompatible input arguments"):
             redefinetrial(self.reg_data, begsample=2000, minlength=2)
 
     def test_offset(self):
 
         # only pre-stimulus with default offset (-1s or -samplerate)
         assert np.all([self.reg_data.time[i] < 0 for i in range(self.nTrials)])
 
@@ -73,29 +72,28 @@
 
         # test vector valued offset
         dummy = redefinetrial(self.reg_data, offset=np.arange(self.nTrials) - self.nTrials // 2)
         # now halft the trials should have only post stimulus time axes
         assert np.sum(np.all([dummy.time[i] >= 0 for i in range(self.nTrials)], axis=0)) == 5
 
         # test exceptions
-        with pytest.raises(SPYValueError, match='expected array of length'):
+        with pytest.raises(SPYValueError, match="expected array of length"):
             redefinetrial(self.reg_data, offset=np.arange(self.nTrials - 1))
 
-        with pytest.raises(SPYTypeError, match='expected scalar, array'):
-            redefinetrial(self.reg_data, offset='no-number')
+        with pytest.raises(SPYTypeError, match="expected scalar, array"):
+            redefinetrial(self.reg_data, offset="no-number")
 
         # with trial selection
-        dummy = redefinetrial(self.reg_data, trials=np.arange(9),
-                              offset=np.arange(1, self.nTrials))
+        dummy = redefinetrial(self.reg_data, trials=np.arange(9), offset=np.arange(1, self.nTrials))
         assert len(dummy.trials) == self.nTrials - 1
         # again only post-stimulus with positive offsets
         assert np.all([dummy.time[i] > 0 for i in range(self.nTrials - 1)])
 
     def test_minlength(self):
-        """ select trials via total lengths in seconds """
+        """select trials via total lengths in seconds"""
 
         # regular trial length is 10 samples, with samplerate=10 this is 1 second
         # so nothing gets thrown out here
         dummy = redefinetrial(self.reg_data, minlength=1)
         assert len(dummy.trials) == self.nTrials
 
         # here we should get an empty object as no trial is 2 seconds long
@@ -112,76 +110,81 @@
         assert len(dummy.trials) == 1
 
         # if we deselect the longer trial we get an empty object
         dummy = redefinetrial(self.irreg_data, trials=[0, 1, 8], minlength=1.5)
         assert dummy.data is None
 
         # for very short minimal length all trials get selected
-        dummy = redefinetrial(self.irreg_data, minlength=.1)
+        dummy = redefinetrial(self.irreg_data, minlength=0.1)
         assert len(dummy.trials) == self.nTrials
 
         # test exceptions
-        with pytest.raises(SPYTypeError, match='expected scalar'):
-            redefinetrial(self.reg_data, minlength='no-number')
+        with pytest.raises(SPYTypeError, match="expected scalar"):
+            redefinetrial(self.reg_data, minlength="no-number")
 
-        with pytest.raises(SPYTypeError, match='expected scalar'):
+        with pytest.raises(SPYTypeError, match="expected scalar"):
             redefinetrial(self.reg_data, minlength=np.arange(10))
 
-        with pytest.raises(SPYValueError, match='expected value to be greater'):
-            redefinetrial(self.reg_data, minlength=-.1)
+        with pytest.raises(SPYValueError, match="expected value to be greater"):
+            redefinetrial(self.reg_data, minlength=-0.1)
 
     def test_toilim(self):
-        """ select/cut trials via latency window """
+        """select/cut trials via latency window"""
 
         # with default offset all time axes are negative/pre-stimulus [-1, ..., -.1]
-        dummy = redefinetrial(self.reg_data, toilim=[-.8, -.2])
+        dummy = redefinetrial(self.reg_data, toilim=[-0.8, -0.2])
         # nothing gets lost here
         assert len(dummy.trials) == self.nTrials
         # now only selected time window
         assert np.all([dummy.time[i] > -1 for i in range(self.nTrials)])
-        assert np.all([dummy.time[i] < -.1 for i in range(self.nTrials)])
+        assert np.all([dummy.time[i] < -0.1 for i in range(self.nTrials)])
         # all trials got shortened to 7 samples
         assert np.all([len(trl) == 7 for trl in dummy.trials])
 
         # sanity check without toilim
         assert not np.all([self.reg_data.time[i] > -1 for i in range(self.nTrials)])
 
         # positive/post-stimulus latencies don't work as there is no data
         # FIXME: Maybe better to also return empty objects here?!
-        with pytest.raises(SPYValueError, match='expected start of latency window < -0.1s'):
-            redefinetrial(self.reg_data, toilim=[.2, .8])
+        with pytest.raises(SPYValueError, match="expected start of latency window < -0.1s"):
+            redefinetrial(self.reg_data, toilim=[0.2, 0.8])
 
         # with new offset the time axis changes into positive/post-stimulus domain
         dummy = redefinetrial(self.reg_data, offset=0)
         # negative/pre-stimulus latency window is now no longer possible
-        with pytest.raises(SPYValueError, match='expected end of latency window > 0.0s'):
-            redefinetrial(dummy, toilim=[-.8, -.2])
+        with pytest.raises(SPYValueError, match="expected end of latency window > 0.0s"):
+            redefinetrial(dummy, toilim=[-0.8, -0.2])
         # but positive latencies now work
-        dummy2 = redefinetrial(dummy, toilim=[.2, .8])
-        assert np.all([dummy2.time[i] >= .2 for i in range(self.nTrials)])
-        assert np.all([dummy2.time[i] <= .8 for i in range(self.nTrials)])
+        dummy2 = redefinetrial(dummy, toilim=[0.2, 0.8])
+        assert np.all([dummy2.time[i] >= 0.2 for i in range(self.nTrials)])
+        assert np.all([dummy2.time[i] <= 0.8 for i in range(self.nTrials)])
         # again all trials got shortened to 7 samples
         assert np.all([len(trl) == 7 for trl in dummy2.trials])
 
         # irregular dataset, only longest trial reaches into post-stimulus/positive time
-        dummy = redefinetrial(self.irreg_data, toilim=[0.1, .4])
+        dummy = redefinetrial(self.irreg_data, toilim=[0.1, 0.4])
         assert len(dummy.trials) == 1
 
         # here the short trial gets kicked out
-        dummy = redefinetrial(self.irreg_data, toilim=[-.8, -.2])
+        dummy = redefinetrial(self.irreg_data, toilim=[-0.8, -0.2])
         assert len(dummy.trials) == self.nTrials - 1
 
         # test exceptions, array_parser gets hit
-        with pytest.raises(SPYValueError, match='expected array of shape'):
-            redefinetrial(self.reg_data, toilim=[2, ])
-        with pytest.raises(SPYTypeError, match='expected array'):
+        with pytest.raises(SPYValueError, match="expected array of shape"):
+            redefinetrial(
+                self.reg_data,
+                toilim=[
+                    2,
+                ],
+            )
+        with pytest.raises(SPYTypeError, match="expected array"):
             redefinetrial(self.reg_data, toilim=-2)
 
     def test_begin_end_sample(self):
-        """ test cutting trials via relative sample numbers """
+        """test cutting trials via relative sample numbers"""
 
         # every trial gets shortened by 3 samples (2-9)
         dummy = redefinetrial(self.reg_data, begsample=2, endsample=9)
         assert np.all([len(trl) == 7 for trl in dummy.trials])
 
         # the irregular trials now all get cut to 10 samples
         dummy = redefinetrial(self.irreg_data, begsample=0, endsample=10)
@@ -193,43 +196,43 @@
         dummy = redefinetrial(self.irreg_data, begsample=0, endsample=10, offset=1)
         assert np.all([len(trl) == 10 for trl in dummy.trials])
         assert np.all([dummy.time[i] > 0 for i in range(self.nTrials)])
 
         # test exceptions
 
         # begsample missing
-        with pytest.raises(SPYValueError, match='expected both'):
+        with pytest.raises(SPYValueError, match="expected both"):
             redefinetrial(self.reg_data, endsample=8)
 
         # endsample missing
-        with pytest.raises(SPYValueError, match='expected both'):
+        with pytest.raises(SPYValueError, match="expected both"):
             redefinetrial(self.reg_data, begsample=8)
 
         # sample numberes are relative to trial start, so negative values are invalid
-        with pytest.raises(SPYValueError, match='expected integers >= 0'):
+        with pytest.raises(SPYValueError, match="expected integers >= 0"):
             redefinetrial(self.reg_data, begsample=-2, endsample=2)
 
         # end > begin
-        with pytest.raises(SPYValueError, match='expected endsample > begsample'):
+        with pytest.raises(SPYValueError, match="expected endsample > begsample"):
             redefinetrial(self.reg_data, begsample=8, endsample=2)
 
         # wrong type
-        with pytest.raises(SPYTypeError, match='expected scalar or array'):
-            redefinetrial(self.reg_data, begsample=2, endsample='d')
+        with pytest.raises(SPYTypeError, match="expected scalar or array"):
+            redefinetrial(self.reg_data, begsample=2, endsample="d")
 
         # wrong length
-        with pytest.raises(SPYValueError, match='expected same sizes'):
-            redefinetrial(self.reg_data, begsample=2, endsample=[2,3])
+        with pytest.raises(SPYValueError, match="expected same sizes"):
+            redefinetrial(self.reg_data, begsample=2, endsample=[2, 3])
 
         # endsample reaches outside data range
-        with pytest.raises(SPYValueError, match='expected integers < 10'):
+        with pytest.raises(SPYValueError, match="expected integers < 10"):
             redefinetrial(self.reg_data, begsample=2, endsample=20)
 
     def test_trl(self):
-        """ setting trialdefinition directly """
+        """setting trialdefinition directly"""
 
         # check that single trial selection works
         dummy = redefinetrial(self.reg_data, trl=[20, 42, 0])
         assert len(dummy.trials) == 1
         assert len(dummy.time[0]) == 22
 
         # redefine to only 3 irreg trials
@@ -237,27 +240,27 @@
         trldef[0] = [5, 15, 0]
         trldef[1] = [18, 25, 0]
         trldef[2] = [20, 35, 0]
         dummy = redefinetrial(self.reg_data, trl=trldef)
         assert len(dummy.trials) == 3
 
         # test invalid input
-        with pytest.raises(SPYError, match='Incompatible input arguments'):
+        with pytest.raises(SPYError, match="Incompatible input arguments"):
             redefinetrial(self.reg_data, trl=trldef, endsample=4)
 
         # wrong shape
-        with pytest.raises(SPYError, match='expected 2-dimensional array'):
+        with pytest.raises(SPYError, match="expected 2-dimensional array"):
             redefinetrial(self.reg_data, trl=trldef[..., None])
 
         # rest of the exceptions get catched via spy.definetrial
 
     def test_cfg(self):
 
         dummy = redefinetrial(self.irreg_data, offset=-5, trials=[0, 5, 7], begsample=0, endsample=6)
         # use cfg from dummy
         dummy2 = redefinetrial(self.irreg_data, dummy.cfg)
         assert dummy == dummy2
         assert np.all([len(trl) == 6 for trl in dummy2.trials])
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestRedefinetrial()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_resampledata.py` & `esi_syncopy-2023.7/syncopy/tests/test_resampledata.py`

 * *Files 10% similar despite different names*

```diff
@@ -26,79 +26,76 @@
     nSamples = 991
     nChannels = 4
     nTrials = 100
     fs = 200
     fNy = fs / 2
 
     # -- use flat white noise as test data --
-    adata = synthdata.white_noise(nTrials=nTrials,
-                                  nChannels=nChannels,
-                                  nSamples=nSamples,
-                                  samplerate=fs,
-                                  seed=42)
+    adata = synthdata.white_noise(
+        nTrials=nTrials, nChannels=nChannels, nSamples=nSamples, samplerate=fs, seed=42
+    )
 
     # original spectrum
     spec = freqanalysis(adata, tapsmofrq=1, keeptrials=False)
     # mean of the flat spectrum
     pow_orig = spec.show(channel=0)[5:].mean()
 
     # for toi tests, -1s offset
-    time_span = [-.8, 4.2]
+    time_span = [-0.8, 4.2]
 
     def test_downsampling(self, **kwargs):
 
         """
         We test for remaining power after
         downsampling.
         """
         # check if we run the default test
         def_test = not len(kwargs)
 
         # write default parameters dict
         if def_test:
-            kwargs = {'resamplefs': self.fs // 2}
-        ds = resampledata(self.adata, method='downsample', **kwargs)
+            kwargs = {"resamplefs": self.fs // 2}
+        ds = resampledata(self.adata, method="downsample", **kwargs)
         lenTrials = np.diff(ds.sampleinfo).squeeze()
         # check for equal trials
         assert np.unique(lenTrials).size == 1
 
         spec_ds = freqanalysis(ds, tapsmofrq=1, keeptrials=False)
 
         # all channels are equal, trim off 0-frequency dip
         pow_ds = spec_ds.show(channel=0)[5:].mean()
 
         if def_test:
 
             # without anti-aliasing we get double the power per freq. bin
             # as we removed half of the frequencies
-            assert np.allclose(2 * self.pow_orig, pow_ds, rtol=.5e-1)
+            assert np.allclose(2 * self.pow_orig, pow_ds, rtol=0.5e-1)
 
             f, ax = mk_spec_ax()
-            ax.plot(spec_ds.freq, spec_ds.show(channel=0), label='downsampled')
-            ax.plot(self.spec.freq, self.spec.show(channel=0), label='original')
+            ax.plot(spec_ds.freq, spec_ds.show(channel=0), label="downsampled")
+            ax.plot(self.spec.freq, self.spec.show(channel=0), label="original")
             ax.legend()
 
         else:
             return spec_ds
 
     def test_aa_filter(self):
 
         # filter with new Nyquist
-        kwargs = {'resamplefs': self.fs // 2,
-                  'lpfreq': self.fs // 4}
+        kwargs = {"resamplefs": self.fs // 2, "lpfreq": self.fs // 4}
 
         spec_ds = self.test_downsampling(**kwargs)
         # all channels are equal, trim off 0-frequency dip
         pow_ds = spec_ds.show(channel=0)[5:].mean()
         # now with the anti-alias filter the powers should be equal
-        np.allclose(self.pow_orig, pow_ds, rtol=.5e-1)
+        np.allclose(self.pow_orig, pow_ds, rtol=0.5e-1)
 
         f, ax = mk_spec_ax()
-        ax.plot(spec_ds.freq, spec_ds.show(channel=0), label='downsampled')
-        ax.plot(self.spec.freq, self.spec.show(channel=0), label='original')
+        ax.plot(spec_ds.freq, spec_ds.show(channel=0), label="downsampled")
+        ax.plot(self.spec.freq, self.spec.show(channel=0), label="original")
         ax.legend()
 
     def test_ds_exceptions(self):
 
         # test non-integer division
         with pytest.raises(SPYValueError) as err:
             self.test_downsampling(resamplefs=self.fs / 3.142)
@@ -112,19 +109,21 @@
         # test wrong order
         with pytest.raises(SPYValueError) as err:
             self.test_downsampling(resamplefs=self.fs // 2, lpfreq=self.fs / 10, order=-1)
         assert "less or equals inf" in str(err.value)
 
     def test_ds_selections(self):
 
-        sel_dicts = helpers.mk_selection_dicts(nTrials=50,
-                                               nChannels=2,
-                                               toi_min=self.time_span[0],
-                                               toi_max=self.time_span[1],
-                                               min_len=3.5)
+        sel_dicts = helpers.mk_selection_dicts(
+            nTrials=50,
+            nChannels=2,
+            toi_min=self.time_span[0],
+            toi_max=self.time_span[1],
+            min_len=3.5,
+        )
         for sd in sel_dicts:
             spec_ds = self.test_downsampling(select=sd, resamplefs=self.fs // 2)
             pow_ds = spec_ds.show(channel=0).mean()
 
             # test for finitenes and make sure we did not loose any power
             assert np.all(np.isfinite(spec_ds.data))
             assert pow_ds >= self.pow_orig
@@ -141,22 +140,25 @@
         ds = resampledata(self.adata, cfg)
         spec_ds = freqanalysis(ds, tapsmofrq=1, keeptrials=False)
 
         # all channels are equal
         pow_ds = spec_ds.show(channel=0).mean()
 
         # with aa filter power does not change
-        assert np.allclose(self.pow_orig, pow_ds, rtol=.5e-1)
+        assert np.allclose(self.pow_orig, pow_ds, rtol=0.5e-1)
 
     def test_ds_parallel(self, testcluster):
 
         ppl.ioff()
         client = dd.Client(testcluster)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (inspect.ismethod(getattr(self, attr)) and "parallel" not in attr)
+        ]
 
         for test_name in all_tests:
             test_method = getattr(self, test_name)
             test_method()
         client.close()
         ppl.ion()
 
@@ -166,110 +168,112 @@
     nSamples = 1000
     nChannels = 4
     nTrials = 100
     fs = 200
     fNy = fs / 2
 
     # -- use flat white noise as test data --
-    adata = synthdata.white_noise(nTrials=nTrials,
-                                  nChannels=nChannels,
-                                  nSamples=nSamples,
-                                  samplerate=fs,
-                                  seed=42)
+    adata = synthdata.white_noise(
+        nTrials=nTrials, nChannels=nChannels, nSamples=nSamples, samplerate=fs, seed=42
+    )
 
     # original spectrum
     spec = freqanalysis(adata, tapsmofrq=1, keeptrials=False)
     # mean of the flat spectrum
     pow_orig = spec.show(channel=0).mean()
 
     # for toi tests, -1s offset
-    time_span = [-.8, 4.2]
+    time_span = [-0.8, 4.2]
 
     def test_resampling(self, **kwargs):
 
         """
         We test for remaining power after
         resampling.
         """
         # check if we run the default test
         def_test = not len(kwargs)
 
         # write default parameters dict
         if def_test:
             # polyphase method: firws acts on the upsampled data!
-            kwargs = {'resamplefs': self.fs * 0.43, 'order': 5000}
+            kwargs = {"resamplefs": self.fs * 0.43, "order": 5000}
 
-        rs = resampledata(self.adata, method='resample', **kwargs)
+        rs = resampledata(self.adata, method="resample", **kwargs)
         lenTrials = np.diff(rs.sampleinfo).squeeze()
         # check for equal trials
         assert np.unique(lenTrials).size == 1
 
         spec_rs = freqanalysis(rs, tapsmofrq=1, keeptrials=False)
 
         # all channels are equal,
         # avoid the nose with 3Hz away from the cut-off
-        pow_rs = spec_rs.show(channel=0,
-                              frequency=[0, kwargs['resamplefs'] / 2 - 3]).mean()
+        pow_rs = spec_rs.show(channel=0, frequency=[0, kwargs["resamplefs"] / 2 - 3]).mean()
 
         if def_test:
             # here we have aa filtering built in,
             # so the power should be unchanged after resampling
-            assert np.allclose(self.pow_orig, pow_rs, rtol=.5e-1)
+            assert np.allclose(self.pow_orig, pow_rs, rtol=0.5e-1)
 
             f, ax = mk_spec_ax()
-            ax.plot(spec_rs.freq, spec_rs.show(channel=0), label='resampled')
-            ax.plot(self.spec.freq, self.spec.show(channel=0), label='original')
-            ax.plot([rs.samplerate / 2, rs.samplerate / 2], [0.001, 0.0025], 'k--', lw=0.5)
+            ax.plot(spec_rs.freq, spec_rs.show(channel=0), label="resampled")
+            ax.plot(self.spec.freq, self.spec.show(channel=0), label="original")
+            ax.plot([rs.samplerate / 2, rs.samplerate / 2], [0.001, 0.0025], "k--", lw=0.5)
             ax.legend()
 
             return
 
         return spec_rs
 
     def test_rs_exceptions(self):
 
         # test wrong method
-        with pytest.raises(SPYValueError, match='Invalid value of `method`'):
-            resampledata(self.adata, method='nothing-real', resamplefs=self.fs // 2)
+        with pytest.raises(SPYValueError, match="Invalid value of `method`"):
+            resampledata(self.adata, method="nothing-real", resamplefs=self.fs // 2)
 
     def test_rs_selections(self):
         np.random.seed(42)
-        sel_dicts = helpers.mk_selection_dicts(nTrials=20,
-                                               nChannels=2,
-                                               toi_min=self.time_span[0],
-                                               toi_max=self.time_span[1],
-                                               min_len=3.5)
+        sel_dicts = helpers.mk_selection_dicts(
+            nTrials=20,
+            nChannels=2,
+            toi_min=self.time_span[0],
+            toi_max=self.time_span[1],
+            min_len=3.5,
+        )
         for sd in sel_dicts:
             print(sd)
             spec_rs = self.test_resampling(select=sd, resamplefs=self.fs / 2.1)
             # remove 3Hz window around the filter cut
             pow_rs = spec_rs.show(channel=0)[:-3].mean()
 
             # test for finitenes and make sure we did not loose power
             assert np.all(np.isfinite(spec_rs.data))
             assert pow_rs >= 0.9 * self.pow_orig
 
     def test_rs_parallel(self, testcluster):
 
         ppl.ioff()
         client = dd.Client(testcluster)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (inspect.ismethod(getattr(self, attr)) and "parallel" not in attr)
+        ]
 
         for test_name in all_tests:
             test_method = getattr(self, test_name)
             test_method()
         client.close()
         ppl.ion()
 
 
 def mk_spec_ax():
 
     fig, ax = ppl.subplots()
-    ax.set_xlabel('frequency (Hz)')
-    ax.set_ylabel('power (a.u.)')
+    ax.set_xlabel("frequency (Hz)")
+    ax.set_ylabel("power (a.u.)")
     return fig, ax
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestDownsampling()
     T2 = TestResampling()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_selectdata.py` & `esi_syncopy-2023.7/syncopy/tests/test_selectdata.py`

 * *Files 14% similar despite different names*

```diff
@@ -14,35 +14,36 @@
 from syncopy.datatype.selector import Selector
 from syncopy.shared.errors import SPYValueError, SPYTypeError
 from syncopy.tests.misc import flush_local_cluster
 
 import syncopy as spy
 
 # map selection keywords to selector attributes (holding the idx to access selected data)
-map_sel_attr = dict(trials='trial_ids',
-                    channel='channel',
-                    latency='time',
-                    taper='taper',
-                    frequency='freq',
-                    channel_i='channel_i',
-                    channel_j='channel_j',
-                    unit='unit',
-                    eventid='eventid'
-                    )
+map_sel_attr = dict(
+    trials="trial_ids",
+    channel="channel",
+    latency="time",
+    taper="taper",
+    frequency="freq",
+    channel_i="channel_i",
+    channel_j="channel_j",
+    unit="unit",
+    eventid="eventid",
+)
 
 
 class TestGeneral:
 
     adata = spy.AnalogData(data=np.ones((2, 2)), samplerate=1)
     csd_data = spy.CrossSpectralData(data=np.ones((2, 2, 2, 2)), samplerate=1)
 
     def test_Selector_init(self):
 
         with pytest.raises(SPYTypeError, match="Wrong type of `data`"):
-            Selector(np.arange(10), {'latency': [0, 4]})
+            Selector(np.arange(10), {"latency": [0, 4]})
 
     def test_invalid_sel_key(self):
 
         # AnalogData has no `frequency`
         with pytest.raises(SPYValueError, match="no `frequency` selection available"):
             spy.selectdata(self.adata, frequency=[1, 10])
         # CrossSpectralData has no `channel` (but channel_i, channel_j)
@@ -53,109 +54,122 @@
 class TestAnalogSelections:
 
     nChannels = 10
     nSamples = 5  # per trial
     nTrials = 3
     samplerate = 2.0
 
-    trldef = np.vstack([np.arange(0, nSamples * nTrials, nSamples),
-                        np.arange(0, nSamples * nTrials, nSamples) + nSamples,
-                        np.ones(nTrials) * -1]).T
+    trldef = np.vstack(
+        [
+            np.arange(0, nSamples * nTrials, nSamples),
+            np.arange(0, nSamples * nTrials, nSamples) + nSamples,
+            np.ones(nTrials) * -1,
+        ]
+    ).T
 
     # this is an array running from 1 - nChannels * nSamples * nTrials
     # with shape: nSamples*nTrials x nChannels
     # and with data[i, j] = i+1 + j * nSamples*nTrials
     data = np.arange(1, nTrials * nChannels * nSamples + 1).reshape(nChannels, nSamples * nTrials).T
 
-    adata = spy.AnalogData(data=data, samplerate=samplerate,
-                           trialdefinition=trldef)
+    adata = spy.AnalogData(data=data, samplerate=samplerate, trialdefinition=trldef)
 
     def test_ad_selection(self):
 
         """
         Create a typical selection and check that the returned data is correct
         """
 
-        selection = {'trials': 1, 'channel': [6, 2], 'latency': [0, 1]}
+        selection = {"trials": 1, "channel": [6, 2], "latency": [0, 1]}
         res = spy.selectdata(self.adata, selection)
 
         # pick the data by hand, latency [0, 1] covers 2nd - 4th sample index
         # as time axis is array([-0.5,  0. ,  0.5,  1. ,  1.5])
 
         # pick trial
-        solution = self.adata.data[self.nSamples:self.nSamples * 2]
+        solution = self.adata.data[self.nSamples : self.nSamples * 2]
         # pick channels and latency
         solution = np.column_stack([solution[1:4, 6], solution[1:4, 2]])
 
         assert np.all(solution == res.data)
 
     def test_ad_valid(self):
 
         """
         Instantiate Selector class and check only its attributes (the idx)
         """
 
         # each selection test is a 2-tuple: (selection kwargs, dict with same kws and the idx "solutions")
         valid_selections = [
             (
-                {'channel': ["channel03", "channel01"],
-                 'latency': [0, 1],
-                 'trials': np.arange(2)},
+                {
+                    "channel": ["channel03", "channel01"],
+                    "latency": [0, 1],
+                    "trials": np.arange(2),
+                },
                 # these are the idx used to access the actual data
-                {'channel': [2, 0],
-                 'latency': 2 * [slice(1, 4, 1)],
-                 'trials': [0, 1]}
+                {"channel": [2, 0], "latency": 2 * [slice(1, 4, 1)], "trials": [0, 1]},
             ),
             (
                 # 2nd selection with some repetitions
-                {'channel': [7, 3, 3],
-                 'trials': [0, 1, 1]},
+                {"channel": [7, 3, 3], "trials": [0, 1, 1]},
                 # 'solutions'
-                {'channel': [7, 3, 3],
-                 'trials': [0, 1, 1]}
-            )
+                {"channel": [7, 3, 3], "trials": [0, 1, 1]},
+            ),
         ]
 
         for selection in valid_selections:
             # instantiate Selector and check attributes
             sel_kwargs, solution = selection
             selector_object = Selector(self.adata, sel_kwargs)
             for sel_kw in sel_kwargs.keys():
                 attr_name = map_sel_attr[sel_kw]
                 assert list(getattr(selector_object, attr_name)) == solution[sel_kw]
 
     def test_ad_invalid(self):
 
         # each selection test is a 3-tuple: (selection kwargs, Error, error message sub-string)
         invalid_selections = [
-            ({'channel': ["channel33", "channel01"]},
-             SPYValueError, "existing names or indices"),
-            ({'channel': "my-non-existing-channel"},
-             SPYValueError, "existing names or indices"),
-            ({'channel': 99},
-             SPYValueError, "existing names or indices"),
-            ({'latency': 1}, SPYTypeError, "expected array_like"),
-            ({'latency': [0, 10]}, SPYValueError, "at least one trial covering the latency window"),
-            ({'latency': 'sth-wrong'}, SPYValueError, "'maxperiod'"),
-            ({'trials': [-3]}, SPYValueError, "all array elements to be bound"),
-            ({'trials': ['1', '6']}, SPYValueError, "expected dtype = numeric"),
-            ({'trials': slice(2)}, SPYTypeError, "expected serializable data type")
+            (
+                {"channel": ["channel33", "channel01"]},
+                SPYValueError,
+                "existing names or indices",
+            ),
+            (
+                {"channel": "my-non-existing-channel"},
+                SPYValueError,
+                "existing names or indices",
+            ),
+            ({"channel": 99}, SPYValueError, "existing names or indices"),
+            ({"latency": 1}, SPYTypeError, "expected array_like"),
+            (
+                {"latency": [0, 10]},
+                SPYValueError,
+                "at least one trial covering the latency window",
+            ),
+            ({"latency": "sth-wrong"}, SPYValueError, "'maxperiod'"),
+            ({"trials": [-3]}, SPYValueError, "all array elements to be bound"),
+            ({"trials": ["1", "6"]}, SPYValueError, "expected dtype = numeric"),
+            ({"trials": slice(2)}, SPYTypeError, "expected serializable data type"),
         ]
 
         for selection in invalid_selections:
             sel_kw, error, err_str = selection
             with pytest.raises(error, match=err_str):
                 spy.selectdata(self.adata, sel_kw)
 
     def test_ad_parallel(self, testcluster):
 
         # collect all tests of current class and repeat them in parallel
         client = dd.Client(testcluster)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and "parallel" not in attr)]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (inspect.ismethod(getattr(self, attr)) and "parallel" not in attr)
+        ]
         for test in all_tests:
             getattr(self, test)()
             flush_local_cluster(testcluster)
         client.close()
 
 
 class TestSpectralSelections:
@@ -163,44 +177,51 @@
     nChannels = 3
     nSamples = 3  # per trial
     nTrials = 3
     nTaper = 2
     nFreqs = 3
     samplerate = 2.0
 
-    trldef = np.vstack([np.arange(0, nSamples * nTrials, nSamples),
-                        np.arange(0, nSamples * nTrials, nSamples) + nSamples,
-                        np.ones(nTrials) * 2]).T
+    trldef = np.vstack(
+        [
+            np.arange(0, nSamples * nTrials, nSamples),
+            np.arange(0, nSamples * nTrials, nSamples) + nSamples,
+            np.ones(nTrials) * 2,
+        ]
+    ).T
 
     # this is an array running from 1 - nChannels * nSamples * nTrials * nFreq * nTaper
-    data = np.arange(1, nChannels * nSamples * nTrials * nFreqs * nTaper + 1).reshape(nSamples * nTrials, nTaper, nFreqs, nChannels)
-    sdata = spy.SpectralData(data=data, samplerate=samplerate,
-                             trialdefinition=trldef)
+    data = np.arange(1, nChannels * nSamples * nTrials * nFreqs * nTaper + 1).reshape(
+        nSamples * nTrials, nTaper, nFreqs, nChannels
+    )
+    sdata = spy.SpectralData(data=data, samplerate=samplerate, trialdefinition=trldef)
     # freq labels
     sdata.freq = [20, 40, 60]
 
     def test_spectral_selection(self):
 
         """
         Create a typical selection and check that the returned data is correct
         """
 
-        selection = {'trials': 1,
-                     'channel': [1, 0],
-                     'latency': [1, 1.5],
-                     'frequency': [25, 50]}
+        selection = {
+            "trials": 1,
+            "channel": [1, 0],
+            "latency": [1, 1.5],
+            "frequency": [25, 50],
+        }
         res = spy.selectdata(self.sdata, selection)
 
         # pick the data by hand, dimord is: ['time', 'taper', 'freq', 'channel']
         # latency [1, 1.5] covers 1st - 2nd sample index
         # as time axis is array([1., 1.5, 2.])
         # frequency covers only 2nd index (40 Hz)
 
         # pick trial
-        solution = self.sdata.data[self.nSamples:self.nSamples * 2]
+        solution = self.sdata.data[self.nSamples : self.nSamples * 2]
         # pick channels, frequency and latency and re-stack
         solution = np.stack([solution[:2, :, [1], 1], solution[:2, :, [1], 0]], axis=-1)
 
         assert np.all(solution == res.data)
 
     def test_spectral_valid(self):
 
@@ -208,57 +229,71 @@
         Instantiate Selector class and check only its attributes (the idx)
         test mainly additional dimensions (taper and freq) here
         """
 
         # each selection test is a 2-tuple: (selection kwargs, dict with same kws and the idx "solutions")
         valid_selections = [
             (
-                {'frequency': np.array([30, 60]),
-                 'taper': [1, 0]},
+                {"frequency": np.array([30, 60]), "taper": [1, 0]},
                 # the 'solutions'
-                {'frequency': slice(1, 3, 1),
-                 'taper': [1, 0]},
+                {"frequency": slice(1, 3, 1), "taper": [1, 0]},
             ),
             # 2nd selection
             (
-                {'frequency': 'all',
-                 'taper': 'taper2',
-                 'latency': [1.2, 1.7],
-                 'trials': np.arange(1, 3)},
+                {
+                    "frequency": "all",
+                    "taper": "taper2",
+                    "latency": [1.2, 1.7],
+                    "trials": np.arange(1, 3),
+                },
                 # the 'solutions'
-                {'frequency': slice(None),
-                 'taper': [1],
-                 'latency': [slice(1, 2, 1), slice(1, 2, 1)],
-                 'trials': [1, 2]},
-            )
+                {
+                    "frequency": slice(None),
+                    "taper": [1],
+                    "latency": [slice(1, 2, 1), slice(1, 2, 1)],
+                    "trials": [1, 2],
+                },
+            ),
         ]
 
         for selection in valid_selections:
             # instantiate Selector and check attributes
             sel_kwargs, solution = selection
             selector_object = Selector(self.sdata, sel_kwargs)
             for sel_kw in sel_kwargs.keys():
                 attr_name = map_sel_attr[sel_kw]
                 res = getattr(selector_object, attr_name)
-                if sel_kw == 'latency':
+                if sel_kw == "latency":
                     assert list(res) == solution[sel_kw]
                 else:
                     assert res == solution[sel_kw]
 
     def test_spectral_invalid(self):
 
         # each selection test is a 3-tuple: (selection kwargs, Error, error message sub-string)
         invalid_selections = [
-            ({'frequency': '40Hz'}, SPYValueError, "'all' or `None` or float or list/array"),
-            ({'frequency': 4}, SPYValueError, "all array elements to be bounded"),
-            ({'frequency': slice(None)}, SPYTypeError, "expected serializable data type"),
-            ({'frequency': range(20, 60)}, SPYTypeError, "expected array_like"),
-            ({'frequency': np.arange(20, 60)}, SPYValueError, "expected array of shape"),
-            ({'taper': 'taper13'}, SPYValueError, "existing names or indices"),
-            ({'taper': [18, 99]}, SPYValueError, "existing names or indices"),
+            (
+                {"frequency": "40Hz"},
+                SPYValueError,
+                "'all' or `None` or float or list/array",
+            ),
+            ({"frequency": 4}, SPYValueError, "all array elements to be bounded"),
+            (
+                {"frequency": slice(None)},
+                SPYTypeError,
+                "expected serializable data type",
+            ),
+            ({"frequency": range(20, 60)}, SPYTypeError, "expected array_like"),
+            (
+                {"frequency": np.arange(20, 60)},
+                SPYValueError,
+                "expected array of shape",
+            ),
+            ({"taper": "taper13"}, SPYValueError, "existing names or indices"),
+            ({"taper": [18, 99]}, SPYValueError, "existing names or indices"),
         ]
 
         for selection in invalid_selections:
             sel_kw, error, err_str = selection
             with pytest.raises(error, match=err_str):
                 spy.selectdata(self.sdata, sel_kw)
 
@@ -267,47 +302,60 @@
 
     nChannels = 3
     nSamples = 3  # per trial
     nTrials = 3
     nFreqs = 3
     samplerate = 2.0
 
-    trldef = np.vstack([np.arange(0, nSamples * nTrials, nSamples),
-                        np.arange(0, nSamples * nTrials, nSamples) + nSamples,
-                        np.ones(nTrials) * 2]).T
+    trldef = np.vstack(
+        [
+            np.arange(0, nSamples * nTrials, nSamples),
+            np.arange(0, nSamples * nTrials, nSamples) + nSamples,
+            np.ones(nTrials) * 2,
+        ]
+    ).T
 
     # this is an array running from 1 - nChannels * nSamples * nTrials * nFreq * nTaper
-    data = np.arange(1, nChannels**2 * nSamples * nTrials * nFreqs + 1).reshape(nSamples * nTrials, nFreqs, nChannels, nChannels)
+    data = np.arange(1, nChannels**2 * nSamples * nTrials * nFreqs + 1).reshape(
+        nSamples * nTrials, nFreqs, nChannels, nChannels
+    )
     csd_data = spy.CrossSpectralData(data=data, samplerate=samplerate)
     csd_data.trialdefinition = trldef
 
     # freq labels
     csd_data.freq = [20, 40, 60]
 
     def test_csd_selection(self):
 
         """
         Create a typical selection and check that the returned data is correct
         """
 
-        selection = {'trials': [1, 0],
-                     'channel_i': [0, 1],
-                     'latency': [1.5, 2],
-                     'frequency': [25, 60]}
+        selection = {
+            "trials": [1, 0],
+            "channel_i": [0, 1],
+            "latency": [1.5, 2],
+            "frequency": [25, 60],
+        }
 
         res = spy.selectdata(self.csd_data, selection)
 
         # pick the data by hand, dimord is: ['time', 'freq', 'channel_i', 'channel_j']
         # latency [1, 1.5] covers 2nd - 3rd sample index
         # as time axis is array([1., 1.5, 2.])
         # frequency covers 2nd and 3rd index (40 and 60Hz)
 
         # pick trials
-        solution = np.concatenate([self.csd_data.data[self.nSamples: self.nSamples * 2],
-                                   self.csd_data.data[: self.nSamples]], axis=0)
+        solution = np.concatenate(
+            [
+                self.csd_data.data[self.nSamples : self.nSamples * 2],
+                self.csd_data.data[: self.nSamples],
+            ],
+            axis=0,
+        )
 
         # pick channels, frequency and latency
         solution = np.concatenate([solution[1:3, 1:3, :2, :], solution[4:6, 1:3, :2, :]])
         assert np.all(solution == res.data)
 
     def test_csd_valid(self):
 
@@ -315,102 +363,114 @@
         Instantiate Selector class and check only its attributes (the idx)
         test mainly additional dimensions (channel_i, channel_j) here
         """
 
         # each selection test is a 2-tuple: (selection kwargs, dict with same kws and the idx "solutions")
         valid_selections = [
             (
-                {'channel_i': [0, 1], 'channel_j': [1, 2], 'latency': [1, 2]},
+                {"channel_i": [0, 1], "channel_j": [1, 2], "latency": [1, 2]},
                 # the 'solutions'
-                {'channel_i': slice(0, 2, 1), 'channel_j': slice(1, 3, 1),
-                 'latency': 3 * [slice(0, 3, 1)]},
+                {
+                    "channel_i": slice(0, 2, 1),
+                    "channel_j": slice(1, 3, 1),
+                    "latency": 3 * [slice(0, 3, 1)],
+                },
             ),
             # 2nd selection
             (
-                {'channel_i': ['channel2', 'channel3'], 'channel_j': 1},
+                {"channel_i": ["channel2", "channel3"], "channel_j": 1},
                 # the 'solutions'
-                {'channel_i': slice(1, 3, 1), 'channel_j': 1},
-            )
+                {"channel_i": slice(1, 3, 1), "channel_j": 1},
+            ),
         ]
 
         for selection in valid_selections:
             # instantiate Selector and check attributes
             sel_kwargs, solution = selection
             selector_object = Selector(self.csd_data, sel_kwargs)
             for sel_kw in sel_kwargs.keys():
                 attr_name = map_sel_attr[sel_kw]
                 res = getattr(selector_object, attr_name)
-                if sel_kw == 'latency':
+                if sel_kw == "latency":
                     assert list(res) == solution[sel_kw]
                 else:
                     assert res == solution[sel_kw]
 
     def test_csd_invalid(self):
 
         # each selection test is a 3-tuple: (selection kwargs, Error, error message sub-string)
         invalid_selections = [
             (
-                {'channel_i': [0, 2]}, NotImplementedError,
-                r"Unordered \(low to high\) or non-contiguous multi-channel-pair selections not supported"
+                {"channel_i": [0, 2]},
+                NotImplementedError,
+                r"Unordered \(low to high\) or non-contiguous multi-channel-pair selections not supported",
             ),
             (
-                {'channel_i': [1, 0]}, NotImplementedError,
-                r"Unordered \(low to high\) or non-contiguous multi-channel-pair selections not supported"
+                {"channel_i": [1, 0]},
+                NotImplementedError,
+                r"Unordered \(low to high\) or non-contiguous multi-channel-pair selections not supported",
             ),
             (
-                {'channel_j': ['channel3', 'channel1']}, NotImplementedError,
-                r"Unordered \(low to high\) or non-contiguous multi-channel-pair selections not supported"
-            )
-
+                {"channel_j": ["channel3", "channel1"]},
+                NotImplementedError,
+                r"Unordered \(low to high\) or non-contiguous multi-channel-pair selections not supported",
+            ),
         ]
 
         for selection in invalid_selections:
             sel_kw, error, err_str = selection
             with pytest.raises(error, match=err_str):
                 spy.selectdata(self.csd_data, sel_kw)
 
 
-def getSpikeData(nChannels = 10, nTrials = 5, samplerate = 1.0, nSpikes = 20):
-    T_max = 2 * nSpikes   # in samples, not seconds!
+def getSpikeData(nChannels=10, nTrials=5, samplerate=1.0, nSpikes=20):
+    T_max = 2 * nSpikes  # in samples, not seconds!
     nSamples = T_max / nTrials
     rng = np.random.default_rng(42)
 
-    data = np.vstack([np.sort(rng.choice(range(T_max), size=nSpikes)),
-                      rng.choice(np.arange(0, nChannels), size=nSpikes),
-                      rng.choice(nChannels // 2, size=nSpikes)]).T
-
-    trldef = np.vstack([np.arange(0, T_max, nSamples),
-                        np.arange(0, T_max, nSamples) + nSamples,
-                        np.ones(nTrials) * -2]).T
-
-    return(spy.SpikeData(data=data,
-                         samplerate=samplerate,
-                         trialdefinition=trldef))
+    data = np.vstack(
+        [
+            np.sort(rng.choice(range(T_max), size=nSpikes)),
+            rng.choice(np.arange(0, nChannels), size=nSpikes),
+            rng.choice(nChannels // 2, size=nSpikes),
+        ]
+    ).T
 
+    trldef = np.vstack(
+        [
+            np.arange(0, T_max, nSamples),
+            np.arange(0, T_max, nSamples) + nSamples,
+            np.ones(nTrials) * -2,
+        ]
+    ).T
 
-class TestSpikeSelections:
+    return spy.SpikeData(data=data, samplerate=samplerate, trialdefinition=trldef)
 
 
+class TestSpikeSelections:
+
     spike_data = getSpikeData()
 
     def test_spike_selection(self):
 
         """
         Create a typical selection and check that the returned data is correct
         """
 
-        selection = {'trials': [2, 4],
-                     'channel': [6, 2],
-                     'unit': [0, 3],
-                     'latency': [-1, 4]}
+        selection = {
+            "trials": [2, 4],
+            "channel": [6, 2],
+            "unit": [0, 3],
+            "latency": [-1, 4],
+        }
         spkd = getSpikeData()
         res = spkd.selectdata(selection)
 
         # hand pick selection from the arrays
-        dat_arr = spkd.data[()] # convert h5py to np.ndarray, see https://github.com/h5py/h5py/issues/474
+        dat_arr = spkd.data[()]  # convert h5py to np.ndarray, see https://github.com/h5py/h5py/issues/474
 
         # these are trial intervals in sample indices!
         trial2 = spkd.trialdefinition[2, :2]
         trial4 = spkd.trialdefinition[4, :2]
 
         # create boolean mask for trials [2, 4]
         bm = (dat_arr[:, 0] >= trial2[0]) & (dat_arr[:, 0] <= trial2[1])
@@ -437,44 +497,64 @@
         used by `_preview_trial` in the end
         """
 
         # each selection test is a 2-tuple: (selection kwargs, dict with same kws and the idx "solutions")
         valid_selections = [
             (
                 # units get apparently indexed on a per trial basis
-                {'trials': np.arange(1, 4), 'channel': ['channel03', 'channel01'], 'unit': [2, 0]},
-                {'trials': [1, 2, 3], 'channel': [2, 0], 'unit': [[], [], [1, 5]]},
+                {
+                    "trials": np.arange(1, 4),
+                    "channel": ["channel3", "channel1"],
+                    "unit": [2, 0],
+                },
+                {"trials": [1, 2, 3], "channel": [2, 0], "unit": [[], [], [1, 5]]},
             ),
             # 2nd selection
             (
                 # time/latency idx can be mixed lists and slices O.0
                 # and channel 'all' selections can still be effectively subsets..
-                {'trials': [0, 4], 'latency': [0, 3], 'channel': 'all'},
-                {'trials': [0, 4], 'latency': [slice(0, 4, 1), [1]], 'channel': [1, 2, 3, 5, 9]},
-            )
+                {"trials": [0, 4], "latency": [0, 3], "channel": "all"},
+                {
+                    "trials": [0, 4],
+                    "latency": [slice(0, 4, 1), [1]],
+                    "channel": [1, 2, 3, 5, 9],
+                },
+            ),
         ]
 
         for selection in valid_selections:
             # instantiate Selector and check attributes
             sel_kwargs, solution = selection
             selector_object = Selector(self.spike_data, sel_kwargs)
             for sel_kw in sel_kwargs.keys():
                 attr_name = map_sel_attr[sel_kw]
                 assert getattr(selector_object, attr_name) == solution[sel_kw]
 
     def test_spike_invalid(self):
 
         # each selection test is a 3-tuple: (selection kwargs, Error, error message sub-string)
         invalid_selections = [
-            ({'channel': ["channel33", "channel01"]}, SPYValueError, "existing names or indices"),
-            ({'channel': "my-non-existing-channel"}, SPYValueError, "existing names or indices"),
-            ({'channel': slice(None)}, SPYTypeError, "expected serializable data type"),
-            ({'unit': 99}, SPYValueError, "existing names or indices"),
-            ({'unit': slice(None)}, SPYTypeError, "expected serializable data type"),
-            ({'latency': [-1, 10]}, SPYValueError, "at least one trial covering the latency window"),
+            (
+                {"channel": ["channel33", "channel01"]},
+                SPYValueError,
+                "existing names or indices",
+            ),
+            (
+                {"channel": "my-non-existing-channel"},
+                SPYValueError,
+                "existing names or indices",
+            ),
+            ({"channel": slice(None)}, SPYTypeError, "expected serializable data type"),
+            ({"unit": 99}, SPYValueError, "existing names or indices"),
+            ({"unit": slice(None)}, SPYTypeError, "expected serializable data type"),
+            (
+                {"latency": [-1, 10]},
+                SPYValueError,
+                "at least one trial covering the latency window",
+            ),
         ]
 
         for selection in invalid_selections:
             sel_kw, error, err_str = selection
             with pytest.raises(error, match=err_str):
                 spy.selectdata(self.spike_data, sel_kw)
 
@@ -482,34 +562,38 @@
 def _getEventData():
     nSamples = 4
     nTrials = 5
     samplerate = 1.0
     eIDs = [0, 111, 31]  # event ids
     rng = np.random.default_rng(42)
 
-    trldef = np.vstack([np.arange(0, nSamples * nTrials, nSamples),
-                        np.arange(0, nSamples * nTrials, nSamples) + nSamples,
-                        np.ones(nTrials) * -1]).T
+    trldef = np.vstack(
+        [
+            np.arange(0, nSamples * nTrials, nSamples),
+            np.arange(0, nSamples * nTrials, nSamples) + nSamples,
+            np.ones(nTrials) * -1,
+        ]
+    ).T
 
     # Use a triple-trigger pattern to simulate EventData w/non-uniform trials
-    data = np.vstack([np.arange(0, nSamples * nTrials, 1),
-                      rng.choice(eIDs, size=nSamples * nTrials)]).T
+    data = np.vstack([np.arange(0, nSamples * nTrials, 1), rng.choice(eIDs, size=nSamples * nTrials)]).T
     edata = spy.EventData(data=data, samplerate=samplerate, trialdefinition=trldef)
     return edata
 
+
 class TestEventSelections:
 
     edata = _getEventData()
 
     def test_event_selection(self):
 
         edata = _getEventData()
 
         # eIDs[1] = 111, a bit funny that here we need an index actually...
-        selection = {'eventid': 1, 'latency': [0, 1], 'trials': [0, 3]}
+        selection = {"eventid": 1, "latency": [0, 1], "trials": [0, 3]}
         res = spy.selectdata(edata, selection)
 
         # hand pick selection from the arrays
         dat_arr = edata.data[()]
 
         # these are trial intervals in sample indices!
         trial0 = edata.trialdefinition[0, :2]
@@ -536,16 +620,16 @@
         used by `_preview_trial` in the end
         """
 
         # each selection test is a 2-tuple: (selection kwargs, dict with same kws and the idx "solutions")
         valid_selections = [
             (
                 # eventids get apparently indexed on a per trial basis
-                {'trials': np.arange(1, 4), 'eventid': [0, 2]},
-                {'trials': [1, 2, 3], 'eventid': [[2], slice(0, 2, 1), []]}
+                {"trials": np.arange(1, 4), "eventid": [0, 2]},
+                {"trials": [1, 2, 3], "eventid": [[2], slice(0, 2, 1), []]},
             ),
         ]
 
         for selection in valid_selections:
             # instantiate Selector and check attributes
             sel_kwargs, solution = selection
             selector_object = Selector(self.edata, sel_kwargs)
@@ -557,24 +641,24 @@
 
         """
         eventid seems to be only indexable ([0, 1, 2]) instead of using the actual
         numerical values ([0, 111, 31]), this should most likely change in the future..
         """
         # each selection test is a 3-tuple: (selection kwargs, Error, error message sub-string)
         invalid_selections = [
-            ({'eventid': [111, 31]}, SPYValueError, "existing names or indices"),
-            ({'eventid': '111'}, SPYValueError, "expected dtype = numeric"),
+            ({"eventid": [111, 31]}, SPYValueError, "existing names or indices"),
+            ({"eventid": "111"}, SPYValueError, "expected dtype = numeric"),
         ]
 
         for selection in invalid_selections:
             sel_kw, error, err_str = selection
             with pytest.raises(error, match=err_str):
                 spy.selectdata(self.edata, sel_kw)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestGeneral()
     T2 = TestAnalogSelections()
     T3 = TestSpectralSelections()
     T4 = TestCrossSpectralSelections()
     T5 = TestSpikeSelections()
     T6 = TestEventSelections()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_specest.py` & `esi_syncopy-2023.7/syncopy/tests/test_specest.py`

 * *Files 5% similar despite different names*

```diff
@@ -46,15 +46,15 @@
     # tStart = -29.5
     # tStop = 70.5
     t0 = -np.abs(tStart * fs).astype(np.intp)
     time = (np.arange(0, (tStop - tStart) * fs, dtype=numType) + tStart * fs) / fs
     N = time.size
     carriers = np.zeros((N, 2), dtype=numType)
     modulators = np.zeros((N, 2), dtype=numType)
-    noise_decay = np.exp(-np.arange(N) / (5*fs))
+    noise_decay = np.exp(-np.arange(N) / (5 * fs))
     fader = np.ones((N,), dtype=numType)
     if fadeIn is None:
         fadeIn = tStart
     if fadeOut is None:
         fadeOut = tStop
     fadeIn = np.arange(0, (fadeIn - tStart) * fs, dtype=np.intp)
     fadeOut = np.arange((fadeOut - tStart) * fs, min(10 * fs, N), dtype=np.intp)
@@ -77,118 +77,163 @@
     sig = np.zeros((N * nTrials, nChannels), dtype=numType)
     trialdefinition = np.zeros((nTrials, 3), dtype=np.intp)
     for ntrial in range(nTrials):
         noise = rng.normal(scale=np.sqrt(noise_power), size=time.shape).astype(numType)
         noise *= noise_decay
         nt1 = ntrial * N
         nt2 = (ntrial + 1) * N
-        sig[nt1 : nt2, ::2] = np.tile(carriers[:, even[(-1)**ntrial]] + noise, (nChan2, 1)).T
-        sig[nt1 : nt2, 1::2] = np.tile(carriers[:, odd[(-1)**ntrial]] + noise, (nChan2, 1)).T
+        sig[nt1:nt2, ::2] = np.tile(carriers[:, even[(-1) ** ntrial]] + noise, (nChan2, 1)).T
+        sig[nt1:nt2, 1::2] = np.tile(carriers[:, odd[(-1) ** ntrial]] + noise, (nChan2, 1)).T
         trialdefinition[ntrial, :] = np.array([nt1, nt2, t0])
 
     # Finally allocate `AnalogData` object that makes use of all this
     tfData = AnalogData(data=sig, samplerate=fs, trialdefinition=trialdefinition)
 
     return tfData, modulators, even, odd, fader
 
 
-class TestMTMFFT():
-
+class TestMTMFFT:
 
     # Construct simple trigonometric signal to check FFT consistency: each
     # channel is a sine wave of frequency `freqs[nchan]` with single unique
     # amplitude `amp` and sampling frequency `fs`
     nChannels = 32
     nTrials = 8
     fs = 1024
-    fband = np.linspace(1, fs/2, int(np.floor(fs/2)))
-    freqs = [88.,  35., 278., 104., 405., 314., 271., 441., 343., 374., 428.,
-             367., 75., 118., 289., 310., 510., 102., 123., 417., 273., 449.,
-             416., 32., 438., 111., 140., 304., 327., 494., 23., 493.]
+    fband = np.linspace(1, fs / 2, int(np.floor(fs / 2)))
+    freqs = [
+        88.0,
+        35.0,
+        278.0,
+        104.0,
+        405.0,
+        314.0,
+        271.0,
+        441.0,
+        343.0,
+        374.0,
+        428.0,
+        367.0,
+        75.0,
+        118.0,
+        289.0,
+        310.0,
+        510.0,
+        102.0,
+        123.0,
+        417.0,
+        273.0,
+        449.0,
+        416.0,
+        32.0,
+        438.0,
+        111.0,
+        140.0,
+        304.0,
+        327.0,
+        494.0,
+        23.0,
+        493.0,
+    ]
     freqs = freqs[:nChannels]
     # freqs = np.random.choice(fband[:-2], size=nChannels, replace=False)
     amp = np.pi
     phases = np.random.permutation(np.linspace(0, 2 * np.pi, nChannels))
     t = np.linspace(0, nTrials, nTrials * fs)
     sig = np.zeros((t.size, nChannels), dtype="float32")
     for nchan in range(nChannels):
-        sig[:, nchan] = amp * \
-            np.sin(2 * np.pi * freqs[nchan] * t + phases[nchan])
+        sig[:, nchan] = amp * np.sin(2 * np.pi * freqs[nchan] * t + phases[nchan])
 
     trialdefinition = np.zeros((nTrials, 3), dtype="int")
     for ntrial in range(nTrials):
-        trialdefinition[ntrial, :] = np.array(
-            [ntrial * fs, (ntrial + 1) * fs, 0])
+        trialdefinition[ntrial, :] = np.array([ntrial * fs, (ntrial + 1) * fs, 0])
 
-    adata = AnalogData(data=sig, samplerate=fs,
-                       trialdefinition=trialdefinition)
+    adata = AnalogData(data=sig, samplerate=fs, trialdefinition=trialdefinition)
 
     # Data selections to be tested w/data generated based on `sig`
-    sigdataSelections = [None,
-                         {"trials": [3, 1, 0],
-                          "channel": ["channel" + str(i) for i in range(12, 28)][::-1]},
-                         {"trials": [0, 1, 2],
-                          "channel": range(0, int(nChannels / 2)),
-                          "latency": [0.25, 0.75]}]
+    sigdataSelections = [
+        None,
+        {
+            "trials": [3, 1, 0],
+            "channel": ["channel" + str(i) for i in range(12, 28)][::-1],
+        },
+        {
+            "trials": [0, 1, 2],
+            "channel": range(0, int(nChannels / 2)),
+            "latency": [0.25, 0.75],
+        },
+    ]
 
     # Data selections to be tested w/`artdata` generated below (use fixed but arbitrary
     # random number seed to randomly select time-points for `toi` (with repetitions)
     seed = np.random.RandomState(13)
-    artdataSelections = [None,
-                         {"trials": [3, 1, 0],
-                          "channel": ["channel" + str(i) for i in range(10, 15)][::-1]},
-                         {"trials": [0, 1, 2],
-                          "channel": range(0, 8),
-                          "latency": [-0.5, 0.6]}]
+    artdataSelections = [
+        None,
+        {
+            "trials": [3, 1, 0],
+            "channel": ["channel" + str(i) for i in range(10, 15)][::-1],
+        },
+        {"trials": [0, 1, 2], "channel": range(0, 8), "latency": [-0.5, 0.6]},
+    ]
 
     # Error tolerances for target amplitudes (depend on data selection!)
     tols = [1, 1, 1.5]
 
     # Error tolerance for frequency-matching
     ftol = 0.25
 
     # Helper function that reduces dataselections (keep `None` selection no matter what)
     def test_cut_selections(self):
         self.sigdataSelections.pop(random.choice([-1, 1]))
         self.artdataSelections.pop(random.choice([-1, 1]))
 
     @staticmethod
     def get_adata():
-        return AnalogData(data=TestMTMFFT.sig, samplerate=TestMTMFFT.fs,
-                       trialdefinition=TestMTMFFT.trialdefinition)
+        return AnalogData(
+            data=TestMTMFFT.sig,
+            samplerate=TestMTMFFT.fs,
+            trialdefinition=TestMTMFFT.trialdefinition,
+        )
 
     def test_output(self):
         # ensure that output type specification is respected
         for select in self.sigdataSelections:
-            spec = freqanalysis(self.adata, method="mtmfft", taper="hann",
-                                output="fourier", select=select)
+            spec = freqanalysis(
+                self.adata,
+                method="mtmfft",
+                taper="hann",
+                output="fourier",
+                select=select,
+            )
             assert "complex" in spec.data.dtype.name
-            spec = freqanalysis(self.adata, method="mtmfft", taper="hann",
-                                output="abs", select=select)
+            spec = freqanalysis(self.adata, method="mtmfft", taper="hann", output="abs", select=select)
             assert "float" in spec.data.dtype.name
-            spec = freqanalysis(self.adata, method="mtmfft", taper="hann",
-                                output="pow", select=select)
+            spec = freqanalysis(self.adata, method="mtmfft", taper="hann", output="pow", select=select)
             assert "float" in spec.data.dtype.name
 
     def test_solution(self):
         # ensure channel-specific frequencies are identified correctly
         for sk, select in enumerate(self.sigdataSelections):
             sel = Selector(self.adata, select)
-            spec = freqanalysis(self.adata, method="mtmfft", taper="hann",
-                                pad="nextpow2", output="pow", select=select)
+            spec = freqanalysis(
+                self.adata,
+                method="mtmfft",
+                taper="hann",
+                pad="nextpow2",
+                output="pow",
+                select=select,
+            )
 
             chanList = np.arange(self.nChannels)[sel.channel]
             amps = np.empty((len(sel.trial_ids) * len(chanList),))
             k = 0
             for nchan, chan in enumerate(chanList):
                 for ntrial in range(len(spec.trials)):
-                    amps[k] = spec.data[ntrial, :, :, nchan].max() / \
-                        self.t.size
-                    assert np.argmax(
-                            spec.data[ntrial, :, :, nchan]) == self.freqs[chan]
+                    amps[k] = spec.data[ntrial, :, :, nchan].max() / self.t.size
+                    assert np.argmax(spec.data[ntrial, :, :, nchan]) == self.freqs[chan]
                     k += 1
 
             # ensure amplitude is consistent across all channels/trials
             assert np.all(np.diff(amps) < self.tols[sk])
 
     def test_normalization(self):
 
@@ -199,21 +244,21 @@
         signal = Ampl * np.cos(2 * np.pi * 50 * np.arange(nSamples) * 1 / fsample)
 
         # single signal/channel is enough
         ad = AnalogData([signal[:, None]], samplerate=fsample)
 
         cfg = StructDict()
         cfg.foilim = [40, 60]
-        cfg.output = 'pow'
+        cfg.output = "pow"
         cfg.taper = None
 
         # -- syncopy's default, padding does NOT change power --
 
         cfg.ft_compat = False
-        cfg.pad = 'maxperlen'  # that's the default -> no padding
+        cfg.pad = "maxperlen"  # that's the default -> no padding
         spec = freqanalysis(ad, cfg)
         peak_power = spec.show().max()
         df_no_pad = np.diff(spec.freq)  # freq. resolution
         assert np.allclose(peak_power, Ampl**2 / 2, atol=1e-5)
 
         cfg.pad = 4  # in seconds, double the size
         spec = freqanalysis(ad, cfg)
@@ -223,15 +268,15 @@
         # yet power stays the same
         peak_power = spec.show().max()
         assert np.allclose(peak_power, Ampl**2 / 2, atol=1e-5)
 
         # -- FT compat mode, padding does dilute the power --
 
         cfg.ft_compat = True
-        cfg.pad = 'maxperlen'  # that's the default
+        cfg.pad = "maxperlen"  # that's the default
         spec = freqanalysis(ad, cfg)
         peak_power = spec.show().max()
         df_no_pad = np.diff(spec.freq)
         # default padding is no padding if all trials are equally sized,
         # so here the results are the same
         assert np.allclose(peak_power, Ampl**2 / 2, atol=1e-5)
 
@@ -243,90 +288,108 @@
         # here half the power is now lost!
         peak_power = spec.show().max()
         assert np.allclose(peak_power, Ampl**2 / 4, atol=1e-5)
 
         # -- works the same with tapering --
 
         cfg.ft_compat = False
-        cfg.pad = 'maxperlen'  # that's the default
-        cfg.taper = 'kaiser'
-        cfg.taper_opt = {'beta': 10}
+        cfg.pad = "maxperlen"  # that's the default
+        cfg.taper = "kaiser"
+        cfg.taper_opt = {"beta": 10}
         spec = freqanalysis(ad, cfg)
         peak_power_no_pad = spec.show().max()
 
         cfg.pad = 4
         spec = freqanalysis(ad, cfg)
         peak_power_with_pad = spec.show().max()
         assert np.allclose(peak_power_no_pad, peak_power_with_pad, atol=1e-5)
 
         cfg.ft_compat = True
-        cfg.pad = 'maxperlen'  # that's the default
-        cfg.taper = 'kaiser'
-        cfg.taper_opt = {'beta': 10}
+        cfg.pad = "maxperlen"  # that's the default
+        cfg.taper = "kaiser"
+        cfg.taper_opt = {"beta": 10}
         spec = freqanalysis(ad, cfg)
         peak_power_no_pad = spec.show().max()
 
         cfg.pad = 4
         spec = freqanalysis(ad, cfg)
         peak_power_with_pad = spec.show().max()
         # again half the power is lost with FT compat
         assert np.allclose(peak_power_no_pad, 2 * peak_power_with_pad, atol=1e-5)
 
     def test_foi(self):
         for select in self.sigdataSelections:
 
             # `foi` lims outside valid bounds
             with pytest.raises(SPYValueError):
-                freqanalysis(TestMTMFFT.get_adata(), method="mtmfft", taper="hann",
-                             foi=[-0.5, self.fs / 3], select=select)
+                freqanalysis(
+                    TestMTMFFT.get_adata(),
+                    method="mtmfft",
+                    taper="hann",
+                    foi=[-0.5, self.fs / 3],
+                    select=select,
+                )
             with pytest.raises(SPYValueError):
-                freqanalysis(TestMTMFFT.get_adata(), method="mtmfft", taper="hann",
-                             foi=[1, self.fs], select=select)
+                freqanalysis(
+                    TestMTMFFT.get_adata(),
+                    method="mtmfft",
+                    taper="hann",
+                    foi=[1, self.fs],
+                    select=select,
+                )
 
-            foi = self.fband[1:int(self.fband.size / 3)]
+            foi = self.fband[1 : int(self.fband.size / 3)]
 
             # offset `foi` by 0.1 Hz - resulting freqs must be unaffected
             ftmp = foi + 0.1
-            spec = freqanalysis(TestMTMFFT.get_adata(), method="mtmfft", taper="hann",
-                                pad="nextpow2", foi=ftmp, select=select)
+            spec = freqanalysis(
+                TestMTMFFT.get_adata(),
+                method="mtmfft",
+                taper="hann",
+                pad="nextpow2",
+                foi=ftmp,
+                select=select,
+            )
             assert np.all(spec.freq == foi)
 
             # unsorted, duplicate entries in `foi` - result must stay the same
             ftmp = np.hstack([foi, np.full(20, foi[0])])
-            spec = freqanalysis(TestMTMFFT.get_adata(), method="mtmfft", taper="hann",
-                                pad="nextpow2", foi=ftmp, select=select)
+            spec = freqanalysis(
+                TestMTMFFT.get_adata(),
+                method="mtmfft",
+                taper="hann",
+                pad="nextpow2",
+                foi=ftmp,
+                select=select,
+            )
             assert np.all(spec.freq == foi)
 
     def test_dpss(self):
 
         for select in self.sigdataSelections:
 
             self.adata.selectdata(select, inplace=True)
             sel = self.adata.selection
             chanList = np.arange(self.nChannels)[sel.channel]
             self.adata.selection = None
 
             # ensure default setting results in single taper
-            spec = freqanalysis(self.adata, method="mtmfft",
-                                tapsmofrq=3, output="pow", select=select)
+            spec = freqanalysis(self.adata, method="mtmfft", tapsmofrq=3, output="pow", select=select)
             assert spec.taper.size == 1
             assert spec.channel.size == len(chanList)
 
             # specify tapers
-            spec = freqanalysis(self.adata, method="mtmfft",
-                                tapsmofrq=7, keeptapers=True, select=select)
+            spec = freqanalysis(self.adata, method="mtmfft", tapsmofrq=7, keeptapers=True, select=select)
             assert spec.channel.size == len(chanList)
 
             # trigger capture of too large tapsmofrq (edge case)
-            spec = freqanalysis(self.adata, method="mtmfft",
-                                tapsmofrq=2, output="pow", select=select)
+            spec = freqanalysis(self.adata, method="mtmfft", tapsmofrq=2, output="pow", select=select)
 
         # non-equidistant data w/multiple tapers
-        artdata = generate_artificial_data(nTrials=5, nChannels=16,
-                                           equidistant=False, inmemory=False)
+        artdata = generate_artificial_data(nTrials=5, nChannels=16, equidistant=False, inmemory=False)
         timeAxis = artdata.dimord.index("time")
         cfg = StructDict()
         cfg.method = "mtmfft"
         cfg.tapsmofrq = 3.3
         cfg.output = "pow"
 
         for select in self.artdataSelections:
@@ -342,17 +405,21 @@
             else:
                 nSamples = max([trl.shape[0] for trl in sel.trials])
             freqs = np.fft.rfftfreq(nSamples, 1 / artdata.samplerate)
             assert spec.freq.size == freqs.size
             assert np.max(spec.freq - freqs) < self.ftol
 
         # same + reversed dimensional order in input object
-        cfg.data = generate_artificial_data(nTrials=5, nChannels=16,
-                                            equidistant=False, inmemory=False,
-                                            dimord=AnalogData._defaultDimord[::-1])
+        cfg.data = generate_artificial_data(
+            nTrials=5,
+            nChannels=16,
+            equidistant=False,
+            inmemory=False,
+            dimord=AnalogData._defaultDimord[::-1],
+        )
         timeAxis = cfg.data.dimord.index("time")
         cfg.output = "abs"
         cfg.keeptapers = True
 
         for select in self.artdataSelections:
             sel = selectdata(cfg.data, select)
             cfg.select = select
@@ -368,18 +435,22 @@
 
             freqs = np.fft.rfftfreq(nSamples, 1 / cfg.data.samplerate)
             assert spec.freq.size == freqs.size
             assert np.max(spec.freq - freqs) < self.ftol
             assert spec.taper.size > 1
 
         # same + overlapping trials
-        cfg.data = generate_artificial_data(nTrials=5, nChannels=16,
-                                            equidistant=False, inmemory=False,
-                                            dimord=AnalogData._defaultDimord[::-1],
-                                            overlapping=True)
+        cfg.data = generate_artificial_data(
+            nTrials=5,
+            nChannels=16,
+            equidistant=False,
+            inmemory=False,
+            dimord=AnalogData._defaultDimord[::-1],
+            overlapping=True,
+        )
         timeAxis = cfg.data.dimord.index("time")
         cfg.keeptapers = False
         cfg.output = "pow"
 
         for select in self.artdataSelections:
 
             sel = selectdata(cfg.data, select)
@@ -399,71 +470,81 @@
             assert np.max(spec.freq - freqs) < self.ftol
             assert spec.taper.size == 1
 
     @skip_low_mem
     def test_parallel(self, testcluster):
         # collect all tests of current class and repeat them using dask
         client = dd.Client(testcluster)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and attr not in ["test_parallel", "test_cut_selections"])]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (
+                inspect.ismethod(getattr(self, attr)) and attr not in ["test_parallel", "test_cut_selections"]
+            )
+        ]
         for test in all_tests:
             getattr(self, test)()
             flush_local_cluster(testcluster)
 
         # now create uniform `cfg` for remaining SLURM tests
         cfg = StructDict()
         cfg.method = "mtmfft"
         cfg.tapsmofrq = 9.3
         cfg.output = "pow"
 
         # no. of HDF5 files that will make up virtual data-set in case of channel-chunking
         chanPerWrkr = 7
-        nFiles = self.nTrials * (int(self.nChannels / chanPerWrkr)
-                                 + int(self.nChannels % chanPerWrkr > 0))
+        nFiles = self.nTrials * (int(self.nChannels / chanPerWrkr) + int(self.nChannels % chanPerWrkr > 0))
 
         # simplest case: equidistant trial spacing, all in memory
         fileCount = [self.nTrials, nFiles]
-        artdata = generate_artificial_data(nTrials=self.nTrials, nChannels=self.nChannels,
-                                           inmemory=True)
+        artdata = generate_artificial_data(nTrials=self.nTrials, nChannels=self.nChannels, inmemory=True)
         for k, chan_per_worker in enumerate([None, chanPerWrkr]):
             cfg.chan_per_worker = chan_per_worker
             spec = freqanalysis(artdata, cfg)
             assert spec.data.is_virtual
             assert len(spec.data.virtual_sources()) == fileCount[k]
 
         # non-equidistant trial spacing
         cfg.keeptapers = False
-        artdata = generate_artificial_data(nTrials=self.nTrials, nChannels=self.nChannels,
-                                           inmemory=True, equidistant=False)
+        artdata = generate_artificial_data(
+            nTrials=self.nTrials,
+            nChannels=self.nChannels,
+            inmemory=True,
+            equidistant=False,
+        )
         timeAxis = artdata.dimord.index("time")
         maxtrlno = np.diff(artdata.sampleinfo).argmax()
         nSamples = artdata.trials[maxtrlno].shape[timeAxis]
         freqs = np.fft.rfftfreq(nSamples, 1 / artdata.samplerate)
         for k, chan_per_worker in enumerate([None, chanPerWrkr]):
             spec = freqanalysis(artdata, cfg)
             assert spec.freq.size == freqs.size
             assert np.allclose(spec.freq, freqs)
             assert spec.taper.size == 1
 
         # equidistant trial spacing, keep tapers
         cfg.output = "abs"
         cfg.keeptapers = True
-        artdata = generate_artificial_data(nTrials=self.nTrials, nChannels=self.nChannels,
-                                           inmemory=False)
+        artdata = generate_artificial_data(nTrials=self.nTrials, nChannels=self.nChannels, inmemory=False)
         for k, chan_per_worker in enumerate([None, chanPerWrkr]):
             spec = freqanalysis(artdata, cfg)
             assert spec.taper.size > 1
 
         # non-equidistant, overlapping trial spacing, throw away trials and tapers
         cfg.keeptapers = False
         cfg.keeptrials = "no"
         cfg.output = "pow"
-        artdata = generate_artificial_data(nTrials=self.nTrials, nChannels=self.nChannels,
-                                           inmemory=False, equidistant=False,
-                                           overlapping=True)
+        artdata = generate_artificial_data(
+            nTrials=self.nTrials,
+            nChannels=self.nChannels,
+            inmemory=False,
+            equidistant=False,
+            overlapping=True,
+        )
         spec = freqanalysis(artdata, cfg)
         timeAxis = artdata.dimord.index("time")
         maxtrlno = np.diff(artdata.sampleinfo).argmax()
         nSamples = artdata.trials[maxtrlno].shape[timeAxis]
         freqs = np.fft.rfftfreq(nSamples, 1 / artdata.samplerate)
         assert spec.freq.size == freqs.size
         assert np.allclose(spec.freq, freqs)
@@ -473,44 +554,51 @@
         assert spec.data.shape == (1, 1, freqs.size, self.nChannels)
 
         client.close()
 
     # FIXME: check polyremoval once supported
 
 
-class TestMTMConvol():
+class TestMTMConvol:
 
     # Construct high-frequency signal modulated by slow oscillating cosine and
     # add time-decaying noise
     nChannels = 6
     nChan2 = int(nChannels / 2)
     nTrials = 3
     seed = 151120
     fadeIn = None
     fadeOut = None
-    tfData, modulators, even, odd, fader = _make_tf_signal(nChannels, nTrials, seed,
-                                                           fadeIn=fadeIn, fadeOut=fadeOut)
+    tfData, modulators, even, odd, fader = _make_tf_signal(
+        nChannels, nTrials, seed, fadeIn=fadeIn, fadeOut=fadeOut
+    )
 
     @staticmethod
     def get_tfdata_mtmconvol():
         """
         High-frequency signal modulated by slow oscillating cosine and time-decaying noise.
         """
-        return _make_tf_signal(TestMTMConvol.nChannels, TestMTMConvol.nTrials, TestMTMConvol.seed,
-                                                           fadeIn=TestMTMConvol.fadeIn, fadeOut=TestMTMConvol.fadeOut)[0]
-
+        return _make_tf_signal(
+            TestMTMConvol.nChannels,
+            TestMTMConvol.nTrials,
+            TestMTMConvol.seed,
+            fadeIn=TestMTMConvol.fadeIn,
+            fadeOut=TestMTMConvol.fadeOut,
+        )[0]
 
     # Data selection dict for the above object
-    dataSelections = [None,
-                      {"trials": [1, 2, 0],
-                       "channel": ["channel" + str(i) for i in range(2, 6)][::-1]},
-                      {"trials": [0, 2],
-                       "channel": range(0, nChan2),
-                       "latency": [-2, 6.8]}]
-                    #    "latency": [-20, 60.8]}] FIXME
+    dataSelections = [
+        None,
+        {
+            "trials": [1, 2, 0],
+            "channel": ["channel" + str(i) for i in range(2, 6)][::-1],
+        },
+        {"trials": [0, 2], "channel": range(0, nChan2), "latency": [-2, 6.8]},
+    ]
+    #    "latency": [-20, 60.8]}] FIXME
 
     # Helper function that reduces dataselections (keep `None` selection no matter what)
     def test_tf_cut_selections(self):
         self.dataSelections.pop(random.choice([-1, 1]))
 
     def test_tf_output(self):
         # Set up basic TF analysis parameters to not slow down things too much
@@ -546,15 +634,15 @@
         cfg.output = "pow"
         cfg.keeptapers = False
 
         # Set up index tuple for slicing computed TF spectra and collect values
         # of expected frequency peaks (for validation of `foi`/`foilim` selections below)
         chanIdx = SpectralData._defaultDimord.index("channel")
         tfIdx = [slice(None)] * len(SpectralData._defaultDimord)
-        maxFreqs = np.hstack([np.arange(325, 336), np.arange(355,366)])
+        maxFreqs = np.hstack([np.arange(325, 336), np.arange(355, 366)])
         allFreqs = np.arange(self.tfData.samplerate / 2 + 1)
         foilimFreqs = np.arange(maxFreqs.min(), maxFreqs.max() + 1)
 
         for select in self.dataSelections:
 
             # Compute TF objects w\w/o`foi`/`foilim`
             cfg.select = select
@@ -578,16 +666,16 @@
                 trlNo = tk
                 timeArr = np.arange(tfData.time[trlNo][0], tfData.time[trlNo][-1])
                 timeSelection = slice(None)
                 if select:
                     trlNo = select["trials"][tk]
                     if "latency" in select.keys():
                         timeArr = np.arange(*select["latency"])
-                        timeStart = int(select['latency'][0] * tfData.samplerate - tfData._t0[trlNo])
-                        timeStop = int(select['latency'][1] * tfData.samplerate - tfData._t0[trlNo])
+                        timeStart = int(select["latency"][0] * tfData.samplerate - tfData._t0[trlNo])
+                        timeStop = int(select["latency"][1] * tfData.samplerate - tfData._t0[trlNo])
                         timeSelection = slice(timeStart, timeStop)
 
                 # Ensure timing array was computed correctly and independent of `foi`/`foilim`
                 assert np.array_equal(timeArr, tfSpec.time[tk])
                 assert np.array_equal(tfSpec.time[tk], tfSpecFoi.time[tk])
                 assert np.array_equal(tfSpecFoi.time[tk], tfSpecFoiLim.time[tk])
 
@@ -595,58 +683,64 @@
 
                     # Get reference channel in input object to determine underlying modulator
                     chanNo = chan
                     if select:
                         if "latency" not in select.keys():
                             chanNo = np.where(tfData.channel == select["channel"][chan])[0][0]
                     if chanNo % 2:
-                        modIdx = self.odd[(-1)**trlNo]
+                        modIdx = self.odd[(-1) ** trlNo]
                     else:
-                        modIdx = self.even[(-1)**trlNo]
+                        modIdx = self.even[(-1) ** trlNo]
                     tfIdx[chanIdx] = chan
                     Zxx = trlArr[tuple(tfIdx)].squeeze()
 
                     # Use SciPy's `find_peaks` to identify frequency peaks in computed TF spectrum:
                     # `peakProfile` is just a sliver of the TF spectrum around the peak frequency; to
                     # better understand what's happening here, look at
                     # plt.figure(); plt.plot(peakProfile); plt.plot(peaks, peakProfile[peaks], 'x')
                     ZxxMax = Zxx.max()
                     ZxxThresh = 0.1 * ZxxMax
                     _, freqPeaks = np.where(Zxx >= (ZxxMax - ZxxThresh))
                     freqMax, freqMin = freqPeaks.max(), freqPeaks.min()
                     modulator = self.modulators[timeSelection, modIdx]
-                    modCounts = [sum(modulator == modulator.min()), sum(modulator == modulator.max())]
+                    modCounts = [
+                        sum(modulator == modulator.min()),
+                        sum(modulator == modulator.max()),
+                    ]
                     for fk, freqPeak in enumerate([freqMin, freqMax]):
                         peakProfile = Zxx[:, freqPeak - 1 : freqPeak + 2].mean(axis=1)
                         peaks, _ = scisig.find_peaks(peakProfile, height=ZxxThresh)
                         assert np.abs(peaks.size - modCounts[fk]) <= 1
 
                     # Ensure that the `foi`/`foilim` selections correspond to the respective
                     # slivers of the full TF spectrum
-                    assert np.allclose(tfSpecFoi.trials[tk][tuple(tfIdx)].squeeze(),
-                                       Zxx[:, maxFreqs])
-                    assert np.allclose(tfSpecFoiLim.trials[tk][tuple(tfIdx)].squeeze(),
-                                       Zxx[:, maxFreqs.min():maxFreqs.max() + 1])
+                    assert np.allclose(tfSpecFoi.trials[tk][tuple(tfIdx)].squeeze(), Zxx[:, maxFreqs])
+                    assert np.allclose(
+                        tfSpecFoiLim.trials[tk][tuple(tfIdx)].squeeze(),
+                        Zxx[:, maxFreqs.min() : maxFreqs.max() + 1],
+                    )
 
     def test_tf_toi(self):
         # Use a Hanning window and throw away trials to speed up things a bit
         cfg = get_defaults(freqanalysis)
         cfg.method = "mtmconvol"
         cfg.taper = "hann"
         cfg.output = "pow"
         cfg.keeptrials = False
         cfg.keeptapers = False
 
         # Test various combinations of `toi` and `t_ftimwin`: `toiArrs` comprises
         # arrays containing the onset, purely pre-onset, purely after onset and
         # non-unit spacing
         toiVals = [0.9, 0.75]
-        toiArrs = [np.arange(-2, 7),
-                   np.arange(-1, 6, 1 / self.tfData.samplerate),
-                   np.arange(1, 6, 2)]
+        toiArrs = [
+            np.arange(-2, 7),
+            np.arange(-1, 6, 1 / self.tfData.samplerate),
+            np.arange(1, 6, 2),
+        ]
         winSizes = [0.5, 1.0]
 
         # Combine `toi`-testing w/in-place data-pre-selection
         for select in self.dataSelections:
             cfg.select = select
             tStart = self.tfData.time[0][0]
             tStop = self.tfData.time[0][-1]
@@ -671,15 +765,15 @@
             dt = TestMTMConvol.get_tfdata_mtmconvol()
             if select is not None and "latency" not in select.keys():
                 cfg.t_ftimwin = 0.05
                 for toi in toiArrs:
                     cfg.toi = toi
                     tfSpec = freqanalysis(cfg, dt)
                     assert np.allclose(cfg.toi, tfSpec.time[0])
-                    assert tfSpec.samplerate == 1/(toi[1] - toi[0])
+                    assert tfSpec.samplerate == 1 / (toi[1] - toi[0])
 
                 # Unevenly sampled array: timing currently in lala-land, but sizes must match
                 cfg.toi = [-1, 2, 6]
                 tfSpec = freqanalysis(cfg, TestMTMConvol.get_tfdata_mtmconvol())
                 assert tfSpec.time[0].size == len(cfg.toi)
 
         # Test correct time-array assembly for ``toi = "all"`` (cut down data signifcantly
@@ -732,78 +826,104 @@
         nChannels = 2
 
         # start harmless: equidistant trials w/multiple tapers
         cfg.toi = 0.0
         # this guy always creates a data set from [-1, ..., 1.9999] seconds
         # no way to change this..
         artdata_len = 3
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           equidistant=True, inmemory=False)
+        artdata = generate_artificial_data(
+            nTrials=nTrials, nChannels=nChannels, equidistant=True, inmemory=False
+        )
         tfSpec = freqanalysis(artdata, **cfg)
         assert tfSpec.taper.size >= 1
         for trl_time in tfSpec.time:
             assert np.allclose(artdata_len / cfg.t_ftimwin, trl_time[0].shape)
 
         cfg.toi = "all"
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           equidistant=True, inmemory=False)
+        artdata = generate_artificial_data(
+            nTrials=nTrials, nChannels=nChannels, equidistant=True, inmemory=False
+        )
         # reduce samples, otherwise the the memory usage explodes (nSamples x win_size x nFreq)
         rdat = artdata.selectdata(latency=[0, 0.5])
         tfSpec = freqanalysis(rdat, **cfg)
         for tk, origTime in enumerate(rdat.time):
             assert np.array_equal(origTime, tfSpec.time[tk])
 
         # non-equidistant trials w/multiple tapers
         cfg.toi = 0.0
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           equidistant=False, inmemory=False)
+        artdata = generate_artificial_data(
+            nTrials=nTrials, nChannels=nChannels, equidistant=False, inmemory=False
+        )
         tfSpec = freqanalysis(artdata, **cfg)
         assert tfSpec.taper.size >= 1
         for tk, trl_time in enumerate(tfSpec.time):
-            assert np.allclose(np.ceil(artdata.time[tk].size / artdata.samplerate / cfg.t_ftimwin), trl_time.size)
+            assert np.allclose(
+                np.ceil(artdata.time[tk].size / artdata.samplerate / cfg.t_ftimwin),
+                trl_time.size,
+            )
 
         cfg.toi = "all"
         # reduce samples, otherwise the the memory usage explodes (nSamples x win_size x nFreq)
         rdat = artdata.selectdata(latency=[0, 0.5])
         tfSpec = freqanalysis(rdat, **cfg)
         for tk, origTime in enumerate(rdat.time):
             assert np.array_equal(origTime, tfSpec.time[tk])
 
         # same + reversed dimensional order in input object
         cfg.toi = 0.0
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           equidistant=False, inmemory=False,
-                                           dimord=AnalogData._defaultDimord[::-1])
+        artdata = generate_artificial_data(
+            nTrials=nTrials,
+            nChannels=nChannels,
+            equidistant=False,
+            inmemory=False,
+            dimord=AnalogData._defaultDimord[::-1],
+        )
         tfSpec = freqanalysis(artdata, cfg)
         assert tfSpec.taper.size >= 1
         for tk, trl_time in enumerate(tfSpec.time):
-            assert np.allclose(np.ceil(artdata.time[tk].size / artdata.samplerate / cfg.t_ftimwin), trl_time.size)
+            assert np.allclose(
+                np.ceil(artdata.time[tk].size / artdata.samplerate / cfg.t_ftimwin),
+                trl_time.size,
+            )
 
         cfg.toi = "all"
         # reduce samples, otherwise the the memory usage explodes (nSamples x win_size x nFreq)
         rdat = artdata.selectdata(latency=[0, 0.5])
         tfSpec = freqanalysis(rdat, cfg)
 
         # same + overlapping trials
         cfg.toi = 0.0
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           equidistant=False, inmemory=False,
-                                           dimord=AnalogData._defaultDimord[::-1],
-                                           overlapping=True)
+        artdata = generate_artificial_data(
+            nTrials=nTrials,
+            nChannels=nChannels,
+            equidistant=False,
+            inmemory=False,
+            dimord=AnalogData._defaultDimord[::-1],
+            overlapping=True,
+        )
         tfSpec = freqanalysis(artdata, cfg)
         assert tfSpec.taper.size >= 1
         for tk, trl_time in enumerate(tfSpec.time):
-            assert np.allclose(np.ceil(artdata.time[tk].size / artdata.samplerate / cfg.t_ftimwin), trl_time.size)
+            assert np.allclose(
+                np.ceil(artdata.time[tk].size / artdata.samplerate / cfg.t_ftimwin),
+                trl_time.size,
+            )
 
     @skip_low_mem
     def test_tf_parallel(self, testcluster):
         # collect all tests of current class and repeat them running concurrently
         client = dd.Client(testcluster)
-        quick_tests = [attr for attr in self.__dir__()
-                       if (inspect.ismethod(getattr(self, attr)) and attr not in ["test_tf_parallel", "test_tf_cut_selections"])]
+        quick_tests = [
+            attr
+            for attr in self.__dir__()
+            if (
+                inspect.ismethod(getattr(self, attr))
+                and attr not in ["test_tf_parallel", "test_tf_cut_selections"]
+            )
+        ]
         slow_tests = []
         slow_tests.append(quick_tests.pop(quick_tests.index("test_tf_output")))
         slow_tests.append(quick_tests.pop(quick_tests.index("test_tf_irregular_trials")))
         for test in quick_tests:
             getattr(self, test)()
             flush_local_cluster(testcluster)
         for test in slow_tests:
@@ -819,88 +939,107 @@
         cfg.output = "pow"
 
         nChannels = 3
         nTrials = 2
 
         # no. of HDF5 files that will make up virtual data-set in case of channel-chunking
         chanPerWrkr = 2
-        nFiles = nTrials * (int(nChannels/chanPerWrkr)
-                            + int(nChannels % chanPerWrkr > 0))
+        nFiles = nTrials * (int(nChannels / chanPerWrkr) + int(nChannels % chanPerWrkr > 0))
 
         # simplest case: equidistant trial spacing, all in memory
         fileCount = [nTrials, nFiles]
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           inmemory=True)
+        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels, inmemory=True)
         for k, chan_per_worker in enumerate([None, chanPerWrkr]):
             cfg.chan_per_worker = chan_per_worker
             tfSpec = freqanalysis(artdata, cfg)
             assert tfSpec.data.is_virtual
             assert len(tfSpec.data.virtual_sources()) == fileCount[k]
 
         # non-equidistant trial spacing
         cfg.keeptapers = False
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           inmemory=True, equidistant=False)
+        artdata = generate_artificial_data(
+            nTrials=nTrials, nChannels=nChannels, inmemory=True, equidistant=False
+        )
         expectedFreqs = np.arange(artdata.samplerate / 2 + 1)
         for chan_per_worker in enumerate([None, chanPerWrkr]):
             tfSpec = freqanalysis(artdata, cfg)
             assert np.array_equal(tfSpec.freq, expectedFreqs)
             assert tfSpec.taper.size == 1
 
         # equidistant trial spacing, keep tapers
         cfg.output = "abs"
         cfg.tapsmofrq = 2
         cfg.keeptapers = True
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           inmemory=False)
+        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels, inmemory=False)
         for chan_per_worker in enumerate([None, chanPerWrkr]):
             tfSpec = freqanalysis(artdata, cfg)
             assert tfSpec.taper.size >= 1
 
         # overlapping trial spacing, throw away trials and tapers
         cfg.keeptapers = False
         cfg.keeptrials = "no"
         cfg.output = "pow"
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           inmemory=False, equidistant=True,
-                                           overlapping=True)
+        artdata = generate_artificial_data(
+            nTrials=nTrials,
+            nChannels=nChannels,
+            inmemory=False,
+            equidistant=True,
+            overlapping=True,
+        )
         expectedFreqs = np.arange(artdata.samplerate / 2 + 1)
         tfSpec = freqanalysis(artdata, cfg)
         assert np.array_equal(tfSpec.freq, expectedFreqs)
         assert tfSpec.taper.size == 1
         assert np.array_equal(np.unique(np.floor(artdata.time[0])), tfSpec.time[0])
-        assert tfSpec.data.shape == (tfSpec.time[0].size, 1, expectedFreqs.size, nChannels)
+        assert tfSpec.data.shape == (
+            tfSpec.time[0].size,
+            1,
+            expectedFreqs.size,
+            nChannels,
+        )
 
         client.close()
 
 
-class TestWavelet():
+class TestWavelet:
 
     # Prepare testing signal: ensure `fadeIn` and `fadeOut` are compatible w/`latency`
     # selection below
     nChannels = 4
     nTrials = 3
     seed = 151120
     fadeIn = -1.5
     fadeOut = 5.5
-    tfData, modulators, even, odd, fader = _make_tf_signal(nChannels, nTrials, seed,
-                                                           fadeIn=fadeIn, fadeOut=fadeOut)
+    tfData, modulators, even, odd, fader = _make_tf_signal(
+        nChannels, nTrials, seed, fadeIn=fadeIn, fadeOut=fadeOut
+    )
 
     @staticmethod
     def get_tfdata_wavelet():
-        return(_make_tf_signal(TestWavelet.nChannels, TestWavelet.nTrials, TestWavelet.seed,
-                                                           fadeIn=TestWavelet.fadeIn, fadeOut=TestWavelet.fadeOut)[0])
+        return _make_tf_signal(
+            TestWavelet.nChannels,
+            TestWavelet.nTrials,
+            TestWavelet.seed,
+            fadeIn=TestWavelet.fadeIn,
+            fadeOut=TestWavelet.fadeOut,
+        )[0]
 
     # Set up in-place data-selection dicts for the constructed object
-    dataSelections = [None,
-                      {"trials": [1, 2, 0],
-                       "channel": ["channel" + str(i) for i in range(2, 4)][::-1]},
-                      {"trials": [0, 2],
-                       "channel": range(0, int(nChannels / 2)),
-                       "latency": [-2, 6.8]}]
+    dataSelections = [
+        None,
+        {
+            "trials": [1, 2, 0],
+            "channel": ["channel" + str(i) for i in range(2, 4)][::-1],
+        },
+        {
+            "trials": [0, 2],
+            "channel": range(0, int(nChannels / 2)),
+            "latency": [-2, 6.8],
+        },
+    ]
 
     # Helper function that reduces dataselections (keep `None` selection no matter what)
     def test_wav_cut_selections(self):
         self.dataSelections.pop(random.choice([-1, 1]))
 
     @skip_low_mem
     def test_wav_solution(self):
@@ -911,36 +1050,39 @@
         cfg.method = "wavelet"
         cfg.wavelet = "Morlet"
         cfg.width = 1
         cfg.output = "pow"
 
         # import pdb; pdb.set_trace()
 
-
         # Set up index tuple for slicing computed TF spectra and collect values
         # of expected frequency peaks (for validation of `foi`/`foilim` selections below)
         chanIdx = SpectralData._defaultDimord.index("channel")
         tfIdx = [slice(None)] * len(SpectralData._defaultDimord)
         modFreqs = [330, 360]
-        maxFreqs = np.hstack([np.arange(modFreqs[0] - 5, modFreqs[0] + 6),
-                              np.arange(modFreqs[1] - 5, modFreqs[1] + 6)])
+        maxFreqs = np.hstack(
+            [
+                np.arange(modFreqs[0] - 5, modFreqs[0] + 6),
+                np.arange(modFreqs[1] - 5, modFreqs[1] + 6),
+            ]
+        )
         foilimFreqs = np.arange(maxFreqs.min(), maxFreqs.max() + 1)
 
         for select in self.dataSelections:
 
             # Timing of `tfData` is identical for all trials, so to speed things up,
             # set up `timeArr` here - if `tfData` is modified, these computations have
             # to be moved inside the `enumerate(tfSpec.trials)`-loop!
             timeArr = np.arange(self.tfData.time[0][0], self.tfData.time[0][-1])
             if select:
                 if "latency" in select.keys():
                     continue
                     timeArr = np.arange(*select["latency"])
-                    timeStart = int(select['latency'][0] * self.tfData.samplerate - self.tfData._t0[0])
-                    timeStop = int(select['latency'][1] * self.tfData.samplerate - self.tfData._t0[0])
+                    timeStart = int(select["latency"][0] * self.tfData.samplerate - self.tfData._t0[0])
+                    timeStop = int(select["latency"][1] * self.tfData.samplerate - self.tfData._t0[0])
                     timeSelection = slice(timeStart, timeStop)
             else:
                 timeSelection = np.where(self.fader == 1.0)[0]
             cfg.toi = timeArr
 
             # Compute TF objects w\w/o`foi`/`foilim`
             cfg.select = select
@@ -975,33 +1117,36 @@
 
                     # Get reference channel in input object to determine underlying modulator
                     chanNo = chan
                     if select:
                         if "latency" not in select.keys():
                             chanNo = np.where(self.tfData.channel == select["channel"][chan])[0][0]
                     if chanNo % 2:
-                        modIdx = self.odd[(-1)**trlNo]
+                        modIdx = self.odd[(-1) ** trlNo]
                     else:
-                        modIdx = self.even[(-1)**trlNo]
+                        modIdx = self.even[(-1) ** trlNo]
                     tfIdx[chanIdx] = chan
                     modulator = self.modulators[timeSelection, modIdx]
-                    modCounts = [sum(modulator == modulator.min()), sum(modulator == modulator.max())]
+                    modCounts = [
+                        sum(modulator == modulator.min()),
+                        sum(modulator == modulator.max()),
+                    ]
 
                     # Be more lenient w/`tfSpec`: don't scan for min/max freq, but all peaks at once
                     # (auto-scale resolution potentially too coarse to differentiate b/w min/max);
                     # consider peak-count equal up to 2 misses
                     Zxx = trlArr[tuple(tfIdx)].squeeze()
                     ZxxMax = Zxx.max()
                     ZxxThresh = 0.2 * ZxxMax
                     _, freqPeaks = np.where(Zxx >= (ZxxMax - ZxxThresh))
                     peakVals, peakCounts = np.unique(freqPeaks, return_counts=True)
                     freqPeak = peakVals[peakCounts.argmax()]
                     modCount = np.ceil(sum(modCounts) / 2)
                     peakProfile = Zxx[:, freqPeak - 1 : freqPeak + 2].mean(axis=1)
-                    peaks, _ = scisig.find_peaks(peakProfile, height=2*ZxxThresh, distance=5)
+                    peaks, _ = scisig.find_peaks(peakProfile, height=2 * ZxxThresh, distance=5)
                     if np.abs(peaks.size - modCount) > 2:
                         modCount = sum(modCounts)
                     assert np.abs(peaks.size - modCount) <= 2
 
                     # Now for `tfSpecFoi`/`tfSpecFoiLim` on the other side be more
                     # stringent and really count maxima/minima (frequency values have
                     # been explicitly queried, must not be too coarse); that said,
@@ -1010,110 +1155,133 @@
                         Zxx = tfObj.trials[tk][tuple(tfIdx)].squeeze()
                         ZxxMax = Zxx.max()
                         ZxxThresh = ZxxMax - 0.1 * ZxxMax
                         for fk, mFreq in enumerate(modFreqs):
                             freqPeak = np.where(np.abs(tfObj.freq - mFreq) < 1)[0][0]
                             peakProfile = Zxx[:, freqPeak - 1 : freqPeak + 2].mean(axis=1)
                             height = (1 - fk * 0.25) * ZxxThresh
-                            peaks, _ = scisig.find_peaks(peakProfile, prominence=0.75*height, height=height, distance=5)
+                            peaks, _ = scisig.find_peaks(
+                                peakProfile,
+                                prominence=0.75 * height,
+                                height=height,
+                                distance=5,
+                            )
                             # if it doesn't fit, use a bigger hammer...
                             if np.abs(peaks.size - modCounts[fk]) > 2:
                                 height = 0.9 * ZxxThresh
-                                peaks, _ = scisig.find_peaks(peakProfile, prominence=0.75*height, height=height, distance=5)
+                                peaks, _ = scisig.find_peaks(
+                                    peakProfile,
+                                    prominence=0.75 * height,
+                                    height=height,
+                                    distance=5,
+                                )
                             assert np.abs(peaks.size - modCounts[fk]) <= 2
 
     def test_wav_toi(self):
         # Don't keep trials to speed things up a bit
         cfg = get_defaults(freqanalysis)
         cfg.method = "wavelet"
         cfg.wavelet = "Morlet"
         cfg.output = "pow"
         cfg.keeptrials = False
 
         # Test time-point arrays comprising onset, purely pre-onset, purely after
         # onset and non-unit spacing
-        toiArrs = [np.arange(-2,7),
-                   np.arange(-1, 6, 1/self.tfData.samplerate),
-                   np.arange(1, 6, 2)]
+        toiArrs = [
+            np.arange(-2, 7),
+            np.arange(-1, 6, 1 / self.tfData.samplerate),
+            np.arange(1, 6, 2),
+        ]
 
         # Combine `toi`-testing w/in-place data-pre-selection
         for select in self.dataSelections:
             if select is not None and "latency" not in select.keys():
                 cfg.select = select
                 for toi in toiArrs:
                     cfg.toi = toi
                     tfSpec = freqanalysis(cfg, self.tfData)
                     assert np.allclose(cfg.toi, tfSpec.time[0])
-                    assert tfSpec.samplerate == 1/(toi[1] - toi[0])
+                    assert tfSpec.samplerate == 1 / (toi[1] - toi[0])
 
         # Test correct time-array assembly for ``toi = "all"`` (cut down data signifcantly
         # to not overflow memory here)
         cfg.select = {"trials": [0], "channel": [0], "latency": [-0.5, 0.5]}
         cfg.toi = "all"
         tfSpec = freqanalysis(cfg, TestWavelet.get_tfdata_wavelet())
-        dt = 1/self.tfData.samplerate
+        dt = 1 / self.tfData.samplerate
         timeArr = np.arange(cfg.select["latency"][0], cfg.select["latency"][1] + dt, dt)
         assert np.allclose(tfSpec.time[0], timeArr)
 
         # Use `toi` array outside trial boundaries
         cfg.toi = self.tfData.time[0][:10]
         with pytest.raises(SPYValueError) as spyval:
             freqanalysis(cfg, TestWavelet.get_tfdata_wavelet())
 
-
         # Unsorted `toi` array
         cfg.toi = [0.3, -0.1, 0.2]
         with pytest.raises(SPYValueError):
             freqanalysis(cfg, TestSuperlet._get_tf_data_superlet())
 
     def test_wav_irregular_trials(self):
         # Set up wavelet to compute "full" TF spectrum for all time-points
         cfg = get_defaults(freqanalysis)
         cfg.method = "wavelet"
         cfg.wavelet = "Morlet"
         cfg.output = "pow"
         cfg.toi = "all"
 
         # start harmless: equidistant trials w/multiple tapers
-        artdata = generate_artificial_data(nTrials=5, nChannels=16,
-                                           equidistant=True, inmemory=False)
+        artdata = generate_artificial_data(nTrials=5, nChannels=16, equidistant=True, inmemory=False)
         tfSpec = freqanalysis(artdata, **cfg)
         for tk, origTime in enumerate(artdata.time):
             assert np.array_equal(origTime, tfSpec.time[tk])
 
         # non-equidistant trials w/multiple tapers
-        artdata = generate_artificial_data(nTrials=5, nChannels=16,
-                                           equidistant=False, inmemory=False)
+        artdata = generate_artificial_data(nTrials=5, nChannels=16, equidistant=False, inmemory=False)
         tfSpec = freqanalysis(artdata, **cfg)
         for tk, origTime in enumerate(artdata.time):
             assert np.array_equal(origTime, tfSpec.time[tk])
 
         # same + reversed dimensional order in input object
-        cfg.data = generate_artificial_data(nTrials=5, nChannels=16,
-                                            equidistant=False, inmemory=False,
-                                            dimord=AnalogData._defaultDimord[::-1])
+        cfg.data = generate_artificial_data(
+            nTrials=5,
+            nChannels=16,
+            equidistant=False,
+            inmemory=False,
+            dimord=AnalogData._defaultDimord[::-1],
+        )
         tfSpec = freqanalysis(cfg)
         for tk, origTime in enumerate(cfg.data.time):
             assert np.array_equal(origTime, tfSpec.time[tk])
 
         # same + overlapping trials
-        cfg.data = generate_artificial_data(nTrials=5, nChannels=16,
-                                           equidistant=False, inmemory=False,
-                                           dimord=AnalogData._defaultDimord[::-1],
-                                           overlapping=True)
+        cfg.data = generate_artificial_data(
+            nTrials=5,
+            nChannels=16,
+            equidistant=False,
+            inmemory=False,
+            dimord=AnalogData._defaultDimord[::-1],
+            overlapping=True,
+        )
         tfSpec = freqanalysis(cfg)
         for tk, origTime in enumerate(cfg.data.time):
             assert np.array_equal(origTime, tfSpec.time[tk])
 
     @skip_low_mem
     def test_wav_parallel(self, testcluster):
         # collect all tests of current class and repeat them running concurrently
         client = dd.Client(testcluster)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and attr not in ["test_wav_parallel", "test_wav_cut_selections"])]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (
+                inspect.ismethod(getattr(self, attr))
+                and attr not in ["test_wav_parallel", "test_wav_cut_selections"]
+            )
+        ]
         for test in all_tests:
             getattr(self, test)()
             flush_local_cluster(testcluster)
 
         # now create uniform `cfg` for remaining SLURM tests
         cfg = StructDict()
         cfg.method = "wavelet"
@@ -1122,82 +1290,101 @@
         cfg.toi = "all"
 
         nChannels = 3
         nTrials = 2
 
         # no. of HDF5 files that will make up virtual data-set in case of channel-chunking
         chanPerWrkr = 2
-        nFiles = nTrials * (int(nChannels/chanPerWrkr) + int(nChannels % chanPerWrkr > 0))
+        nFiles = nTrials * (int(nChannels / chanPerWrkr) + int(nChannels % chanPerWrkr > 0))
 
         # simplest case: equidistant trial spacing, all in memory
         fileCount = [nTrials, nFiles]
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           inmemory=True)
+        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels, inmemory=True)
         for k, chan_per_worker in enumerate([None, chanPerWrkr]):
             cfg.chan_per_worker = chan_per_worker
             tfSpec = freqanalysis(artdata, cfg)
             assert tfSpec.data.is_virtual
             assert len(tfSpec.data.virtual_sources()) == fileCount[k]
 
         # non-equidistant trial spacing
         cfg.keeptapers = False
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           inmemory=True, equidistant=False)
+        artdata = generate_artificial_data(
+            nTrials=nTrials, nChannels=nChannels, inmemory=True, equidistant=False
+        )
         for chan_per_worker in enumerate([None, chanPerWrkr]):
             tfSpec = freqanalysis(artdata, cfg)
             assert 1 > tfSpec.freq.min() > 0
             assert tfSpec.freq.max() == (self.tfData.samplerate / 2)
 
         # equidistant trial spacing
         cfg.output = "abs"
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           inmemory=False)
+        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels, inmemory=False)
         for chan_per_worker in enumerate([None, chanPerWrkr]):
             tfSpec = freqanalysis(artdata, cfg)
             for tk, origTime in enumerate(artdata.time):
                 assert np.array_equal(origTime, tfSpec.time[tk])
 
         # overlapping trial spacing, throw away trials
         cfg.keeptrials = "no"
         cfg.foilim = [1, 250]
         expectedFreqs = np.arange(1, cfg.foilim[1] + 1)
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           inmemory=False, equidistant=True,
-                                           overlapping=True)
+        artdata = generate_artificial_data(
+            nTrials=nTrials,
+            nChannels=nChannels,
+            inmemory=False,
+            equidistant=True,
+            overlapping=True,
+        )
         tfSpec = freqanalysis(artdata, cfg)
         assert np.allclose(tfSpec.freq, expectedFreqs)
-        assert tfSpec.data.shape == (tfSpec.time[0].size, 1, expectedFreqs.size, nChannels)
+        assert tfSpec.data.shape == (
+            tfSpec.time[0].size,
+            1,
+            expectedFreqs.size,
+            nChannels,
+        )
 
         client.close()
 
-class TestSuperlet():
 
+class TestSuperlet:
     @staticmethod
     def _get_tf_data_superlet():
-        return _make_tf_signal(TestSuperlet.nChannels, TestSuperlet.nTrials, TestSuperlet.seed,
-                                                           fadeIn=TestSuperlet.fadeIn, fadeOut=TestSuperlet.fadeOut)[0]
-
+        return _make_tf_signal(
+            TestSuperlet.nChannels,
+            TestSuperlet.nTrials,
+            TestSuperlet.seed,
+            fadeIn=TestSuperlet.fadeIn,
+            fadeOut=TestSuperlet.fadeOut,
+        )[0]
 
     # Prepare testing signal: ensure `fadeIn` and `fadeOut` are compatible w/`latency`
     # selection below
     nChannels = 4
     nTrials = 3
     seed = 151120
     fadeIn = -9.5
     fadeOut = 50.5
-    tfData, modulators, even, odd, fader = _make_tf_signal(nChannels, nTrials, seed,
-                                                           fadeIn=fadeIn, fadeOut=fadeOut)
+    tfData, modulators, even, odd, fader = _make_tf_signal(
+        nChannels, nTrials, seed, fadeIn=fadeIn, fadeOut=fadeOut
+    )
 
     # Set up in-place data-selection dicts for the constructed object
-    dataSelections = [None,
-                      {"trials": [1, 2, 0],
-                       "channel": ["channel" + str(i) for i in range(2, 4)][::-1]},
-                      {"trials": [0, 2],
-                       "channel": range(0, int(nChannels / 2)),
-                       "latency": [-2, 6.8]}]
+    dataSelections = [
+        None,
+        {
+            "trials": [1, 2, 0],
+            "channel": ["channel" + str(i) for i in range(2, 4)][::-1],
+        },
+        {
+            "trials": [0, 2],
+            "channel": range(0, int(nChannels / 2)),
+            "latency": [-2, 6.8],
+        },
+    ]
 
     # Helper function that reduces dataselections (keep `None` selection no matter what)
     def test_slet_cut_selections(self):
         self.dataSelections.pop(random.choice([-1, 1]))
 
     @skip_low_mem
     def test_slet_solution(self):
@@ -1210,29 +1397,33 @@
         cfg.output = "pow"
 
         # Set up index tuple for slicing computed TF spectra and collect values
         # of expected frequency peaks (for validation of `foi`/`foilim` selections below)
         chanIdx = SpectralData._defaultDimord.index("channel")
         tfIdx = [slice(None)] * len(SpectralData._defaultDimord)
         modFreqs = [330, 360]
-        maxFreqs = np.hstack([np.arange(modFreqs[0] - 5, modFreqs[0] + 6),
-                              np.arange(modFreqs[1] - 5, modFreqs[1] + 6)])
+        maxFreqs = np.hstack(
+            [
+                np.arange(modFreqs[0] - 5, modFreqs[0] + 6),
+                np.arange(modFreqs[1] - 5, modFreqs[1] + 6),
+            ]
+        )
         foilimFreqs = np.arange(maxFreqs.min(), maxFreqs.max() + 1)
 
         for select in self.dataSelections:
 
             # Timing of `tfData` is identical for all trials, so to speed things up,
             # set up `timeArr` here - if `tfData` is modified, these computations have
             # to be moved inside the `enumerate(tfSpec.trials)`-loop!
             timeArr = np.arange(self.tfData.time[0][0], self.tfData.time[0][-1])
             if select:
                 if "latency" in select.keys():
                     timeArr = np.arange(*select["latency"])
-                    timeStart = int(select['latency'][0] * self.tfData.samplerate - self.tfData._t0[0])
-                    timeStop = int(select['latency'][1] * self.tfData.samplerate - self.tfData._t0[0])
+                    timeStart = int(select["latency"][0] * self.tfData.samplerate - self.tfData._t0[0])
+                    timeStop = int(select["latency"][1] * self.tfData.samplerate - self.tfData._t0[0])
                     timeSelection = slice(timeStart, timeStop)
             else:
                 timeSelection = np.where(self.fader == 1.0)[0]
             cfg.toi = timeArr
 
             # Skip below tests if `toi` and an in-place time-selection clash
             if select is not None and "latency" in select.keys():
@@ -1271,38 +1462,51 @@
 
                     # Get reference channel in input object to determine underlying modulator
                     chanNo = chan
                     if select:
                         if "latency" not in select.keys():
                             chanNo = np.where(self.tfData.channel == select["channel"][chan])[0][0]
                     if chanNo % 2:
-                        modIdx = self.odd[(-1)**trlNo]
+                        modIdx = self.odd[(-1) ** trlNo]
                     else:
-                        modIdx = self.even[(-1)**trlNo]
+                        modIdx = self.even[(-1) ** trlNo]
                     tfIdx[chanIdx] = chan
                     modulator = self.modulators[timeSelection, modIdx]
-                    modCounts = [sum(modulator == modulator.min()), sum(modulator == modulator.max())]
+                    modCounts = [
+                        sum(modulator == modulator.min()),
+                        sum(modulator == modulator.max()),
+                    ]
 
                     # Now for `tfSpecFoi`/`tfSpecFoiLim` on the other side be more
                     # stringent and really count maxima/minima (frequency values have
                     # been explicitly queried, must not be too coarse); that said,
                     # the peak-profile is quite rugged, so adjust `height` if necessary
                     for tfObj in [tfSpecFoi, tfSpecFoiLim]:
                         Zxx = tfObj.trials[tk][tuple(tfIdx)].squeeze()
                         ZxxMax = Zxx.max()
                         ZxxThresh = ZxxMax - 0.1 * ZxxMax
                         for fk, mFreq in enumerate(modFreqs):
                             freqPeak = np.where(np.abs(tfObj.freq - mFreq) < 1)[0][0]
                             peakProfile = Zxx[:, freqPeak - 1 : freqPeak + 2].mean(axis=1)
                             height = (1 - fk * 0.25) * ZxxThresh
-                            peaks, _ = scisig.find_peaks(peakProfile, prominence=0.75*height, height=height, distance=5)
+                            peaks, _ = scisig.find_peaks(
+                                peakProfile,
+                                prominence=0.75 * height,
+                                height=height,
+                                distance=5,
+                            )
                             # if it doesn't fit, use a bigger hammer...
                             if np.abs(peaks.size - modCounts[fk]) > 2:
                                 height = 0.9 * ZxxThresh
-                                peaks, _ = scisig.find_peaks(peakProfile, prominence=0.75*height, height=height, distance=5)
+                                peaks, _ = scisig.find_peaks(
+                                    peakProfile,
+                                    prominence=0.75 * height,
+                                    height=height,
+                                    distance=5,
+                                )
                             assert np.abs(peaks.size - modCounts[fk]) <= 2
 
                     # Be more lenient w/`tfSpec`: don't scan for min/max freq, but all peaks at once
                     # (auto-scale resolution potentially too coarse to differentiate b/w min/max);
 
     def test_slet_toi(self):
         # Don't keep trials to speed things up a bit
@@ -1310,36 +1514,38 @@
         cfg.method = "superlet"
         cfg.order_max = 2
         cfg.output = "pow"
         cfg.keeptrials = False
 
         # Test time-point arrays comprising onset, purely pre-onset, purely after
         # onset and non-unit spacing
-        toiArrs = [np.arange(-2,7),
-                   np.arange(-1, 6, 1/self.tfData.samplerate),
-                   np.arange(1, 6, 2)]
+        toiArrs = [
+            np.arange(-2, 7),
+            np.arange(-1, 6, 1 / self.tfData.samplerate),
+            np.arange(1, 6, 2),
+        ]
 
         toiArrs = [random.choice(toiArrs)]
 
         # Combine `toi`-testing w/in-place data-pre-selection
         for select in self.dataSelections:
             if select is not None and "latency" not in select.keys():
                 cfg.select = select
                 for toi in toiArrs:
                     cfg.toi = toi
                     tfSpec = freqanalysis(cfg, self.tfData)
                     assert np.allclose(cfg.toi, tfSpec.time[0])
-                    assert tfSpec.samplerate == 1/(toi[1] - toi[0])
+                    assert tfSpec.samplerate == 1 / (toi[1] - toi[0])
 
         # Test correct time-array assembly for ``toi = "all"`` (cut down data signifcantly
         # to not overflow memory here)
         cfg.select = {"trials": [0], "channel": [0], "latency": [-0.5, 0.5]}
         cfg.toi = "all"
         tfSpec = freqanalysis(cfg, self.tfData)
-        dt = 1/self.tfData.samplerate
+        dt = 1 / self.tfData.samplerate
         timeArr = np.arange(cfg.select["latency"][0], cfg.select["latency"][1] + dt, dt)
         assert np.allclose(tfSpec.time[0], timeArr)
 
         # Use `toi` array outside trial boundaries
         cfg.toi = self.tfData.time[0][:10]
         with pytest.raises(SPYValueError) as spyval:
             freqanalysis(cfg, TestSuperlet._get_tf_data_superlet())
@@ -1360,104 +1566,130 @@
         cfg.output = "pow"
         cfg.toi = "all"
 
         nTrials = 2
         nChannels = 2
 
         # start harmless: equidistant trials w/multiple tapers
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           equidistant=True, inmemory=False)
+        artdata = generate_artificial_data(
+            nTrials=nTrials, nChannels=nChannels, equidistant=True, inmemory=False
+        )
         tfSpec = freqanalysis(artdata, **cfg)
         for tk, origTime in enumerate(artdata.time):
             assert np.array_equal(origTime, tfSpec.time[tk])
 
         # non-equidistant trials w/multiple tapers
-        artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                           equidistant=False, inmemory=False)
+        artdata = generate_artificial_data(
+            nTrials=nTrials, nChannels=nChannels, equidistant=False, inmemory=False
+        )
         tfSpec = freqanalysis(artdata, **cfg)
         for tk, origTime in enumerate(artdata.time):
             assert np.array_equal(origTime, tfSpec.time[tk])
 
         # same + reversed dimensional order in input object
-        cfg.data = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                            equidistant=False, inmemory=False,
-                                            dimord=AnalogData._defaultDimord[::-1])
+        cfg.data = generate_artificial_data(
+            nTrials=nTrials,
+            nChannels=nChannels,
+            equidistant=False,
+            inmemory=False,
+            dimord=AnalogData._defaultDimord[::-1],
+        )
         tfSpec = freqanalysis(cfg)
         for tk, origTime in enumerate(cfg.data.time):
             assert np.array_equal(origTime, tfSpec.time[tk])
 
         # same + overlapping trials
-        cfg.data = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
-                                            equidistant=False, inmemory=False,
-                                            dimord=AnalogData._defaultDimord[::-1],
-                                            overlapping=True)
+        cfg.data = generate_artificial_data(
+            nTrials=nTrials,
+            nChannels=nChannels,
+            equidistant=False,
+            inmemory=False,
+            dimord=AnalogData._defaultDimord[::-1],
+            overlapping=True,
+        )
         tfSpec = freqanalysis(cfg)
         for tk, origTime in enumerate(cfg.data.time):
             assert np.array_equal(origTime, tfSpec.time[tk])
 
     @skip_low_mem
     def test_slet_parallel(self, testcluster):
         # collect all tests of current class and repeat them running concurrently
         client = dd.Client(testcluster)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and attr not in ["test_slet_parallel", "test_cut_slet_selections"])]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (
+                inspect.ismethod(getattr(self, attr))
+                and attr not in ["test_slet_parallel", "test_cut_slet_selections"]
+            )
+        ]
         for test in all_tests:
             getattr(self, test)()
             flush_local_cluster(testcluster)
 
         # now create uniform `cfg` for remaining SLURM tests
         cfg = StructDict()
         cfg.method = "superlet"
         cfg.order_max = 2
         cfg.output = "pow"
         cfg.toi = "all"
 
         # no. of HDF5 files that will make up virtual data-set in case of channel-chunking
         chanPerWrkr = 2
-        nFiles = self.nTrials * (int(self.nChannels/chanPerWrkr)
-                                 + int(self.nChannels % chanPerWrkr > 0))
+        nFiles = self.nTrials * (int(self.nChannels / chanPerWrkr) + int(self.nChannels % chanPerWrkr > 0))
 
         # simplest case: equidistant trial spacing, all in memory
         fileCount = [self.nTrials, nFiles]
-        artdata = generate_artificial_data(nTrials=self.nTrials, nChannels=self.nChannels,
-                                           inmemory=True)
+        artdata = generate_artificial_data(nTrials=self.nTrials, nChannels=self.nChannels, inmemory=True)
         for k, chan_per_worker in enumerate([None, chanPerWrkr]):
             cfg.chan_per_worker = chan_per_worker
             tfSpec = freqanalysis(artdata, cfg)
             assert tfSpec.data.is_virtual
             assert len(tfSpec.data.virtual_sources()) == fileCount[k]
 
         # non-equidistant trial spacing
         cfg.keeptapers = False
-        artdata = generate_artificial_data(nTrials=self.nTrials, nChannels=self.nChannels,
-                                           inmemory=True, equidistant=False)
+        artdata = generate_artificial_data(
+            nTrials=self.nTrials,
+            nChannels=self.nChannels,
+            inmemory=True,
+            equidistant=False,
+        )
         for chan_per_worker in enumerate([None, chanPerWrkr]):
             tfSpec = freqanalysis(artdata, cfg)
             assert 1 > tfSpec.freq.min() > 0
             assert tfSpec.freq.max() == (self.tfData.samplerate / 2)
 
         # equidistant trial spacing
         cfg.output = "abs"
-        artdata = generate_artificial_data(nTrials=self.nTrials, nChannels=self.nChannels,
-                                           inmemory=False)
+        artdata = generate_artificial_data(nTrials=self.nTrials, nChannels=self.nChannels, inmemory=False)
         for chan_per_worker in enumerate([None, chanPerWrkr]):
             tfSpec = freqanalysis(artdata, cfg)
             for tk, origTime in enumerate(artdata.time):
                 assert np.array_equal(origTime, tfSpec.time[tk])
 
         # overlapping trial spacing, throw away trials
         cfg.keeptrials = "no"
         cfg.foilim = [1, 250]
         expectedFreqs = np.arange(1, cfg.foilim[1] + 1)
-        artdata = generate_artificial_data(nTrials=self.nTrials, nChannels=self.nChannels,
-                                           inmemory=False, equidistant=True,
-                                           overlapping=True)
+        artdata = generate_artificial_data(
+            nTrials=self.nTrials,
+            nChannels=self.nChannels,
+            inmemory=False,
+            equidistant=True,
+            overlapping=True,
+        )
         tfSpec = freqanalysis(artdata, cfg)
         assert np.allclose(tfSpec.freq, expectedFreqs)
-        assert tfSpec.data.shape == (tfSpec.time[0].size, 1, expectedFreqs.size, self.nChannels)
+        assert tfSpec.data.shape == (
+            tfSpec.time[0].size,
+            1,
+            expectedFreqs.size,
+            self.nChannels,
+        )
 
         client.close()
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestMTMConvol()
     T2 = TestMTMFFT()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_spike_psth.py` & `esi_syncopy-2023.7/syncopy/tests/test_spike_psth.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,65 +11,56 @@
 # syncopy imports
 import syncopy as spy
 from syncopy.shared.errors import SPYValueError
 from syncopy import synthdata as sd
 from syncopy.statistics.spike_psth import available_outputs
 
 
-def get_spike_data(nTrials = 10, seed=None):
-    return sd.poisson_noise(nTrials,
-                            nUnits=3,
-                            nChannels=2,
-                            nSpikes=10_000,
-                            samplerate=10_000,
-                            seed=seed)
+def get_spike_data(nTrials=10, seed=None):
+    return sd.poisson_noise(nTrials, nUnits=3, nChannels=2, nSpikes=10_000, samplerate=10_000, seed=seed)
 
 
 def get_spike_cfg():
     cfg = spy.StructDict()
     cfg.binsize = 0.3
-    cfg.latency = [-.5, 1.5]
+    cfg.latency = [-0.5, 1.5]
     return cfg
 
 
 class TestPSTH:
 
     # synthetic spike data
     spd = get_spike_data(seed=42)
 
     def test_psth_binsize(self):
 
         cfg = spy.StructDict()
-        cfg.latency = 'maxperiod'  # default
+        cfg.latency = "maxperiod"  # default
 
         # directly in seconds
         cfg.binsize = 0.2
         counts = spy.spike_psth(self.spd, cfg)
 
         assert isinstance(counts, spy.TimeLockData)
         # check that all trials have the same 'time locked' time axis
         # as enfored by TimeLockData.trialdefinition setter
         assert len(set([t.size for t in counts.time])) == 1
 
         # check that time steps correspond to binsize
         assert np.allclose(np.diff(counts.time[0]), cfg.binsize)
 
         # automatic binsize selection
-        cfg.binsize = 'rice'
-        counts = spy.spike_psth(self.spd,
-                                cfg,
-                                keeptrials=True)
+        cfg.binsize = "rice"
+        counts = spy.spike_psth(self.spd, cfg, keeptrials=True)
         # number of bins is length of time axis
         nBins_rice = counts.time[0].size
         assert len(set([t.size for t in counts.time])) == 1
 
-        cfg.binsize = 'sqrt'
-        counts = spy.spike_psth(self.spd,
-                                cfg,
-                                keeptrials=True)
+        cfg.binsize = "sqrt"
+        counts = spy.spike_psth(self.spd, cfg, keeptrials=True)
         # number of bins is length of time axis
         nBins_sqrt = counts.time[0].size
         assert len(set([t.size for t in counts.time])) == 1
 
         # sqrt rule gives more bins than Rice rule
         assert nBins_sqrt > nBins_rice
 
@@ -81,15 +72,15 @@
         # directly in seconds
         cfg.binsize = 0.1
 
         trl_starts = self.spd.trialintervals[:, 0]
         trl_ends = self.spd.trialintervals[:, 1]
 
         # -- bins stretch over the largest common time window --
-        cfg.latency = 'maxperiod'  # frontend default
+        cfg.latency = "maxperiod"  # frontend default
         counts = spy.spike_psth(self.spd, cfg)
 
         # sampling interval for histogram output
         delta_t = 1 / counts.samplerate
 
         # check that histogram time points are less than 1
         # delta_t away from the maximal interval boundaries
@@ -97,41 +88,41 @@
         assert np.abs(trl_ends.max() - counts.time[0][-1]) < delta_t
 
         # check that there are NaNs as not all trials have data
         # in this maximal interval (due to start/end randomization)
         assert np.any(np.isnan(counts.data[:]))
 
         # -- bins stretch over the minimal interval present in all trials --
-        cfg.latency = 'minperiod'
+        cfg.latency = "minperiod"
         counts = spy.spike_psth(self.spd, cfg)
 
         # check that histogram time points are less than 1
         # delta_t away from the minimal interval boundaries
         assert np.abs(trl_starts.max() - counts.time[0][0]) < delta_t
         assert np.abs(trl_ends.min() - counts.time[0][-1]) < delta_t
 
         # check that there are NO NaNs as all trials have data
         # in this minimal interval
         assert not np.any(np.isnan(counts.data[:]))
 
         # -- prestim --> only events with t < 0
-        cfg.latency = 'prestim'
+        cfg.latency = "prestim"
         counts = spy.spike_psth(self.spd, cfg)
 
         assert np.all(counts.time[0] <= 0)
 
         # -- poststim --> only events with t > 0
-        cfg.latency = 'poststim'
+        cfg.latency = "poststim"
         counts = spy.spike_psth(self.spd, cfg)
 
         assert np.all(counts.time[0] >= 0)
 
         # -- finally the manual latency interval --
         # this is way to big, so we have many NaNs (empty bins)
-        cfg.latency = [-.5, 1.5]   # in seconds
+        cfg.latency = [-0.5, 1.5]  # in seconds
         assert cfg.latency[0] < trl_starts.min()
         assert cfg.latency[1] > trl_ends.max()
 
         counts = spy.spike_psth(self.spd, cfg)
         # check that histogram time points are less than 1
         # delta_t away from the manual set interval boundaries
         assert np.abs(cfg.latency[0] - counts.time[0][0]) <= delta_t
@@ -169,79 +160,92 @@
         counts = spy.spike_psth(self.spd, cfg)
 
         # check that 3 trials were excluded
         assert len(self.spd.trials) - len(counts.trials) == 3
 
         # setting latency to 'maxperiod' with vartriallen=False
         # excludes all trials which raises an error
-        cfg.latency = 'maxperiod'
-        with pytest.raises(SPYValueError,
-                           match='no trial that completely covers the latency window'):
+        cfg.latency = "maxperiod"
+        with pytest.raises(SPYValueError, match="no trial that completely covers the latency window"):
             counts = spy.spike_psth(self.spd, cfg)
 
         # setting latency to 'minperiod' with vartriallen=False
         # excludes no trials by definition of 'minperiod'
-        cfg.latency = 'minperiod'
+        cfg.latency = "minperiod"
         counts = spy.spike_psth(self.spd, cfg)
         # check that 0 trials were excluded
         assert len(self.spd.trials) - len(counts.trials) == 0
 
     def test_psth_outputs(self):
 
         cfg = spy.StructDict()
-        cfg.latency = 'minperiod'  # to avoid NaNs
-        cfg.output = 'spikecount'
+        cfg.latency = "minperiod"  # to avoid NaNs
+        cfg.output = "spikecount"
         cfg.binsize = 0.1  # in seconds
 
         counts = spy.spike_psth(self.spd, cfg)
 
         # -- plot single trial statistics --
         # single trials have high variance, see below
 
         last_data = np.zeros(counts.time[0].size)
         for chan in counts.channel:
             bars = counts.show(trials=5, channel=chan)
-            ppl.bar(counts.time[0], bars, alpha=0.7, bottom=last_data,
-                    width=0.9 / counts.samplerate, label=chan)
+            ppl.bar(
+                counts.time[0],
+                bars,
+                alpha=0.7,
+                bottom=last_data,
+                width=0.9 / counts.samplerate,
+                label=chan,
+            )
             # for stacking
             last_data += bars
         ppl.legend()
-        ppl.xlabel('time (s)')
-        ppl.ylabel('spike counts')
+        ppl.xlabel("time (s)")
+        ppl.ylabel("spike counts")
 
         # -- plot mean and variance --
 
         # shows that each channel-unit combination
         # has a flat distribution as expected for poisson noise
         # however the absolute intensity differs: we have more and less
         # active channels/units by synthetic data costruction
 
         ppl.figure()
         ppl.title("Trial statistics")
         last_data = np.zeros(len(counts.time[0]))
         for chan in range(len(counts.channel)):
             bars = counts.avg[:, chan]
             yerr = counts.var[:, chan]
-            ppl.bar(counts.time[0], bars, alpha=0.7, bottom=last_data,
-                    width=0.9 / counts.samplerate, label=chan, yerr=yerr, capsize=2)
+            ppl.bar(
+                counts.time[0],
+                bars,
+                alpha=0.7,
+                bottom=last_data,
+                width=0.9 / counts.samplerate,
+                label=chan,
+                yerr=yerr,
+                capsize=2,
+            )
             # for stacking
             last_data += bars
         ppl.legend()
-        ppl.xlabel('time (s)')
-        ppl.ylabel('spike counts')
+        ppl.xlabel("time (s)")
+        ppl.ylabel("spike counts")
 
-        cfg.output = 'rate'  # the default
+        cfg.output = "rate"  # the default
         rates = spy.spike_psth(self.spd, cfg)
 
         # check that the rates are just the counts times samplerate
         assert counts * counts.samplerate == rates
 
         # this gives the spike histogram as normalized density
-        cfg.output = 'proportion'
-        cfg.latency = 'maxperiod'  # to provoke NaNs
+        cfg.output = "proportion"
+        cfg.latency = "maxperiod"  # to provoke NaNs
         spike_densities = spy.spike_psth(self.spd, cfg)
 
         # check that there are NaNs as not all trials have data
         # in this maximal interval (due to start/end randomization)
         assert np.any(np.isnan(spike_densities.data[:]))
 
         # check for one arbitrary trial should be enough
@@ -252,139 +256,134 @@
     def test_psth_exceptions(self):
 
         cfg = spy.StructDict()
 
         # -- output validation --
 
         # invalid string
-        cfg.output = 'counts'
-        with pytest.raises(SPYValueError,
-                           match="expected one of"):
+        cfg.output = "counts"
+        with pytest.raises(SPYValueError, match="expected one of"):
             spy.spike_psth(self.spd, cfg)
 
         # invalid type
         cfg.output = 12
-        with pytest.raises(SPYValueError,
-                           match="expected one of"):
+        with pytest.raises(SPYValueError, match="expected one of"):
             spy.spike_psth(self.spd, cfg)
 
         # -- binsize validation --
 
-        cfg.output = 'rate'
+        cfg.output = "rate"
         # no negative binsizes
         cfg.binsize = -0.2
-        with pytest.raises(SPYValueError,
-                           match="expected value to be greater"):
+        with pytest.raises(SPYValueError, match="expected value to be greater"):
             spy.spike_psth(self.spd, cfg)
 
         cfg.latency = [0, 0.2]
         # binsize larger than time interval
         cfg.binsize = 0.3
-        with pytest.raises(SPYValueError,
-                           match="less or equals 0.2"):
+        with pytest.raises(SPYValueError, match="less or equals 0.2"):
             spy.spike_psth(self.spd, cfg)
 
         # not available rule
-        cfg.binsize = 'sth'
-        with pytest.raises(SPYValueError,
-                           match="expected one of"):
+        cfg.binsize = "sth"
+        with pytest.raises(SPYValueError, match="expected one of"):
             spy.spike_psth(self.spd, cfg)
 
         # -- latency validation --
 
         cfg.binsize = 0.1
         # not available latency
-        cfg.latency = 'sth'
-        with pytest.raises(SPYValueError,
-                           match="expected one of"):
+        cfg.latency = "sth"
+        with pytest.raises(SPYValueError, match="expected one of"):
             spy.spike_psth(self.spd, cfg)
 
         # latency not ordered
         cfg.latency = [0.1, 0]
-        with pytest.raises(SPYValueError,
-                           match="expected start < end"):
+        with pytest.raises(SPYValueError, match="expected start < end"):
             spy.spike_psth(self.spd, cfg)
 
         # latency completely outside of data
         cfg.latency = [-999, -99]
-        with pytest.raises(SPYValueError,
-                           match="expected end of latency window"):
+        with pytest.raises(SPYValueError, match="expected end of latency window"):
             spy.spike_psth(self.spd, cfg)
         cfg.latency = [99, 999]
-        with pytest.raises(SPYValueError,
-                           match="expected start of latency window"):
+        with pytest.raises(SPYValueError, match="expected start of latency window"):
             spy.spike_psth(self.spd, cfg)
 
     def test_psth_chan_unit_mapping(self):
         """
         Test that non-existent channel/unit combinations
         are accounted for correctly
         """
 
         # check that unit 1 really is there
         assert np.any(self.spd.data[:, 2] == 1)
-        counts = spy.spike_psth(self.spd, output='spikecount')
-        assert 'channel0_unit1' in counts.channel
-        assert 'channel1_unit1' in counts.channel
+        counts = spy.spike_psth(self.spd, output="spikecount")
+        assert "channel0_unit1" in counts.channel
+        assert "channel1_unit1" in counts.channel
 
         # get rid of unit 1
         pruned_spd = self.spd.selectdata(unit=[0, 2])
         # check that unit 1 really is gone
         assert np.all(pruned_spd.data[:, 2] != 1)
 
-        pruned_counts = spy.spike_psth(pruned_spd, output='spikecount')
+        pruned_counts = spy.spike_psth(pruned_spd, output="spikecount")
         # check that unit 1 really is gone
         assert len(pruned_counts.channel) < len(counts.channel)
-        assert 'channel0_unit1' not in pruned_counts.channel
-        assert 'channel1_unit1' not in pruned_counts.channel
+        assert "channel0_unit1" not in pruned_counts.channel
+        assert "channel1_unit1" not in pruned_counts.channel
 
         # check that counts for remaining channel/units are unchanged
         for chan in pruned_counts.channel:
-            assert np.array_equal(counts.show(trials=4, channel=chan),
-                                  pruned_counts.show(trials=4, channel=chan),
-                                  equal_nan=True)
+            assert np.array_equal(
+                counts.show(trials=4, channel=chan),
+                pruned_counts.show(trials=4, channel=chan),
+                equal_nan=True,
+            )
 
         # now the same with an active in-place selection
         # Already fixed: #332
         # get rid of unit 1
         # self.spd.selectdata(unit=[0, 2], inplace=True)
 
-        pruned_counts2 = spy.spike_psth(self.spd, output='spikecount', select={'unit': [0, 2]})
+        pruned_counts2 = spy.spike_psth(self.spd, output="spikecount", select={"unit": [0, 2]})
 
         # check that unit 1 really is gone
         assert len(pruned_counts2.channel) < len(counts.channel)
-        assert 'channel0_unit1' not in pruned_counts2.channel
-        assert 'channel1_unit1' not in pruned_counts2.channel
+        assert "channel0_unit1" not in pruned_counts2.channel
+        assert "channel1_unit1" not in pruned_counts2.channel
         # check that counts for remaining channel/units are unchanged
         for chan in pruned_counts2.channel:
-            assert np.array_equal(counts.show(trials=4, channel=chan),
-                                  pruned_counts2.show(trials=4, channel=chan),
-                                  equal_nan=True)
+            assert np.array_equal(
+                counts.show(trials=4, channel=chan),
+                pruned_counts2.show(trials=4, channel=chan),
+                equal_nan=True,
+            )
 
     def test_parallel_selection(self, testcluster):
 
         cfg = spy.StructDict()
-        cfg.latency = 'minperiod'
+        cfg.latency = "minperiod"
         cfg.parallel = True
 
         client = dd.Client(testcluster)
 
         # test standard run
         counts = spy.spike_psth(self.spd, cfg)
         # check that there are NO NaNs as all trials
         # have data in `minperiod` by definition
         assert not np.any(np.isnan(counts.data[:]))
 
         # test channel selection
-        cfg.select = {'channel': 0}
+        cfg.select = {"channel": 0}
         counts = spy.spike_psth(self.spd, cfg)
-        assert all(['channel1' not in chan for chan in counts.channel])
+        assert all(["channel1" not in chan for chan in counts.channel])
 
         client.close()
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestPSTH()
     spd = T1.spd
     trl0 = spd.trials[0]
-    spd.selectdata(unit=[0,2], inplace=True)
+    spd.selectdata(unit=[0, 2], inplace=True)
     arr1 = spd.selection._get_trial(1)
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_spyio.py` & `esi_syncopy-2023.7/syncopy/tests/test_spyio.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,79 +8,88 @@
 import tempfile
 import shutil
 import h5py
 import time
 import pytest
 import numpy as np
 from glob import glob
-import matplotlib.pyplot as ppl
 
 # Local imports
-import syncopy as spy
 from syncopy.datatype import AnalogData
-from syncopy.io import save, load, load_ft_raw, load_tdt, load_nwb
+from syncopy.io import save, load, load_ft_raw, load_tdt
 from syncopy.shared.filetypes import FILE_EXT
-from syncopy.shared.errors import (
-    SPYValueError,
-    SPYIOError,
-    SPYError,
-    SPYTypeError
-)
+from syncopy.shared.errors import SPYValueError, SPYIOError, SPYError, SPYTypeError
 import syncopy.datatype as swd
 from syncopy.tests.misc import generate_artificial_data
+from syncopy import __pynwb__
 
+skip_no_pynwb = pytest.mark.skipif(
+    not __pynwb__, reason=f"This test requires the 'pynwb' package to be installed."
+)
 
 
 # Decorator to detect if test data dir is available
-on_esi = os.path.isdir('/cs/slurm/syncopy')
+on_esi = os.path.isdir("/cs/slurm/syncopy")
 skip_no_esi = pytest.mark.skipif(not on_esi, reason="ESI fs not available")
-skip_no_nwb = pytest.mark.skipif(not spy.__nwb__, reason="pynwb not installed")
 
 
-class TestSpyIO():
+class TestSpyIO:
 
     # Allocate test-datasets for AnalogData, SpectralData, SpikeData and EventData objects
     nc = 10
     ns = 30
     nt = 5
     nf = 15
     nd = 50
     data = {}
     trl = {}
 
     # Generate 2D array simulating an AnalogData array
     data["AnalogData"] = np.arange(1, nc * ns + 1).reshape(ns, nc)
-    trl["AnalogData"] = np.vstack([np.arange(0, ns, 5),
-                                   np.arange(5, ns + 5, 5),
-                                   np.ones((int(ns / 5), )),
-                                   np.ones((int(ns / 5), )) * np.pi]).T
+    trl["AnalogData"] = np.vstack(
+        [
+            np.arange(0, ns, 5),
+            np.arange(5, ns + 5, 5),
+            np.ones((int(ns / 5),)),
+            np.ones((int(ns / 5),)) * np.pi,
+        ]
+    ).T
 
     # Generate a 4D array simulating a SpectralData array
     data["SpectralData"] = np.arange(1, nc * ns * nt * nf + 1).reshape(ns, nt, nc, nf)
     trl["SpectralData"] = trl["AnalogData"]
 
     # Generate a 4D array simulating a CorssSpectralData array
     data["CrossSpectralData"] = np.arange(1, nc * nc * ns * nf + 1).reshape(ns, nf, nc, nc)
     trl["CrossSpectralData"] = trl["AnalogData"]
 
     # Use a fixed random number generator seed to simulate a 2D SpikeData array
     seed = np.random.RandomState(13)
-    data["SpikeData"] = np.vstack([seed.choice(ns, size=nd),
-                                   seed.choice(nc, size=nd),
-                                   seed.choice(int(nc / 2), size=nd)]).T.astype(int)
+    data["SpikeData"] = np.vstack(
+        [
+            seed.choice(ns, size=nd),
+            seed.choice(nc, size=nd),
+            seed.choice(int(nc / 2), size=nd),
+        ]
+    ).T.astype(int)
     trl["SpikeData"] = trl["AnalogData"]
 
     # Generate bogus trigger timings
-    data["EventData"] = np.vstack([np.arange(0, ns, 5),
-                                   np.zeros((int(ns / 5), ))]).T.astype(int)
+    data["EventData"] = np.vstack([np.arange(0, ns, 5), np.zeros((int(ns / 5),))]).T.astype(int)
     data["EventData"][1::2, 1] = 1
     trl["EventData"] = trl["AnalogData"]
 
     # Define data classes to be used in tests below
-    classes = ["AnalogData", "SpectralData", "CrossSpectralData", "SpikeData", "EventData"]
+    classes = [
+        "AnalogData",
+        "SpectralData",
+        "CrossSpectralData",
+        "SpikeData",
+        "EventData",
+    ]
 
     # Test correct handling of object log
     def test_logging(self):
         with tempfile.TemporaryDirectory() as tdir:
             fname = os.path.join(tdir, "dummy")
             dummy = generate_artificial_data(inmemory=True)
             ldum = len(dummy._log)
@@ -180,15 +189,20 @@
                 # container with extension and tag
                 container = "test_container.spy"
                 tag = "sometag"
                 save(dummy, container=os.path.join(tdir, container), tag=tag)
                 assert len(glob(os.path.join(tdir, container, "test_container_sometag*"))) == 2
 
                 # explicit overwrite
-                save(dummy, container=os.path.join(tdir, container), tag=tag, overwrite=True)
+                save(
+                    dummy,
+                    container=os.path.join(tdir, container),
+                    tag=tag,
+                    overwrite=True,
+                )
                 assert len(glob(os.path.join(tdir, container, "test_container_sometag*"))) == 2
 
                 # implicit overwrite
                 dummy.save()
                 assert len(glob(os.path.join(tdir, container, "test_container_sometag*"))) == 2
 
                 # attempted overwrite w/o keyword
@@ -229,37 +243,41 @@
                 dummy.save(container=os.path.join(tdir, container))
                 dummy2 = load(os.path.join(tdir, container))
                 for attr in ["data", "sampleinfo", "trialinfo"]:
                     assert np.array_equal(getattr(dummy, attr), getattr(dummy2, attr))
                 del dummy2
 
                 # load single file via dataclass
-                dummy2 = load(os.path.join(tdir, container),
-                              dataclass=dummy._classname_to_extension())
+                dummy2 = load(
+                    os.path.join(tdir, container),
+                    dataclass=dummy._classname_to_extension(),
+                )
 
                 # save and load single file via tag
                 container2 = "another_single_container_" + dummy._classname_to_extension()[1:]
                 dummy2.save(container=os.path.join(tdir, container2), tag="sometag")
                 dummy3 = load(os.path.join(tdir, container2), tag="sometag")
                 for attr in ["data", "sampleinfo", "trialinfo"]:
                     assert np.array_equal(getattr(dummy2, attr), getattr(dummy3, attr))
                 del dummy, dummy2, dummy3
 
                 # tag mismatch in single-file container
                 with pytest.raises(SPYIOError):
                     load(os.path.join(tdir, container2), tag="invalid")
 
                 # dataclass mismatch in single-file container
-                wrong_ext = getattr(swd, list(set(self.classes).difference([dclass]))[0])()._classname_to_extension()
+                wrong_ext = getattr(
+                    swd, list(set(self.classes).difference([dclass]))[0]
+                )()._classname_to_extension()
                 with pytest.raises(SPYIOError):
                     load(os.path.join(tdir, container), dataclass=wrong_ext)
 
                 # invalid dataclass specification
                 with pytest.raises(SPYValueError):
-                    load(os.path.join(tdir, container), dataclass='.invalid')
+                    load(os.path.join(tdir, container), dataclass=".invalid")
 
     # Test saving multiple objects to the same container
     def test_multi_saveload(self):
         with tempfile.TemporaryDirectory() as tdir:
             for dk, dclass in enumerate(self.classes):
                 dummy = getattr(swd, dclass)(self.data[dclass], samplerate=1000)
                 dummy.trialdefintion = self.trl[dclass]
@@ -275,67 +293,80 @@
                         load(os.path.join(tdir, container), dataclass=not_yet)
 
                 # try to load everything but the first class at the beginning
                 if dk == 0:
                     ext_list = []
                     for attr in self.classes[1:]:
                         ext_list.append(getattr(swd, attr)()._classname_to_extension())
-                    fname = os.path.join(os.path.join(tdir, container + FILE_EXT["dir"]),
-                                         container + dummy._classname_to_extension())
+                    fname = os.path.join(
+                        os.path.join(tdir, container + FILE_EXT["dir"]),
+                        container + dummy._classname_to_extension(),
+                    )
                     with pytest.raises(SPYValueError):
                         load(fname, dataclass=ext_list)
 
                 del dummy
 
             # load all files created above
             container = "multi_container"
             objDict = load(os.path.join(tdir, container))
             assert len(objDict.keys()) == len(self.classes)
             all_ext = []
             for attr in self.classes:
                 all_ext.append(getattr(swd, attr)()._classname_to_extension())
             fnameList = []
             for ext in all_ext:
-                fnameList.append(os.path.join(os.path.join(tdir, container + FILE_EXT["dir"]),
-                                              container + ext))
+                fnameList.append(
+                    os.path.join(os.path.join(tdir, container + FILE_EXT["dir"]), container + ext)
+                )
             for name, obj in objDict.items():
                 assert obj.filename in fnameList
                 fname = fnameList.pop(fnameList.index(obj.filename))
                 assert name in fname
             assert len(fnameList) == 0
             del objDict
 
             # load single file from joint container via dataclass
             dummy = load(os.path.join(tdir, container), dataclass="analog")
-            assert dummy.filename == os.path.join(os.path.join(tdir, container + FILE_EXT["dir"]),
-                                                  container + ".analog")
+            assert dummy.filename == os.path.join(
+                os.path.join(tdir, container + FILE_EXT["dir"]), container + ".analog"
+            )
             dummy.save(tag="2ndanalog")
             del dummy
 
             # load single file from joint container using tag
             dummy = load(os.path.join(tdir, container), tag="2ndanalog")
             dummy.save(tag="3rdanalog")
             del dummy
 
             # load single file from joint container using dataclass and tag
             dummy = load(os.path.join(tdir, container), dataclass="analog", tag="3rdanalog")
 
             # load single file from joint container using multiple dataclasses and single tag
-            dummy2 = load(os.path.join(tdir, container), dataclass=["analog", "spectral"],
-                          tag="3rdanalog")
+            dummy2 = load(
+                os.path.join(tdir, container),
+                dataclass=["analog", "spectral"],
+                tag="3rdanalog",
+            )
             assert dummy2.filename == dummy.filename
 
             # load single file from joint container using single dataclass and multiple tags
-            dummy3 = load(os.path.join(tdir, container), dataclass="analog",
-                          tag=["3rdanalog", "invalid"])
+            dummy3 = load(
+                os.path.join(tdir, container),
+                dataclass="analog",
+                tag=["3rdanalog", "invalid"],
+            )
             assert dummy3.filename == dummy.filename
 
             # load single file from joint container using multiple dataclasses and tags
-            dummy4 = load(os.path.join(tdir, container), dataclass=["analog", "spectral"],
-                          tag=["3rdanalog", "invalid"])
+            dummy4 = load(
+                os.path.join(tdir, container),
+                dataclass=["analog", "spectral"],
+                tag=["3rdanalog", "invalid"],
+            )
             assert dummy4.filename == dummy.filename
             del dummy, dummy2, dummy3, dummy4
 
             # load multiple files from joint container using single tag
             objDict = load(os.path.join(tdir, container), tag="analog")
             assert len(objDict.keys()) == 2
             wanted = ["2nd", "3rd"]
@@ -369,176 +400,200 @@
             wanted = ["spectral", "spike"]
             objDict = load(os.path.join(tdir, container), dataclass=wanted)
             assert len(wanted) == len(objDict.keys())
             for ext in wanted:
                 basename = container + "." + ext
                 assert basename in objDict.keys()
                 fname = objDict[basename].filename
-                assert fname == os.path.join(os.path.join(tdir, container + FILE_EXT["dir"]),
-                                             basename)
+                assert fname == os.path.join(os.path.join(tdir, container + FILE_EXT["dir"]), basename)
             del objDict
 
             # load multiple files from joint container via multiple dataclasses and single tag
-            objDict = load(os.path.join(tdir, container), tag="analog",
-                           dataclass=["analog", "analog"])
+            objDict = load(
+                os.path.join(tdir, container),
+                tag="analog",
+                dataclass=["analog", "analog"],
+            )
             wanted = ["2nd", "3rd"]
             for name, obj in objDict.items():
                 assert isinstance(obj, AnalogData)
                 inWanted = [tag in name for tag in wanted]
                 assert any(inWanted)
                 inWanted.pop(inWanted.index(True))
             del objDict
 
             # load multiple files from joint container via single dataclass and multiple tags
-            objDict = load(os.path.join(tdir, container), tag=["2nd", "3rd"],
-                           dataclass="analog")
+            objDict = load(os.path.join(tdir, container), tag=["2nd", "3rd"], dataclass="analog")
             wanted = ["2nd", "3rd"]
             for name, obj in objDict.items():
                 assert isinstance(obj, AnalogData)
                 inWanted = [tag in name for tag in wanted]
                 assert any(inWanted)
                 inWanted.pop(inWanted.index(True))
             del objDict
 
             # load multiple files from joint container via multiple dataclasses and tags
-            objDict = load(os.path.join(tdir, container), tag=["2nd", "3rd"],
-                           dataclass=["analog", "analog"])
+            objDict = load(
+                os.path.join(tdir, container),
+                tag=["2nd", "3rd"],
+                dataclass=["analog", "analog"],
+            )
             wanted = ["2nd", "3rd"]
             for name, obj in objDict.items():
                 assert isinstance(obj, AnalogData)
                 inWanted = [tag in name for tag in wanted]
                 assert any(inWanted)
                 inWanted.pop(inWanted.index(True))
             del obj, objDict
 
             # invalid combinations of tag/dataclass
             with pytest.raises(SPYIOError):
                 load(os.path.join(tdir, container), dataclass="analog", tag="invalid")
             with pytest.raises(SPYIOError):
                 load(os.path.join(tdir, container), dataclass="spike", tag="2nd")
             with pytest.raises(SPYIOError):
-                load(os.path.join(tdir, container), dataclass=["analog", "spike"],
-                     tag="invalid")
+                load(
+                    os.path.join(tdir, container),
+                    dataclass=["analog", "spike"],
+                    tag="invalid",
+                )
             with pytest.raises(SPYIOError):
-                load(os.path.join(tdir, container), dataclass=["spike", "invalid"],
-                     tag="2nd")
+                load(
+                    os.path.join(tdir, container),
+                    dataclass=["spike", "invalid"],
+                    tag="2nd",
+                )
             with pytest.raises(SPYIOError):
-                load(os.path.join(tdir, container), dataclass="analog",
-                     tag=["invalid", "stillinvalid"])
+                load(
+                    os.path.join(tdir, container),
+                    dataclass="analog",
+                    tag=["invalid", "stillinvalid"],
+                )
             with pytest.raises(SPYIOError):
-                load(os.path.join(tdir, container), dataclass="spike",
-                     tag=["2nd", "3rd"])
+                load(os.path.join(tdir, container), dataclass="spike", tag=["2nd", "3rd"])
             with pytest.raises(SPYIOError):
-                load(os.path.join(tdir, container), dataclass=["spike", "event"],
-                     tag=["2nd", "3rd"])
+                load(
+                    os.path.join(tdir, container),
+                    dataclass=["spike", "event"],
+                    tag=["2nd", "3rd"],
+                )
             with pytest.raises(SPYIOError):
-                load(os.path.join(tdir, container), dataclass=["analog", "analog"],
-                     tag=["invalid", "stillinvalid"])
+                load(
+                    os.path.join(tdir, container),
+                    dataclass=["analog", "analog"],
+                    tag=["invalid", "stillinvalid"],
+                )
             with pytest.raises(SPYIOError):
-                load(os.path.join(tdir, container), dataclass=["spike", "event"],
-                     tag=["invalid", "stillinvalid"])
+                load(
+                    os.path.join(tdir, container),
+                    dataclass=["spike", "event"],
+                    tag=["invalid", "stillinvalid"],
+                )
             with pytest.raises(SPYValueError):
                 load(os.path.join(tdir, container), dataclass="invalid", tag="2nd")
             with pytest.raises(SPYValueError):
-                load(os.path.join(tdir, container), dataclass=["invalid", "stillinvalid"],
-                     tag="2nd")
+                load(
+                    os.path.join(tdir, container),
+                    dataclass=["invalid", "stillinvalid"],
+                    tag="2nd",
+                )
 
 
 @skip_no_esi
 class TestFTImporter:
 
     """At the moment only ft_datatype_raw is supported"""
 
-    mat_file_dir = '/cs/slurm/syncopy/MAT-Files'
+    mat_file_dir = "/cs/slurm/syncopy/MAT-Files"
 
     def test_read_hdf(self):
         """Test MAT-File v73 reader, uses h5py"""
 
-        mat_name = 'matdata-v73.mat'
+        mat_name = "matdata-v73.mat"
         fname = os.path.join(self.mat_file_dir, mat_name)
 
         dct = load_ft_raw(fname)
-        assert 'Data_K' in dct
-        AData = dct['Data_K']
+        assert "Data_K" in dct
+        AData = dct["Data_K"]
 
         assert isinstance(AData, AnalogData)
         assert len(AData.trials) == 393
         assert len(AData.channel) == 218
 
         # list only structure names
         slist = load_ft_raw(fname, list_only=True)
-        assert 'Data_K' in slist
-        assert 'Data_KB' in slist
+        assert "Data_K" in slist
+        assert "Data_KB" in slist
 
         # additional fields of Matlab structures
         # get attached to .info dict
         # hdf reader does NOT support nested fields
-        dct = load_ft_raw(fname, include_fields=('chV1',))
-        AData2 = dct['Data_K']
-        assert 'chV1' in AData2.info
-        assert len(AData2.info['chV1']) == 30
-        assert isinstance(AData2.info['chV1'][0], str)
+        dct = load_ft_raw(fname, include_fields=("chV1",))
+        AData2 = dct["Data_K"]
+        assert "chV1" in AData2.info
+        assert len(AData2.info["chV1"]) == 30
+        assert isinstance(AData2.info["chV1"][0], str)
 
         # test loading a subset of structures
-        dct = load_ft_raw(fname, select_structures=('Data_KB',))
-        assert 'Data_KB' in dct
-        assert 'Data_K' not in dct
+        dct = load_ft_raw(fname, select_structures=("Data_KB",))
+        assert "Data_KB" in dct
+        assert "Data_K" not in dct
 
         # test str sequence parsing
         try:
-            dct = load_ft_raw(fname, select_structures=(3, 'sth'))
+            dct = load_ft_raw(fname, select_structures=(3, "sth"))
         except SPYTypeError as err:
-            assert 'expected str found int' in str(err)
+            assert "expected str found int" in str(err)
 
         try:
-            dct = load_ft_raw(fname, include_fields=(3, 'sth'))
+            dct = load_ft_raw(fname, include_fields=(3, "sth"))
         except SPYTypeError as err:
-            assert 'expected str found int' in str(err)
+            assert "expected str found int" in str(err)
 
     def test_read_dict(self):
         """Test MAT-File v7 reader, based on scipy.io.loadmat"""
 
-        mat_name = 'matdataK-v7.mat'
+        mat_name = "matdataK-v7.mat"
         fname = os.path.join(self.mat_file_dir, mat_name)
 
         dct = load_ft_raw(fname)
-        assert 'Data_K' in dct
-        AData = dct['Data_K']
+        assert "Data_K" in dct
+        AData = dct["Data_K"]
 
         assert isinstance(AData, AnalogData)
         assert len(AData.trials) == 393
         assert len(AData.channel) == 218
 
         slist = load_ft_raw(fname, list_only=True)
-        assert 'Data_K' in slist
+        assert "Data_K" in slist
 
         # additional fields of Matlab structures
         # get attached to .info dict
         # here nested structures are also now forbidden
-        dct = load_ft_raw(fname, include_fields=('ch',))
-        AData2 = dct['Data_K']
+        dct = load_ft_raw(fname, include_fields=("ch",))
+        AData2 = dct["Data_K"]
         # sadly here it is actually nested
         assert len(AData2.info) == 0
 
 
 @skip_no_esi
 class TestTDTImporter:
 
-    tdt_dir = '/cs/slurm/syncopy/Tdt_reader/session-25'
+    tdt_dir = "/cs/slurm/syncopy/Tdt_reader/session-25"
     start_code, end_code = 23000, 30020
 
     def test_load_tdt(self):
 
         AData = load_tdt(self.tdt_dir)
 
         assert isinstance(AData, AnalogData)
         # check meta info parsing
         assert len(AData.info.keys()) == 13
         # that is apparently fixed
-        assert AData.dimord == ['time', 'channel']
+        assert AData.dimord == ["time", "channel"]
         assert len(AData.channel) == 9
 
         # it's only one big trial here
         assert AData.trials[0].shape == (3170560, 9)
 
         # test median subtr
         AData2 = load_tdt(self.tdt_dir, subtract_median=True)
@@ -549,72 +604,27 @@
 
         # test automatic trialdefinition
         AData = load_tdt(self.tdt_dir, self.start_code, self.end_code)
         assert len(AData.trials) == 659
 
     def test_exceptions(self):
 
-        with pytest.raises(SPYIOError, match='Cannot read'):
-            load_tdt('non/existing/path')
+        with pytest.raises(SPYIOError, match="Cannot read"):
+            load_tdt("non/existing/path")
 
-        with pytest.raises(SPYValueError, match='Invalid value of `start_code`'):
+        with pytest.raises(SPYValueError, match="Invalid value of `start_code`"):
             load_tdt(self.tdt_dir, start_code=None, end_code=self.end_code)
 
-        with pytest.raises(SPYValueError, match='Invalid value of `end_code`'):
+        with pytest.raises(SPYValueError, match="Invalid value of `end_code`"):
             load_tdt(self.tdt_dir, start_code=self.start_code, end_code=None)
 
-        with pytest.raises(SPYValueError, match='Invalid value of `start_code`'):
+        with pytest.raises(SPYValueError, match="Invalid value of `start_code`"):
             load_tdt(self.tdt_dir, start_code=999999, end_code=self.end_code)
 
-        with pytest.raises(SPYValueError, match='Invalid value of `end_code`'):
+        with pytest.raises(SPYValueError, match="Invalid value of `end_code`"):
             load_tdt(self.tdt_dir, start_code=self.start_code, end_code=999999)
 
 
-@skip_no_esi
-@skip_no_nwb
-class TestNWBImporter:
-
-    nwb_filename = '/cs/slurm/syncopy/NWBdata/test.nwb'
-
-    def test_load_nwb(self):
-
-        spy_filename = self.nwb_filename.split('/')[-1][:-4] + '.spy'
-        out = load_nwb(self.nwb_filename, memuse=2000)
-        edata, adata1, adata2 = list(out.values())
-
-        assert isinstance(adata2, spy.AnalogData)
-        assert isinstance(edata, spy.EventData)
-        assert np.any(~np.isnan(adata2.data))
-        assert np.any(adata2.data != 0)
-
-        snippet = adata2.selectdata(latency=[30, 32])
-
-        snippet.singlepanelplot(latency=[30, 30.3], channel=3)
-        ppl.gcf().suptitle('raw data')
-
-        # Bandpass filter
-        lfp = spy.preprocessing(snippet, filter_class='but', freq=[10, 100],
-                                filter_type='bp', order=8)
-
-        # Downsample
-        lfp = spy.resampledata(lfp, resamplefs=2000, method='downsample')
-        lfp.info = adata2.info
-        lfp.singlepanelplot(channel=3)
-        ppl.gcf().suptitle('bp-filtered 10-100Hz and resampled')
-
-        spec = spy.freqanalysis(lfp, foilim=[5, 150])
-        spec.singlepanelplot(channel=[1, 3])
-        ppl.gcf().suptitle('bp-filtered 10-100Hz and resampled')
-
-        # test save and load
-        with tempfile.TemporaryDirectory() as tdir:
-            lfp.save(os.path.join(tdir, spy_filename))
-            lfp2 = spy.load(os.path.join(tdir, spy_filename))
-
-            assert np.allclose(lfp.data, lfp2.data)
-
-
-if __name__ == '__main__':
+if __name__ == "__main__":
     T0 = TestSpyIO()
     T1 = TestFTImporter()
     T2 = TestTDTImporter()
-    T3 = TestNWBImporter()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_spytools.py` & `esi_syncopy-2023.7/syncopy/tests/test_spytools.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,67 +1,75 @@
 # -*- coding: utf-8 -*-
-# 
+#
 # Ensure tooling functions shared across the package work as intended
-# 
+#
 
 # Builtin/3rd party package imports
 import numpy as np
 import pytest
 
 # Local imports
 from syncopy.shared.tools import best_match
 from syncopy.shared.errors import SPYValueError
 
 
-class TestBestMatch():
+class TestBestMatch:
 
     # Source-arrays with integer elements
     intSource = np.arange(10)
     randIntSource = np.random.choice(intSource, size=intSource.size, replace=False)
 
     # Source-arrays with floating point elements
     floatSource = np.array([1.5, 1.5, 2.2, 6.2, 8.8])
     randFloatSource = np.random.choice(floatSource, size=floatSource.size, replace=False)
 
     # Selections defined by ordered/unordered int/float arrays
     intSelection = intSource[:4]
     randIntSelection = np.random.choice(intSelection, size=intSelection.size, replace=False)
-    floatSelection = np.array([1.9, 9., 1., -0.4, 1.2, 0.2, 9.3])
+    floatSelection = np.array([1.9, 9.0, 1.0, -0.4, 1.2, 0.2, 9.3])
     sortFloatSelection = np.sort(floatSelection)
-    
+
     def test_intsource(self):
-        
+
         for source in [self.intSource, self.randIntSource]:
-            for selection in [self.intSelection, self.randIntSelection, 
-                              self.floatSelection, self.sortFloatSelection]:
+            for selection in [
+                self.intSelection,
+                self.randIntSelection,
+                self.floatSelection,
+                self.sortFloatSelection,
+            ]:
                 expectedVal = np.round(selection)
                 expectedIdx = np.array([np.where(source == elem)[0][0] for elem in expectedVal])
                 val, idx = best_match(source, selection)
                 assert np.array_equal(val, expectedVal)
                 assert np.array_equal(idx, expectedIdx)
-                
+
                 val, idx = best_match(source, selection, squash_duplicates=True)
                 _, sidx = np.unique(expectedVal, return_index=True)
                 sidx.sort()
                 assert np.array_equal(val, expectedVal[sidx])
                 assert np.array_equal(idx, expectedIdx[sidx])
-                
+
                 with pytest.raises(SPYValueError):
                     best_match(source, selection, tol=1e-6)
-                
+
                 val, idx = best_match(source, [selection.min(), selection.max()], span=True)
-                expectedVal = np.array([elem for elem in source 
-                                        if selection.min() <= elem <= selection.max()])
+                expectedVal = np.array(
+                    [elem for elem in source if selection.min() <= elem <= selection.max()]
+                )
                 expectedIdx = np.array([np.where(source == elem)[0][0] for elem in expectedVal])
-        
 
     def test_floatsource(self):
         for source in [self.floatSource, self.randFloatSource]:
-            for selection in [self.intSelection, self.randIntSelection, 
-                              self.floatSelection, self.sortFloatSelection]:
+            for selection in [
+                self.intSelection,
+                self.randIntSelection,
+                self.floatSelection,
+                self.sortFloatSelection,
+            ]:
                 expectedVal = np.array([source[np.argmin(np.abs(source - elem))] for elem in selection])
                 expectedIdx = np.array([np.where(source == elem)[0][0] for elem in expectedVal])
                 val, idx = best_match(source, selection)
                 assert np.array_equal(val, expectedVal)
                 assert np.array_equal(idx, expectedIdx)
 
                 val, idx = best_match(source, selection, squash_duplicates=True)
@@ -70,11 +78,11 @@
                 assert np.array_equal(val, expectedVal[sidx])
                 assert np.array_equal(idx, expectedIdx[sidx])
 
                 with pytest.raises(SPYValueError):
                     best_match(source, selection, tol=1e-6)
 
                 val, idx = best_match(source, [selection.min(), selection.max()], span=True)
-                expectedVal = np.array([elem for elem in source 
-                                        if selection.min() <= elem <= selection.max()])
+                expectedVal = np.array(
+                    [elem for elem in source if selection.min() <= elem <= selection.max()]
+                )
                 expectedIdx = np.array([np.where(source == elem)[0][0] for elem in expectedVal])
-
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_statistics.py` & `esi_syncopy-2023.7/syncopy/tests/test_statistics.py`

 * *Files 5% similar despite different names*

```diff
@@ -23,15 +23,15 @@
 
 class TestSumStatistics:
 
     # initialize rng instance
     rng = np.random.default_rng(helpers.test_seed)
 
     # lognormal distribution parameters
-    mu, sigma = 2, .5
+    mu, sigma = 2, 0.5
 
     nTrials = 4
     nChannels = 3
     nSamples = 10
     nFreq = 10
     nTaper = 2
 
@@ -63,291 +63,281 @@
                 # -- test average --
 
                 # top-level function
                 spy_res1 = spy.mean(spy_data, dim=dim)
                 # check only one trial
                 npy_res = np.mean(spy_data.trials[test_trial], axis=axis)
 
-                self._check_trial('mean', dim, spy_res1, npy_res, trial=test_trial)
+                self._check_trial("mean", dim, spy_res1, npy_res, trial=test_trial)
 
                 # --- test variance ---
 
                 # top-level function
                 spy_res1 = spy.var(spy_data, dim=dim)
                 # check only one trial
                 npy_res = np.var(spy_data.trials[test_trial], axis=axis)
 
-                self._check_trial('var', dim, spy_res1, npy_res, trial=test_trial)
+                self._check_trial("var", dim, spy_res1, npy_res, trial=test_trial)
 
                 # --- test standard deviation ---
 
                 # top-level function
                 spy_res1 = spy.std(spy_data, dim=dim)
                 # check only one trial
                 npy_res = np.std(spy_data.trials[test_trial], axis=axis)
 
-                self._check_trial('std', dim, spy_res1, npy_res, trial=test_trial)
+                self._check_trial("std", dim, spy_res1, npy_res, trial=test_trial)
 
                 # --- test median ---
 
                 # top-level function
                 spy_res1 = spy.median(spy_data, dim=dim)
                 # check only one trial
                 npy_res = np.median(spy_data.trials[test_trial], axis=axis)
 
-                self._check_trial('median', dim, spy_res1, npy_res, trial=test_trial)
+                self._check_trial("median", dim, spy_res1, npy_res, trial=test_trial)
 
     def _check_trial(self, operation, dim, spy_res, npy_res, trial=1):
         """
         Test that direct numpy stats give the same results
         for a single trial
         """
 
         # show returns list of trials, pick only one
         # show also squeezes out the singleton dimension
         # which remains after the statistic got computed!
         trial_result_spy = spy_res.show()[trial]
         assert np.allclose(trial_result_spy, npy_res)
 
         # check the dimension label was set to the statistical operation
-        if dim not in ['freq', 'time']:
+        if dim not in ["freq", "time"]:
             assert getattr(spy_res, dim) == operation
         # numerical dimension labels get set to 0 (axis is gone)
-        elif dim == 'time':
+        elif dim == "time":
             assert spy_res.time[trial] == 0
-        elif dim == 'freq':
+        elif dim == "freq":
             assert spy_res.freq == 0
 
     def test_trial_statistics(self):
         """
         Test statistics over trials
         """
 
         # --- test statistics trial average against CR trial average ---
 
         spec = spy.freqanalysis(self.adata, keeptrials=True)
         # trigger trial average after spectral estimation
-        spec1 = spy.mean(spec, dim='trials')
+        spec1 = spy.mean(spec, dim="trials")
 
         # trial average via CR keeptrials
         spec2 = spy.freqanalysis(self.adata, keeptrials=False)
 
         assert len(spec1.trials) == 1
         assert np.allclose(spec1.data, spec2.data)
 
         # --- test trial var and std ---
         for data in self.data_types:
 
-            spy_var = spy.var(data, dim='trials')
+            spy_var = spy.var(data, dim="trials")
 
             # reshape to get rid of trial stacking along time axis
             # array has shape (nTrials, nSamples, ..rest-of-dims..)
             arr = data.data[()].reshape(self.nTrials, self.nSamples, *data.data.shape[1:])
             # now compute directly over the trial axis
             npy_var = np.var(arr, axis=0)
 
             assert len(spy_var.trials) == 1
             assert np.allclose(npy_var, spy_var.data)
 
-            spy_std = spy.std(data, dim='trials')
+            spy_std = spy.std(data, dim="trials")
 
             # reshape to get rid of trial stacking along time axis
             # array has shape (nTrials, nSamples, ..rest-of-dims..)
             arr = data.data[()].reshape(self.nTrials, self.nSamples, *data.data.shape[1:])
             # now compute directly over the trial axis
             npy_std = np.std(arr, axis=0)
 
             assert len(spy_var.trials) == 1
             assert np.allclose(npy_std, spy_std.data)
 
     def test_selections(self):
 
         # got 10 samples with 1s samplerate,so time is [-1, ..., 8]
-        sdict1 = {'trials': [1, 3], 'latency': [2, 6]}
-        res = spy.mean(self.adata, dim='channel', select=sdict1)
+        sdict1 = {"trials": [1, 3], "latency": [2, 6]}
+        res = spy.mean(self.adata, dim="channel", select=sdict1)
         assert len(res.trials) == 2
         assert self.adata.time[0].min() == -1
         assert res.time[0].min() == 2
         assert self.adata.time[0].max() == 8
         assert res.time[0].max() == 6
 
         # freq axis is [0, ..., 9]
-        sdict2 = {'channel': [0, 2], 'frequency': [1, 5]}
-        res = spy.var(self.spec_data, dim='trials', select=sdict2)
-        assert np.all(res.channel == np.array(['channel1', 'channel3']))
+        sdict2 = {"channel": [0, 2], "frequency": [1, 5]}
+        res = spy.var(self.spec_data, dim="trials", select=sdict2)
+        assert np.all(res.channel == np.array(["channel1", "channel3"]))
         assert np.all(res.freq == np.arange(1, 6))
 
         # check at least a few times that the statistics are indeed
         # computed correctly on the trimmed down data
-        sdict3 = {'trials': [1, 3]}
-        res = spy.mean(self.crossspec_data, dim='trials', select=sdict3)
+        sdict3 = {"trials": [1, 3]}
+        res = spy.mean(self.crossspec_data, dim="trials", select=sdict3)
         # reshape to extract trial separated arrays
-        arr = self.crossspec_data.data[()].reshape(self.nTrials, self.nSamples,
-                                                   self.nFreq, self.nChannels, self.nChannels)
+        arr = self.crossspec_data.data[()].reshape(
+            self.nTrials, self.nSamples, self.nFreq, self.nChannels, self.nChannels
+        )
         # now cut out the same 2 trials and average
         npy_res = arr[1::2].mean(axis=0)
         assert np.allclose(npy_res, res.data)
 
-        sdict4 = {'channel': [0, 2]}
-        res = spy.mean(self.spec_data, dim='channel', select=sdict4)
+        sdict4 = {"channel": [0, 2]}
+        res = spy.mean(self.spec_data, dim="channel", select=sdict4)
         # now cut out the same 2 channels and average, dimord is (time, taper, freq, channel)
         npy_res = self.spec_data.data[..., ::2].mean(axis=-1)
         # check only 1st trial
-        assert np.allclose(npy_res[:self.nSamples], res.show(trials=0))
+        assert np.allclose(npy_res[: self.nSamples], res.show(trials=0))
 
         # one last time for the freq axis
-        sdict5 = {'frequency': [1, 4]}
-        res = spy.median(self.spec_data, dim='freq', select=sdict5)
+        sdict5 = {"frequency": [1, 4]}
+        res = spy.median(self.spec_data, dim="freq", select=sdict5)
         # cut out same frequencies directly from the dataset array
         npy_res = np.median(self.spec_data.data[..., 1:5, :], axis=2)
         # check only 2nd trial
-        assert np.allclose(npy_res[self.nSamples:2 * self.nSamples], res.show(trials=1))
+        assert np.allclose(npy_res[self.nSamples : 2 * self.nSamples], res.show(trials=1))
 
     def test_exceptions(self):
 
         with pytest.raises(SPYValueError) as err:
-            spy.mean(self.adata, dim='sth')
+            spy.mean(self.adata, dim="sth")
         assert "expected one of ['time', 'channel']" in str(err.value)
 
         # unequal trials and trial average can't work
         with pytest.raises(SPYValueError) as err:
             # to not screw sth up
             adata_cpy = spy.copy(self.adata)
             trldef = adata_cpy.trialdefinition
             trldef[2] = [21, 25, -1]
             adata_cpy.trialdefinition = trldef
-            spy.mean(adata_cpy, dim='trials')
+            spy.mean(adata_cpy, dim="trials")
         assert "found trials of different shape" in str(err.value)
 
     def test_stat_parallel(self, testcluster):
         client = dd.Client(testcluster)
         self.test_selections()
         # should have no effect here
         self.test_trial_statistics()
         client.close()
 
     def test_itc(self, do_plot=True):
 
-        adata = sd.white_noise(nTrials=100,
-                               nSamples=1000,
-                               nChannels=2,
-                               samplerate=500,
-                               seed=42)
+        adata = sd.white_noise(nTrials=100, nSamples=1000, nChannels=2, samplerate=500, seed=42)
 
         # add simple 60Hz armonic
-        adata += sd.harmonic(nTrials=100,
-                             freq=60,
-                             nSamples=1000,
-                             nChannels=2,
-                             samplerate=500)
+        adata += sd.harmonic(nTrials=100, freq=60, nSamples=1000, nChannels=2, samplerate=500)
 
         trials = []
         # add frequency drift along trials
         # note same initial phase
         # so ITC will be ~1 at time 0
         freq = 30
         dfreq = 2 / 100  # frequency difference
         for trl in adata.trials:
             # start at 0s
             dat = np.cos(2 * np.pi * freq * (adata.time[0] + 1))
             trials.append(trl + np.c_[dat, dat])
             freq += dfreq
         adata = spy.AnalogData(data=trials, samplerate=500)
 
-        tf_spec = spy.freqanalysis(adata, method='mtmconvol',
-                                   t_ftimwin=0.5,
-                                   output='fourier',
-                                   foilim=[0, 100])
+        tf_spec = spy.freqanalysis(
+            adata, method="mtmconvol", t_ftimwin=0.5, output="fourier", foilim=[0, 100]
+        )
 
         # test also taper averaging
-        spec = spy.freqanalysis(adata, foilim=[0, 100],
-                                output='fourier', tapsmofrq=.5, keeptapers=True)
+        spec = spy.freqanalysis(adata, foilim=[0, 100], output="fourier", tapsmofrq=1, keeptapers=True)
 
         # -- calculate itc --
         itc = spy.itc(spec)
         tf_itc = spy.itc(tf_spec)
 
-        assert isinstance(tf_itc, spy.SpectralData)
-
-        assert np.all(np.imag(itc.data[()]) == 0)
-        assert itc.data[()].max() <= 1
-        assert itc.data[()].min() >= 0
-
-        # high itc around the in phase 60Hz
-        assert np.all(itc.show(frequency=60) > 0.6)
-        # low (time averaged) itc around the drifters
-        assert np.all(itc.show(frequency=30) < 0.25)
-
-        assert np.all(np.imag(tf_itc.data[()]) == 0)
-        assert tf_itc.data[()].max() <= 1
-        assert tf_itc.data[()].min() >= 0
-        assert np.allclose(tf_itc.time[0], tf_spec.time[0])
-
         if do_plot:
 
             # plot tf power spectrum
             # after 500 samples / 1s
             fig, ax = ppl.subplots()
             for idx in [0, 50, 99]:
                 power500 = np.abs(tf_spec.trials[idx][500, 0, :, 0])
                 ax.plot(tf_spec.freq, power500, label=f"trial {idx}")
             # note that the 30Hz peak wandered to ~31Hz
             # hence the phases will decorrelate over time
             ax.legend()
-            ax.set_xlabel('frequency (Hz)')
+            ax.set_xlabel("frequency (Hz)")
 
             # plot ITCs
             itc.singlepanelplot(channel=1)
             tf_itc.singlepanelplot(channel=1)
 
             # plot tf ITC time profiles
             fig, ax = ppl.subplots()
-            ax.set_xlabel('time (s)')
+            ax.set_xlabel("time (s)")
             for frq in [31, 60, 10]:
                 itc_profile = tf_itc.show(frequency=frq, channel=0)
                 ax.plot(tf_itc.time[0], itc_profile, label=f"{frq}Hz")
             ax.legend()
             ax.set_title("time dependent ITC")
 
+        assert isinstance(tf_itc, spy.SpectralData)
 
-class TestJackknife:
+        assert np.all(np.imag(itc.data[()]) == 0)
+        assert itc.data[()].max() <= 1
+        assert itc.data[()].min() >= 0
 
+        # high itc around the in phase 60Hz
+        assert np.all(itc.show(frequency=60) > 0.6)
+        # low (time averaged) itc around the drifters
+        assert np.all(itc.show(frequency=30) < 0.25)
+
+        assert np.all(np.imag(tf_itc.data[()]) == 0)
+        assert tf_itc.data[()].max() <= 1
+        assert tf_itc.data[()].min() >= 0
+        assert np.allclose(tf_itc.time[0], tf_spec.time[0])
+
+
+
+class TestJackknife:
     def test_jk_avg(self):
         """
         Mean estimation via jackknifing yields exactly the same
         results as the direct estimate (sample mean/variance)
         and hence can directly serve as a straightforward test
         """
 
         # create test data
         nTrials = 10
-        adata = spy.AnalogData(data=[i * np.ones((5, 3)) for i in range(nTrials)],
-                               samplerate=7)
+        adata = spy.AnalogData(data=[i * np.ones((5, 3)) for i in range(nTrials)], samplerate=7)
 
         # to test for property propagation
-        adata.channel = [f'chV_{i}' for i in range(1, 4)]
-        raw_est = spy.mean(adata, dim='time', keeptrials=True)
+        adata.channel = [f"chV_{i}" for i in range(1, 4)]
+        raw_est = spy.mean(adata, dim="time", keeptrials=True)
 
         # first compute all the leave-one-out (loo) replicates
         replicates = jk.trial_avg_replicates(raw_est)
         # as many replicates as there are trials
         assert len(replicates.trials) == len(adata.trials)
 
         # direct estimate is just the trial average
-        direct_est = spy.mean(raw_est, dim='trials')
+        direct_est = spy.mean(raw_est, dim="trials")
 
         # now compute bias and variance
         bias, variance = jk.bias_var(direct_est, replicates)
 
         # no bias for mean estimation
         assert np.allclose(bias.data[()], np.zeros(bias.data.shape))
 
         # jackknife variance is here the same as sample variance over trials
         # yet we have to correct for the denominator (N-1 vs. N)
-        direct_var = spy.var(raw_est, dim='trials').trials[0]
+        direct_var = spy.var(raw_est, dim="trials").trials[0]
         assert np.allclose(nTrials / (nTrials - 1) * direct_var, variance.trials[0])
 
         # check properties
         assert np.all(bias.channel == adata.channel)
         assert bias.samplerate == adata.samplerate
         assert np.all(variance.channel == adata.channel)
         assert variance.samplerate == adata.samplerate
@@ -363,26 +353,24 @@
         Jackknife cross-spectral densities, here again the trivial average/variance
         yields the same results.
         """
         nTrials = 10
         nSamples = 500
         adata = 10 * sd.white_noise(nTrials=nTrials, nSamples=nSamples, seed=helpers.test_seed)
         # to test for property propagation
-        adata.channel = [f'chV_{i}' for i in range(1, 3)]
+        adata.channel = [f"chV_{i}" for i in range(1, 3)]
 
         # -- still trivial CSDs --
 
         # single trial cross spectra (not densities!)
-        cross_spectra = spy.connectivityanalysis(adata,
-                                                 method='csd',
-                                                 keeptrials=True)
+        cross_spectra = spy.connectivityanalysis(adata, method="csd", keeptrials=True)
 
         # direct cross spectral density estimate (must be a trial average)
         # is here just the trial average
-        csd = spy.mean(cross_spectra, dim='trials')
+        csd = spy.mean(cross_spectra, dim="trials")
 
         # compute avg replicates
         replicates_avg = jk.trial_avg_replicates(cross_spectra)
 
         # now compute bias and variance
         bias, variance = jk.bias_var(csd, replicates_avg)
 
@@ -393,99 +381,106 @@
 
         # now again as this is still just a simple average
         # there can be no real bias
         assert np.allclose(bias.data[()], np.zeros(bias.data.shape), atol=1e-5)
 
         # direct variances still coincide,
         # `show` strips of empty time axis
-        direct_var = spy.var(cross_spectra, dim='trials').show()
-        assert np.allclose(direct_var * (nTrials / (nTrials - 1) + 0j),
-                           variance.show())
+        direct_var = spy.var(cross_spectra, dim="trials").show()
+        assert np.allclose(direct_var * (nTrials / (nTrials - 1) + 0j), variance.show())
 
     def test_jk_coh(self, **kwargs):
         """
         Jackknife a coherence analysis.
 
         For the coherence confidence intervals see:
         "Tables of the distribution of the coefficient of coherence for
         stationary bivariate Gaussian processes, Sandia monograph by Amos and Koopmans(1963)"
         """
 
         nTrials = 50
         nSamples = 1000
         # sufficient to check this entry
-        show_kwargs = {'channel_i': 0, 'channel_j': 1}
+        show_kwargs = {"channel_i": 0, "channel_j": 1}
         adata = sd.white_noise(nTrials=nTrials, nSamples=nSamples, seed=helpers.test_seed)
 
         # confidence for 100 trials from
         # above mentioned publication for squared coherence
         ci95 = {30: 0.98, 50: 0.06, 100: 0.03}
 
         # important to match between
         # replicates and direct estimate!
-        output = 'pow'
+        output = "pow"
 
         # direct estimate
-        coh = spy.connectivityanalysis(adata,
-                                       method='coh',
-                                       output=output)
+        coh = spy.connectivityanalysis(adata, method="coh", output=output)
 
         # first check that we got the right statistics
         # by asserting that less of 5% of the freq. bins are outside
         # the (one-sided) 95% conf. interval
         assert np.sum(coh.show(**show_kwargs) > ci95[nTrials]) / coh.freq.size < 0.05
 
         # single trial cross spectra (not densities!)
-        cross_spectra = spy.connectivityanalysis(adata,
-                                                 method='csd',
-                                                 keeptrials=True)
+        cross_spectra = spy.connectivityanalysis(adata, method="csd", keeptrials=True)
 
         # first create trivial avg/csd replicates
         replicates_avg = jk.trial_avg_replicates(cross_spectra)
 
         # -- compute coherence replicates --
 
         # from those compute jackknife replicates of the coherence
         CR = NormalizeCrossSpectra(output=output)
         replicates_coh = CrossSpectralData(dimord=coh.dimord)
         log_dict = {}
         # now fire up CR on all loo averages to get
         # the coherence jackknife replicates
         CR.initialize(replicates_avg, replicates_coh._stackingDim, chan_per_worker=None)
-        CR.compute(replicates_avg, replicates_coh, parallel=kwargs.get("parallel"),
-                   log_dict=log_dict)
+        CR.compute(
+            replicates_avg,
+            replicates_coh,
+            parallel=kwargs.get("parallel"),
+            log_dict=log_dict,
+        )
 
         assert len(replicates_coh.trials) == nTrials
         assert np.all(replicates_coh.channel_i == adata.channel)
 
         # now compute bias and variance
         bias, variance = jk.bias_var(coh, replicates_coh)
 
         # here we have some actual bias
         assert not np.allclose(bias.data[()], np.zeros(bias.data.shape), atol=1e-5)
 
-
         # look at the 0,1 entry
-        b01, v01, c01 = (bias.show(**show_kwargs),
-                         variance.show(**show_kwargs),
-                         coh.show(**show_kwargs))
+        b01, v01, c01 = (
+            bias.show(**show_kwargs),
+            variance.show(**show_kwargs),
+            coh.show(**show_kwargs),
+        )
 
         # standard error of the mean
         SEM = np.sqrt(v01 / nTrials)
 
         fig, ax = ppl.subplots()
         ax.set_title(f"Coherence of white noise with nTrials={nTrials}")
         ax.set_xlabel("frequency (Hz)")
         ax.set_ylabel("Coherence $|C|^2$")
-        ax.plot(coh.freq, c01, marker='.')
-        ax.fill_between(coh.freq, c01, c01 + 1.96 * SEM, color='k', alpha=0.3, label='95% Jackknife CI')
+        ax.plot(coh.freq, c01, marker=".")
+        ax.fill_between(
+            coh.freq,
+            c01,
+            c01 + 1.96 * SEM,
+            color="k",
+            alpha=0.3,
+            label="95% Jackknife CI",
+        )
         # truncate to 0 for negative values
         ci2 = c01 - 1.96 * SEM
         ci2[ci2 < 0] = 0
-        ax.fill_between(coh.freq, c01, ci2, color='k', alpha=0.3)
+        ax.fill_between(coh.freq, c01, ci2, color="k", alpha=0.3)
         ax.set_xlim((100, 150))
         ax.legend()
 
         # calculate the z-scores from the jackknife estimate
         # and jackknife variance for 0 coherence
         # as 0-hypothesis (we have uncorrelated noise)
         Zs = (c01 - b01) / SEM
@@ -493,88 +488,93 @@
         # now get p-values from survival function
         pvals = st.norm.sf(Zs)
 
         fig, ax = ppl.subplots()
         ax.set_title("Jackknife p-values for $H_0$: $|C|^2 = 0$")
         ax.set_xlabel("Coherence $|C|^2$")
         ax.set_ylabel("p-value")
-        ax.plot(c01, pvals, 'o', alpha=0.4, c='k', ms=3.5, mec='w')
-        ax.plot([0, c01.max()], [0.05, 0.05], 'k--', label='5%')
+        ax.plot(c01, pvals, "o", alpha=0.4, c="k", ms=3.5, mec="w")
+        ax.plot([0, c01.max()], [0.05, 0.05], "k--", label="5%")
         ax.legend()
 
         # turns out, the jackknife confidence intervals are quite conservative
         # and no frequency bin has a coherence high enough to reject the C=0 hypothesis
         # we could still expect to get a 'significant' coherence max 5% of the time
         assert np.sum(pvals < 0.05) / coh.freq.size < 0.05
 
         # finally fire up the frontend and compare results
-        res = spy.connectivityanalysis(adata, method='coh', jackknife=True, output=output)
+        res = spy.connectivityanalysis(adata, method="coh", jackknife=True, output=output)
 
         assert np.allclose(res.jack_var, variance.data, atol=1e-5)
         assert np.allclose(res.jack_bias, bias.data, atol=1e-5)
 
     def test_jk_frontend(self):
 
         # no seed needed here
-        adata = spy.AnalogData(data=[i * np.random.randn(5, 3) for i in range(3)],
-                               samplerate=7)
+        adata = spy.AnalogData(data=[i * np.random.randn(5, 3) for i in range(3)], samplerate=7)
 
         # test check for boolean type
-        with pytest.raises(SPYTypeError, match='expected boolean'):
-            spy.connectivityanalysis(adata, method='coh', jackknife=3)
+        with pytest.raises(SPYTypeError, match="expected boolean"):
+            spy.connectivityanalysis(adata, method="coh", jackknife=3)
 
         # check that jack attributes are not appended if no jackknifing was done
-        res = spy.connectivityanalysis(adata, method='corr')
-        assert not hasattr(res, 'jack_var')
-        assert not hasattr(res, 'jack_bias')
-
-        res = spy.connectivityanalysis(adata, method='coh', jackknife=False)
-        assert not hasattr(res, 'jack_var')
-        assert not hasattr(res, 'jack_bias')
-
-        res = spy.connectivityanalysis(adata, method='granger', jackknife=False)
-        assert not hasattr(res, 'jack_var')
-        assert not hasattr(res, 'jack_bias')
+        res = spy.connectivityanalysis(adata, method="corr")
+        assert not hasattr(res, "jack_var")
+        assert not hasattr(res, "jack_bias")
+
+        res = spy.connectivityanalysis(adata, method="coh", jackknife=False)
+        assert not hasattr(res, "jack_var")
+        assert not hasattr(res, "jack_bias")
+
+        res = spy.connectivityanalysis(adata, method="granger", jackknife=False)
+        assert not hasattr(res, "jack_var")
+        assert not hasattr(res, "jack_bias")
 
     def test_jk_granger(self):
 
-        AdjMat = np.zeros((2,2))
+        AdjMat = np.zeros((2, 2))
         # weak coupling 1 -> 0
         AdjMat[1, 0] = 0.025
         nTrials = 35
         adata = sd.ar2_network(nTrials=nTrials, AdjMat=AdjMat, seed=42)
         # true causality is at 200Hz
         flims = [190, 210]
 
         # direct estimate
-        res = spy.connectivityanalysis(adata, method='granger',
-                                       jackknife=True,
-                                       tapsmofrq=5)
+        res = spy.connectivityanalysis(adata, method="granger", jackknife=True, tapsmofrq=5)
         # there will be bias
         assert not np.allclose(res.jack_bias, np.zeros(res.data.shape), atol=1e-5)
 
-        b10, v10, g10 = (res.jack_bias[0, :, 1, 0],
-                         res.jack_var[0, :, 1, 0],
-                         res.show(channel_i=1, channel_j=0)
-                         )
+        b10, v10, g10 = (
+            res.jack_bias[0, :, 1, 0],
+            res.jack_var[0, :, 1, 0],
+            res.show(channel_i=1, channel_j=0),
+        )
         # standard error of the mean
         SEM = np.sqrt(v10 / nTrials)
 
         # plot confidence intervals
         fig, ax = ppl.subplots()
         ax.set_title(f"Granger causality between weakly coupled AR(2)")
         ax.set_xlabel("frequency (Hz)")
         ax.set_ylabel("Granger")
         ax.plot(res.freq, g10, label=f"nTrials={nTrials}")
-        ax.fill_between(res.freq, g10, g10 + 1.96 * SEM, color='k', alpha=0.3, label='95% Jackknife CI')
+        ax.fill_between(
+            res.freq,
+            g10,
+            g10 + 1.96 * SEM,
+            color="k",
+            alpha=0.3,
+            label="95% Jackknife CI",
+        )
         ci2 = g10 - 1.96 * SEM
         ci2[ci2 < 0] = 0
-        ax.fill_between(res.freq, g10, ci2, color='k', alpha=0.3)
-        ax.plot([flims[0], flims[0]], [0, 0.04], '--', alpha=0.5, lw=2, c='red')
-        ax.plot([flims[-1], flims[-1]], [0, 0.04], '--', alpha=0.5, lw=2, c='red')
+        ax.fill_between(res.freq, g10, ci2, color="k", alpha=0.3)
+        ax.plot([flims[0], flims[0]], [0, 0.04], "--", alpha=0.5, lw=2, c="red")
+        ax.plot([flims[-1], flims[-1]], [0, 0.04], "--", alpha=0.5, lw=2, c="red")
         ax.legend()
         fig.tight_layout()
 
         # calculate the z-scores from the jackknife estimate
         # and jackknife variance for 0 granger causality
         # as 0-hypothesis
         Zs = (g10 - b10) / SEM
@@ -585,25 +585,33 @@
         # boolean indices of frequency interval with true causality
         bi = (res.freq > flims[0]) & (res.freq < flims[-1])
 
         fig, ax = ppl.subplots()
         ax.set_title("Jackknife p-values for $H_0$: Granger = 0")
         ax.set_xlabel("Granger causality")
         ax.set_ylabel("p-value")
-        ax.plot(g10[~bi], pvals[~bi], 'o', alpha=0.4, c='k', ms=5, mec='w')
-        ax.plot(g10[bi], pvals[bi], 'o', alpha=0.4, c='red', ms=4, label=f'{flims[0]}Hz-{flims[-1]}Hz')
+        ax.plot(g10[~bi], pvals[~bi], "o", alpha=0.4, c="k", ms=5, mec="w")
+        ax.plot(
+            g10[bi],
+            pvals[bi],
+            "o",
+            alpha=0.4,
+            c="red",
+            ms=4,
+            label=f"{flims[0]}Hz-{flims[-1]}Hz",
+        )
 
-        ax.plot([0, g10.max()], [0.05, 0.05], 'k--', label='5%')
+        ax.plot([0, g10.max()], [0.05, 0.05], "k--", label="5%")
         ax.legend()
 
         # make sure most (>95%) frequency bins outside the causality region have high p-value
         assert np.sum(pvals[~bi] > 0.05) / (res.freq.size - np.sum(bi)) > 0.95
 
         # check that at least 80% of causality values within the freq interval are below
         # the 5% significance interval and hence are deteceted as true positives
         assert np.sum(pvals[bi] < 0.05) / bi[bi].size > 0.8
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
 
     T1 = TestSumStatistics()
     T2 = TestJackknife()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_synthdata.py` & `esi_syncopy-2023.7/syncopy/tests/test_synthdata.py`

 * *Files 13% similar despite different names*

```diff
@@ -37,30 +37,30 @@
     assert len(adata.trials) == cfg.nTrials
     assert len(adata.channel) == cfg.nChannels
     assert len(adata.time[0]) == cfg.nSamples
     assert adata.samplerate == cfg.samplerate
 
     # with nTrials=None, the decorator gets bypassed
     # and returns the single trial array
-    cfg['nTrials'] = None
+    cfg["nTrials"] = None
     arr = ones(cfg)
     assert isinstance(arr, np.ndarray)
     assert arr.shape == (cfg.nSamples, cfg.nChannels)
 
 
 class TestSynthData:
 
     nTrials = 100
     nChannels = 1
     nSamples = 1000
     samplerate = 1000
 
     def test_white_noise_without_seed(self):
         """Without seed set, the data should not be identical.
-           Note: This does not use collect trials.
+        Note: This does not use collect trials.
         """
         cfg = StructDict()
         cfg.nSamples = self.nSamples
         cfg.nChannels = self.nChannels
         cfg.nTrials = None
         # that is the default
         # cfg.seed = None
@@ -70,15 +70,15 @@
         assert isinstance(wn1, np.ndarray)
         assert isinstance(wn2, np.ndarray)
 
         assert not np.allclose(wn1, wn2)
 
     def test_white_noise_with_seed(self):
         """With seed set, the data should be identical.
-           Note: This does not use @collect_trials.
+        Note: This does not use @collect_trials.
         """
         cfg = StructDict()
         cfg.nSamples = self.nSamples
         cfg.nChannels = self.nChannels
         cfg.nTrials = None
         cfg.seed = 42
 
@@ -90,71 +90,105 @@
 
         assert np.allclose(wn1, wn2)
 
     def test_collect_trials_wn_seed_array(self):
         """Uses @collect_trials."""
         # Trials must differ within an object if seed_per_trial is left at default (true):
         seed = 42
-        wn1 = white_noise(nSamples=self.nSamples, nChannels=self.nChannels, nTrials=self.nTrials, seed=seed)
+        wn1 = white_noise(
+            nSamples=self.nSamples,
+            nChannels=self.nChannels,
+            nTrials=self.nTrials,
+            seed=seed,
+        )
         assert isinstance(wn1, AnalogData)
         assert not np.allclose(wn1.show(trials=0), wn1.show(trials=1))
 
         # However, using the same seed for a new instance must lead to identical trials between instances:
-        wn2 = white_noise(nSamples=self.nSamples, nChannels=self.nChannels, nTrials=self.nTrials, seed=seed)
+        wn2 = white_noise(
+            nSamples=self.nSamples,
+            nChannels=self.nChannels,
+            nTrials=self.nTrials,
+            seed=seed,
+        )
         assert np.allclose(wn1.show(trials=0), wn2.show(trials=0))
         assert np.allclose(wn1.show(trials=1), wn2.show(trials=1))
 
     def test_collect_trials_wn_seed_scalar(self):
         """Uses @collect_trials."""
         # Trials must be identical within an object if seed_per_trial is False (and a seed is used).
         seed = 42
-        wn1 = white_noise(nSamples=self.nSamples, nChannels=self.nChannels, nTrials=self.nTrials, seed=seed, seed_per_trial=False)
+        wn1 = white_noise(
+            nSamples=self.nSamples,
+            nChannels=self.nChannels,
+            nTrials=self.nTrials,
+            seed=seed,
+            seed_per_trial=False,
+        )
         assert isinstance(wn1, AnalogData)
         assert np.allclose(wn1.show(trials=0), wn1.show(trials=1))
 
         # And also, using the same scalar seed again should lead to an identical object.
-        wn2 = white_noise(nSamples=self.nSamples, nChannels=self.nChannels, nTrials=self.nTrials, seed=seed, seed_per_trial=False)
+        wn2 = white_noise(
+            nSamples=self.nSamples,
+            nChannels=self.nChannels,
+            nTrials=self.nTrials,
+            seed=seed,
+            seed_per_trial=False,
+        )
         assert np.allclose(wn1.show(trials=0), wn2.show(trials=0))
         assert np.allclose(wn1.show(trials=1), wn2.show(trials=1))
 
     def test_collect_trials_wn_no_seed(self):
         """Uses @collect_trials."""
         # Trials must differ within an object if seed is None:
         seed = None
-        wn1 = white_noise(nSamples=self.nSamples, nChannels=self.nChannels, nTrials=self.nTrials, seed=seed)
+        wn1 = white_noise(
+            nSamples=self.nSamples,
+            nChannels=self.nChannels,
+            nTrials=self.nTrials,
+            seed=seed,
+        )
         assert isinstance(wn1, AnalogData)
         assert not np.allclose(wn1.show(trials=0), wn1.show(trials=1))
 
         # And instances must also differ:
-        wn2 = white_noise(nSamples=self.nSamples, nChannels=self.nChannels, nTrials=self.nTrials, seed=seed)
+        wn2 = white_noise(
+            nSamples=self.nSamples,
+            nChannels=self.nChannels,
+            nTrials=self.nTrials,
+            seed=seed,
+        )
         assert not np.allclose(wn1.show(trials=0), wn2.show(trials=0))
         assert not np.allclose(wn1.show(trials=1), wn2.show(trials=1))
 
-
     #### Tests for AR2_network
 
     def test_ar2_without_seed(self):
         """Without seed set, the data should not be identical.
-           Note: This does not use collect trials.
+        Note: This does not use collect trials.
         """
         num_channels = 2
-        arn1 = ar2_network(nSamples=self.nSamples, seed=None, nTrials=None)  # 2 channels, via default adj matrix
+        arn1 = ar2_network(
+            nSamples=self.nSamples, seed=None, nTrials=None
+        )  # 2 channels, via default adj matrix
         arn2 = ar2_network(nSamples=self.nSamples, seed=None, nTrials=None)
         assert isinstance(arn1, np.ndarray)
         assert isinstance(arn2, np.ndarray)
         assert arn1.shape == (self.nSamples, num_channels)
         assert arn2.shape == (self.nSamples, num_channels)
 
         assert not np.allclose(arn1, arn2)
 
     def test_ar2_with_seed(self):
         """With seed set, the data should be identical.
-           Note: This does not use collect trials.
+        Note: This does not use collect trials.
         """
         seed = 42
         arn1 = ar2_network(nSamples=self.nSamples, seed=seed, seed_per_trial=False, nTrials=None)
         arn2 = ar2_network(nSamples=self.nSamples, seed=seed, seed_per_trial=False, nTrials=None)
 
         assert np.allclose(arn1, arn2)
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     T1 = TestSynthData()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_timelockanalysis.py` & `esi_syncopy-2023.7/syncopy/tests/test_timelockanalysis.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,52 +20,52 @@
     nTrials = 10
     nChannels = 3
     nSamples = 500
     fs = 200
 
     # create simple white noise
     # "real" data gets created for semantic test
-    adata = synthdata.white_noise(nTrials=nTrials, samplerate=fs,
-                                  nSamples=nSamples,
-                                  nChannels=nChannels,
-                                  seed=42)
+    adata = synthdata.white_noise(
+        nTrials=nTrials, samplerate=fs, nSamples=nSamples, nChannels=nChannels, seed=42
+    )
 
     # change trial sizes, original interval is [-1, 1.495] seconds
     trldef = adata.trialdefinition
     trldef[1] = [500, 700, -200]  # [-1, -0.005] seconds
     trldef[2] = [680, 960, -20]  # [-0.1, 1.295] seconds
     trldef[3] = [1000, 1200, -100]  # [-0.5, 0.495] seconds
     adata.trialdefinition = trldef
 
     # in samples
-    overlap = (np.min(adata.trialintervals[:, 1]) -\
-        np.max(adata.trialintervals[:, 0])) * fs + 1
+    overlap = (np.min(adata.trialintervals[:, 1]) - np.max(adata.trialintervals[:, 0])) * fs + 1
 
     def test_timelockanalysis(self):
 
         # create bigger dataset for statistics only here
         # moderate phase diffusion, same initial phase/value!
-        adata = synthdata.phase_diffusion(nTrials=300,
-                                          rand_ini=False,
-                                          samplerate=self.fs,
-                                          nSamples=self.nSamples,
-                                          nChannels=self.nChannels,
-                                          freq=40,
-                                          eps=0.01,
-                                          seed=42)
+        adata = synthdata.phase_diffusion(
+            nTrials=300,
+            rand_ini=False,
+            samplerate=self.fs,
+            nSamples=self.nSamples,
+            nChannels=self.nChannels,
+            freq=40,
+            eps=0.01,
+            seed=42,
+        )
 
         # change trial sizes, original interval is [-1, 1.495] seconds
         trldef = adata.trialdefinition
         trldef[1] = [500, 700, -200]  # [-1, -0.005] seconds
         trldef[2] = [680, 960, -20]  # [-0.1, 1.295] seconds
         trldef[3] = [1000, 1200, -100]  # [-0.5, 0.495] seconds
         adata.trialdefinition = trldef
 
         cfg = spy.StructDict()
-        cfg.latency = 'maxperiod'  # default
+        cfg.latency = "maxperiod"  # default
         cfg.covariance = True
         cfg.keeptrials = False
 
         tld = spy.timelockanalysis(adata, cfg)
 
         assert isinstance(tld, spy.TimeLockData)
         # check that all trials have the same 'time locked' time axis
@@ -77,17 +77,17 @@
 
         assert isinstance(tld.avg, h5py.Dataset)
         assert isinstance(tld.var, h5py.Dataset)
         assert isinstance(tld.cov, h5py.Dataset)
 
         # check that the results are the same when kicking
         # out the offending trial via the same latency selection
-        ad = spy.selectdata(adata, latency='maxperiod')
-        avg = spy.mean(ad, dim='trials')
-        var = spy.var(ad, dim='trials')
+        ad = spy.selectdata(adata, latency="maxperiod")
+        avg = spy.mean(ad, dim="trials")
+        var = spy.var(ad, dim="trials")
 
         assert np.allclose(avg.data, tld.avg)
         assert np.allclose(var.data, tld.var)
 
         # over time the phases will diffuse and be uniform in [-pi, pi],
         # hence by transforming the random variable with x = cos(phases) the
         # signal values are distributed like 1 / sqrt(1 - x**2) * 1 / pi
@@ -111,138 +111,129 @@
         assert np.allclose(np.diagonal(tld.cov), variances)
 
         # here just check that off-diagonals have vastly lower covariance
         assert np.all(np.diagonal(tld.cov)[:-1] > 5 * np.diagonal(tld.cov, offset=1))
 
         # plot the Syncopy objects which have the same data as .avg and .var
         fig, ax = avg.singlepanelplot()
-        ax.set_title('Trial mean')
+        ax.set_title("Trial mean")
         fig.tight_layout()
         fig, ax = var.singlepanelplot()
-        ax.set_title('Trial variance')
+        ax.set_title("Trial variance")
         fig.tight_layout()
 
     def test_latency(self):
 
         """Test all available `latency` (time window interval) settings"""
 
         # first make sure we have unequal trials
         assert np.any(np.diff(self.adata.sampleinfo, n=2, axis=0) != 0)
 
         # check that now all trial have been cut to the overlap interval
-        tld = spy.timelockanalysis(self.adata, latency='minperiod')
+        tld = spy.timelockanalysis(self.adata, latency="minperiod")
         assert np.all(np.diff(tld.sampleinfo) == self.overlap)
 
         # check that trials got kicked out and all times are smaller 0
-        tld = spy.timelockanalysis(self.adata, latency='prestim')
+        tld = spy.timelockanalysis(self.adata, latency="prestim")
         assert 3 == len(self.adata.trials) - len(tld.trials)
         # only trigger relative negative times ("pre-stimulus")
         assert np.all([tld.time[i] <= 0 for i in range(len(tld.trials))])
 
         # check that trials got kicked out and all times are larger than 0
-        tld = spy.timelockanalysis(self.adata, latency='poststim')
+        tld = spy.timelockanalysis(self.adata, latency="poststim")
         assert 3 == len(self.adata.trials) - len(tld.trials)
         # only trigger relative positive times ("post-stimulus")
         assert np.all([tld.time[i] >= 0 for i in range(len(tld.trials))])
 
         # finally manually set a latency window which excludes only 2 trials
-        tld = spy.timelockanalysis(self.adata, latency=[-.1, 0.5])
+        tld = spy.timelockanalysis(self.adata, latency=[-0.1, 0.5])
         assert 2 == len(self.adata.trials) - len(tld.trials)
 
         # and ultimately check that we have no dangling selections
         # after all this
         assert self.adata.selection is None
         assert tld.selection is None
 
     def test_exceptions(self):
 
         cfg = spy.StructDict()
 
         # -- latency validation --
 
         # not available latency
-        cfg.latency = 'sth'
-        with pytest.raises(SPYValueError,
-                           match="expected one of"):
+        cfg.latency = "sth"
+        with pytest.raises(SPYValueError, match="expected one of"):
             spy.timelockanalysis(self.adata, cfg)
 
         # latency not ordered
         cfg.latency = [0.1, 0]
-        with pytest.raises(SPYValueError,
-                           match="expected start < end"):
+        with pytest.raises(SPYValueError, match="expected start < end"):
             spy.timelockanalysis(self.adata, cfg)
 
         # latency completely outside of data
         cfg.latency = [-999, -99]
-        with pytest.raises(SPYValueError,
-                           match="expected end of latency window"):
+        with pytest.raises(SPYValueError, match="expected end of latency window"):
             spy.timelockanalysis(self.adata, cfg)
 
         cfg.latency = [99, 999]
-        with pytest.raises(SPYValueError,
-                           match="expected start of latency window"):
+        with pytest.raises(SPYValueError, match="expected start of latency window"):
             spy.timelockanalysis(self.adata, cfg)
 
         # here we need to manually wipe the selection due to the
         # exceptioned runs above
         self.adata.selection = None
 
         # -- trial selection with both selection and keyword --
-        with pytest.raises(SPYValueError,
-                           match="expected either `trials != 'all'`"):
-            spy.timelockanalysis(self.adata, trials=0, select={'trials': 8})
-
+        with pytest.raises(SPYValueError, match="expected either `trials != 'all'`"):
+            spy.timelockanalysis(self.adata, trials=0, select={"trials": 8})
 
         # -- remaining parameters --
 
         cfg.latency = None
-        cfg.covariance = 'fd'
-        with pytest.raises(SPYTypeError,
-                           match="expected bool"):
+        cfg.covariance = "fd"
+        with pytest.raises(SPYTypeError, match="expected bool"):
             spy.timelockanalysis(self.adata, cfg)
 
-        with pytest.raises(SPYTypeError,
-                           match="expected bool"):
+        with pytest.raises(SPYTypeError, match="expected bool"):
             spy.timelockanalysis(self.adata, keeptrials=2)
 
-        with pytest.raises(SPYValueError,
-                           match="expected positive integer"):
-            spy.timelockanalysis(self.adata, ddof='2')
+        with pytest.raises(SPYValueError, match="expected positive integer"):
+            spy.timelockanalysis(self.adata, ddof="2")
 
         # here we need to manually wipe the selection due to the
         # exception runs above
         self.adata.selection = None
 
     def test_parallel_selection(self, testcluster):
 
         cfg = spy.StructDict()
-        cfg.latency = 'minperiod'
+        cfg.latency = "minperiod"
         cfg.parallel = True
 
         client = dd.Client(testcluster)
 
         # test standard run
         tld = spy.timelockanalysis(self.adata, cfg)
         # check that there are NO NaNs
         assert not np.any(np.isnan(tld.data[:]))
 
         # test channel selection
-        cfg.select = {'channel': 0}
+        cfg.select = {"channel": 0}
         tld = spy.timelockanalysis(self.adata, cfg)
-        assert all(['channel2' not in chan for chan in tld.channel])
+        assert all(["channel2" not in chan for chan in tld.channel])
         assert self.adata.selection is None
 
         # trial selection via FT compat parameter
         cfg.trials = [5, 6]
         cfg.select = None
         tld = spy.timelockanalysis(self.adata, cfg)
         assert len(tld.trials) == 2
         assert self.adata.selection is None
 
         # and via normal selection
-        tld2 = spy.timelockanalysis(self.adata, select={'trials': [5, 6]})
+        tld2 = spy.timelockanalysis(self.adata, select={"trials": [5, 6]})
         assert np.all(tld2.data[()] == tld.data[()])
         client.close()
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestTimelockanalysis()
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_tools.py` & `esi_syncopy-2023.7/syncopy/tests/test_tools.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,16 +2,16 @@
 #
 # Test shared tools.
 #
 
 import syncopy as spy
 import copy
 
-class TestTools:
 
+class TestTools:
     def test_structdict_shallow_copy(self):
         """Test for fix of issue #394: 'Copying a spy.StructDict returns a dict'."""
         cfg = spy.StructDict()
         cfg.a = 0.5
         cfg.b = "test"
         cfg.c = [1, 2, 3]
         assert type(cfg) == spy.shared.tools.StructDict
@@ -29,30 +29,30 @@
         assert cfg2.c == cfg.c
 
         # Check the list was shallow-copied.
         cfg.c.append(4)
         assert cfg2.c == cfg.c
 
     def test_structdict_from_dict(self):
-        my_dict = {'a': 0.5, 'b': 'test', 'c' : [1, 2, 3]}
+        my_dict = {"a": 0.5, "b": "test", "c": [1, 2, 3]}
         cfg = spy.StructDict(my_dict)
         assert type(cfg) == spy.shared.tools.StructDict
         assert cfg.a == 0.5
         assert cfg.b == "test"
         assert cfg.c == [1, 2, 3]
 
     def test_copy_Welch_cfg(self):
         from syncopy.tests.test_welch import TestWelch
+
         cfg = TestWelch.get_welch_cfg()
         cfg.method = "abs"
         cfg2 = cfg.copy()
         assert type(cfg2) == spy.shared.tools.StructDict
         assert cfg2.method == "abs"
 
-
     def test_structdict_shallow_copy_ext(self):
         """Test for fix of issue #394: 'Copying a spy.StructDict returns a dict'."""
         cfg = spy.StructDict()
         cfg.a = 0.5
         cfg.b = "test"
         cfg.c = [1, 2, 3]
         assert type(cfg) == spy.shared.tools.StructDict
@@ -87,25 +87,25 @@
         assert cfg2.c != cfg.c
         assert 4 in cfg.c
         assert 4 not in cfg2.c
         assert cfg2.a == cfg.a
         assert cfg2.b == cfg.b
         assert cfg2.a == 0.5
         assert cfg2.b == "test"
-        assert cfg2.c == [1,2,3]
-        assert cfg.c == [1,2,3,4]
+        assert cfg2.c == [1, 2, 3]
+        assert cfg.c == [1, 2, 3, 4]
 
     def test_structdict_deepcopy_ext(self):
         cfg = spy.StructDict()
         cfg.a = 0.5
         cfg.b = "test"
         cfg.c = [1, 2, 3]
         assert type(cfg) == spy.shared.tools.StructDict
 
-        cfg2 = copy.deepcopy(cfg) # Use copy.deepcopy instead of cfg.deepcopy() this time.
+        cfg2 = copy.deepcopy(cfg)  # Use copy.deepcopy instead of cfg.deepcopy() this time.
 
         assert type(cfg2) == spy.shared.tools.StructDict
         assert cfg2.a == cfg.a
         assert cfg2.b == cfg.b
         assert cfg2.c == cfg.c
 
         # Check the list was deep-copied.
@@ -113,25 +113,25 @@
         assert cfg2.c != cfg.c
         assert 4 in cfg.c
         assert 4 not in cfg2.c
         assert cfg2.a == cfg.a
         assert cfg2.b == cfg.b
         assert cfg2.a == 0.5
         assert cfg2.b == "test"
-        assert cfg2.c == [1,2,3]
-        assert cfg.c == [1,2,3,4]
+        assert cfg2.c == [1, 2, 3]
+        assert cfg.c == [1, 2, 3, 4]
 
     def test_structdict_deepcopy_ext_with_explicit_deep(self):
         cfg = spy.StructDict()
         cfg.a = 0.5
         cfg.b = "test"
         cfg.c = [1, 2, 3]
         assert type(cfg) == spy.shared.tools.StructDict
 
-        cfg2 = cfg.copy(deep=True) # Use copy.deepcopy instead of cfg.deepcopy() this time.
+        cfg2 = cfg.copy(deep=True)  # Use copy.deepcopy instead of cfg.deepcopy() this time.
 
         assert type(cfg2) == spy.shared.tools.StructDict
         assert cfg2.a == cfg.a
         assert cfg2.b == cfg.b
         assert cfg2.c == cfg.c
 
         # Check the list was deep-copied.
@@ -139,25 +139,25 @@
         assert cfg2.c != cfg.c
         assert 4 in cfg.c
         assert 4 not in cfg2.c
         assert cfg2.a == cfg.a
         assert cfg2.b == cfg.b
         assert cfg2.a == 0.5
         assert cfg2.b == "test"
-        assert cfg2.c == [1,2,3]
-        assert cfg.c == [1,2,3,4]
+        assert cfg2.c == [1, 2, 3]
+        assert cfg.c == [1, 2, 3, 4]
 
     def test_structdict_deepcopy_ext_with_implicit_deep(self):
         cfg = spy.StructDict()
         cfg.a = 0.5
         cfg.b = "test"
         cfg.c = [1, 2, 3]
         assert type(cfg) == spy.shared.tools.StructDict
 
-        cfg2 = cfg.copy() # Use copy.deepcopy instead of cfg.deepcopy() this time.
+        cfg2 = cfg.copy()  # Use copy.deepcopy instead of cfg.deepcopy() this time.
 
         assert type(cfg2) == spy.shared.tools.StructDict
         assert cfg2.a == cfg.a
         assert cfg2.b == cfg.b
         assert cfg2.c == cfg.c
 
         # Check the list was deep-copied.
@@ -165,14 +165,13 @@
         assert cfg2.c != cfg.c
         assert 4 in cfg.c
         assert 4 not in cfg2.c
         assert cfg2.a == cfg.a
         assert cfg2.b == cfg.b
         assert cfg2.a == 0.5
         assert cfg2.b == "test"
-        assert cfg2.c == [1,2,3]
-        assert cfg.c == [1,2,3,4]
+        assert cfg2.c == [1, 2, 3]
+        assert cfg.c == [1, 2, 3, 4]
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     T1 = TestTools()
-
```

### Comparing `esi_syncopy-2023.5/syncopy/tests/test_welch.py` & `esi_syncopy-2023.7/syncopy/tests/test_welch.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,64 +12,63 @@
 import matplotlib.pyplot as plt
 from syncopy.shared.errors import SPYValueError
 from syncopy.shared.const_def import spectralConversions
 from syncopy import synthdata
 from syncopy.tests.helpers import teardown, test_seed
 
 
-class TestWelch():
+class TestWelch:
     """
     Test the frontend (user API) for running Welch's method for estimation of power spectra.
     """
 
     # White noise
-    adata = synthdata.white_noise(nTrials=2, nChannels=3, nSamples=20000, samplerate=1000,
-                                  seed=test_seed)
+    adata = synthdata.white_noise(nTrials=2, nChannels=3, nSamples=20000, samplerate=1000, seed=test_seed)
     do_plot = True
 
     def setup_class(cls):
-        plt.close('all')    # Close plots that are still open.
+        plt.close("all")  # Close plots that are still open.
 
     @staticmethod
     def get_welch_cfg():
         """
         Get a reasonable Welch cfg for testing purposes.
         """
         cfg = spy.get_defaults(spy.freqanalysis)
         cfg.method = "welch"
         cfg.t_ftimwin = 0.5  # Window length in seconds.
-        cfg.toi = 0.0        # Overlap between periodograms (0.5 = 50 percent overlap).
+        cfg.toi = 0.0  # Overlap between periodograms (0.5 = 50 percent overlap).
         return cfg
 
     def test_mtmconvolv_res(self):
         """Internal function mainly for interactive debugging purposes,
-           to better see what we are working with.
+        to better see what we are working with.
 
-           Welch is implemented as a post-processing of mtmfftconvolv, so it
-           is helpful to be sure about its input.
+        Welch is implemented as a post-processing of mtmfftconvolv, so it
+        is helpful to be sure about its input.
         """
         cfg = TestWelch.get_welch_cfg()
         cfg.method = "mtmconvol"
         res = spy.freqanalysis(cfg, self.adata)
 
         # Test basic output properties.
         assert len(res.dimord) == 4
         assert len(res.data.shape) == 4
-        assert res.dimord.index('time') == 0
-        assert res.dimord.index('taper') == 1
-        assert res.dimord.index('freq') == 2
-        assert res.dimord.index('channel') == 3
+        assert res.dimord.index("time") == 0
+        assert res.dimord.index("taper") == 1
+        assert res.dimord.index("freq") == 2
+        assert res.dimord.index("channel") == 3
 
         # Test ouput shape.
         # The 'time' dimension is the important difference between mtmconvolv and Welch:
         # 20.000 samples per trial at 1000 samplerate => 20 sec of data. With window length of
         # 0.5 sec and no overlap, we should get 40 periodograms per trial, so 80 in total.
-        assert res.data.shape[res.dimord.index('time')] == 80
-        assert res.data.shape[res.dimord.index('taper')] == 1
-        assert res.data.shape[res.dimord.index('channel')] == 3
+        assert res.data.shape[res.dimord.index("time")] == 80
+        assert res.data.shape[res.dimord.index("taper")] == 1
+        assert res.data.shape[res.dimord.index("channel")] == 3
 
         # Test output trialdefinition
         assert res.trialdefinition.shape[0] == 2  # nTrials
 
         if self.do_plot:
             _, ax = res.singlepanelplot(trials=0, channel=0)
             ax.set_title("mtmconvolv result.")
@@ -80,26 +79,30 @@
         """
         cfg = TestWelch.get_welch_cfg()
         res = spy.freqanalysis(cfg, self.adata)
 
         # Test basic output properties. Same as for mtmconvolv.
         assert len(res.dimord) == 4
         assert len(res.data.shape) == 4
-        assert res.dimord.index('time') == 0
-        assert res.dimord.index('taper') == 1
-        assert res.dimord.index('freq') == 2
-        assert res.dimord.index('channel') == 3
+        assert res.dimord.index("time") == 0
+        assert res.dimord.index("taper") == 1
+        assert res.dimord.index("freq") == 2
+        assert res.dimord.index("channel") == 3
 
         # Test ouput shape:
-        assert res.data.shape[res.dimord.index('time')] == 2  # 1 averaged periodogram per trial left, so 2 periodograms for the 2 trials.
-        assert res.data.shape[res.dimord.index('taper')] == 1
-        assert res.data.shape[res.dimord.index('channel')] == 3
+        assert (
+            res.data.shape[res.dimord.index("time")] == 2
+        )  # 1 averaged periodogram per trial left, so 2 periodograms for the 2 trials.
+        assert res.data.shape[res.dimord.index("taper")] == 1
+        assert res.data.shape[res.dimord.index("channel")] == 3
 
         # Test output trialdefinition
-        assert res.trialdefinition.shape[0] == 2  # nTrials should be left intact, as we did not set trial averaging.
+        assert (
+            res.trialdefinition.shape[0] == 2
+        )  # nTrials should be left intact, as we did not set trial averaging.
 
         if self.do_plot:
             _, ax = res.singlepanelplot(trials=0, logscale=False)
             ax.set_title("Welch result")
             # ax.set_ylabel("Power")
             ax.set_xlabel("Frequency")
 
@@ -112,16 +115,16 @@
         We select toi and ftimwin in a way that leads to a comparable number of
         windows between the two cases.
         """
         foilim = [10, 70]
 
         cfg_no_overlap = TestWelch.get_welch_cfg()
         cfg_no_overlap.method = "mtmconvol"
-        cfg_no_overlap.toi = 0.0        # overlap [0, 1]
-        cfg_no_overlap.t_ftimwin = 0.25   # window length in sec
+        cfg_no_overlap.toi = 0.0  # overlap [0, 1]
+        cfg_no_overlap.t_ftimwin = 0.25  # window length in sec
         cfg_no_overlap.foilim = foilim
         cfg_no_overlap.output = "abs"
 
         cfg_with_overlap = TestWelch.get_welch_cfg()
         cfg_with_overlap.method = "mtmconvol"
         cfg_with_overlap.toi = 0.8
         cfg_with_overlap.t_ftimwin = 1.2
@@ -132,63 +135,81 @@
         samplerate = 1000
         wn = synthdata.white_noise(nTrials=1, nChannels=3, nSamples=nSamples, samplerate=samplerate)
 
         spec_short_windows = spy.freqanalysis(cfg_no_overlap, wn)
         spec_long_windows = spy.freqanalysis(cfg_with_overlap, wn)
 
         # Check number of windows, we want something similar/comparable.
-        assert spec_short_windows.dimord.index('time') == spec_long_windows.dimord.index('time')
-        ti = spec_short_windows.dimord.index('time')
-        assert spec_short_windows.data.shape[ti] == 120, f"Window count without overlap is: {spec_short_windows.data.shape[ti]} (shape: {spec_short_windows.data.shape})"
-        assert spec_long_windows.data.shape[ti] == 125, f"Window count with overlap is: {spec_long_windows.data.shape[ti]} (shape: {spec_long_windows.data.shape})"
+        assert spec_short_windows.dimord.index("time") == spec_long_windows.dimord.index("time")
+        ti = spec_short_windows.dimord.index("time")
+        assert (
+            spec_short_windows.data.shape[ti] == 120
+        ), f"Window count without overlap is: {spec_short_windows.data.shape[ti]} (shape: {spec_short_windows.data.shape})"
+        assert (
+            spec_long_windows.data.shape[ti] == 125
+        ), f"Window count with overlap is: {spec_long_windows.data.shape[ti]} (shape: {spec_long_windows.data.shape})"
 
         # Check windows lengths, these should be different.
-        assert spec_short_windows.dimord.index('freq') == spec_long_windows.dimord.index('freq')
-        fi = spec_short_windows.dimord.index('freq')
-        assert spec_short_windows.data.shape[fi] == 15, f"Window length without overlap is: {spec_short_windows.data.shape[fi]} (shape: {spec_short_windows.data.shape})"
-        assert spec_long_windows.data.shape[fi] == 73, f"Window length with overlap is: {spec_long_windows.data.shape[fi]} (shape: {spec_long_windows.data.shape})"
+        assert spec_short_windows.dimord.index("freq") == spec_long_windows.dimord.index("freq")
+        fi = spec_short_windows.dimord.index("freq")
+        assert (
+            spec_short_windows.data.shape[fi] == 15
+        ), f"Window length without overlap is: {spec_short_windows.data.shape[fi]} (shape: {spec_short_windows.data.shape})"
+        assert (
+            spec_long_windows.data.shape[fi] == 73
+        ), f"Window length with overlap is: {spec_long_windows.data.shape[fi]} (shape: {spec_long_windows.data.shape})"
 
-        var_dim='time'
+        var_dim = "time"
         var_short_windows = spy.var(spec_short_windows, dim=var_dim)
         var_long_windows = spy.var(spec_long_windows, dim=var_dim)
 
         if self.do_plot:
-            plot_trial=0  # Which one does not matter, they are all white noise.
+            plot_trial = 0  # Which one does not matter, they are all white noise.
             _, ax0 = var_short_windows.singlepanelplot(trials=plot_trial, logscale=False)
-            ax0.set_title(f"mtmconvolv overlap effect: Windows without overlap\n(toi={cfg_no_overlap.toi}, f_timwin={cfg_no_overlap.t_ftimwin}).")
+            ax0.set_title(
+                f"mtmconvolv overlap effect: Windows without overlap\n(toi={cfg_no_overlap.toi}, f_timwin={cfg_no_overlap.t_ftimwin})."
+            )
             ax0.set_ylabel("Variance")
             _, ax1 = var_long_windows.singlepanelplot(trials=plot_trial, logscale=False)
-            ax1.set_title(f"mtmconvolv overlap effect: Windows with overlap\n(toi={cfg_with_overlap.toi}, f_timwin={cfg_with_overlap.t_ftimwin}).")
+            ax1.set_title(
+                f"mtmconvolv overlap effect: Windows with overlap\n(toi={cfg_with_overlap.toi}, f_timwin={cfg_with_overlap.t_ftimwin})."
+            )
             ax1.set_ylabel("Variance")
 
-        chan=0
+        chan = 0
         assert np.mean(var_short_windows.show(channel=chan)) > np.mean(var_long_windows.show(channel=chan))
 
     def test_welch_size_effect(self):
         """
         Compare variance over different Welch estimates based on signal length and overlap.
 
         (Variance can be computed along trials.)
 
         Compare a long signal without overlap versus a short signal with overlap, that result in the
         same window count. We expect to see higher variance for the shorter signal.
 
         Potential nice-to-have for later: investigate sweet spot for the overlap parameter as a function of signal length.
         """
-        wn_long = synthdata.white_noise(nTrials=20, nChannels=1, nSamples=10000, samplerate=1000, seed=42)  # 10 seconds of signal
-        wn_short = synthdata.white_noise(nTrials=20, nChannels=1, nSamples=1000, samplerate=1000, seed=42)  # 1  second of signal
+        wn_long = synthdata.white_noise(
+            nTrials=20, nChannels=1, nSamples=10000, samplerate=1000, seed=42
+        )  # 10 seconds of signal
+        wn_short = synthdata.white_noise(
+            nTrials=20, nChannels=1, nSamples=1000, samplerate=1000, seed=42
+        )  # 1  second of signal
 
         foilim = [5, 200]  # Shared between cases.
 
         cfg_long_no_overlap = TestWelch.get_welch_cfg()  # Results in 100 windows of length 100.
-        cfg_long_no_overlap.toi = 0.0         # overlap [0, 1[
-        cfg_long_no_overlap.t_ftimwin = 0.1   # window length in sec
+        cfg_long_no_overlap.toi = 0.0  # overlap [0, 1[
+        cfg_long_no_overlap.t_ftimwin = 0.1  # window length in sec
         cfg_long_no_overlap.foilim = foilim
 
-        cfg_short_with_overlap = TestWelch.get_welch_cfg()  # Results in 100 windows of length 20, with 50% overlap.
+        cfg_short_with_overlap = (
+            TestWelch.get_welch_cfg()
+        )  # Results in 100 windows of length 20, with 50% overlap.
         cfg_short_with_overlap.toi = 0.5
         cfg_short_with_overlap.t_ftimwin = 0.02
         cfg_short_with_overlap.foilim = foilim
 
         # Check the number of windows that Welch will average over.
         # To do this, we run mtmconvol and check the output size.
         # This is to verify that the number of windows is equal, and as expected.
@@ -197,157 +218,192 @@
         cfg_mtm_short = cfg_short_with_overlap.copy()
         cfg_mtm_short.method = "mtmconvol"
 
         spec_long_no_overlap = spy.freqanalysis(cfg_long_no_overlap, wn_long)
         spec_short_with_overlap = spy.freqanalysis(cfg_short_with_overlap, wn_short)
 
         # We got one Welch estimate per trial so far. Now compute the variance over trials:
-        var_dim='trials'
+        var_dim = "trials"
         var_longsig_no_overlap = spy.var(spec_long_no_overlap, dim=var_dim)
         var_shortsig_with_overlap = spy.var(spec_short_with_overlap, dim=var_dim)
 
-        assert var_longsig_no_overlap.dimord.index('time') == 0
+        assert var_longsig_no_overlap.dimord.index("time") == 0
         assert var_longsig_no_overlap.data.shape[0] == 1
         assert var_shortsig_with_overlap.data.shape[0] == 1
 
         if self.do_plot:
-            mn_long, var_long = np.mean(var_longsig_no_overlap.show(trials=0)), np.var(var_longsig_no_overlap.show(trials=0))
-
-            mn_short, var_short = np.mean(var_shortsig_with_overlap.show(trials=0)), np.var(var_shortsig_with_overlap.show(trials=0))
+            mn_long, var_long = np.mean(var_longsig_no_overlap.show(trials=0)), np.var(
+                var_longsig_no_overlap.show(trials=0)
+            )
+
+            mn_short, var_short = np.mean(var_shortsig_with_overlap.show(trials=0)), np.var(
+                var_shortsig_with_overlap.show(trials=0)
+            )
             _, ax = plt.subplots()
-            title = f"Long signal: (toi={cfg_long_no_overlap.toi}, f_timwin={cfg_long_no_overlap.t_ftimwin})\n"
+            title = (
+                f"Long signal: (toi={cfg_long_no_overlap.toi}, f_timwin={cfg_long_no_overlap.t_ftimwin})\n"
+            )
             title += f"Short signal: (toi={cfg_short_with_overlap.toi}, f_timwin={cfg_short_with_overlap.t_ftimwin})"
-            ax.bar([1, 2], [mn_long, mn_short], yerr=[var_long, var_short], width=0.5, capsize=2)
+            ax.bar(
+                [1, 2],
+                [mn_long, mn_short],
+                yerr=[var_long, var_short],
+                width=0.5,
+                capsize=2,
+            )
             ax.set_title(title)
             ax.set_ylabel("Variance")
-            ax.set_xlabel('')
-            ax.set_xticklabels(['long', 'short'])
-        chan=0
-        assert np.mean(var_longsig_no_overlap.show(channel=chan)) < np.mean(var_shortsig_with_overlap.show(channel=chan))
+            ax.set_xlabel("")
+            ax.set_xticklabels(["long", "short"])
+        chan = 0
+        assert np.mean(var_longsig_no_overlap.show(channel=chan)) < np.mean(
+            var_shortsig_with_overlap.show(channel=chan)
+        )
 
     def test_welch_overlap_effect(self):
 
         sig_lengths = np.linspace(1000, 4000, num=4, dtype=int)
         overlaps = np.linspace(0.0, 0.99, num=10)
         variances = np.zeros((sig_lengths.size, overlaps.size), dtype=float)  # Filled in loop below.
 
         foilim = [5, 200]  # Shared between cases.
         f_timwin = 0.2
 
         for sigl_idx, sig_len in enumerate(sig_lengths):
             for overl_idx, overlap in enumerate(overlaps):
-                wn = synthdata.white_noise(nTrials=20, nChannels=1, nSamples=sig_len, samplerate=1000, seed=test_seed)
+                wn = synthdata.white_noise(
+                    nTrials=20,
+                    nChannels=1,
+                    nSamples=sig_len,
+                    samplerate=1000,
+                    seed=test_seed,
+                )
 
                 cfg = TestWelch.get_welch_cfg()  # Results in 100 windows of length 100.
                 cfg.toi = overlap
                 cfg.t_ftimwin = f_timwin
                 cfg.foilim = foilim
 
                 spec = spy.freqanalysis(cfg, wn)
 
                 # We got one Welch estimate per trial so far. Now compute the variance over trials:
-                spec_var = spy.var(spec, dim='trials')
+                spec_var = spy.var(spec, dim="trials")
                 mvar = np.mean(spec_var.show(channel=0))
                 variances[sigl_idx, overl_idx] = mvar
 
         fig = plt.figure()
-        ax = fig.add_subplot(projection='3d')
+        ax = fig.add_subplot(projection="3d")
         for row_idx in range(variances.shape[0]):
-            ax.scatter(np.tile(sig_lengths[row_idx], overlaps.size), overlaps, variances[row_idx, :], label=f"Signal len {sig_lengths[row_idx]}")
-        ax.set_xlabel('Signal length (number of samples)')
-        ax.set_ylabel('Window overlap')
-        ax.set_zlabel('Mean variance of the Welch estimate')
-        ax.set_title('Variance of Welsh estimate as a function of signal length and overlap.\nColors represent different signal lengths.')
+            ax.scatter(
+                np.tile(sig_lengths[row_idx], overlaps.size),
+                overlaps,
+                variances[row_idx, :],
+                label=f"Signal len {sig_lengths[row_idx]}",
+            )
+        ax.set_xlabel("Signal length (number of samples)")
+        ax.set_ylabel("Window overlap")
+        ax.set_zlabel("Mean variance of the Welch estimate")
+        ax.set_title(
+            "Variance of Welsh estimate as a function of signal length and overlap.\nColors represent different signal lengths."
+        )
         # plt.show()  # We could run 'plt.legend()' before this line, but it's a bit large.
 
         # Now for the tests.
         # For a fixed overlap, the variance should decrease with signal length:
         for overlap_idx in range(variances.shape[1]):
             for siglen_idx in range(1, variances.shape[0]):
                 assert variances[siglen_idx, overlap_idx] < variances[siglen_idx - 1, overlap_idx]
 
         # For short signals, there is a benefit in using medium overlap:
-        assert np.argmin(variances[0, :]) == overlaps.size // 2, f"Expected {overlaps.size // 2}, got {np.argmin(variances[0, :])}."
+        assert (
+            np.argmin(variances[0, :]) == overlaps.size // 2
+        ), f"Expected {overlaps.size // 2}, got {np.argmin(variances[0, :])}."
         # Note: For humans, looking at the plot above will illustrate this a lot better.
 
-
     def test_welch_replay(self):
         """Test replay with settings from output cfg."""
         # only preprocessing makes sense to chain atm
         first_cfg = TestWelch.get_welch_cfg()
         first_res = spy.freqanalysis(self.adata, cfg=first_cfg)
 
         # Now replay with cfg from preceding frontend call:
         replay_res = spy.freqanalysis(self.adata, cfg=first_res.cfg)
 
         # same results
         assert np.allclose(first_res.data[:], replay_res.data[:])
         assert first_res.cfg == replay_res.cfg
 
-
     def test_welch_trial_averaging(self):
         cfg = TestWelch.get_welch_cfg()
-        cfg.keeptrials = False  # Activate trial averaging. This happens during mtmfftconvolv, Welch just gets less input.
+        cfg.keeptrials = (
+            False  # Activate trial averaging. This happens during mtmfftconvolv, Welch just gets less input.
+        )
 
         res = spy.freqanalysis(cfg, self.adata)
         # Test basic output properties.
         assert len(res.dimord) == 4
         assert len(res.data.shape) == 4
-        assert res.dimord.index('time') == 0
-        assert res.dimord.index('taper') == 1
-        assert res.dimord.index('freq') == 2
-        assert res.dimord.index('channel') == 3
+        assert res.dimord.index("time") == 0
+        assert res.dimord.index("taper") == 1
+        assert res.dimord.index("freq") == 2
+        assert res.dimord.index("channel") == 3
 
         # Test ouput shape:
         # The time dimensions is the important thing, trial averaging of the 2 trials leads to only 1 left:
-        assert res.data.shape[res.dimord.index('time')] == 1
-        assert res.data.shape[res.dimord.index('taper')] == 1  # Nothing special expected here.
-        assert res.data.shape[res.dimord.index('channel')] == 3  # Nothing special expected here.
+        assert res.data.shape[res.dimord.index("time")] == 1
+        assert res.data.shape[res.dimord.index("taper")] == 1  # Nothing special expected here.
+        assert res.data.shape[res.dimord.index("channel")] == 3  # Nothing special expected here.
 
         # Another relevant assertion specific to this test case: trialdefinition
         assert res.trialdefinition.shape[0] == 1  # trial averaging has been performed, so only 1 trial left.
 
         if self.do_plot:
             _, ax = res.singlepanelplot(trials=0, channel=0)
             ax.set_title("Welsh result with trial averaging.")
 
     def test_welch_with_multitaper(self):
         cfg = TestWelch.get_welch_cfg()
         cfg.tapsmofrq = 2  # Activate multi-tapering, which is fine.
-        cfg.keeptapers = False  # Disable averaging over tapers (taper dimension), which is NOT allowed with Welsh.
+        cfg.keeptapers = (
+            False  # Disable averaging over tapers (taper dimension), which is NOT allowed with Welsh.
+        )
 
         res = spy.freqanalysis(cfg, self.adata)
-        assert res.data.shape[res.dimord.index('taper')] == 1  # Averaging over tapers expected.
-        assert res.data.shape[res.dimord.index('channel')] == 3  # Nothing special expected here.
+        assert res.data.shape[res.dimord.index("taper")] == 1  # Averaging over tapers expected.
+        assert res.data.shape[res.dimord.index("channel")] == 3  # Nothing special expected here.
 
     def test_parallel(self, testcluster):
         plt.ioff()
         self.do_plot = False
         client = dd.Client(testcluster)
-        all_tests = [attr for attr in self.__dir__()
-                     if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr and attr.startswith('test'))]
+        all_tests = [
+            attr
+            for attr in self.__dir__()
+            if (inspect.ismethod(getattr(self, attr)) and "parallel" not in attr and attr.startswith("test"))
+        ]
 
         for test in all_tests:
             test_method = getattr(self, test)
             test_method()
         client.close()
         self.do_plot = True
         plt.ion()
 
-
     def test_welch_rejects_keeptaper(self):
         cfg = TestWelch.get_welch_cfg()
         cfg.tapsmofrq = 2  # Activate multi-tapering, which is fine.
-        cfg.keeptapers = True  # Disable averaging over tapers (taper dimension), which is NOT allowed with Welsh.
+        cfg.keeptapers = (
+            True  # Disable averaging over tapers (taper dimension), which is NOT allowed with Welsh.
+        )
         with pytest.raises(SPYValueError, match="keeptapers"):
             _ = spy.freqanalysis(cfg, self.adata)
 
     def test_welch_rejects_invalid_tois(self):
         cfg = TestWelch.get_welch_cfg()
-        for toi in ['all', np.linspace(0.0, 1.0, 5)]:
+        for toi in ["all", np.linspace(0.0, 1.0, 5)]:
             cfg.toi = toi
             with pytest.raises(SPYValueError, match="toi"):
                 _ = spy.freqanalysis(cfg, self.adata)
 
     def test_welch_rejects_invalid_output(self):
         cfg = TestWelch.get_welch_cfg()
         for output in spectralConversions.keys():
@@ -355,12 +411,14 @@
                 cfg.output = output
                 with pytest.raises(SPYValueError, match="output"):
                     _ = spy.freqanalysis(cfg, self.adata)
 
     def teardown_class(cls):
         teardown()
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     if TestWelch.do_plot:
         import matplotlib.pyplot as plt
+
         plt.ion()
     T1 = TestWelch()
```

### Comparing `esi_syncopy-2023.5/PKG-INFO` & `esi_syncopy-2023.7/PKG-INFO`

 * *Files 6% similar despite different names*

```diff
@@ -1,53 +1,56 @@
 Metadata-Version: 2.1
 Name: esi-syncopy
-Version: 2023.5
+Version: 2023.7
 Summary: A toolkit for user-friendly large-scale electrophysiology data analysis. Syncopy is compatible with the Matlab toolbox FieldTrip.
 Home-page: https://syncopy.org
 License: BSD-3-Clause
 Author: Stefan Fürtinger
 Author-email: sfuerti@esi-frankfurt.de
-Requires-Python: >=3.8,<4.0
+Requires-Python: >=3.8,<3.12
 Classifier: Environment :: Console
 Classifier: Framework :: Jupyter
 Classifier: License :: OSI Approved :: BSD License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: Topic :: Scientific/Engineering
+Requires-Dist: bokeh (>=3.1.1,<4.0.0)
 Requires-Dist: dask-jobqueue (>=0.8)
 Requires-Dist: dask[distributed] (>=2022.6)
 Requires-Dist: fooof (>=1.0)
 Requires-Dist: h5py (>=2.9)
 Requires-Dist: matplotlib (>=3.5)
 Requires-Dist: natsort (>=8.1.0,<9.0.0)
 Requires-Dist: numpy (>=1.10)
 Requires-Dist: psutil (>=5.9)
-Requires-Dist: scipy (>=1.5)
+Requires-Dist: scipy (>=1.10.0)
 Requires-Dist: tqdm (>=4.31)
 Project-URL: Repository, https://github.com/esi-neuroscience/syncopy
 Description-Content-Type: text/x-rst
 
 .. image:: https://raw.githubusercontent.com/esi-neuroscience/syncopy/master/doc/source/_static/syncopy_logo_small.png
 	   :alt: Syncopy-Logo
 
 Systems Neuroscience Computing in Python
 ========================================
 
 
-|Conda Version| |PyPi Version| |License|
+|Conda Version| |PyPi Version| |License| |DOI|
 
 .. |Conda Version| image:: https://img.shields.io/conda/vn/conda-forge/esi-syncopy.svg
    :target: https://anaconda.org/conda-forge/esi-syncopy
 .. |PyPI version| image:: https://badge.fury.io/py/esi-syncopy.svg
    :target: https://badge.fury.io/py/esi-syncopy
 .. |License| image:: https://img.shields.io/github/license/esi-neuroscience/syncopy
+.. |DOI| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.8191941.svg
+   :target: https://doi.org/10.5281/zenodo.8191941
 
 |Master Tests| |Master Coverage|
 
 .. |Master Tests| image:: https://github.com/esi-neuroscience/syncopy/actions/workflows/cov_test_workflow.yml/badge.svg?branch=master
    :target: https://github.com/esi-neuroscience/syncopy/actions/workflows/cov_test_workflow.yml
 .. |Master Coverage| image:: https://codecov.io/gh/esi-neuroscience/syncopy/branch/master/graph/badge.svg?token=JEI3QQGNBQ
    :target: https://codecov.io/gh/esi-neuroscience/syncopy
```

