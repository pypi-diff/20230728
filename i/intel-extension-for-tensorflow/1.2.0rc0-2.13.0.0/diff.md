# Comparing `tmp/intel_extension_for_tensorflow-1.2.0rc0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip` & `tmp/intel_extension_for_tensorflow-2.13.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,34 +1,45 @@
-Zip file size: 130636 bytes, number of entries: 32
--rw-rw-r--  2.0 unx     1898 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/__init__.py
--rw-rw-r--  2.0 unx      243 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/__main__.py
--rwxr-xr-x  2.0 unx        0 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/core/utils/protobuf/__init__.py
--rwxr-xr-x  2.0 unx     3575 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/core/utils/protobuf/config_pb2.py
--rwxr-xr-x  2.0 unx        0 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/__init__.py
--rw-rw-r--  2.0 unx     1403 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/config.py
--rw-rw-r--  2.0 unx     1214 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/device.py
--rw-rw-r--  2.0 unx    26593 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/experimental_ops_override.py
--rw-rw-r--  2.0 unx     2190 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/gen_itex_version.py
--rw-rw-r--  2.0 unx    31025 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/launch.py
--rw-rw-r--  2.0 unx     1028 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/version.py
--rw-rw-r--  2.0 unx     1169 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/ops/__init__.py
--rw-rw-r--  2.0 unx     2531 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/ops/activations.py
--rw-rw-r--  2.0 unx    17867 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/ops/layer_norm.py
--rw-rw-r--  2.0 unx     2668 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/ops/load_ops_library.py
--rw-rw-r--  2.0 unx     3451 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/ops/ops_grad.py
--rw-rw-r--  2.0 unx    10707 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/ops/optimizers.py
--rw-rw-r--  2.0 unx    28487 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/ops/recurrent.py
--rw-rw-r--  2.0 unx     1038 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/test_func/__init__.py
--rw-rw-r--  2.0 unx     1116 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/test_func/eager_test.py
--rw-rw-r--  2.0 unx    17632 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/test_func/keras_parameterized.py
--rw-rw-r--  2.0 unx    38058 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/test_func/keras_testing_utils.py
--rw-rw-r--  2.0 unx     6345 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/test_func/test.py
--rw-rw-r--  2.0 unx   135335 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/python/test_func/test_util.py
--rw-rw-r--  2.0 unx    81897 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/third-party-programs/THIRD-PARTY-PROGRAMS
--rw-rw-r--  2.0 unx    28478 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onednn.txt
--rw-rw-r--  2.0 unx    21801 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onevpl.txt
--rw-rw-r--  2.0 unx    10775 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow-1.2.0rc0.dist-info/LICENSE.txt
--rw-rw-r--  2.0 unx     3391 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow-1.2.0rc0.dist-info/METADATA
--rw-rw-r--  2.0 unx      103 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow-1.2.0rc0.dist-info/WHEEL
--rw-rw-r--  2.0 unx       31 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow-1.2.0rc0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3643 b- defN 23-Apr-11 11:02 intel_extension_for_tensorflow-1.2.0rc0.dist-info/RECORD
-32 files, 485692 bytes uncompressed, 124442 bytes compressed:  74.4%
+Zip file size: 153251 bytes, number of entries: 43
+-rw-r--r--  2.0 unx     1895 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/__init__.py
+-rw-r--r--  2.0 unx      243 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/__main__.py
+-rwxr-xr-x  2.0 unx        0 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/core/utils/protobuf/__init__.py
+-rwxr-xr-x  2.0 unx     3416 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/core/utils/protobuf/config_pb2.py
+-rwxr-xr-x  2.0 unx        0 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/__init__.py
+-rw-r--r--  2.0 unx     1403 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/config.py
+-rw-r--r--  2.0 unx      942 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/device.py
+-rw-r--r--  2.0 unx    20774 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/experimental_ops_override.py
+-rw-r--r--  2.0 unx     2220 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/gen_itex_version.py
+-rw-r--r--  2.0 unx    34934 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/launch.py
+-rw-r--r--  2.0 unx     1092 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/version.py
+-rw-r--r--  2.0 unx      931 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/fp8/__init__.py
+-rw-r--r--  2.0 unx     4634 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/fp8/autocast.py
+-rw-r--r--  2.0 unx     3331 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/fp8/recipe.py
+-rw-r--r--  2.0 unx     1437 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/ops/__init__.py
+-rw-r--r--  2.0 unx     2531 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/ops/activations.py
+-rw-r--r--  2.0 unx    10611 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/ops/group_norm.py
+-rw-r--r--  2.0 unx    17931 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/ops/layer_norm.py
+-rw-r--r--  2.0 unx     2668 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/ops/load_ops_library.py
+-rw-r--r--  2.0 unx    11810 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/ops/mlp.py
+-rw-r--r--  2.0 unx     5246 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/ops/multi_head_attention.py
+-rw-r--r--  2.0 unx     4625 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/ops/ops_grad.py
+-rw-r--r--  2.0 unx    10717 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/ops/optimizers.py
+-rw-r--r--  2.0 unx    28376 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/ops/recurrent.py
+-rw-r--r--  2.0 unx     1038 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/test_func/__init__.py
+-rw-r--r--  2.0 unx     1116 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/test_func/eager_test.py
+-rw-r--r--  2.0 unx    17632 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/test_func/keras_parameterized.py
+-rw-r--r--  2.0 unx    37244 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/test_func/keras_testing_utils.py
+-rw-r--r--  2.0 unx     6345 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/test_func/test.py
+-rw-r--r--  2.0 unx   135744 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/test_func/test_util.py
+-rw-r--r--  2.0 unx      848 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/transformer/__init__.py
+-rw-r--r--  2.0 unx    18767 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/transformer/attention.py
+-rw-r--r--  2.0 unx     7846 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/transformer/base.py
+-rw-r--r--  2.0 unx     2515 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/transformer/common.py
+-rw-r--r--  2.0 unx    16277 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/python/transformer/layer.py
+-rw-r--r--  2.0 unx    81897 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/third-party-programs/THIRD-PARTY-PROGRAMS
+-rw-r--r--  2.0 unx    28478 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onednn.txt
+-rw-r--r--  2.0 unx    21801 b- defN 23-Jul-28 01:16 intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onevpl.txt
+-rw-r--r--  2.0 unx    10775 b- defN 23-Jul-28 01:17 intel_extension_for_tensorflow-2.13.0.0.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     3642 b- defN 23-Jul-28 01:17 intel_extension_for_tensorflow-2.13.0.0.dist-info/METADATA
+-rw-r--r--  2.0 unx      103 b- defN 23-Jul-28 01:17 intel_extension_for_tensorflow-2.13.0.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       31 b- defN 23-Jul-28 01:17 intel_extension_for_tensorflow-2.13.0.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     4893 b- defN 23-Jul-28 01:17 intel_extension_for_tensorflow-2.13.0.0.dist-info/RECORD
+43 files, 568759 bytes uncompressed, 144977 bytes compressed:  74.5%
```

## zipnote {}

```diff
@@ -27,26 +27,44 @@
 
 Filename: intel_extension_for_tensorflow/python/launch.py
 Comment: 
 
 Filename: intel_extension_for_tensorflow/python/version.py
 Comment: 
 
+Filename: intel_extension_for_tensorflow/python/fp8/__init__.py
+Comment: 
+
+Filename: intel_extension_for_tensorflow/python/fp8/autocast.py
+Comment: 
+
+Filename: intel_extension_for_tensorflow/python/fp8/recipe.py
+Comment: 
+
 Filename: intel_extension_for_tensorflow/python/ops/__init__.py
 Comment: 
 
 Filename: intel_extension_for_tensorflow/python/ops/activations.py
 Comment: 
 
+Filename: intel_extension_for_tensorflow/python/ops/group_norm.py
+Comment: 
+
 Filename: intel_extension_for_tensorflow/python/ops/layer_norm.py
 Comment: 
 
 Filename: intel_extension_for_tensorflow/python/ops/load_ops_library.py
 Comment: 
 
+Filename: intel_extension_for_tensorflow/python/ops/mlp.py
+Comment: 
+
+Filename: intel_extension_for_tensorflow/python/ops/multi_head_attention.py
+Comment: 
+
 Filename: intel_extension_for_tensorflow/python/ops/ops_grad.py
 Comment: 
 
 Filename: intel_extension_for_tensorflow/python/ops/optimizers.py
 Comment: 
 
 Filename: intel_extension_for_tensorflow/python/ops/recurrent.py
@@ -66,32 +84,47 @@
 
 Filename: intel_extension_for_tensorflow/python/test_func/test.py
 Comment: 
 
 Filename: intel_extension_for_tensorflow/python/test_func/test_util.py
 Comment: 
 
+Filename: intel_extension_for_tensorflow/python/transformer/__init__.py
+Comment: 
+
+Filename: intel_extension_for_tensorflow/python/transformer/attention.py
+Comment: 
+
+Filename: intel_extension_for_tensorflow/python/transformer/base.py
+Comment: 
+
+Filename: intel_extension_for_tensorflow/python/transformer/common.py
+Comment: 
+
+Filename: intel_extension_for_tensorflow/python/transformer/layer.py
+Comment: 
+
 Filename: intel_extension_for_tensorflow/third-party-programs/THIRD-PARTY-PROGRAMS
 Comment: 
 
 Filename: intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onednn.txt
 Comment: 
 
 Filename: intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onevpl.txt
 Comment: 
 
-Filename: intel_extension_for_tensorflow-1.2.0rc0.dist-info/LICENSE.txt
+Filename: intel_extension_for_tensorflow-2.13.0.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: intel_extension_for_tensorflow-1.2.0rc0.dist-info/METADATA
+Filename: intel_extension_for_tensorflow-2.13.0.0.dist-info/METADATA
 Comment: 
 
-Filename: intel_extension_for_tensorflow-1.2.0rc0.dist-info/WHEEL
+Filename: intel_extension_for_tensorflow-2.13.0.0.dist-info/WHEEL
 Comment: 
 
-Filename: intel_extension_for_tensorflow-1.2.0rc0.dist-info/top_level.txt
+Filename: intel_extension_for_tensorflow-2.13.0.0.dist-info/top_level.txt
 Comment: 
 
-Filename: intel_extension_for_tensorflow-1.2.0rc0.dist-info/RECORD
+Filename: intel_extension_for_tensorflow-2.13.0.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## intel_extension_for_tensorflow/__init__.py

```diff
@@ -14,16 +14,16 @@
 # ==============================================================================
 '''Init file for graph optimizer config, custom ops'''
 
 import tensorflow  # pylint: disable=unused-import
 import intel_extension_for_tensorflow_lib  # pylint: disable=unused-import
 from intel_extension_for_tensorflow.python.config import set_config  # pylint: disable=unused-import
 from intel_extension_for_tensorflow.python.config import get_config  # pylint: disable=unused-import
-from intel_extension_for_tensorflow.python.device import set_backend  # pylint: disable=unused-import
 from intel_extension_for_tensorflow.python.device import get_backend  # pylint: disable=unused-import
+from intel_extension_for_tensorflow.python.device import is_xehpc  # pylint: disable=unused-import
 from intel_extension_for_tensorflow.python import ops  # pylint: disable=unused-import,line-too-long
 from intel_extension_for_tensorflow.python.version import __version__  # pylint: disable=unused-import
 from intel_extension_for_tensorflow.python import version  # pylint: disable=unused-import
 from intel_extension_for_tensorflow.python import test_func  # pylint: disable=unused-import
 
 from intel_extension_for_tensorflow.core.utils.protobuf.config_pb2 import *  # pylint: disable=unused-import,wildcard-import,unused-wildcard-import
 from intel_extension_for_tensorflow.python.experimental_ops_override import experimental_ops_override
```

## intel_extension_for_tensorflow/core/utils/protobuf/config_pb2.py

```diff
@@ -9,33 +9,31 @@
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n%itex/core/utils/protobuf/config.proto\x12\x04itex\"\x0c\n\nGPUOptions\"\xb6\x02\n\x0cGraphOptions\x12\"\n\x0conednn_graph\x18\x01 \x01(\x0e\x32\x0c.itex.Toggle\x12 \n\nlayout_opt\x18\x02 \x01(\x0e\x32\x0c.itex.Toggle\x12\x1e\n\x08remapper\x18\x03 \x01(\x0e\x32\x0c.itex.Toggle\x12*\n\x14\x61uto_mixed_precision\x18\x04 \x01(\x0e\x32\x0c.itex.Toggle\x12\x45\n\x1c\x61uto_mixed_precision_options\x18\x05 \x01(\x0b\x32\x1f.itex.AutoMixedPrecisionOptions\x12\x1e\n\x08sharding\x18\x06 \x01(\x0e\x32\x0c.itex.Toggle\x12-\n\x0fsharding_config\x18\x07 \x01(\x0b\x32\x14.itex.ShardingConfig\"\x8a\x01\n\x0b\x43onfigProto\x12%\n\x0bgpu_options\x18\x01 \x01(\x0b\x32\x10.itex.GPUOptions\x12)\n\rgraph_options\x18\x02 \x01(\x0b\x32\x12.itex.GraphOptions\x12)\n\rdebug_options\x18\x03 \x01(\x0b\x32\x12.itex.DebugOptions\"\x9e\x02\n\x19\x41utoMixedPrecisionOptions\x12\x15\n\rallowlist_add\x18\x01 \x01(\t\x12\x15\n\rinferlist_add\x18\x02 \x01(\t\x12\x15\n\rclearlist_add\x18\x03 \x01(\t\x12\x14\n\x0c\x64\x65nylist_add\x18\x04 \x01(\t\x12\x18\n\x10\x61llowlist_remove\x18\x05 \x01(\t\x12\x18\n\x10inferlist_remove\x18\x06 \x01(\t\x12\x18\n\x10\x63learlist_remove\x18\x07 \x01(\t\x12\x17\n\x0f\x64\x65nylist_remove\x18\x08 \x01(\t\x12\x18\n\x10unsafe_force_all\x18\t \x01(\x08\x12%\n\tdata_type\x18\n \x01(\x0e\x32\x12.itex.ITEXDataType\"[\n\x0c\x44\x65\x62ugOptions\x12%\n\x1d\x61uto_mixed_precision_log_path\x18\x01 \x01(\t\x12$\n\x0expu_force_sync\x18\x02 \x01(\x0e\x32\x0c.itex.Toggle\"K\n\x0eShardingConfig\x12\x11\n\tauto_mode\x18\x01 \x01(\x08\x12&\n\x07\x64\x65vices\x18\x02 \x03(\x0b\x32\x15.itex.ShardingDevices\"a\n\x0fShardingDevices\x12\x13\n\x0b\x64\x65vice_type\x18\x01 \x01(\t\x12\x12\n\ndevice_num\x18\x02 \x01(\x05\x12\x12\n\nbatch_size\x18\x03 \x01(\x05\x12\x11\n\tstage_num\x18\x04 \x01(\x05*&\n\x06Toggle\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\x06\n\x02ON\x10\x01\x12\x07\n\x03OFF\x10\x02*@\n\x0cITEXDataType\x12\x15\n\x11\x44\x45\x46\x41ULT_DATA_TYPE\x10\x00\x12\x0b\n\x07\x46LOAT16\x10\x01\x12\x0c\n\x08\x42\x46LOAT16\x10\x02\x62\x06proto3')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n%itex/core/utils/protobuf/config.proto\x12\x04itex\"\xb6\x02\n\x0cGraphOptions\x12\"\n\x0conednn_graph\x18\x01 \x01(\x0e\x32\x0c.itex.Toggle\x12 \n\nlayout_opt\x18\x02 \x01(\x0e\x32\x0c.itex.Toggle\x12\x1e\n\x08remapper\x18\x03 \x01(\x0e\x32\x0c.itex.Toggle\x12*\n\x14\x61uto_mixed_precision\x18\x04 \x01(\x0e\x32\x0c.itex.Toggle\x12\x45\n\x1c\x61uto_mixed_precision_options\x18\x05 \x01(\x0b\x32\x1f.itex.AutoMixedPrecisionOptions\x12\x1e\n\x08sharding\x18\x06 \x01(\x0e\x32\x0c.itex.Toggle\x12-\n\x0fsharding_config\x18\x07 \x01(\x0b\x32\x14.itex.ShardingConfig\"c\n\x0b\x43onfigProto\x12)\n\rgraph_options\x18\x01 \x01(\x0b\x32\x12.itex.GraphOptions\x12)\n\rdebug_options\x18\x02 \x01(\x0b\x32\x12.itex.DebugOptions\"\x9e\x02\n\x19\x41utoMixedPrecisionOptions\x12\x15\n\rallowlist_add\x18\x01 \x01(\t\x12\x15\n\rinferlist_add\x18\x02 \x01(\t\x12\x15\n\rclearlist_add\x18\x03 \x01(\t\x12\x14\n\x0c\x64\x65nylist_add\x18\x04 \x01(\t\x12\x18\n\x10\x61llowlist_remove\x18\x05 \x01(\t\x12\x18\n\x10inferlist_remove\x18\x06 \x01(\t\x12\x18\n\x10\x63learlist_remove\x18\x07 \x01(\t\x12\x17\n\x0f\x64\x65nylist_remove\x18\x08 \x01(\t\x12\x18\n\x10unsafe_force_all\x18\t \x01(\x08\x12%\n\tdata_type\x18\n \x01(\x0e\x32\x12.itex.ITEXDataType\"[\n\x0c\x44\x65\x62ugOptions\x12%\n\x1d\x61uto_mixed_precision_log_path\x18\x01 \x01(\t\x12$\n\x0expu_force_sync\x18\x02 \x01(\x0e\x32\x0c.itex.Toggle\"K\n\x0eShardingConfig\x12\x11\n\tauto_mode\x18\x01 \x01(\x08\x12&\n\x07\x64\x65vices\x18\x02 \x03(\x0b\x32\x15.itex.ShardingDevices\"a\n\x0fShardingDevices\x12\x13\n\x0b\x64\x65vice_type\x18\x01 \x01(\t\x12\x12\n\ndevice_num\x18\x02 \x01(\x05\x12\x12\n\nbatch_size\x18\x03 \x01(\x05\x12\x11\n\tstage_num\x18\x04 \x01(\x05*&\n\x06Toggle\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\x06\n\x02ON\x10\x01\x12\x07\n\x03OFF\x10\x02*@\n\x0cITEXDataType\x12\x15\n\x11\x44\x45\x46\x41ULT_DATA_TYPE\x10\x00\x12\x0b\n\x07\x46LOAT16\x10\x01\x12\x0c\n\x08\x42\x46LOAT16\x10\x02\x62\x06proto3')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'itex.core.utils.protobuf.config_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
-  _TOGGLE._serialized_start=1073
-  _TOGGLE._serialized_end=1111
-  _ITEXDATATYPE._serialized_start=1113
-  _ITEXDATATYPE._serialized_end=1177
-  _GPUOPTIONS._serialized_start=47
-  _GPUOPTIONS._serialized_end=59
-  _GRAPHOPTIONS._serialized_start=62
-  _GRAPHOPTIONS._serialized_end=372
-  _CONFIGPROTO._serialized_start=375
-  _CONFIGPROTO._serialized_end=513
-  _AUTOMIXEDPRECISIONOPTIONS._serialized_start=516
-  _AUTOMIXEDPRECISIONOPTIONS._serialized_end=802
-  _DEBUGOPTIONS._serialized_start=804
-  _DEBUGOPTIONS._serialized_end=895
-  _SHARDINGCONFIG._serialized_start=897
-  _SHARDINGCONFIG._serialized_end=972
-  _SHARDINGDEVICES._serialized_start=974
-  _SHARDINGDEVICES._serialized_end=1071
+  _TOGGLE._serialized_start=1019
+  _TOGGLE._serialized_end=1057
+  _ITEXDATATYPE._serialized_start=1059
+  _ITEXDATATYPE._serialized_end=1123
+  _GRAPHOPTIONS._serialized_start=48
+  _GRAPHOPTIONS._serialized_end=358
+  _CONFIGPROTO._serialized_start=360
+  _CONFIGPROTO._serialized_end=459
+  _AUTOMIXEDPRECISIONOPTIONS._serialized_start=462
+  _AUTOMIXEDPRECISIONOPTIONS._serialized_end=748
+  _DEBUGOPTIONS._serialized_start=750
+  _DEBUGOPTIONS._serialized_end=841
+  _SHARDINGCONFIG._serialized_start=843
+  _SHARDINGCONFIG._serialized_end=918
+  _SHARDINGDEVICES._serialized_start=920
+  _SHARDINGDEVICES._serialized_end=1017
 # @@protoc_insertion_point(module_scope)
```

## intel_extension_for_tensorflow/python/device.py

```diff
@@ -16,18 +16,12 @@
 """device"""
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
 from intel_extension_for_tensorflow.python._pywrap_itex import *
 
-_VALID_DEVICE_BACKENDS = frozenset({"CPU", "GPU", "AUTO"})
-
-def set_backend(backend):
-  if backend.upper() in _VALID_DEVICE_BACKENDS:
-    ITEX_SetBackend(backend.upper())
-  else:
-    raise ValueError("Cannot specify %s as XPU backend, Only %s is VALID"
-                     % (backend, _VALID_DEVICE_BACKENDS))
-
 def get_backend():
   return ITEX_GetBackend()
+
+def is_xehpc():
+  return ITEX_IsXeHPC()
```

## intel_extension_for_tensorflow/python/experimental_ops_override.py

```diff
@@ -21,25 +21,28 @@
 """ITEX optimization for some TensorFlow API."""
 import builtins
 import logging
 import types
 import numpy as np
 import tensorflow as tf
 
+from tensorflow.python.framework import config
 from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
-from tensorflow.python.framework import config
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import gen_array_ops
 from tensorflow.python.ops import math_ops
+from tensorflow.python.ops import state_ops
 from tensorflow.python.util import compat
 
+from intel_extension_for_tensorflow.python.device import get_backend
 from intel_extension_for_tensorflow.python.ops.layer_norm import _layer_norm
 from intel_extension_for_tensorflow.python.ops.activations import gelu as itex_gelu
-from intel_extension_for_tensorflow.python.ops.recurrent import ItexLSTM
+from intel_extension_for_tensorflow.python.ops.recurrent import gpu_lstm
+from intel_extension_for_tensorflow.python.ops.recurrent import is_itex_supported_inputs
 
 format_str = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
 logging.basicConfig(level=logging.INFO, format=format_str)
 logger = logging.getLogger(__name__)
 
 def copy_func(f, name=None):
   '''
@@ -48,189 +51,39 @@
   '''
   fn = types.FunctionType(f.__code__, f.__globals__, name or f.__name__,
        f.__defaults__, f.__closure__)
   # in case f was given attrs (note this dict is a shallow copy):
   fn.__dict__.update(f.__dict__)
   return fn
 
+@tf.custom_gradient
+def itex_optimized_bmm(x, w, b):
+  result = tf.raw_ops.BatchMatMulV2(x=x, y=w)
+  result = tf.nn.bias_add(result, b)
+  def grad(upstream):
+    '''
+    # This is the gradient using BMM
+    dx = tf.raw_ops.BatchMatMulV2(x=upstream, y=w, adj_y=True)
+    dy = tf.raw_ops.BatchMatMulV2(x=x, y=upstream, adj_x=True)
+    dy = tf.math.reduce_sum(dy,axis=0)
+    db = tf.raw_ops.BiasAddGrad(out_backprop=upstream)
+    '''
+    upstream_shape = tf.shape(upstream)
+    # upstream rank must > 3
+    batch_size = tf.math.reduce_prod(upstream_shape[:-1])
+    upstream_2d = tf.reshape(upstream,[batch_size,-1])
+    x_2d = tf.reshape(x,[batch_size,-1])
+    dx = tf.raw_ops.BatchMatMulV2(x=upstream, y=w, adj_y=True)
+    dy = tf.matmul(x_2d,upstream_2d,transpose_a=True)
+    db = tf.raw_ops.BiasAddGrad(out_backprop=upstream_2d)
+    return dx, dy, db
+  return result, grad
+
 def itex_dense_layer_call(self, inputs, training=None):
   r"""ITEX optimized dense layer"""
-  if training is None:
-    training = tf.keras.backend.learning_phase() # scalar integer tensor or Python integer
-  def tensordot(a, b, axes, name=None):
-    r"""Tensor contraction of a and b along specified axes and outer product.
-    Tensordot (also known as tensor contraction) sums the product of elements
-    from `a` and `b` over the indices specified by `axes`.
-    This operation corresponds to `numpy.tensordot(a, b, axes)`.
-    Example 1: When `a` and `b` are matrices (order 2), the case `axes=1`
-    is equivalent to matrix multiplication.
-    Example 2: When `a` and `b` are matrices (order 2), the case
-    `axes = [[1], [0]]` is equivalent to matrix multiplication.
-    Example 3: When `a` and `b` are matrices (order 2), the case `axes=0` gives
-    the outer product, a tensor of order 4.
-    Example 4: Suppose that \\(a_{ijk}\\) and \\(b_{lmn}\\) represent two
-    tensors of order 3. Then, `contract(a, b, [[0], [2]])` is the order 4 tensor
-    \\(c_{jklm}\\) whose entry
-    corresponding to the indices \\((j,k,l,m)\\) is given by:
-    \\( c_{jklm} = \sum_i a_{ijk} b_{lmi} \\).
-    In general, `order(c) = order(a) + order(b) - 2*len(axes[0])`.
-    Args:
-    a: `Tensor` of type `float32` or `float64`.
-    b: `Tensor` with the same type as `a`.
-    axes: Either a scalar `N`, or a list or an `int32` `Tensor` of shape [2, k].
-      If axes is a scalar, sum over the last N axes of a and the first N axes of
-      b in order. If axes is a list or `Tensor` the first and second row contain
-      the set of unique integers specifying axes along which the contraction is
-      computed, for `a` and `b`, respectively. The number of axes for `a` and
-      `b` must be equal. If `axes=0`, computes the outer product between `a` and
-      `b`.
-    name: A name for the operation (optional).
-    Returns:
-    A `Tensor` with the same type as `a`.
-    Raises:
-    ValueError: If the shapes of `a`, `b`, and `axes` are incompatible.
-    IndexError: If the values in axes exceed the rank of the corresponding
-        tensor.
-    """
-
-    def _tensordot_reshape(a, axes, flipped=False):
-      """Helper method to perform transpose and reshape for contraction op.
-      This method is helpful in reducing `math_ops.tensordot` to
-      `math_ops.matmul` using `array_ops.transpose` and
-      `array_ops.reshape`. The method takes a tensor and performs the
-      correct transpose and reshape operation for a given set of indices.
-      It returns the reshaped tensor as well as a list of indices
-      necessary to reshape the tensor again after matrix multiplication.
-      Args:
-      a: `Tensor`.
-      axes: List or `int32` `Tensor` of unique indices specifying valid axes of
-          `a`.
-      flipped: An optional `bool`. Defaults to `False`. If `True`, the method
-          assumes that `a` is the second argument in the contraction operation.
-      Returns:
-      A tuple `(reshaped_a, free_dims, free_dims_static)` where `reshaped_a` is
-      the tensor `a` reshaped to allow contraction via `matmul`, `free_dims` is
-      either a list of integers or an `int32` `Tensor`, depending on whether
-      the shape of a is fully specified, and free_dims_static is either a list
-      of integers and None values, or None, representing the inferred
-      static shape of the free dimensions
-      """
-      if a.get_shape().is_fully_defined() and isinstance(axes, (list, tuple)): # pylint: disable=no-else-return
-        shape_a = a.get_shape().as_list()
-        axes = [i if i >= 0 else i + len(shape_a) for i in axes]
-        free = [i for i in builtins.range(len(shape_a)) if i not in axes]
-        free_dims = [shape_a[i] for i in free]
-        prod_free = int(np.prod([shape_a[i] for i in free]))
-        prod_axes = int(np.prod([shape_a[i] for i in axes]))
-        perm = list(axes) + free if flipped else free + list(axes)
-        new_shape = [prod_axes, prod_free] if flipped else [
-            prod_free, prod_axes]
-        if (perm != np.arange(len(shape_a))).any():
-          a_trans = array_ops.transpose(a, perm)
-        else:
-          a_trans = a
-        if a_trans.get_shape().as_list() != new_shape:
-          reshaped_a = array_ops.reshape(a_trans, new_shape)
-        else:
-          reshaped_a = a_trans
-        return reshaped_a, free_dims, free_dims
-      else:
-        if a.get_shape().ndims is not None and isinstance(axes, (list, tuple)):
-          shape_a = a.get_shape().as_list()
-          axes = [i if i >= 0 else i + len(shape_a) for i in axes]
-          free = [i for i in builtins.range(len(shape_a)) if i not in axes]
-          axes_dims = [shape_a[i] for i in axes]
-          free_dims = [shape_a[i] for i in free]
-          free_dims_static = free_dims
-          axes = ops.convert_to_tensor(axes, dtype=dtypes.int32, name="axes")
-          free = ops.convert_to_tensor(free, dtype=dtypes.int32, name="free")
-          shape_a = array_ops.shape(a)
-        else:
-          free_dims_static = None
-          shape_a = array_ops.shape(a)
-          rank_a = array_ops.rank(a)
-          axes = ops.convert_to_tensor(axes, dtype=dtypes.int32, name="axes")
-          axes = array_ops.where(axes >= 0, axes, axes + rank_a)
-          free, _ = gen_array_ops.list_diff(range(rank_a), axes, dtypes.int32)
-        free_dims = array_ops.gather(shape_a, free)
-        axes_dims = array_ops.gather(shape_a, axes)
-        prod_free_dims = tf.math.reduce_prod(free_dims)
-        prod_axes_dims = tf.math.reduce_prod(axes_dims)
-        if flipped:
-          perm = array_ops.concat([axes, free], 0)
-          new_shape = array_ops.stack([prod_axes_dims, prod_free_dims])
-        else:
-          perm = array_ops.concat([free, axes], 0)
-          new_shape = array_ops.stack([prod_free_dims, prod_axes_dims])
-        reshaped_a = array_ops.reshape(array_ops.transpose(a, perm), new_shape)
-        return reshaped_a, free_dims, free_dims_static
-
-    def _tensordot_axes(a, axes):
-      """Generates two sets of contraction axes for the two tensor arguments."""
-      a_shape = a.get_shape()
-      if isinstance(axes, compat.integral_types):
-        if axes < 0:
-          raise ValueError(f"`axes` must be at least 0. Received: {axes}.")
-        if a_shape.ndims is not None: # pylint: disable=no-else-return
-          if axes > a_shape.ndims:
-            raise ValueError(f"`axes` must not be larger than the number of "
-                             f"dimensions of tensor {a}.  Received {axes}, vs "
-                             f"tensor dimensions {a_shape.ndims}.")
-          return (list(builtins.range(a_shape.ndims - axes,
-                        a_shape.ndims)), list(builtins.range(axes)))
-        else:
-          rank = array_ops.rank(a)
-          return (range(rank - axes, rank,
-                  dtype=dtypes.int32), range(axes, dtype=dtypes.int32))
-      elif isinstance(axes, (list, tuple)):
-        if len(axes) != 2:
-          raise ValueError(
-              f"`axes` must be an integer or have length 2. Received {axes}.")
-        a_axes = axes[0]
-        b_axes = axes[1]
-        if isinstance(a_axes, compat.integral_types) and \
-                isinstance(b_axes, compat.integral_types):
-          a_axes = [a_axes]
-          b_axes = [b_axes]
-        if len(a_axes) != len(b_axes):
-          raise ValueError(f"Different number of contraction axes `a` and `b`, "
-                           f"{len(a_axes)} != {len(b_axes)}.")
-        return a_axes, b_axes
-      else:
-        axes = ops.convert_to_tensor(axes, name="axes", dtype=dtypes.int32)
-        return axes[0], axes[1]
-
-    with ops.name_scope(name, "Tensordot", [a, b, axes]) as name: # pylint: disable=redefined-argument-from-local
-      a = ops.convert_to_tensor(a, name="a")
-      b = ops.convert_to_tensor(b, name="b")
-      a_axes, b_axes = _tensordot_axes(a, axes)
-      a_reshape, a_free_dims, a_free_dims_static = _tensordot_reshape(a, a_axes)
-      b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(
-          b, b_axes, True)
-      ab_matmul = tf.matmul(a_reshape, b_reshape)
-    if self.use_bias:
-      ab_matmul = tf.nn.bias_add(ab_matmul, self.bias)
-
-    with ops.name_scope(name, "Tensordot", [a, b, axes]) as name: # pylint: disable=redefined-argument-from-local
-      if isinstance(a_free_dims, list) and isinstance(b_free_dims, list):
-        if (ab_matmul.get_shape().is_fully_defined() and # pylint: disable=no-else-return
-                ab_matmul.get_shape().as_list() == a_free_dims + b_free_dims):
-          return ab_matmul
-        else:
-          return array_ops.reshape(
-              ab_matmul, a_free_dims + b_free_dims, name=name)
-      else:
-        a_free_dims = ops.convert_to_tensor(a_free_dims, dtype=dtypes.int32)
-        b_free_dims = ops.convert_to_tensor(b_free_dims, dtype=dtypes.int32)
-        product = array_ops.reshape(
-          ab_matmul, array_ops.concat([a_free_dims, b_free_dims], 0), name=name)
-        if a_free_dims_static is not None and b_free_dims_static is not None:
-          product.set_shape(a_free_dims_static + b_free_dims_static)
-        return product
-
   if inputs.dtype.base_dtype != self._compute_dtype_object.base_dtype: # pylint: disable=protected-access
     inputs = tf.cast(inputs, dtype=self._compute_dtype_object) # pylint: disable=protected-access
 
   is_ragged = isinstance(inputs, tf.RaggedTensor)
   if is_ragged:
     # In case we encounter a RaggedTensor with a fixed last dimension
     # (last dimension not ragged), we can flatten the input and restore
@@ -293,31 +146,22 @@
       outputs = tf.nn.bias_add(outputs, self.bias)
     if self.activation is not None:
       outputs = self.activation(outputs)
     if is_ragged:
       outputs = original_inputs.with_flat_values(outputs)
   # Broadcast kernel to inputs.
   else:
-    if training == True:
-      outputs = tensordot(inputs, self.kernel, [[rank - 1], [0]])
-      # TODO(itex): We cannot do softmax/sigmoid before reshape in tensordot.
-      # It will affect loss functions with logits. We should use BMM instead.
-      if self.activation is not None:
-        outputs = self.activation(outputs)
+    if training == True and self.use_bias:
+      outputs = itex_optimized_bmm(inputs, self.kernel, self.bias)
     else:
       outputs = tf.raw_ops.BatchMatMulV2(x=inputs, y=self.kernel)
       if self.use_bias:
         outputs = tf.nn.bias_add(outputs, self.bias)
-      if self.activation is not None:
-        outputs = self.activation(outputs)
-    # Reshape the output back to the original ndim of the input.
-    if not tf.executing_eagerly():
-      shape = inputs.shape.as_list()
-      output_shape = shape[:-1] + [self.kernel.shape[-1]]
-      outputs.set_shape(output_shape)
+    if self.activation is not None:
+      outputs = self.activation(outputs)
 
     if is_ragged:
       outputs = original_inputs.with_flat_values(outputs)
 
   return outputs
 
 def _can_use_onednn_layer_norm(self, ndims):
@@ -347,19 +191,26 @@
   return can_use_onednn_layer_norm
 
 def experimental_ops_override():
   '''
   using itex api in some tf and keras functions.
   '''
   try:
-    from keras.utils import tf_utils # pylint: disable=import-outside-toplevel
     from pkg_resources import packaging # pylint: disable=import-outside-toplevel
     version = packaging.version.parse
     if version(tf.__version__) < version("2.9.0"):
       return
+    if version(tf.__version__).release >= version("2.13").release:
+        # New versions of Keras require importing from `keras.src` when
+        # importing internal symbols.
+        from keras.src import backend # pylint: disable=import-outside-toplevel
+        from keras.src.utils import tf_utils # pylint: disable=import-outside-toplevel
+    else:
+        from keras import backend # pylint: disable=import-outside-toplevel
+        from keras.utils import tf_utils # pylint: disable=import-outside-toplevel
     tf_ln_call = copy_func(tf.keras.layers.LayerNormalization.call)
     tf_lstm_call = copy_func(tf.keras.layers.LSTM.call)
     tf_lstm_build = copy_func(tf.keras.layers.LSTM.build)
 
   except BaseException: # pylint: disable=broad-except
     return
   def itex_layer_norm_build(self, input_shape):
@@ -536,53 +387,106 @@
     return outputs
 
   def itex_lstm_build(self, input_shape):
     tf_lstm_build(self, input_shape)
     self._could_use_itex_kernel = (
         self.activation in (tf.keras.activations.tanh, tf.nn.tanh) and
         self.recurrent_activation in (tf.keras.activations.sigmoid, tf.nn.sigmoid) and
-        self.use_bias)
+        self.use_bias) and (config.list_logical_devices('XPU'))
+    # TODO: use ITEX get_backend to check GPU.
     if config.list_logical_devices('XPU'):
       # Only show the message when there is GPU available, itex LSTM only support GPU currently
       if self._could_use_itex_kernel:
         logging.debug('Layer %s will use ITEX kernels when running on GPU.' % self.name)
       else:
         logging.warning('Layer %s will not use ITEX kernels since it '
                         'doesn\'t meet the criteria. It will '
                         'use a generic GPU kernel as fallback when running '
                         'on GPU.' % self.name)
 
-  
+  def itex_lstm_call(self, inputs, mask=None, training=None, initial_state=None):
+    # if is ragged tensor or on CPU, fall back
+    if (not self._could_use_itex_kernel):
+      return tf_lstm_call(self, inputs, mask, training, initial_state)
+    if (isinstance(inputs, tf.RaggedTensor)):
+      return tf_lstm_call(self, inputs, mask, training, initial_state)
+    # when mask is not None:
+    if mask is not None:
+      if isinstance(mask, list):
+        mask = mask[0]
+      if not is_itex_supported_inputs(mask, self.time_major):
+        return tf_lstm_call(self, inputs, mask, training, initial_state)
+
+    inputs, row_lengths = backend.convert_inputs_if_ragged(inputs)
+    is_ragged_input = (row_lengths is not None)
+    self._validate_args_if_ragged(is_ragged_input, mask)
+
+    inputs, initial_state, _ = self._process_inputs(
+      inputs, initial_state, None)
+    self._maybe_reset_cell_dropout_mask(self.cell)
+    gpu_lstm_kwargs = {
+        'cell': self.cell,
+        'inputs': inputs,
+        'mask': mask,
+        'training': training,
+        'initial_state': initial_state,
+        'sequence_lengths': row_lengths,
+        'go_backwards': self.go_backwards,
+        'time_major': self.time_major,
+    }
+    last_output, outputs, new_h, new_c = gpu_lstm(**gpu_lstm_kwargs)
+    states = [new_h, new_c]
+    if self.stateful:
+      #Below cast is caused by states has differnet datat type with input when set stateful in official tensorflow
+      #Maybe remove this in the future
+      states = [math_ops.cast(i, self.states[0].dtype) for i  in states]
+      updates = [
+          state_ops.assign(self_state, state)
+          for self_state, state in zip(self.states, states)
+      ]
+      self.add_update(updates)
+
+    if self.return_sequences:
+      output = backend.maybe_convert_to_ragged(
+          is_ragged_input, outputs, row_lengths, go_backwards=self.go_backwards)
+    else:
+      output = last_output
+
+    if self.return_state:
+      return [output] + list(states)
+    return output
+
   try:
     import tensorflow_addons as tfa # pylint: disable=import-outside-toplevel
     tfa.layers.InstanceNormalization.call = itex_instance_norm_call
   except BaseException: # pylint: disable=broad-except
     logger.warning("itex experimental ops override: tensorflow_addons is not installed.") # pylint: disable=line-too-long
   try:
     tf.keras.layers.Dense.call = itex_dense_layer_call
     tf.keras.layers.LayerNormalization.call = itex_layer_norm_call
     tf.keras.layers.LayerNormalization.build = itex_layer_norm_build
-    
+
     from tensorflow.nn import gelu # pylint: disable=import-outside-toplevel
     gelu = itex_gelu
     tf.nn.gelu = itex_gelu
-    if config.list_logical_devices('XPU'):
-      # TODO(itex): Complement the complete implementation of CPU, and When
-      # CPU of XPU is launched, this workaround may be re-edit.
-      tf.keras.layers.LSTM.call = ItexLSTM.call
-      tf.keras.layers.LSTM.build = itex_lstm_build
-      from tensorflow.python import keras # pylint: disable=import-outside-toplevel
-      keras.layers.LSTM.call = ItexLSTM.call
-      keras.layers.LSTM.build = itex_lstm_build
+    tf.keras.layers.LSTM.call = itex_lstm_call
+    tf.keras.layers.LSTM.build = itex_lstm_build
     logger.info("itex experimental ops override is enabled.")
   except BaseException: # pylint: disable=broad-except
     logger.error("Cannot override itex ops.")
   try:
     import keras # pylint: disable=import-outside-toplevel
-    keras.layers.core.dense.Dense.call = itex_dense_layer_call
-    keras.layers.LayerNormalization.call = itex_layer_norm_call
-    keras.layers.LayerNormalization.build = itex_layer_norm_build
-    if config.list_logical_devices('XPU'):
-      keras.layers.LSTM.call = ItexLSTM.call
+    if version(tf.__version__).release >= version("2.13").release:
+      keras.src.layers.core.dense.Dense.call = itex_dense_layer_call
+      keras.src.layers.normalization.layer_normalization.LayerNormalization.call = itex_layer_norm_call
+      keras.src.layers.normalization.layer_normalization.LayerNormalization.build = itex_layer_norm_build
+      keras.src.layers.rnn.lstm.LSTM.call = itex_lstm_call
+      keras.src.layers.rnn.lstm.LSTM.build = itex_lstm_build
+    else:
+      keras.layers.core.dense.Dense.call = itex_dense_layer_call
+      keras.layers.LayerNormalization.call = itex_layer_norm_call
+      keras.layers.LayerNormalization.build = itex_layer_norm_build
+      keras.layers.LSTM.call = itex_lstm_call
       keras.layers.LSTM.build = itex_lstm_build
+  
   except BaseException: # pylint: disable=broad-except
     logger.warning("itex experimental ops override: Keras is not installed.") # pylint: disable=line-too-long
```

## intel_extension_for_tensorflow/python/gen_itex_version.py

```diff
@@ -46,21 +46,21 @@
 
   return "0.0.0"
 
 def generate_version(header_in, header_out):
   hash_value = git_hash(header_in)
   jax_version = get_jax_version()
 
-  [major, minor, patch] = version.__version__.split(".")
+  [major, minor, patch1, patch2] = version.__version__.split(".")
 
   with open(os.path.expanduser(header_in)) as inf:
     content = inf.read()
     content = content.replace("@ITEX_VERSION_MAJOR@", major)
     content = content.replace("@ITEX_VERSION_MINOR@", minor)
-    content = content.replace("@ITEX_VERSION_PATCH@", patch)
+    content = content.replace("@ITEX_VERSION_PATCH@", ".".join([patch1, patch2]))
     content = content.replace("@ITEX_VERSION_HASH@", hash_value)
     content = content.replace("@JAX_VERSION_STRING@", jax_version)
 
     header_out = os.path.expanduser(header_out)
     header_out_dir = os.path.dirname(header_out)
     if not os.path.exists(header_out_dir):
       os.makedirs(header_out_dir, exist_ok=True)
```

## intel_extension_for_tensorflow/python/launch.py

```diff
@@ -11,14 +11,15 @@
 #   distributed under the License is distributed on an "AS IS" BASIS,
 #   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #   See the License for the specific language governing permissions and
 #   limitations under the License.
 # ============================================================================
 
 from __future__ import absolute_import, division, print_function, unicode_literals
+import math
 import sys
 import platform
 import subprocess
 import os
 from os.path import expanduser
 import re
 import glob
@@ -273,38 +274,39 @@
                                      enable_jemalloc=False,
                                      use_default_allocator=False):
     '''
     Set multi-thread configuration and enable LLVM openMP and TCMalloc/JeMalloc.
     '''
     self.set_memory_allocator(
         enable_tcmalloc, enable_jemalloc, use_default_allocator)
-    self.set_env("OMP_NUM_THREADS", str(ncore_per_instance))
     if set_kmp_affinity:
       if len(self.cpuinfo.get_node_logical_cores(0)) > len(
           self.cpuinfo.get_node_physical_cores(0)):
         # HT is on
         self.set_env("KMP_AFFINITY",
                      "granularity=fine,verbose,compact,1,0")
       else:
         # HT is off
         self.set_env("KMP_AFFINITY",
                      "granularity=fine,verbose,compact,")
 
     self.set_env("KMP_BLOCKTIME", "1")
     if num_inter is None:
       self.set_env("TF_NUM_INTEROP_THREADS", "1")
+      self.set_env("OMP_NUM_THREADS", str(ncore_per_instance))
     else:
       try:
         num = int(num_inter)
         assert num >= -1
       except ValueError:
         logger.error(
             "tf_num_interop_threads should be an integer >= -1, "
             "but input is %s.", 'num_inter')
         sys.exit(-1)
+      self.set_env("OMP_NUM_THREADS", str(ncore_per_instance // num))
       self.set_env("TF_NUM_INTEROP_THREADS", num_inter)
     if num_intra is None:
       self.set_env("TF_NUM_INTRAOP_THREADS", str(ncore_per_instance))
     else:
       try:
         num = int(num_intra)
         assert num >= 0
@@ -328,14 +330,15 @@
 class MultiInstanceLauncher(Launcher):
   r"""
    Launcher for single instance and multi-instance
    """
 
   def launch(self, args):
     processes = []
+    processes_tune = []
     cores = []
     set_kmp_affinity = True
     enable_taskset = False
     if args.core_list:  # user specify what cores will be used by params
       cores = [int(x) for x in args.core_list.split(",")]
       if args.ncore_per_instance == -1:
         logger.error(
@@ -482,15 +485,17 @@
                                         args.tf_num_interop_threads,
                                         args.tf_num_intraop_threads,
                                         set_kmp_affinity,
                                         args.enable_tcmalloc,
                                         args.enable_jemalloc,
                                         args.use_default_allocator)
     self.set_itex(args.enable_itex_amp, args.enable_itex_layout_opt)
-    os.environ["LAUNCH_CMD"] = "#"
+    # os.environ["LAUNCH_CMD"] = "#"
+    cmd_run = []
+    cmd_tune = []
     for i in range(args.ninstances):
       cmd = []
       cur_process_cores = ""
       if not args.disable_numactl or enable_taskset:
         if not args.disable_numactl:
           cmd = ["numactl"]
         elif enable_taskset:
@@ -535,28 +540,83 @@
         cmd.append("-m")
       cmd.append(args.program)
       log_name = args.log_file_prefix + \
           "_instance_{}_cores_".format(
               i) + cur_process_cores.replace(',', '_') + ".log"
       log_name = os.path.join(args.log_path, log_name)
       cmd.extend(args.program_args)
-      os.environ["LAUNCH_CMD"] += " ".join(cmd) + ",#"
+      # os.environ["LAUNCH_CMD"] += " ".join(cmd) + ",#"
       cmd_s = " ".join(cmd)
+      if not args.disable_numactl:
+        cmd_tune.append("{} > tmp_itex_launcher_tune_result_{}.log 2>&1".format(cmd_s, i))
+      elif enable_taskset:
+        cmd_tune.append("{} > tmp_itex_launcher_tune_result_{}.log 2>&1".format(cmd, i))
       if args.log_path:
-        cmd_s = "{} 2>&1 | tee {}".format(cmd_s, log_name)
-      logger.info(cmd_s)
+        cmd_s = "{} > {} 2>&1".format(cmd_s, log_name)
+      if not args.disable_numactl:
+        cmd_run.append(cmd_s)
+      elif enable_taskset:
+        cmd_run.append(cmd)
+
+    if args.tune:
+      candidates = [1]
+      while (True):
+        if args.ncore_per_instance % (2 * candidates[-1]) == 0:
+          candidates.append(2 * candidates[-1])
+        else:
+          break
+      if candidates[-1] != args.ncore_per_instance:
+        candidates.append(args.ncore_per_instance)
+      tune_time = [-1 for i in range(len(candidates))]
+      logger.info("Start to tune TF_NUM_INTEROP_THREADS, candidates are {}".format(candidates))
+      loop = len(candidates) // args.ninstances + (1 if len(candidates) % args.ninstances > 0 else 0)
+      for l in range(loop):
+        for i in range(args.ninstances):
+          if l * args.ninstances + i >= len(candidates):
+            break
+          candidate = candidates[l * args.ninstances + i]
+          logger.info(candidate)
+          timer = "/usr/bin/time -o tmp_itex_launcher_tune_inter_op_{}_time.log -f \"%e\" ".format(candidate)
+          os.environ["TF_NUM_INTEROP_THREADS"] = str(candidate)
+          os.environ["OMP_NUM_THREADS"] = str(args.ncore_per_instance // candidate)
+          process = subprocess.Popen(timer + cmd_tune[i], env=os.environ, shell=True)
+          processes_tune.append(process)
+        for i in range(len(processes_tune)):
+          processes_tune[i].wait()
+          if processes_tune[i].returncode != 0:
+            raise subprocess.CalledProcessError(
+              returncode=processes_tune[i].returncode, cmd=cmd_tune[i])
+        processes_tune = []
+
+      for i in range(len(candidates)):
+        with open("tmp_itex_launcher_tune_inter_op_{}_time.log".format(candidates[i])) as file:
+          content = file.readline()
+          tune_time[i] = float(content)
+        os.remove("tmp_itex_launcher_tune_inter_op_{}_time.log".format(candidates[i]))
+      for i in range(min(len(candidates),args.ninstances)):
+        os.remove("tmp_itex_launcher_tune_result_{}.log".format(i))
+      if -1 in tune_time:
+        logger.error("--tune failed")
+        sys.exit()
+      best_config = candidates[tune_time.index(min(tune_time))]
+      os.environ["TF_NUM_INTEROP_THREADS"] = str(best_config)
+      os.environ["OMP_NUM_THREADS"] = str(args.ncore_per_instance // best_config)
+      logger.info("launcher tune result: TF_NUM_INTEROP_THREADS={}".format(best_config))
+
+    for i in range(args.ninstances):
+      logger.info(cmd_run[i])
       if not args.disable_numactl:
-        process = subprocess.Popen(cmd_s, env=os.environ, shell=True)
+        process = subprocess.Popen(cmd_run[i], env=os.environ, shell=True)
       elif enable_taskset:
-        process = subprocess.Popen(cmd, env=os.environ)
+        process = subprocess.Popen(cmd_run[i], env=os.environ)
       processes.append(process)
 
       if args.instance_idx != -1:  # launches single instance, instance_idx, only
         break
-    os.environ["LAUNCH_CMD"] = os.environ["LAUNCH_CMD"][:-2]
+    # os.environ["LAUNCH_CMD"] = os.environ["LAUNCH_CMD"][:-2]
     for process in processes:
       process.wait()
       if process.returncode != 0:
         raise subprocess.CalledProcessError(
             returncode=process.returncode, cmd=cmd_s)
 
 
@@ -565,14 +625,17 @@
   group = parser.add_argument_group("ITEX Parameters")
   # ITEX control
   group.add_argument("--enable_itex_amp", action='store_true', default=False,
                      help="Enable ITEX AMP")
   group.add_argument("--enable_itex_layout_opt", action='store_true', \
                       default=False,
                      help="Enable ITEX layout opt")
+  group.add_argument("--enable_op_parallelism", action='store_true', \
+                      default=False,
+                     help="If true, set TF_NUM_INTEROP_THREADS=2, by default it is 1.")
 
 
 def add_memory_allocator_params(parser):
 
   group = parser.add_argument_group("Memory Allocator Parameters")
   # allocator control
   group.add_argument("--enable_tcmalloc", action='store_true', default=False,
@@ -677,15 +740,16 @@
                            "'python -m'.")
 
   parser.add_argument("--no_python", default=False, action="store_true",
                       help="Do not prepend the --program script \
                             with \"python\" - just exec "
                            "it directly. Useful when the script is \
                             not a Python script.")
-
+  parser.add_argument("--tune", default=False, action="store_true",
+                      help="tune the inter num threads and run a py script.")
   add_memory_allocator_params(parser)
   add_itex_params(parser)
   add_multi_instance_params(parser)
   # positional
   parser.add_argument("program", type=str,
                       help="The full path to the proram/script to be launched. "
                            "followed by all the arguments for the script")
@@ -716,18 +780,36 @@
     file_handler.setFormatter(log_formatter)
     logger.addHandler(file_handler)
 
   if args.latency_mode and args.throughput_mode:
     raise RuntimeError(
         "Either args.latency_mode or args.throughput_mode should be set")
 
-  if not args.no_python and not args.program.endswith(".py"):
+  if not args.no_python and not (args.program.endswith(".py") or args.module):
     logger.error(
         "For non Python script, you should use '--no_python' parameter.")
     sys.exit()
+  if args.no_python and args.tune:
+    logger.error(
+        "For non Python script, you should not use '--tune' parameter.")
+    sys.exit()
+  if args.tune and (args.tf_num_interop_threads is not None or
+    "TF_NUM_INTEROP_THREADS" in os.environ):
+    logger.error(
+        "You should not use '--tune' parameter when number of interop threads "
+        "is set")
+    sys.exit()
+  if args.enable_op_parallelism and (
+    args.tf_num_interop_threads is not None or "TF_NUM_INTEROP_THREADS" in os.environ):
+    logger.error(
+        "You should not use '--enable_op_parallelism' parameter when number of interop threads "
+        "is set")
+    sys.exit()
+  if args.enable_op_parallelism:
+    args.tf_num_interop_threads = "2"
 
   # Verify LD_PRELOAD
   if "LD_PRELOAD" in os.environ:
     lst_valid = []
     tmp_ldpreload = os.environ["LD_PRELOAD"]
     for item in tmp_ldpreload.split(":"):
       if item != "":
```

## intel_extension_for_tensorflow/python/version.py

```diff
@@ -12,14 +12,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 '''version information for Intel  Extension for TensorFlow*'''
 
-__version__ = '1.2.0rc0'
-__git_desc__= '0b1ee1b4'
+__version__ = '2.13.0.0'
+__git_desc__= 'd7dc3d85'
 VERSION = __version__
 GIT_VERSION = 'v' + __version__ + '-' + __git_desc__
-COMPILER_VERSION = 'dpcpp-'
-ONEDNN_GIT_VERSION = 'v3.1.0-ad34c124'
+COMPILER_VERSION = 'dpcpp-2023.2.0.20230622'
+ONEDNN_GPU_GIT_VERSION = 'v3.2.0-67bc621a'
+ONEDNN_CPU_GIT_VERSION = 'v3.2.0-f7bceb51'
 TF_COMPATIBLE_VERSION = '>= 2.8.0'
```

## intel_extension_for_tensorflow/python/ops/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2022 Intel Corporation
+# Copyright (c) 2023 Intel Corporation
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,8 +14,11 @@
 # ==============================================================================
 
 # pylint: disable=g-bad-import-order,unused-import,missing-module-docstring,unused-import,line-too-long
 from intel_extension_for_tensorflow.python.ops.activations import gelu
 from intel_extension_for_tensorflow.python.ops import ops_grad as _ops_grad
 from intel_extension_for_tensorflow.python.ops.optimizers import AdamWithWeightDecayOptimizer
 from intel_extension_for_tensorflow.python.ops.layer_norm import LayerNormalization
+from intel_extension_for_tensorflow.python.ops.group_norm import GroupNormalization
 from intel_extension_for_tensorflow.python.ops.recurrent import ItexLSTM
+from intel_extension_for_tensorflow.python.ops.mlp import FusedDenseBiasAddGelu
+from intel_extension_for_tensorflow.python.ops.multi_head_attention import scaled_dot_product_attention
```

## intel_extension_for_tensorflow/python/ops/layer_norm.py

```diff
@@ -32,16 +32,19 @@
 from tensorflow.python.ops import math_ops
 from tensorflow.python.ops import nn
 
 from keras import backend as K
 from keras import constraints
 from keras import initializers
 from keras import regularizers
-from keras.engine.base_layer import Layer
-from keras.utils import control_flow_util
+from keras.layers import Layer
+try:
+  from keras.utils import control_flow_util
+except ImportError:
+  from keras.src.utils import control_flow_util
 
 def _layer_norm(
     x,
     scale,
     offset,  # pylint: disable=invalid-name
     epsilon=0.001,
     is_training=True,
```

## intel_extension_for_tensorflow/python/ops/ops_grad.py

```diff
@@ -12,14 +12,17 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 # pylint: disable=missing-module-docstring
 from tensorflow.python.framework import ops
 from intel_extension_for_tensorflow.python.ops.load_ops_library import load_ops_library
+from tensorflow.python.ops import math_ops
+from tensorflow.python.ops import gen_nn_ops
+from tensorflow.python.ops import array_ops
 
 @ops.RegisterGradient("Gelu")
 def _gelu_grad(op, grad):
   return load_ops_library.gelu_grad(
       grad, op.inputs[0], op.get_attr("approximate")
   )
 
@@ -87,7 +90,35 @@
       rnn_mode=op.get_attr("rnn_mode"),
       dropout=op.get_attr("dropout"),
       recurrent_dropout=op.get_attr("recurrent_dropout"),
       # seed=op.get_attr("seed"),
       # seed2=op.get_attr("seed2"),
       num_proj=op.get_attr("num_proj"),
       var_seq_length=op.get_attr("var_seq_length")) + (None, None, None,)
+
+@ops.RegisterGradient("ScaledDotProductAttention")
+def _scaled_dot_product_attention_grad(op, *grad):
+  dq, dk, dv = load_ops_library.scaled_dot_product_attention_grad(
+      query=op.inputs[0],
+      key=op.inputs[1],
+      value=op.inputs[2],
+      dropout_mask=op.inputs[4],
+      atten=op.outputs[1],
+      atten_dp=op.outputs[2],
+      output_backprop=grad[0],
+      dropout_prob=op.get_attr("dropout_prob"))
+  return (dq, dk, dv, None, None)
+
+      
+@ops.RegisterGradient("FusedDenseBiasAddGelu")
+def _itex_fused_dense_bias_add_gelu_grad(op, *grad):
+  feature = op.inputs[0]
+  weights = op.inputs[1]
+  workspace = op.outputs[1]
+  output_backprop = grad[0]
+  dgelu = math_ops.mul(workspace, output_backprop)
+  # TODO(zw): fuse dbias and dweights ? replace reduce_sum with biasAddgrad
+  # dbias = math_ops.reduce_sum(dgelu, axis=0)  
+  dbias = gen_nn_ops.bias_add_grad(dgelu)
+  dweights = math_ops.matmul(feature, dgelu, transpose_a=True)
+  dfeature = math_ops.matmul(dgelu, weights, transpose_b=True)
+  return dfeature, dweights, dbias
```

## intel_extension_for_tensorflow/python/ops/optimizers.py

```diff
@@ -116,15 +116,15 @@
   def _apply_dense(self, grad, var):
     """A dummy docstring."""
     m = self.get_slot(var, "m")
     v = self.get_slot(var, "v")
     beta_1_power, beta_2_power = self._get_beta_accumulators()
     param_name = self._get_variable_name(var.name)
     if self._do_use_weight_decay(param_name): # pylint: disable=no-else-return
-      return load_ops_library.apply_adam_with_weight_decay(
+      return load_ops_library.itex_apply_adam_with_weight_decay(
           var,
           m,
           v,
           math_ops.cast(beta_1_power, var.dtype.base_dtype),
           math_ops.cast(beta_2_power, var.dtype.base_dtype),
           math_ops.cast(self._lr_t, var.dtype.base_dtype),
           math_ops.cast(self._beta_1_t, var.dtype.base_dtype),
@@ -150,15 +150,15 @@
   def _resource_apply_dense(self, grad, var): # pylint: disable=arguments-differ
     """A dummy docstring."""
     m = self.get_slot(var, "m")
     v = self.get_slot(var, "v")
     beta_1_power, beta_2_power = self._get_beta_accumulators()
     param_name = self._get_variable_name(var.name)
     if self._do_use_weight_decay(param_name): # pylint: disable=no-else-return
-      return load_ops_library.resource_apply_adam_with_weight_decay(
+      return load_ops_library.itex_resource_apply_adam_with_weight_decay(
           var.handle,
           m.handle,
           v.handle,
           math_ops.cast(beta_1_power, grad.dtype.base_dtype),
           math_ops.cast(beta_2_power, grad.dtype.base_dtype),
           math_ops.cast(self._lr_t, grad.dtype.base_dtype),
           math_ops.cast(self._beta_1_t, grad.dtype.base_dtype),
```

## intel_extension_for_tensorflow/python/ops/recurrent.py

```diff
@@ -30,20 +30,27 @@
 from tensorflow.python.ops import nn
 from tensorflow.python.ops import state_ops
 from tensorflow.python.ops import variables
 from tensorflow.python.platform import tf_logging as logging
 
 from tensorflow.python import keras
 from keras import activations
-from keras import backend
+try:
+  from keras.src import backend
+except ImportError:
+  from keras import backend
 from keras import constraints
 from keras import initializers
 from keras import regularizers
-from keras.engine.input_spec import InputSpec
-from keras.layers import LSTMV1
+try:
+  from keras.engine.input_spec import InputSpec
+  from keras.layers import LSTMV1
+except ImportError:
+  from keras.src.engine.input_spec import InputSpec
+  from keras.src.layers import LSTMV1
 
 _ITEX_AVAILABLE_MSG = 'Layer %s will use ITEX kernels when running on GPU.'
 _ITEX_NOT_AVAILABLE_MSG = ('Layer %s will not use ITEX kernels since it '
                            'doesn\'t meet the criteria. It will '
                            'use a generic GPU kernel as fallback when running '
                            'on GPU.')
 
@@ -326,15 +333,15 @@
     # into the layer, it is padded and the row lengths are used for masking.
     inputs, row_lengths = backend.convert_inputs_if_ragged(inputs)
     is_ragged_input = (row_lengths is not None)
     self._validate_args_if_ragged(is_ragged_input, mask)
 
     # TODO: support ragged_input and mask in the future
     self._could_use_itex_kernel = (self._could_use_itex_kernel and \
-       (not is_ragged_input) and (mask is None))
+       (not is_ragged_input))
 
     # LSTM does not support constants. Ignore it during process.
     inputs, initial_state, _ = self._process_inputs(
         inputs, initial_state, None)
 
     self._maybe_reset_cell_dropout_mask(self.cell)
 
@@ -586,15 +593,14 @@
         dropout_mask=dp_mask,
         recurrent_dropout=recurrent_dropout,
         recurrent_dropout_mask=rec_dp_mask,
         sequence_lengths=sequence_lengths,
         rnn_mode='lstm',
         var_seq_length=True,
         is_training=training)
-    # TODO: below reshape operation is added as tensorflow shape inference c api bug, maybe remove this once rebase tensorflow==2.12.0
     outputs = array_ops.reshape(outputs,
                                 [array_ops.shape(inputs)[0], \
                                 array_ops.shape(inputs)[1], \
                                 array_ops.shape(init_h)[1]])
     h = array_ops.reshape(h, [array_ops.shape(init_h)[0], \
                           array_ops.shape(init_h)[1]])
     c = array_ops.reshape(c, [array_ops.shape(init_c)[0], \
@@ -618,15 +624,14 @@
         dropout=dropout,
         dropout_mask=dp_mask,
         recurrent_dropout=recurrent_dropout,
         recurrent_dropout_mask=rec_dp_mask,
         sequence_lengths=0,
         rnn_mode='lstm',
         is_training=training)
-    # TODO: below reshape operation is added as tensorflow shape inference c api bug, maybe remove this once rebase tensorflow==2.12.0
     outputs = array_ops.reshape(outputs,
                                 [array_ops.shape(inputs)[0], \
                                 array_ops.shape(inputs)[1], \
                                 array_ops.shape(init_h)[1]])
     h = array_ops.reshape(h, [array_ops.shape(init_h)[0], \
                           array_ops.shape(init_h)[1]])
     c = array_ops.reshape(c, [array_ops.shape(init_c)[0], \
```

## intel_extension_for_tensorflow/python/test_func/keras_testing_utils.py

```diff
@@ -22,45 +22,27 @@
 import itertools
 import threading
 
 import tensorflow.compat.v2 as tf
 
 import numpy as np
 from intel_extension_for_tensorflow.python.test_func import test_util
+from tensorflow.python.keras.engine import base_layer_utils
+from tensorflow.python.keras.optimizer_v2 import adadelta as adadelta_v2
+from tensorflow.python.keras.optimizer_v2 import adagrad as adagrad_v2
+from tensorflow.python.keras.optimizer_v2 import adam as adam_v2
+from tensorflow.python.keras.optimizer_v2 import adamax as adamax_v2
+from tensorflow.python.keras.optimizer_v2 import gradient_descent as gradient_descent_v2
+from tensorflow.python.keras.optimizer_v2 import nadam as nadam_v2
+from tensorflow.python.keras.optimizer_v2 import rmsprop as rmsprop_v2
+from tensorflow.python.keras.utils import tf_contextlib
+from tensorflow.python.keras.utils import tf_inspect
 from keras import backend
 from keras import layers
 from keras import models
-from keras.engine import base_layer_utils
-try:
-  from keras.optimizers.optimizer_v2 import adadelta as adadelta_v2
-  from keras.optimizers.optimizer_v2 import adagrad as adagrad_v2
-  from keras.optimizers.optimizer_v2 import adam as adam_v2
-  from keras.optimizers.optimizer_v2 import adamax as adamax_v2
-  from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_v2
-  from keras.optimizers.optimizer_v2 import nadam as nadam_v2
-  from keras.optimizers.optimizer_v2 import rmsprop as rmsprop_v2
-except ImportError:
-  try:
-    from keras.optimizer_v2 import adadelta as adadelta_v2
-    from keras.optimizer_v2 import adagrad as adagrad_v2
-    from keras.optimizer_v2 import adam as adam_v2
-    from keras.optimizer_v2 import adamax as adamax_v2
-    from keras.optimizer_v2 import gradient_descent as gradient_descent_v2
-    from keras.optimizer_v2 import nadam as nadam_v2
-    from keras.optimizer_v2 import rmsprop as rmsprop_v2
-  except ImportError:
-    from keras.optimizers.legacy import adadelta as adadelta_v2
-    from keras.optimizers.legacy import adagrad as adagrad_v2
-    from keras.optimizers.legacy import adam as adam_v2
-    from keras.optimizers.legacy import adamax as adamax_v2
-    from keras.optimizers.legacy import gradient_descent as gradient_descent_v2
-    from keras.optimizers.legacy import nadam as nadam_v2
-    from keras.optimizers.legacy import rmsprop as rmsprop_v2
-from keras.utils import tf_contextlib
-from keras.utils import tf_inspect
 
 
 def string_test(actual, expected):
   np.testing.assert_array_equal(actual, expected)
 
 
 def numeric_test(actual, expected):
```

## intel_extension_for_tensorflow/python/test_func/test_util.py

```diff
@@ -69,24 +69,26 @@
 from tensorflow.python import pywrap_sanitizers
 from tensorflow.python import tf2
 from tensorflow.python.client import device_lib
 from tensorflow.python.client import pywrap_tf_session
 from tensorflow.python.client import session
 from tensorflow.python.compat.compat import forward_compatibility_horizon
 from tensorflow.python.eager import backprop
+from tensorflow.python.framework import indexed_slices
 from tensorflow.python.eager import context
 from tensorflow.python.eager import def_function
 from tensorflow.python.eager import tape
 from tensorflow.python.framework import config
 from tensorflow.python.framework import device as pydev
 from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import errors
 from tensorflow.python.framework import errors_impl
 from tensorflow.python.framework import gpu_util
 from tensorflow.python.framework import importer
+from tensorflow.python.framework import indexed_slices
 from tensorflow.python.framework import ops
 from tensorflow.python.framework import random_seed
 from tensorflow.python.framework import sparse_tensor
 from tensorflow.python.framework import tensor_shape
 from tensorflow.python.framework import tensor_util
 from tensorflow.python.framework import tfrt_utils
 from tensorflow.python.framework import versions
@@ -981,18 +983,14 @@
 
   Returns:
     The decorated function.
   """
 
   def decorator(self, **kwargs):
     """Sets DEBUG_SAVEALL, runs the test, and checks for new garbage."""
-    # Force-load `distribution_strategy_context` to prevent GC at
-    # test time when using eager. Remove once b/117329403 is resolved.
-    tape.distribution_strategy_context.get_strategy()
-
     gc.disable()
     previous_debug_flags = gc.get_debug()
     gc.set_debug(gc.DEBUG_SAVEALL)
     gc.collect()
     previous_garbage = len(gc.garbage)
     result = f(self, **kwargs)
     gc.collect()
@@ -2313,14 +2311,16 @@
 
 @tf_export("test.TestCase")
 class TensorFlowTestCase(googletest.TestCase):
   """Base class for tests that need to test TensorFlow."""
 
   def __init__(self, methodName="runTest"):  # pylint: disable=invalid-name
     super(TensorFlowTestCase, self).__init__(methodName)
+    # Set Test Mode for all unit tests
+    os.environ['_ITEX_TEST_MODE'] = '1'
     # Make sure we get unfiltered stack traces during the test
     traceback_utils.disable_traceback_filtering()
     if is_xla_enabled():
       pywrap_tf_session.TF_SetXlaAutoJitMode("2")
       pywrap_tf_session.TF_SetXlaMinClusterSize(1)
       pywrap_tf_session.TF_SetXlaEnableLazyCompilation(False)
       pywrap_tf_session.TF_SetTfXlaCpuGlobalJit(True)
@@ -2517,39 +2517,47 @@
     """
     if not actual.startswith(expected_start):
       fail_msg = "%r does not start with %r" % (actual, expected_start)
       fail_msg += " : %r" % (msg) if msg else ""
       self.fail(fail_msg)
 
   def _eval_tensor(self, tensor):
-    """A dummy docstring."""
     if tensor is None:
       return None
     elif callable(tensor):
       return self._eval_helper(tensor())
     else:
       try:
+        # for compatibility with TF1 test cases
         if sparse_tensor.is_sparse(tensor):
           return sparse_tensor.SparseTensorValue(tensor.indices.numpy(),
                                                  tensor.values.numpy(),
                                                  tensor.dense_shape.numpy())
         elif ragged_tensor.is_ragged(tensor):
           return ragged_tensor_value.RaggedTensorValue(
               self._eval_tensor(tensor.values),
               self._eval_tensor(tensor.row_splits))
-        elif isinstance(tensor, ops.IndexedSlices):
-          return ops.IndexedSlicesValue(
+        elif isinstance(tensor, indexed_slices.IndexedSlices):
+          return indexed_slices.IndexedSlicesValue(
               values=tensor.values.numpy(),
               indices=tensor.indices.numpy(),
-              dense_shape=tensor.dense_shape.numpy())
-        # Convert tensors and composite tensors to numpy arrays.
-        return nest.map_structure(lambda t: t.numpy(), tensor,
-                                  expand_composites=True)
+              dense_shape=None
+              if tensor.dense_shape is None else tensor.dense_shape.numpy())
+        else:
+          if hasattr(tensor, "numpy") and callable(tensor.numpy):
+            return tensor.numpy()
+          else:
+            # Try our best to convert CompositeTensor components to NumPy
+            # arrays. Officially, we don't support NumPy arrays as
+            # CompositeTensor components. So don't be surprised if this doesn't
+            # work.
+            return nest.map_structure(lambda t: t.numpy(), tensor,
+                                      expand_composites=True)
       except AttributeError as e:
-        six.raise_from(ValueError("Unsupported type %s." % type(tensor)), e)
+        raise ValueError(f"Unsupported type {type(tensor).__name__!r}.") from e
 
   def _eval_helper(self, tensors):
     if tensors is None:
       return None
     return nest.map_structure(self._eval_tensor, tensors)
 
   def evaluate(self, tensors):
```

## Comparing `intel_extension_for_tensorflow-1.2.0rc0.dist-info/LICENSE.txt` & `intel_extension_for_tensorflow-2.13.0.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `intel_extension_for_tensorflow-1.2.0rc0.dist-info/METADATA` & `intel_extension_for_tensorflow-2.13.0.0.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: intel-extension-for-tensorflow
-Version: 1.2.0rc0
+Version: 2.13.0.0
 Summary: Intel Extension for Tensorflow*
 Home-page: https://github.com/intel/intel-extension-for-tensorflow
 Download-URL: https://github.com/intel/intel-extension-for-tensorflow/tags
 Author: Intel Corporation
 Author-email: itex.maintainers@intel.com
 License: Apache 2.0
 Project-URL: Bug Tracker, https://github.com/intel/intel-extension-for-tensorflow/issues
@@ -25,42 +25,45 @@
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 License-File: LICENSE.txt
 Requires-Dist: grpcio (>=1.8.6)
 Requires-Dist: wheel
-Requires-Dist: tensorflow (>=2.12)
+Requires-Dist: tensorflow (==2.13)
 Requires-Dist: numpy (<1.24)
 Provides-Extra: cpu
-Requires-Dist: intel-extension-for-tensorflow-lib (==1.2.0.0rc0) ; extra == 'cpu'
+Requires-Dist: intel-extension-for-tensorflow-lib (==2.13.0.0.0) ; extra == 'cpu'
 Provides-Extra: gpu
-Requires-Dist: intel-extension-for-tensorflow-lib (==1.2.0.1rc0) ; extra == 'gpu'
+Requires-Dist: intel-extension-for-tensorflow-lib (==2.13.0.0.1) ; extra == 'gpu'
+Provides-Extra: xpu
+Requires-Dist: intel-extension-for-tensorflow-lib (==2.13.0.0.2) ; extra == 'xpu'
 
 # Intel Extension for TensorFlow*
 
-[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://pypi.org/project/intel-extension-for-tensorflow)
-[![version](https://img.shields.io/badge/release-1.1.0-green)](https://github.com/intel/intel-extension-for-tensorflow/releases)
+[![Python](https://img.shields.io/pypi/pyversions/intel_extension_for_tensorflow)](https://badge.fury.io/py/intel-extension-for-tensorflow)
+[![PyPI version](https://badge.fury.io/py/intel-extension-for-tensorflow.svg)](https://badge.fury.io/py/intel-extension-for-tensorflow)
+[![version](https://img.shields.io/github/v/release/intel/intel-extension-for-tensorflow?color=brightgreen)](https://github.com/intel/intel-extension-for-tensorflow/releases)
 
 Intel Extension for TensorFlow* is a heterogeneous, high performance deep learning extension plugin based on TensorFlow [PluggableDevice](https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.md) interface to bring Intel XPU(GPU, CPU, etc) devices into [TensorFlow](https://github.com/tensorflow/tensorflow) open source community for AI workload acceleration. It allows flexibly plugging an XPU into TensorFlow on-demand, and exposing computing power inside Intel's hardware.
 
 Documentation: [**Intel Extension for TensorFlow\* online document website**](https://intel.github.io/intel-extension-for-tensorflow/).
 
 ## Installation
 
-### Install for GPU
+### Install for XPU
 ```
-pip install tensorflow==2.12.0
-pip install --upgrade intel-extension-for-tensorflow[gpu]==1.2.0rc0
+pip install tensorflow==2.13.0
+pip install --upgrade intel-extension-for-tensorflow[xpu]
 ```
-Please refer to [GPU installation](https://intel.github.io/intel-extension-for-tensorflow/latest/docs/install/install_for_gpu.html) for details.
+Please refer to [XPU installation](https://intel.github.io/intel-extension-for-tensorflow/latest/docs/install/install_for_xpu.html) for details.
 
-### Install for CPU [Experimental]
+### Install for CPU
 ```
-pip install tensorflow==2.12.0
-pip install --upgrade intel-extension-for-tensorflow[cpu]==1.2.0rc0
+pip install tensorflow==2.13.0
+pip install --upgrade intel-extension-for-tensorflow[cpu]
 ```
 
 ## Security
 See Intel's [Security Center](https://www.intel.com/content/www/us/en/security-center/default.html) for information on how to report a potential security issue or vulnerability.
 
 See also: [Security Policy](https://intel.github.io/intel-extension-for-tensorflow/latest/SECURITY.html)
```

## Comparing `intel_extension_for_tensorflow-1.2.0rc0.dist-info/RECORD` & `intel_extension_for_tensorflow-2.13.0.0.dist-info/RECORD`

 * *Files 19% similar despite different names*

```diff
@@ -1,32 +1,43 @@
-intel_extension_for_tensorflow/__init__.py,sha256=HO5tjJj9IHVU7pbzu7cLTz6vS18FoJnS14fP5kfunJ8,1898
+intel_extension_for_tensorflow/__init__.py,sha256=U9MQGXNYC0gHKv_V0Urw0cFjgK8zJZ8B3oUwrtBTOT0,1895
 intel_extension_for_tensorflow/__main__.py,sha256=VY9K7rQScqRvOS_nIs4DAj4FPKB6YEefzcxJqlwW6A0,243
 intel_extension_for_tensorflow/core/utils/protobuf/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-intel_extension_for_tensorflow/core/utils/protobuf/config_pb2.py,sha256=nfE_Z9atjkGIeQFIvB4UZoTDyHMvV3g3rNxg9H-wrLI,3575
+intel_extension_for_tensorflow/core/utils/protobuf/config_pb2.py,sha256=vVVnnYuKoPhOz7XJf0nrJFoalwboqOvY57TdRuOYhC4,3416
 intel_extension_for_tensorflow/python/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 intel_extension_for_tensorflow/python/config.py,sha256=q3CHAtRHOkwSE0L_LAteGj98wxYbUOQrqEokAiVCHk8,1403
-intel_extension_for_tensorflow/python/device.py,sha256=lcHDdGA2N6hDwloSfJZLkySy4_GSxY7fJs-4kh9yz78,1214
-intel_extension_for_tensorflow/python/experimental_ops_override.py,sha256=1VvimSIDwVYKttJgIpXrWnWPep7kFlB_1ZKBYHijI10,26593
-intel_extension_for_tensorflow/python/gen_itex_version.py,sha256=s9Lqd7_Jhg76y6_sp1K2a4iQeUGcs-r3_TingzA-FzI,2190
-intel_extension_for_tensorflow/python/launch.py,sha256=LgPe4WAu9iSv5C13yugQPktp2MsNp3RvKtC7kG6bBao,31025
-intel_extension_for_tensorflow/python/version.py,sha256=4kESuBfZoiTOFggr_THilTz6vIVMIcXF_fhhUnJXBYs,1028
-intel_extension_for_tensorflow/python/ops/__init__.py,sha256=hI7KF7Xy5e39_h56lEMOSz_JJbR7UDx_EV5J037_oA8,1169
+intel_extension_for_tensorflow/python/device.py,sha256=4mAJnMruc3hu7JdcWqZ50n1vosTU_4cxpcjnD2Z8NZ8,942
+intel_extension_for_tensorflow/python/experimental_ops_override.py,sha256=mFgIoPDrNNP8-SjfaIldZ8SE1ax-4y7r6sOc2gdWeEU,20774
+intel_extension_for_tensorflow/python/gen_itex_version.py,sha256=Uog_rwAGYmTkN46mgwbbksoXgaxgmqjsA-p9_777lWQ,2220
+intel_extension_for_tensorflow/python/launch.py,sha256=fgT_oRa1u9AUtQ3a-3OEJaQdbJohNVN7ItgXS3Tb99M,34934
+intel_extension_for_tensorflow/python/version.py,sha256=IXBBz_Tg-COSUP3_TB-v8uGGCE1IAzbfMiD7ZQFclSM,1092
+intel_extension_for_tensorflow/python/fp8/__init__.py,sha256=wvWL7SA8jwJei6pTO8svL8Zit2cOW1y5NauzQIgCSmk,931
+intel_extension_for_tensorflow/python/fp8/autocast.py,sha256=OuJo6xzqSXDYGFUVC8IeZZ0nQXLdQoRRQN6qtBOUJVc,4634
+intel_extension_for_tensorflow/python/fp8/recipe.py,sha256=_IgjPOXXNFhdG2TU1wTV5dXFm-s_NyzhvlrLhYZ2B1s,3331
+intel_extension_for_tensorflow/python/ops/__init__.py,sha256=Sr8dUYMvRrTqWMgKqUtlk0CElSvIJf2iiVuBsv150dM,1437
 intel_extension_for_tensorflow/python/ops/activations.py,sha256=Sll7242juaaP77YoDamXV2_fa_wOQVflLZ4V1UU8aZ0,2531
-intel_extension_for_tensorflow/python/ops/layer_norm.py,sha256=KhoKgq-PXAGRyeoFfyNcdMT1q94YM12tL6Dwl_V8xFQ,17867
+intel_extension_for_tensorflow/python/ops/group_norm.py,sha256=aMCw0WZUA6eGhNq3w7VRI9Bse6U_0eODuynW-q2aCfM,10611
+intel_extension_for_tensorflow/python/ops/layer_norm.py,sha256=jLbU3nIa9Skc-jfVx2DAO0w_Xh2GBBtwi7yLoWk4mws,17931
 intel_extension_for_tensorflow/python/ops/load_ops_library.py,sha256=HAWSWdLGnpUhdUX5zqoZXApM3lPPY4luWIdQRz1Kt38,2668
-intel_extension_for_tensorflow/python/ops/ops_grad.py,sha256=XoDuTCKMFdCxw1B3LyQwMw1mxSt2Ose0MqkHlmVhoX8,3451
-intel_extension_for_tensorflow/python/ops/optimizers.py,sha256=TFVrhflnVM1oIOCe4DRwN_lCVr7zMeah7QacktquYW4,10707
-intel_extension_for_tensorflow/python/ops/recurrent.py,sha256=IXF1uG-Ip_46y2IBThNrXOL_BESZhYU1JYkazGtOUkk,28487
+intel_extension_for_tensorflow/python/ops/mlp.py,sha256=-cuBiYFyn8jEyYx8ZTUWe9AjO9O3WN2etYtWABZOkxM,11810
+intel_extension_for_tensorflow/python/ops/multi_head_attention.py,sha256=3izl2NIPNtMcOCyPpMXOMCH-_lRjJTyVMg7x0ELI5hM,5246
+intel_extension_for_tensorflow/python/ops/ops_grad.py,sha256=cA2sL04O5AFckSRd_s9plLDkQwyqNlPM9ynISIxLFug,4625
+intel_extension_for_tensorflow/python/ops/optimizers.py,sha256=mXts0LY2iD28eKyafxTq4zvbdBZfWMTqgeNxr6_e08A,10717
+intel_extension_for_tensorflow/python/ops/recurrent.py,sha256=J0RUfBagziaXfq_uCTtqMtFMMzHZw4DMehXCNr6bP08,28376
 intel_extension_for_tensorflow/python/test_func/__init__.py,sha256=eUT0rwTDAVlL6uaKzP2CFwyLcF9IsdyHcLRYuySiy0k,1038
 intel_extension_for_tensorflow/python/test_func/eager_test.py,sha256=O1hsCIPlYaJdSM5VBk7Dy9lYVh96IZMxhKNGUrjbQcs,1116
 intel_extension_for_tensorflow/python/test_func/keras_parameterized.py,sha256=RUlfk7OO2JlZl7fOYQCpZzQQ_pA-juQ2rDPIxMnKLo0,17632
-intel_extension_for_tensorflow/python/test_func/keras_testing_utils.py,sha256=NI6rCfgafWETp-8LxCENb504zHIbgFbg8j1CIIQpjOs,38058
+intel_extension_for_tensorflow/python/test_func/keras_testing_utils.py,sha256=vh5TiYzFKpq10jca4dqTjkUfmC0z-hcSo7Nd37aEas4,37244
 intel_extension_for_tensorflow/python/test_func/test.py,sha256=EC1dGZZ5Xr_8PnMU5FzHhDYFcsA3U2-oWk63NVmg59c,6345
-intel_extension_for_tensorflow/python/test_func/test_util.py,sha256=WKc-z-hCEsNk_FiI2vc_H-Qx7oJvuSEmkGlZ1T20Tb8,135335
+intel_extension_for_tensorflow/python/test_func/test_util.py,sha256=_tznRH8XCrz3XVu-gAL_kSE7jL_RXhwdMt7Yom39dAo,135744
+intel_extension_for_tensorflow/python/transformer/__init__.py,sha256=N-WIQjy4H0ThVFH7dm167vLxc9MrlSkmSV3lQLp0dDM,848
+intel_extension_for_tensorflow/python/transformer/attention.py,sha256=6-71uAX6_PlRY_Nk4Y-x3UbVKh_DHoyYecbwTc3MNPY,18767
+intel_extension_for_tensorflow/python/transformer/base.py,sha256=rxadBq3KwLoX2Px5_R1kcPUf36XCC2sTEcUtxYSyR7c,7846
+intel_extension_for_tensorflow/python/transformer/common.py,sha256=phgJ584pmj2a9o5n88lWCx962Kq5LZeqnbD4LALm8mQ,2515
+intel_extension_for_tensorflow/python/transformer/layer.py,sha256=dnck-CUBtTqu92j_rtUBltG0Fce9CB0VDeAiDWPbBMI,16277
 intel_extension_for_tensorflow/third-party-programs/THIRD-PARTY-PROGRAMS,sha256=38wrUdgRWcBxKeftLvRLlSSrFsbXbY91XNeA1Ln8sz8,81897
 intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onednn.txt,sha256=0YEJW0dBbtk9OTCDxdI_p8Li3WTYdHSkKsXA5_5-CaM,28478
 intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onevpl.txt,sha256=i34dYM774UHk35qAHdW2rbqF3370_zixaOb3bxc5TWs,21801
-intel_extension_for_tensorflow-1.2.0rc0.dist-info/LICENSE.txt,sha256=fAUT-S_f-DGBz-dvO3RJVmMav-KLvB9io_1yKXuR5po,10775
-intel_extension_for_tensorflow-1.2.0rc0.dist-info/METADATA,sha256=Culk_Fo9Tun3dAyuffonxXU8R_-s2Fq2H8F5OqfeiHo,3391
-intel_extension_for_tensorflow-1.2.0rc0.dist-info/WHEEL,sha256=3oXbtF4vYwmQyf0LQIagKtl0VO3gP86556qKiLb0bDc,103
-intel_extension_for_tensorflow-1.2.0rc0.dist-info/top_level.txt,sha256=0DYGIzJhxYAd-U188jctMBN6lvmX-0gKs1TfZLd_gz0,31
-intel_extension_for_tensorflow-1.2.0rc0.dist-info/RECORD,,
+intel_extension_for_tensorflow-2.13.0.0.dist-info/LICENSE.txt,sha256=fAUT-S_f-DGBz-dvO3RJVmMav-KLvB9io_1yKXuR5po,10775
+intel_extension_for_tensorflow-2.13.0.0.dist-info/METADATA,sha256=zX-4t2KVozjxlVw6DVgz0EY4fxzvx_qenXcgUYSz_Cc,3642
+intel_extension_for_tensorflow-2.13.0.0.dist-info/WHEEL,sha256=eBaYnBzd9EaKS6xIeGqcHdkvfb_UmlKjiG3njEtAaRY,103
+intel_extension_for_tensorflow-2.13.0.0.dist-info/top_level.txt,sha256=0DYGIzJhxYAd-U188jctMBN6lvmX-0gKs1TfZLd_gz0,31
+intel_extension_for_tensorflow-2.13.0.0.dist-info/RECORD,,
```

